2017-08-25 05:51:59.739586: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-25 05:51:59.739824: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-25 05:51:59.739839: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-08-25 05:51:58.399678 Start.
Training Salamon's ICASSP 2017 convnet on BirdVox-70k. 
Training set: unit07, unit10, unit01.
Validation set: unit02, unit03.
Test set: unit05.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 128, 104, 1)       0         
_________________________________________________________________
batch_normalization_1 (Batch (None, 128, 104, 1)       4         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 128, 104, 24)      624       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 64, 26, 24)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 64, 26, 48)        28848     
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 32, 6, 48)         0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 32, 6, 48)         57648     
_________________________________________________________________
flatten_1 (Flatten)          (None, 9216)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 64)                589888    
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 65        
=================================================================
Total params: 677,077
Trainable params: 677,075
Non-trainable params: 2
_________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.891113  0.420108  0.800537  0.857453
1   0.968140  0.156784  0.957153  0.158303
2   0.974609  0.119565  0.940186  0.241777
3   0.976318  0.108581  0.962891  0.193952
4   0.978149  0.103590  0.976929  0.112187
5   0.982178  0.089637  0.968628  0.138363
6   0.983398  0.072939  0.982544  0.072693
7   0.982178  0.084747  0.989014  0.057018
8   0.983276  0.074504  0.979248  0.107779
9   0.985840  0.069290  0.989258  0.057358
10  0.988281  0.064637  0.974731  0.095175
11  0.981567  0.086296  0.981201  0.089499
12  0.983643  0.074475  0.981445  0.085610
13  0.983521  0.084797  0.982300  0.092298
14  0.983398  0.076291  0.984375  0.096736
15  0.986694  0.070259  0.986572  0.079433
16  0.987671  0.063005  0.988037  0.073860
17  0.982666  0.094778  0.975342  0.169205
18  0.982056  0.107954  0.982666  0.121458
19  0.700439  4.638679  0.503174  8.058695
20  0.502075  8.070039  0.496460  8.155076
21  0.499634  8.099713  0.489624  8.257241
22  0.495605  8.157716  0.492188  8.209933
23  0.493164  8.191779  0.499390  8.089186
24  0.504028  8.012504  0.508423  7.939877
25  0.501343  8.052450  0.497803  8.108056
26  0.496704  8.107292  0.495117  8.084946
27  0.497681  8.039808  0.501831  7.970292
28  0.492798  8.111890  0.496216  8.055225
29  0.498657  8.014511  0.489868  8.152962
30  0.489990  8.149583  0.497803  8.023681
31  0.502319  7.950490  0.502930  7.939632

2017-08-25 10:15:14.538359 Finish.
Total elapsed time: 04:23:16.54.
