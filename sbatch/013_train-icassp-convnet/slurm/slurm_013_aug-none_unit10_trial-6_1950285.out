2017-08-25 05:29:59.525341: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-25 05:29:59.525623: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-25 05:29:59.525638: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-08-25 05:29:58.502377 Start.
Training Salamon's ICASSP 2017 convnet on BirdVox-70k. 
Training set: unit01, unit02, unit03.
Validation set: unit05, unit07.
Test set: unit10.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 128, 104, 1)       0         
_________________________________________________________________
batch_normalization_1 (Batch (None, 128, 104, 1)       4         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 128, 104, 24)      624       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 64, 26, 24)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 64, 26, 48)        28848     
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 32, 6, 48)         0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 32, 6, 48)         57648     
_________________________________________________________________
flatten_1 (Flatten)          (None, 9216)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 64)                589888    
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 65        
=================================================================
Total params: 677,077
Trainable params: 677,075
Non-trainable params: 2
_________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.740967  1.842078  0.832397  0.507427
1   0.894897  0.307299  0.855591  0.385422
2   0.912598  0.265985  0.897583  0.280013
3   0.925171  0.229746  0.904663  0.306725
4   0.933105  0.205621  0.910522  0.271565
5   0.939575  0.182868  0.917847  0.333088
6   0.946045  0.173940  0.954590  0.143763
7   0.948853  0.158268  0.919556  0.283073
8   0.954590  0.150189  0.955933  0.172527
9   0.953369  0.149808  0.950195  0.173407
10  0.958618  0.138787  0.944702  0.187028
11  0.954956  0.142198  0.959839  0.152696
12  0.961182  0.120728  0.962402  0.139132
13  0.967896  0.118620  0.943726  0.227855
14  0.961670  0.133204  0.952026  0.159529
15  0.963257  0.128846  0.942017  0.176339
16  0.967163  0.114641  0.963135  0.147549
17  0.966064  0.118307  0.878784  0.409414
18  0.968140  0.106395  0.937256  0.274510
19  0.964233  0.127396  0.950806  0.213354
20  0.964600  0.113729  0.956421  0.156393
21  0.923462  0.841524  0.562622  6.873992
22  0.574585  6.831122  0.725952  4.506570
23  0.602905  6.432074  0.866699  2.230879
24  0.623535  6.089207  0.834839  2.714669
25  0.500977  8.036331  0.497803  8.082736
26  0.492310  8.166591  0.501099  8.022932
27  0.503906  7.974970  0.495117  8.111985
28  0.495850  8.097436  0.509888  7.870830
29  0.506104  7.928532  0.492188  8.147810
30  0.497314  8.063646  0.499146  8.032068
31  0.498535  8.039542  0.500488  8.006182

2017-08-25 09:52:59.360564 Finish.
Total elapsed time: 04:23:01.36.
