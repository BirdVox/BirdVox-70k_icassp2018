2018-01-31 14:12:07.551718: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-31 14:12:07.551992: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-31 14:12:07.552004: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-01-31 14:11:59.622690 Start.
Training Salamon's ICASSP 2017 convnet on BirdVox-70k. 
Training set: unit07, unit10, unit01.
Validation set: unit02, unit03.
Test set: unit05.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 128, 104, 1)       0         
_________________________________________________________________
batch_normalization_1 (Batch (None, 128, 104, 1)       4         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 128, 104, 24)      624       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 64, 26, 24)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 64, 26, 48)        28848     
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 32, 6, 48)         0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 32, 6, 48)         57648     
_________________________________________________________________
flatten_1 (Flatten)          (None, 9216)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 64)                589888    
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 65        
=================================================================
Total params: 677,077
Trainable params: 677,075
Non-trainable params: 2
_________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.782715  0.520180  0.779419  0.654217
1   0.884521  0.313640  0.860107  0.441827
2   0.910034  0.254450  0.883911  0.447644
3   0.925415  0.229498  0.905396  0.394485
4   0.939941  0.193691  0.923706  0.309779
5   0.942871  0.178326  0.904175  0.494898
6   0.944458  0.179360  0.936890  0.262670
7   0.920654  0.255627  0.885254  0.833285
8   0.934326  0.251372  0.911011  0.400158
9   0.948486  0.180867  0.957764  0.167638
10  0.937256  0.216538  0.935547  0.330551
11  0.940674  0.209490  0.916870  0.385950
12  0.948120  0.188048  0.961304  0.168554
13  0.947266  0.178169  0.900391  0.598435
14  0.949219  0.183144  0.932007  0.434512
15  0.921387  0.242411  0.907471  0.475454
16  0.943726  0.206303  0.951050  0.212581
17  0.959229  0.171423  0.941650  0.289344
18  0.952759  0.183679  0.863647  0.416684
19  0.932495  0.286538  0.942871  0.255979
20  0.950562  0.196950  0.936646  0.358227
21  0.709717  3.604056  0.506104  8.014434
22  0.605469  6.399630  0.691040  5.041319
23  0.551636  7.213984  0.542236  7.371618
24  0.578125  6.809442  0.611206  6.301095
25  0.618408  6.178736  0.602905  6.422330
26  0.587402  6.713985  0.620605  6.208139
27  0.546387  7.402306  0.618896  6.231140
28  0.549194  7.352867  0.613403  6.314613
29  0.558716  7.195366  0.609619  6.372805
30  0.555664  7.241011  0.606323  6.422239
31  0.544067  7.424311  0.616333  6.256108

2018-01-31 17:54:11.381706 Finish.
Total elapsed time: 03:42:12.38.
