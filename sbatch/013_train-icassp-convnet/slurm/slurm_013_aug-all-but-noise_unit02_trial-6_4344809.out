2018-01-31 14:12:09.408957: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-31 14:12:09.409219: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-31 14:12:09.409231: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-01-31 14:12:03.734248 Start.
Training Salamon's ICASSP 2017 convnet on BirdVox-70k. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 128, 104, 1)       0         
_________________________________________________________________
batch_normalization_1 (Batch (None, 128, 104, 1)       4         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 128, 104, 24)      624       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 64, 26, 24)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 64, 26, 48)        28848     
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 32, 6, 48)         0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 32, 6, 48)         57648     
_________________________________________________________________
flatten_1 (Flatten)          (None, 9216)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 64)                589888    
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 65        
=================================================================
Total params: 677,077
Trainable params: 677,075
Non-trainable params: 2
_________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.872070  0.405078  0.706543  1.058781
1   0.947510  0.168367  0.687744  0.659201
2   0.954468  0.160859  0.691895  0.678820
3   0.967041  0.136184  0.637573  1.298544
4   0.970337  0.117878  0.717896  0.755006
5   0.971680  0.111242  0.693115  0.861460
6   0.967896  0.126353  0.696045  1.137667
7   0.977051  0.095183  0.722412  0.873762
8   0.978149  0.087310  0.724121  0.671031
9   0.896606  1.371182  0.490356  8.165989
10  0.504395  7.935577  0.493042  8.111839
11  0.501709  7.970599  0.498291  8.022390
12  0.500000  7.992996  0.500244  7.987131
13  0.504028  7.925144  0.502075  7.954731
14  0.511353  7.805491  0.499023  8.000783
15  0.506470  7.880963  0.497314  8.025867
16  0.494019  8.077478  0.495361  8.055180
17  0.506226  7.881183  0.504761  7.903775
18  0.502808  7.934230  0.503174  7.927737
19  0.497559  8.016670  0.498657  7.998591
20  0.488403  8.161557  0.494629  8.061820
21  0.502808  7.930996  0.504761  7.899441
22  0.489136  8.148166  0.489502  8.141970
23  0.503418  7.919797  0.500366  7.968145
24  0.499146  7.987335  0.493042  8.084382
25  0.500610  7.963497  0.502319  7.936036
26  0.493896  8.070126  0.506592  7.867553
27  0.499268  7.984162  0.503662  7.913955
28  0.512573  7.771762  0.496094  8.034364
29  0.500488  7.964201  0.492188  8.096438
30  0.493530  8.074948  0.502930  7.925021
31  0.493652  8.072858  0.500732  7.959924

2018-01-31 17:55:07.056872 Finish.
Total elapsed time: 03:43:04.06.
