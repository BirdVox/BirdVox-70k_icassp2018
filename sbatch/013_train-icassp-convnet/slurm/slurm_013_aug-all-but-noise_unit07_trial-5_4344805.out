2018-01-31 14:12:08.098018: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-31 14:12:08.098844: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-31 14:12:08.098860: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-01-31 14:12:05.393766 Start.
Training Salamon's ICASSP 2017 convnet on BirdVox-70k. 
Training set: unit10, unit01, unit02.
Validation set: unit03, unit05.
Test set: unit07.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 128, 104, 1)       0         
_________________________________________________________________
batch_normalization_1 (Batch (None, 128, 104, 1)       4         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 128, 104, 24)      624       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 64, 26, 24)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 64, 26, 48)        28848     
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 32, 6, 48)         0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 32, 6, 48)         57648     
_________________________________________________________________
flatten_1 (Flatten)          (None, 9216)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 64)                589888    
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 65        
=================================================================
Total params: 677,077
Trainable params: 677,075
Non-trainable params: 2
_________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.707397  0.973727  0.711548  0.610899
1   0.848511  0.406840  0.841797  0.386514
2   0.889160  0.312409  0.920410  0.240982
3   0.921753  0.259886  0.899658  0.293895
4   0.925293  0.241273  0.939087  0.197142
5   0.946289  0.194757  0.917114  0.253384
6   0.943237  0.196502  0.960327  0.145142
7   0.941162  0.194380  0.957397  0.148418
8   0.945923  0.178564  0.925293  0.210330
9   0.946533  0.188793  0.944092  0.175656
10  0.947388  0.173223  0.968018  0.117042
11  0.954590  0.156516  0.964966  0.132188
12  0.953491  0.156710  0.930908  0.222451
13  0.955688  0.148944  0.960205  0.129160
14  0.959473  0.143052  0.963379  0.145970
15  0.956299  0.147338  0.969971  0.110173
16  0.960449  0.136609  0.916992  0.252376
17  0.951904  0.149687  0.954834  0.153649
18  0.955444  0.145687  0.969971  0.114337
19  0.956421  0.143387  0.969360  0.118543
20  0.961426  0.138225  0.954224  0.142577
21  0.957886  0.143783  0.942383  0.188553
22  0.957397  0.140231  0.959595  0.139410
23  0.883789  0.309950  0.961670  0.131137
24  0.959229  0.156408  0.946411  0.202204
25  0.962524  0.143621  0.977539  0.097293
26  0.958130  0.142691  0.963623  0.125441
27  0.954346  0.147864  0.955566  0.138654
28  0.961304  0.133586  0.968994  0.114178
29  0.968628  0.123244  0.969482  0.111050
30  0.963989  0.126970  0.938477  0.205892
31  0.957275  0.139862  0.936768  0.201241

2018-01-31 17:57:49.606658 Finish.
Total elapsed time: 03:45:44.61.
