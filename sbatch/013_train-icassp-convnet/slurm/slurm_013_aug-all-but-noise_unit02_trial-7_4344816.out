2018-01-31 14:12:08.813318: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-31 14:12:08.813534: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-31 14:12:08.813546: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-01-31 14:12:08.813551: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-31 14:12:08.813556: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-01-31 14:12:03.741509 Start.
Training Salamon's ICASSP 2017 convnet on BirdVox-70k. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 128, 104, 1)       0         
_________________________________________________________________
batch_normalization_1 (Batch (None, 128, 104, 1)       4         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 128, 104, 24)      624       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 64, 26, 24)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 64, 26, 48)        28848     
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 32, 6, 48)         0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 32, 6, 48)         57648     
_________________________________________________________________
flatten_1 (Flatten)          (None, 9216)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 64)                589888    
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 65        
=================================================================
Total params: 677,077
Trainable params: 677,075
Non-trainable params: 2
_________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.846680  0.611313  0.718262  0.663777
1   0.936035  0.205463  0.739502  1.251145
2   0.957764  0.150045  0.787842  0.595726
3   0.956177  0.152784  0.756958  0.732223
4   0.969360  0.132982  0.786621  0.892101
5   0.969360  0.131076  0.768799  1.343247
6   0.971191  0.113019  0.756226  0.611341
7   0.977783  0.092339  0.818359  0.592602
8   0.978027  0.089911  0.792114  0.455387
9   0.975098  0.095836  0.791748  0.714801
10  0.977417  0.093365  0.799316  0.557434
11  0.976929  0.090191  0.810791  0.712149
12  0.978760  0.085398  0.784912  0.865339
13  0.979004  0.084051  0.825317  0.551497
14  0.982300  0.074239  0.777222  0.963153
15  0.980103  0.081805  0.826538  0.582951
16  0.977295  0.087642  0.789795  0.753384
17  0.982544  0.077494  0.839600  0.473159
18  0.979614  0.076600  0.831543  0.434458
19  0.977783  0.083919  0.789551  0.449029
20  0.979736  0.081206  0.839478  0.436889
21  0.981323  0.079699  0.794922  0.449299
22  0.570435  6.785409  0.496216  8.069881
23  0.506958  7.895937  0.495117  8.082253
24  0.500854  7.988717  0.507568  7.879734
25  0.496216  8.059014  0.491211  8.137176
26  0.496948  8.044253  0.498291  8.021447
27  0.510010  7.833354  0.494507  8.079283
28  0.494995  8.070379  0.509155  7.843547
29  0.501709  7.961261  0.503906  7.925261
30  0.499878  7.988587  0.507446  7.867057
31  0.507690  7.862357  0.513672  7.766212

2018-01-31 18:03:12.336891 Finish.
Total elapsed time: 03:51:09.34.
