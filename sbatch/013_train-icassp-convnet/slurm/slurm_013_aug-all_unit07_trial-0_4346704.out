2018-01-31 16:00:41.293962: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-31 16:00:41.294253: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-31 16:00:41.294265: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-01-31 16:00:41.294270: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-31 16:00:41.294275: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-01-31 16:00:33.984024 Start.
Training Salamon's ICASSP 2017 convnet on BirdVox-70k. 
Training set: unit10, unit01, unit02.
Validation set: unit03, unit05.
Test set: unit07.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 128, 104, 1)       0         
_________________________________________________________________
batch_normalization_1 (Batch (None, 128, 104, 1)       4         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 128, 104, 24)      624       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 64, 26, 24)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 64, 26, 48)        28848     
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 32, 6, 48)         0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 32, 6, 48)         57648     
_________________________________________________________________
flatten_1 (Flatten)          (None, 9216)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 64)                589888    
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 65        
=================================================================
Total params: 677,077
Trainable params: 677,075
Non-trainable params: 2
_________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.801147  0.560345  0.894409  0.324325
1   0.914917  0.258251  0.916748  0.241333
2   0.924438  0.239568  0.915161  0.240652
3   0.939209  0.206818  0.918457  0.217195
4   0.938721  0.203261  0.899902  0.259343
5   0.943359  0.185952  0.831421  0.361767
6   0.949097  0.169273  0.937256  0.178901
7   0.953247  0.164565  0.947876  0.165916
8   0.952515  0.155907  0.931274  0.188537
9   0.953735  0.165580  0.901733  0.235442
10  0.954712  0.153588  0.948120  0.159415
11  0.953857  0.162033  0.935425  0.170874
12  0.959229  0.146405  0.906616  0.239609
13  0.947388  0.177594  0.908325  0.246536
14  0.948730  0.188409  0.879150  0.256035
15  0.945068  0.177362  0.920654  0.187759
16  0.955811  0.155770  0.904053  0.250878
17  0.953613  0.160522  0.939209  0.174792
18  0.963501  0.139275  0.799072  0.374190
19  0.956543  0.150848  0.888672  0.285708
20  0.953857  0.172782  0.942139  0.193549
21  0.948486  0.188282  0.945068  0.164070
22  0.961548  0.158586  0.917114  0.205063
23  0.961060  0.156283  0.960205  0.162174
24  0.952637  0.164004  0.949951  0.164493
25  0.960083  0.143610  0.947876  0.166572
26  0.959839  0.150014  0.913696  0.260193
27  0.959595  0.139940  0.909180  0.286676
28  0.949219  0.189434  0.881226  0.302881
29  0.578857  6.732631  0.501709  8.088680
30  0.504272  8.042445  0.505859  8.012627
31  0.627197  6.034165  0.500488  8.014484

2018-01-31 19:55:29.782165 Finish.
Total elapsed time: 03:54:56.78.
