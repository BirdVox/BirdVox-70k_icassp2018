2017-08-24 20:42:02.642598: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-24 20:42:02.642826: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-24 20:42:02.642841: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-08-24 20:42:01.578964 Start.
Training Salamon's ICASSP 2017 convnet on BirdVox-70k. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 128, 104, 1)       0         
_________________________________________________________________
batch_normalization_1 (Batch (None, 128, 104, 1)       4         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 128, 104, 24)      624       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 64, 26, 24)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 64, 26, 48)        28848     
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 32, 6, 48)         0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 32, 6, 48)         57648     
_________________________________________________________________
flatten_1 (Flatten)          (None, 9216)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 64)                589888    
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 65        
=================================================================
Total params: 677,077
Trainable params: 677,075
Non-trainable params: 2
_________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.510742  0.749055  0.507080  0.711758
1   0.496216  0.705562  0.496460  0.701315
2   0.489258  2.317708  0.511108  7.629313
3   0.567017  6.187400  0.504517  7.917255
4   0.499512  7.995289  0.496216  8.046358
5   0.495361  8.058873  0.495728  8.052035
6   0.494507  8.070675  0.500000  7.982336
7   0.502808  7.936917  0.499390  7.990783
8   0.494995  8.060289  0.499268  7.991646
9   0.492676  8.096259  0.499023  7.994603
10  0.503540  7.922182  0.498291  8.005461
11  0.501099  7.960334  0.504883  7.899650
12  0.504883  7.899324  0.497803  8.011881
13  0.494263  8.068027  0.503418  7.921787
14  0.496704  8.028561  0.500366  7.969924
15  0.489014  8.150674  0.501343  7.953889
16  0.499023  7.990650  0.503906  7.912597
17  0.497070  8.021382  0.497192  8.019244
18  0.504761  7.898406  0.502441  7.935205
19  0.500610  7.964230  0.500244  7.969906
20  0.498779  7.993106  0.500610  7.963764
21  0.504517  7.901348  0.512329  7.776659
22  0.495972  8.037305  0.497925  8.006040
23  0.502808  7.928076  0.491333  8.110892
24  0.497070  8.019316  0.500122  7.970557
25  0.496094  8.034678  0.501343  7.950900
26  0.496460  8.028654  0.495850  8.038297
27  0.509033  7.828040  0.502808  7.927214
28  0.503174  7.921305  0.499512  7.979620
29  0.504028  7.907553  0.507568  7.851058
30  0.507568  7.851005  0.501221  7.952151
31  0.498291  7.998813  0.504883  7.893682

2017-08-25 01:00:11.292959 Finish.
Total elapsed time: 04:18:10.29.
