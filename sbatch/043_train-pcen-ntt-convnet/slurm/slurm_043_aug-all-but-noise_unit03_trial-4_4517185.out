2018-02-12 20:00:44.692981: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:00:44.693304: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:00:44.693315: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 20:00:07.541220 Start.
Training NTT-like convnet on BirdVox-70k with PCEN input. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 16)            1168        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 4)             68          bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_dense2[0][0]                  
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
flatten (Flatten)                (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             65          flatten[0][0]                    
====================================================================================================
Total params: 678,577
Trainable params: 678,575
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.956787  0.190948  0.784180  0.580436
1    0.972534  0.129102  0.777954  0.595580
2    0.974365  0.115520  0.791626  0.471630
3    0.976318  0.097504  0.797241  0.510830
4    0.975708  0.101402  0.786743  0.492918
5    0.973145  0.115536  0.768188  0.563092
6    0.974487  0.110589  0.803589  0.488808
7    0.975464  0.101132  0.776123  0.582164
8    0.974121  0.107274  0.794434  0.513077
9    0.977539  0.097067  0.810303  0.499627
10   0.958496  0.184489  0.761841  0.654642
11   0.954956  0.196460  0.761963  0.608716
12   0.969727  0.137365  0.774780  0.586535
13   0.937622  0.627609  0.578369  6.841208
14   0.545166  7.378440  0.512817  7.897552
15   0.505127  8.018286  0.497437  8.139370
16   0.504272  8.026837  0.502441  8.054212
17   0.617798  6.194613  0.625732  6.078160
18   0.827759  2.817500  0.614502  6.251572
19   0.608398  6.300313  0.498779  8.042817
20   0.505859  7.928487  0.497437  8.061454
21   0.493042  8.130394  0.490967  8.162412
22   0.501831  7.988249  0.500244  8.012618
23   0.502075  7.982566  0.503418  7.960317
24   0.502808  7.969254  0.502319  7.976258
25   0.511963  7.821773  0.500244  8.007862
26   0.503662  7.952662  0.490112  8.167973
27   0.500732  7.997980  0.492432  8.129632
28   0.498413  8.033608  0.504395  7.937583
29   0.499634  8.012827  0.506470  7.903191
30   0.499390  8.015419  0.496094  8.067315
31   0.482422  8.284637  0.508545  7.867529
32   0.501587  7.977819  0.498657  8.023883
33   0.501221  7.982378  0.499512  8.008981
34   0.503784  7.940230  0.494141  8.093328
35   0.499268  8.010954  0.499878  8.000579
36   0.496094  8.060267  0.505005  7.917556
37   0.489746  8.160175  0.495239  8.071954
38   0.486206  8.215321  0.505615  7.905244
39   0.491699  8.126454  0.493042  8.104398
40   0.504395  7.922768  0.504395  7.922120
41   0.500732  7.979861  0.499878  7.992838
42   0.495605  8.060314  0.500488  7.981830
43   0.499146  8.002604  0.490112  8.145980
44   0.498413  8.013022  0.507690  7.864493
45   0.503784  7.926154  0.505981  7.890509
46   0.503540  7.928829  0.508301  7.852330
47   0.507568  7.863420  0.504883  7.905649
48   0.501831  7.953732  0.499023  7.997926
49   0.501221  7.962348  0.499878  7.983209
50   0.510864  7.807535  0.502319  7.943240
51   0.498047  8.010853  0.504761  7.903323
52   0.495117  8.056591  0.499756  7.982173
53   0.495728  8.045951  0.501831  7.948210
54   0.498901  7.994504  0.498291  8.003829
55   0.512939  7.769918  0.497925  8.008915
56   0.498291  8.002730  0.501221  7.955685
57   0.510864  7.801630  0.490601  8.124397
58   0.496704  8.026791  0.501099  7.956459
59   0.501465  7.950373  0.503662  7.915168
60   0.497070  8.020040  0.509155  7.824904
61   0.523682  7.595960  0.550415  7.162986
62   0.527100  7.642445  0.490479  8.276010
63   0.506226  8.015438  0.494995  8.191695
64   0.499146  8.121788  0.487793  8.302159
65   0.500244  8.099424  0.502441  8.062130
66   0.505859  8.005440  0.505859  8.003930
67   0.500977  8.081289  0.506714  7.987524
68   0.500732  8.082757  0.502563  8.052102
69   0.500366  8.086460  0.503296  8.038206
70   0.501587  8.064786  0.501221  8.069742
71   0.491821  8.220353  0.498413  8.113233
72   0.504028  8.021901  0.507202  7.969935
73   0.517334  7.805862  0.502441  8.045147
74   0.502197  8.048366  0.501343  8.061434
75   0.499390  8.092245  0.497314  8.125033
76   0.503296  8.027997  0.502441  8.041152
77   0.503784  8.018920  0.502686  8.036047
78   0.500854  8.065007  0.491699  8.212026
79   0.504517  8.004912  0.509033  7.931598
80   0.486206  8.299036  0.507812  7.950295
81   0.492432  8.197739  0.499878  8.077259
82   0.502197  8.039435  0.496948  8.123602
83   0.499146  8.087768  0.501953  8.042099
84   0.500366  8.067278  0.503906  8.009824
85   0.493408  8.178653  0.501221  8.052353
86   0.499512  8.079536  0.500610  8.061468
87   0.493286  8.179174  0.500610  8.060776
88   0.501587  8.044704  0.501221  8.050278
89   0.502197  8.034219  0.502441  8.029969
90   0.501831  8.039503  0.497681  8.106097
91   0.500732  8.056618  0.500000  8.068134
92   0.492065  8.195746  0.492310  8.191534
93   0.491211  8.208975  0.497192  8.112300
94   0.502563  8.025472  0.498291  8.094082
95   0.509888  7.906919  0.502563  8.024727
96   0.496826  8.116964  0.499146  8.079345
97   0.495483  8.138142  0.497559  8.104465
98   0.501343  8.043248  0.501709  8.037123
99   0.509033  7.918854  0.492554  8.184255
100  0.511597  7.877107  0.507446  7.943791
101  0.494385  8.154111  0.505371  7.976825
102  0.498291  8.090739  0.503906  8.000029
103  0.509277  7.913258  0.494507  8.151133
104  0.490723  8.211933  0.498901  8.079915
105  0.500854  8.048247  0.490723  8.211367
106  0.503052  8.012466  0.506714  7.953261
107  0.497192  8.106559  0.498169  8.090650
108  0.498291  8.088522  0.498779  8.080494
109  0.496704  8.113795  0.504272  7.991662
110  0.492432  8.182379  0.505371  7.973688
111  0.497192  8.105393  0.489746  8.225297
112  0.491699  8.193710  0.488647  8.242798
113  0.498779  8.079402  0.497681  8.097023
114  0.496216  8.120558  0.490845  8.207059
115  0.506104  7.961055  0.507446  7.939354
116  0.489136  8.234437  0.505737  7.966806
117  0.491943  8.189101  0.504761  7.982475
118  0.497437  8.100499  0.495728  8.128020
119  0.500122  8.057168  0.497192  8.104371
120  0.495483  8.131903  0.503906  7.996130
121  0.499023  8.074823  0.503906  7.996113
122  0.499390  8.068906  0.502808  8.013810
123  0.500977  8.043319  0.499634  8.064959
124  0.492554  8.179075  0.503174  8.007897
125  0.495239  8.135786  0.496826  8.110207
126  0.503662  8.000024  0.509521  7.905582
127  0.502319  8.021666  0.502686  8.015763

2018-02-13 12:17:28.858210 Finish.
Total elapsed time: 16:17:21.86.
