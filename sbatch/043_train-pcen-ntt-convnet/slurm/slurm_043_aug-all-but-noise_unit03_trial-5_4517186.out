2018-02-12 20:00:36.195717: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:00:36.196151: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:00:36.196165: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 20:00:15.936540 Start.
Training NTT-like convnet on BirdVox-70k with PCEN input. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 16)            1168        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 4)             68          bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_dense2[0][0]                  
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
flatten (Flatten)                (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             65          flatten[0][0]                    
====================================================================================================
Total params: 678,577
Trainable params: 678,575
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.969360  0.119047  0.841187  0.373848
1    0.970581  0.118924  0.864258  0.500508
2    0.972412  0.100060  0.870361  0.415080
3    0.976685  0.107337  0.873291  0.392769
4    0.977905  0.088981  0.867554  0.357234
5    0.974731  0.099527  0.876587  0.421879
6    0.975342  0.095934  0.872070  0.440062
7    0.973877  0.107899  0.847168  0.407081
8    0.974976  0.100541  0.871826  0.364223
9    0.749268  3.862880  0.576660  6.700185
10   0.609741  6.179311  0.502441  8.058134
11   0.508789  7.950411  0.651489  5.616825
12   0.534668  7.526307  0.500610  8.088645
13   0.487427  8.298535  0.493408  8.199886
14   0.496338  8.150862  0.496338  8.149183
15   0.502197  8.053290  0.514893  7.847290
16   0.502563  8.044789  0.489624  8.252178
17   0.493164  8.194064  0.498657  8.104507
18   0.508667  7.942240  0.492554  8.201056
19   0.500244  8.076273  0.505371  7.992830
20   0.494263  8.171131  0.500122  8.075961
21   0.489136  8.252361  0.493164  8.186769
22   0.487793  8.272720  0.481934  8.366553
23   0.503052  8.025596  0.505249  7.989619
24   0.500854  8.059921  0.506226  7.972829
25   0.498291  8.100227  0.493530  8.176477
26   0.501099  8.054030  0.505005  7.990616
27   0.503906  8.007894  0.499023  8.086171
28   0.496094  8.132989  0.501465  8.046019
29   0.510620  7.898074  0.498901  8.086582
30   0.506592  7.962269  0.504395  7.997331
31   0.505981  7.971414  0.501343  8.045846
32   0.493774  8.167513  0.492188  8.192774
33   0.507690  7.942593  0.497437  8.107565
34   0.496338  8.124985  0.494995  8.146343
35   0.500610  8.055563  0.494385  8.155637
36   0.505737  7.972397  0.503418  8.009523
37   0.499756  8.068304  0.499512  8.071996
38   0.500244  8.059958  0.507324  7.945611
39   0.500854  8.005091  0.496582  8.052362
40   0.505493  7.908145  0.501221  7.974741
41   0.501465  7.969874  0.492676  8.109126
42   0.498291  8.018881  0.497070  8.037653
43   0.506226  7.891069  0.494873  8.071445
44   0.499023  8.004704  0.515747  7.737524
45   0.498535  8.011383  0.501831  7.958305
46   0.492920  8.099858  0.501709  7.959231
47   0.493286  8.093022  0.502319  7.948526
48   0.494019  8.080392  0.503418  7.930077
49   0.504272  7.916007  0.502075  7.950591
50   0.498779  8.002706  0.497192  8.027579
51   0.500732  7.970732  0.513916  7.760147
52   0.501343  7.960202  0.496826  8.031818
53   0.498901  7.998360  0.499146  7.994096
54   0.504150  7.913949  0.503418  7.925272
55   0.501099  7.961908  0.507446  7.860375
56   0.507568  7.858107  0.492188  8.102996
57   0.503540  7.921704  0.509277  7.829936
58   0.510376  7.812133  0.498047  8.008403
59   0.505371  7.891367  0.497437  8.017595
60   0.501587  7.951174  0.490967  8.120233
61   0.490479  8.127779  0.510010  7.816170
62   0.498413  8.000826  0.501343  7.953902
63   0.503296  7.922558  0.513062  7.766667
64   0.494995  8.054498  0.503296  7.921975
65   0.498413  7.999642  0.500366  7.968330
66   0.494507  8.061580  0.504517  7.901840
67   0.499390  7.983425  0.504395  7.903488
68   0.510864  7.800207  0.512085  7.780611
69   0.489380  8.142458  0.491943  8.101466
70   0.515991  7.717971  0.487915  8.165460
71   0.513306  7.760568  0.499756  7.976482
72   0.505981  7.877136  0.500732  7.960725
73   0.503052  7.923664  0.496948  8.020885
74   0.500366  7.966318  0.499390  7.981812
75   0.501343  7.950605  0.500244  7.968054
76   0.495361  8.045836  0.509644  7.818084
77   0.497314  8.014585  0.500977  7.956150
78   0.497437  8.012539  0.501221  7.952164
79   0.501343  7.950177  0.502075  7.938461
80   0.506958  7.961593  0.505005  7.992963
81   0.505493  7.983341  0.498901  8.088190
82   0.505859  7.975043  0.502441  8.029241
83   0.501953  8.036372  0.503784  8.006167
84   0.496826  8.117712  0.503296  8.012855
85   0.494995  8.146130  0.505859  7.970522
86   0.492065  8.192404  0.495972  8.129008
87   0.494385  8.154191  0.498047  8.094785
88   0.504639  7.988191  0.499634  8.068527
89   0.501099  8.044614  0.504028  7.997102
90   0.500122  8.059800  0.497070  8.108736
91   0.503174  8.010131  0.499390  8.070907
92   0.497192  8.106127  0.498169  8.090200
93   0.502930  8.013301  0.504639  7.985597
94   0.494019  8.156634  0.506958  7.947941
95   0.508667  7.920280  0.492310  8.183820
96   0.500366  8.053867  0.500366  8.053777
97   0.515381  7.811691  0.497314  8.102815
98   0.503540  8.002408  0.506104  7.961031
99   0.502319  8.021976  0.498657  8.080957
100  0.488159  8.250127  0.494629  8.145812
101  0.500610  8.049374  0.503418  8.004093
102  0.501465  8.035552  0.493530  8.163423
103  0.504150  7.992230  0.499878  8.061080
104  0.503418  8.004010  0.509155  7.911525
105  0.487061  8.267642  0.500244  8.055140
106  0.501587  8.033492  0.498779  8.078740
107  0.502686  8.015775  0.500610  8.049221
108  0.501831  8.029543  0.508789  7.917391
109  0.496338  8.118079  0.500977  8.043311
110  0.500977  8.043310  0.497681  8.096433
111  0.497192  8.104302  0.504028  7.994120
112  0.499390  8.068886  0.503662  8.000022
113  0.508179  7.927223  0.499146  8.072821
114  0.480957  8.365984  0.505005  7.978379
115  0.504639  7.984291  0.501465  8.035438
116  0.506836  7.948873  0.499390  8.068885
117  0.497070  8.106272  0.508179  7.927029
118  0.502808  8.013800  0.493164  8.169230
119  0.491699  8.140739  0.498413  8.022654
120  0.488647  8.176318  0.497192  8.038452
121  0.496338  8.050857  0.502686  7.948553
122  0.498657  8.011844  0.501343  7.968156
123  0.496948  8.037442  0.495728  8.056162
124  0.496460  8.043809  0.499146  8.000337
125  0.496094  8.048375  0.506470  7.882355
126  0.494263  8.076392  0.494263  8.075827
127  0.498047  8.014957  0.500610  7.973553

2018-02-13 13:32:29.805689 Finish.
Total elapsed time: 17:32:14.81.
