2018-02-12 20:00:41.218421: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:00:41.218773: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:00:41.218784: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 20:00:11.116741 Start.
Training NTT-like convnet on BirdVox-70k with PCEN input. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 16)            1168        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 4)             68          bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_dense2[0][0]                  
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
flatten (Flatten)                (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             65          flatten[0][0]                    
====================================================================================================
Total params: 678,577
Trainable params: 678,575
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.968384  0.130917  0.837280  0.547863
1    0.973145  0.108485  0.845947  0.408800
2    0.972534  0.115936  0.849854  0.534382
3    0.968628  0.130718  0.845947  0.714846
4    0.979248  0.094119  0.852539  0.566212
5    0.970215  0.124181  0.849243  0.636109
6    0.972656  0.120707  0.856934  0.466318
7    0.959351  0.198957  0.780029  0.605809
8    0.923584  0.404965  0.835938  0.488634
9    0.952881  0.211130  0.836670  0.397553
10   0.961304  0.174146  0.844238  0.376378
11   0.969604  0.138285  0.845337  0.505452
12   0.586304  6.603708  0.495605  8.174811
13   0.506104  7.999925  0.502930  8.046187
14   0.506104  7.991235  0.493408  8.192404
15   0.493408  8.189550  0.494751  8.165263
16   0.503296  8.025298  0.494629  8.162902
17   0.511841  7.883689  0.504272  8.003997
18   0.492798  8.187499  0.497681  8.107438
19   0.500488  8.061011  0.510742  7.894634
20   0.491943  8.196683  0.502075  8.032481
21   0.504272  7.996294  0.508789  7.922770
22   0.506104  7.965434  0.503052  8.014039
23   0.498413  8.088306  0.502441  8.022911
24   0.495850  8.128761  0.492676  8.179547
25   0.490601  8.212681  0.502808  8.015636
26   0.509888  7.901274  0.499268  8.072224
27   0.499878  8.062197  0.499023  8.075796
28   0.507812  7.933989  0.498901  8.077488
29   0.504639  7.984905  0.496948  8.108762
30   0.508423  7.923732  0.497559  8.098771
31   0.502197  8.023945  0.506958  7.947158
32   0.495117  8.137967  0.496460  8.116286
33   0.500610  8.049359  0.501343  8.037492
34   0.499268  8.068663  0.506470  7.893793
35   0.494263  8.119215  0.501465  7.996099
36   0.501587  7.988527  0.502075  7.975961
37   0.493774  8.104639  0.497070  8.048772
38   0.498047  8.030444  0.498657  8.018144
39   0.499023  8.010100  0.503662  7.934070
40   0.502197  7.955608  0.497559  8.027839
41   0.496704  8.039947  0.512573  7.785515
42   0.498169  8.013881  0.479370  8.312366
43   0.497559  8.021324  0.505615  7.891859
44   0.498535  8.003826  0.496826  8.030209
45   0.500000  7.978847  0.498291  8.005366
46   0.498291  8.004725  0.498535  8.000223
47   0.506348  7.875136  0.496338  8.034206
48   0.498413  8.000675  0.497314  8.017765
49   0.501709  7.947334  0.498901  7.991741
50   0.506226  7.874668  0.506348  7.872432
51   0.501831  7.944184  0.511108  7.796042
52   0.500732  7.961252  0.495605  8.042792
53   0.492798  8.087382  0.507568  7.851746
54   0.494385  8.061786  0.502441  7.933215
55   0.501831  7.942834  0.499634  7.977759
56   0.503784  7.911503  0.516479  7.709025
57   0.495239  8.047574  0.501587  7.946311
58   0.494385  8.061075  0.487549  8.170004
59   0.504150  7.905291  0.490234  8.127105
60   0.494995  8.051174  0.502441  7.932432
61   0.507568  7.850670  0.493530  8.074448
62   0.509644  7.817545  0.507568  7.850611
63   0.498047  8.002394  0.496216  8.031572
64   0.490845  8.117193  0.492065  8.097721
65   0.498413  8.059690  0.497559  8.239478
66   0.485840  8.390199  0.494263  8.228326
67   0.483154  8.392612  0.503052  8.059867
68   0.488403  8.287693  0.503662  8.034643
69   0.499146  8.102187  0.510010  7.922451
70   0.500610  8.070398  0.494995  8.157731
71   0.504761  7.997829  0.493042  8.184458
72   0.494141  8.164944  0.499878  8.070829
73   0.497070  8.114753  0.500488  8.058447
74   0.498169  8.094838  0.506104  7.966039
75   0.507568  7.941682  0.506836  7.952803
76   0.505859  7.967981  0.510376  7.894667
77   0.510132  7.898179  0.502197  8.025682
78   0.504028  7.995853  0.506104  7.962117
79   0.503296  8.007137  0.505371  7.973477
80   0.501465  8.036268  0.500244  8.055790
81   0.498169  8.089117  0.499023  8.075235
82   0.499512  8.067280  0.497559  8.098686
83   0.504761  7.982543  0.499390  8.069064
84   0.499512  8.067058  0.499878  8.061122
85   0.501953  8.027650  0.503174  8.007953
86   0.487427  8.261751  0.501831  8.029569
87   0.497559  8.098424  0.497559  8.098417
88   0.499023  8.074801  0.496338  8.118083
89   0.504272  7.996678  0.498535  8.167494
90   0.488770  8.297157  0.502441  8.057685
91   0.497070  8.128900  0.505493  7.982230
92   0.500244  8.056683  0.509033  7.908383
93   0.508179  7.915575  0.486816  8.250303
94   0.501099  8.017809  0.495605  8.100934
95   0.503906  7.964798  0.495605  8.093548
96   0.501343  7.998926  0.500244  8.013426
97   0.489380  8.183915  0.498291  8.039234
98   0.502197  7.974569  0.505737  7.915816
99   0.491211  8.145268  0.500244  7.999183
100  0.500732  7.989482  0.494751  8.082976
101  0.504639  7.923619  0.495850  8.062062
102  0.504272  7.926236  0.499634  7.998685
103  0.498779  8.010927  0.498291  8.017372
104  0.498047  8.020037  0.506226  7.888460
105  0.497437  8.027494  0.500122  7.983631
106  0.486938  8.192854  0.494995  8.063491
107  0.497192  8.027624  0.505615  7.892536
108  0.498901  7.998840  0.496582  8.035111
109  0.498535  8.003335  0.499756  7.983260
110  0.494751  8.062493  0.490112  8.135908
111  0.497437  8.018657  0.499756  7.981214
112  0.513550  7.760881  0.498291  8.003733
113  0.501587  7.950817  0.506714  7.868723
114  0.496216  8.035760  0.504639  7.901165
115  0.499878  7.976776  0.490234  8.130240
116  0.512451  7.775800  0.500244  7.970165
117  0.509033  7.829825  0.496948  8.022274
118  0.498779  7.992887  0.496704  8.025782
119  0.499146  7.986690  0.492065  8.099398
120  0.489014  8.147901  0.497314  8.015423
121  0.498535  7.995832  0.500610  7.962624
122  0.489014  8.147391  0.505615  7.882616
123  0.506104  7.874736  0.496338  8.030332
124  0.497070  8.018575  0.499878  7.973738
125  0.507080  7.858852  0.498169  8.000853
126  0.504272  7.903493  0.496216  8.031883
127  0.500977  7.955940  0.497925  8.004550

2018-02-13 12:20:57.179847 Finish.
Total elapsed time: 16:20:46.18.
