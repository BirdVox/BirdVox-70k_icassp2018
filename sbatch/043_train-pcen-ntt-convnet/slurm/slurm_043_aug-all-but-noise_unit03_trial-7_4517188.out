2018-02-12 20:00:35.413392: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:00:35.414001: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:00:35.414026: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 20:00:16.756832 Start.
Training NTT-like convnet on BirdVox-70k with PCEN input. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 16)            1168        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 4)             68          bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_dense2[0][0]                  
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
flatten (Flatten)                (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             65          flatten[0][0]                    
====================================================================================================
Total params: 678,577
Trainable params: 678,575
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.972290  0.126068  0.879883  0.505968
1    0.971924  0.132578  0.853271  0.767695
2    0.976318  0.111979  0.862915  0.707305
3    0.978149  0.095108  0.862183  0.695922
4    0.975464  0.102629  0.885620  0.473182
5    0.978149  0.098350  0.864014  0.775052
6    0.976929  0.097422  0.853882  0.919937
7    0.979126  0.095388  0.874390  0.638635
8    0.980469  0.089604  0.875000  0.507617
9    0.982788  0.081419  0.875000  0.441982
10   0.982300  0.084115  0.877930  0.509556
11   0.694580  4.558161  0.496338  8.174786
12   0.507690  7.979920  0.508667  7.955527
13   0.505981  7.993365  0.511108  7.906087
14   0.503662  8.022651  0.498291  8.106147
15   0.500244  8.072237  0.494263  8.166442
16   0.498535  8.095784  0.488770  8.251543
17   0.497681  8.106552  0.494019  8.164324
18   0.504272  7.998004  0.502686  8.022612
19   0.506470  7.960806  0.499634  8.070235
20   0.501953  8.032218  0.498291  8.090657
21   0.500488  8.054748  0.492432  8.184148
22   0.507935  7.933886  0.497070  8.108642
23   0.498657  8.082766  0.495483  8.133647
24   0.502563  8.019299  0.500366  8.054502
25   0.494263  8.152703  0.491089  8.203696
26   0.496948  8.048551  0.502319  7.961096
27   0.504272  7.927070  0.499268  8.004555
28   0.503052  7.942620  0.503174  7.939262
29   0.494873  8.070489  0.499512  7.995529
30   0.495850  8.053074  0.502563  7.945258
31   0.490356  8.139195  0.495850  8.050985
32   0.496826  8.034853  0.501709  7.956469
33   0.501465  7.959873  0.505615  7.893234
34   0.489380  8.151630  0.493164  8.090878
35   0.504272  7.913391  0.501587  7.955821
36   0.498657  8.002167  0.502930  7.933700
37   0.502930  7.933366  0.496948  8.028396
38   0.504272  7.911319  0.500366  7.973288
39   0.496948  8.027487  0.490845  8.124505
40   0.496338  8.036657  0.502563  7.937137
41   0.493896  8.075052  0.500000  7.977493
42   0.498169  8.006442  0.497559  8.015933
43   0.497070  8.023488  0.501099  7.959041
44   0.495117  8.054183  0.501953  7.944988
45   0.497925  8.009004  0.509766  7.820031
46   0.491333  8.113697  0.496338  8.033717
47   0.493896  8.072456  0.491089  8.117035
48   0.505859  7.881386  0.510498  7.807264
49   0.498779  7.993926  0.498169  8.003497
50   0.499146  7.987775  0.499634  7.979840
51   0.500244  7.969966  0.504028  7.909496
52   0.499390  7.983314  0.487183  8.177791
53   0.501221  7.953865  0.499512  7.980988
54   0.507812  7.848537  0.502075  7.939889
55   0.497681  8.025350  0.516602  7.840703
56   0.538940  7.459679  0.508057  7.949683
57   0.540771  7.421426  0.501465  8.053460
58   0.541748  7.403209  0.513428  7.858871
59   0.550903  7.254103  0.506714  7.965800
60   0.531250  7.566917  0.502686  8.020242
61   0.500366  8.030468  0.498169  8.051401
62   0.495605  8.085536  0.498169  8.039238
63   0.508789  7.866156  0.493164  8.111971
64   0.495361  8.074426  0.503174  7.947619
65   0.499512  8.004192  0.489502  8.162119
66   0.492798  8.108212  0.498657  8.013539
67   0.498291  8.018314  0.501709  7.962830
68   0.509888  7.831587  0.493408  8.093503
69   0.500977  7.972138  0.497925  8.020117
70   0.491211  8.126551  0.511353  7.804870
71   0.500000  7.985333  0.502441  7.945906
72   0.497437  8.025230  0.503296  7.931365
73   0.502686  7.940673  0.501343  7.961710
74   0.503296  7.930204  0.506836  7.873363
75   0.489258  8.153321  0.494385  8.071182
76   0.494507  8.068977  0.500366  7.975180
77   0.505981  7.961577  0.500732  8.048337
78   0.507080  7.937995  0.511475  7.860886
79   0.495728  8.107165  0.499756  8.038758
80   0.498047  8.062668  0.494263  8.119917
81   0.490356  8.179550  0.497803  8.058334
82   0.508667  7.882897  0.500977  8.003351
83   0.500610  8.007227  0.497803  8.050083
84   0.506104  7.915988  0.489258  8.182832
85   0.501831  7.980785  0.503418  7.953923
86   0.497803  8.041980  0.495239  8.081416
87   0.505493  7.916601  0.506470  7.899715
88   0.501465  7.978267  0.504883  7.922562
89   0.504639  7.925311  0.495483  8.070145
90   0.496582  8.051572  0.498779  8.015503
91   0.503906  7.932785  0.497314  8.036909
92   0.505859  7.899770  0.507690  7.869681
93   0.499268  8.003111  0.501831  7.961407
94   0.497681  8.026781  0.502441  7.950103
95   0.500610  7.978554  0.497803  8.022585
96   0.504150  7.920697  0.499268  7.997859
97   0.496948  8.034190  0.504028  7.920681
98   0.503052  7.935648  0.504395  7.913648
99   0.496582  8.037637  0.491943  8.111037
100  0.504028  7.917852  0.510376  7.816142
101  0.496704  8.033619  0.506226  7.881348
102  0.510254  7.816677  0.511108  7.802612
103  0.493652  8.080486  0.501709  7.951635
104  0.505249  7.894813  0.499512  7.985900
105  0.497559  8.016681  0.503052  7.928756
106  0.499512  7.984864  0.500732  7.965079
107  0.500000  7.976452  0.490234  8.131841
108  0.496826  8.026471  0.494385  8.065118
109  0.507080  7.862466  0.498779  7.994547
110  0.496338  8.033231  0.500366  7.968778
111  0.488403  8.159277  0.497681  8.011160
112  0.500977  7.958417  0.501709  7.946545
113  0.504883  7.895765  0.495972  8.037652
114  0.493042  8.084193  0.509644  7.819364
115  0.504883  7.895112  0.499268  7.984488
116  0.493408  8.077766  0.495483  8.044553
117  0.500488  7.964644  0.508545  7.836087
118  0.504883  7.894365  0.491821  8.102495
119  0.502808  7.927256  0.501587  7.946629
120  0.486816  8.182027  0.494019  8.067132
121  0.499756  7.975599  0.506104  7.874338
122  0.496704  8.024131  0.509766  7.815847
123  0.492432  8.092146  0.493530  8.074588
124  0.501343  7.950002  0.492676  8.088141
125  0.506226  7.872096  0.498779  7.990781
126  0.498535  7.994652  0.501221  7.951818
127  0.501587  7.945964  0.493408  8.076338

2018-02-13 13:32:31.086018 Finish.
Total elapsed time: 17:32:15.09.
