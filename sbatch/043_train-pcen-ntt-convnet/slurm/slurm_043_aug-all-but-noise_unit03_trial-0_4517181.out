2018-02-12 20:00:45.436028: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:00:45.436362: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:00:45.436375: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 19:59:56.871829 Start.
Training NTT-like convnet on BirdVox-70k with PCEN input. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 16)            1168        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 4)             68          bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_dense2[0][0]                  
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
flatten (Flatten)                (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             65          flatten[0][0]                    
====================================================================================================
Total params: 678,577
Trainable params: 678,575
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.973633  0.113147  0.790649  0.850595
1    0.977417  0.102157  0.821411  0.739907
2    0.975586  0.115143  0.808472  1.024425
3    0.976807  0.108196  0.823486  0.667605
4    0.976440  0.110400  0.798340  0.752353
5    0.976196  0.101549  0.820068  0.763873
6    0.979858  0.099742  0.825439  0.655470
7    0.977173  0.099192  0.825195  0.830862
8    0.981812  0.087729  0.813599  0.575661
9    0.974854  0.103661  0.813721  1.466361
10   0.978271  0.096900  0.817505  0.610173
11   0.980103  0.090031  0.845337  0.577904
12   0.975830  0.105945  0.822632  1.254806
13   0.979980  0.111833  0.814209  0.535853
14   0.981812  0.082458  0.825439  0.529519
15   0.981445  0.094816  0.838745  0.519675
16   0.982788  0.093028  0.818848  0.480941
17   0.980103  0.089846  0.842773  0.400433
18   0.981934  0.081948  0.838867  0.730471
19   0.983032  0.084349  0.833252  0.756530
20   0.983765  0.079173  0.844116  0.458487
21   0.982056  0.083488  0.856323  0.483163
22   0.984375  0.075514  0.821777  0.664129
23   0.923706  0.451527  0.843262  0.644352
24   0.958252  0.210743  0.814087  0.690111
25   0.966187  0.153388  0.807983  0.519022
26   0.974243  0.131802  0.836670  0.643411
27   0.977051  0.120662  0.847168  0.475067
28   0.980103  0.103587  0.827759  0.694326
29   0.981201  0.106777  0.833740  0.504370
30   0.976318  0.110367  0.839600  0.492762
31   0.978149  0.110566  0.849854  0.526216
32   0.981812  0.096134  0.834229  0.675471
33   0.980591  0.117588  0.830078  0.497432
34   0.975708  0.116398  0.835693  0.535456
35   0.932251  0.801713  0.490723  8.270695
36   0.496948  8.160616  0.487671  8.303543
37   0.509033  7.954911  0.502808  8.051417
38   0.499268  8.105364  0.515015  7.848671
39   0.494995  8.168893  0.506226  7.985568
40   0.509399  7.932394  0.507812  7.956056
41   0.501465  8.056672  0.501953  8.047182
42   0.500000  8.077215  0.488770  8.256843
43   0.494385  8.165091  0.500977  8.057649
44   0.495605  8.143143  0.505859  7.976833
45   0.508179  7.938514  0.491455  8.207168
46   0.497803  8.104043  0.501953  8.036365
47   0.507202  7.951054  0.507446  7.946440
48   0.505859  7.971407  0.499634  8.071164
49   0.491943  8.194590  0.504028  7.999297
50   0.505127  7.981134  0.499756  8.067271
51   0.489624  8.230188  0.494141  8.157017
52   0.493530  8.166524  0.500244  8.057994
53   0.502319  8.024267  0.498535  8.084995
54   0.494751  8.145754  0.505493  7.972389
55   0.504028  7.995804  0.499390  8.070387
56   0.505127  7.977752  0.510864  7.885127
57   0.497437  8.101425  0.493652  8.162296
58   0.510132  7.896572  0.503662  8.000753
59   0.494629  8.146267  0.504150  7.992720
60   0.500854  8.045776  0.488647  8.242468
61   0.488281  8.248318  0.492310  8.183340
62   0.492676  8.177396  0.507812  7.933383
63   0.498169  8.088786  0.498535  8.082854
64   0.503784  7.998225  0.495483  8.131995
65   0.503418  8.004086  0.504150  7.992263
66   0.504028  7.994216  0.500366  8.053229
67   0.493286  8.167335  0.498535  8.082720
68   0.502808  8.013848  0.499634  8.064996
69   0.486084  8.283387  0.495728  8.127945
70   0.497803  8.094492  0.494751  8.143676
71   0.493774  8.159413  0.499146  8.072838
72   0.492676  8.177115  0.504761  7.982326
73   0.496826  8.107081  0.491699  8.115705
74   0.504639  7.908351  0.500000  7.980005
75   0.497070  8.025797  0.501099  7.960815
76   0.499634  7.983595  0.503784  7.916908
77   0.491943  8.105245  0.501343  7.954990
78   0.498779  7.995502  0.502930  7.928995
79   0.490356  8.129136  0.486084  8.196954
80   0.506470  7.915073  0.497437  8.126250
81   0.507568  7.956829  0.491943  8.204824
82   0.496826  8.123989  0.507446  7.951029
83   0.497681  8.107108  0.499512  8.076406
84   0.498169  8.097081  0.512817  7.860081
85   0.497681  8.103296  0.496826  8.116356
86   0.498779  8.084259  0.511108  7.884956
87   0.494873  8.146131  0.498779  8.082691
88   0.500000  8.062595  0.499023  8.077939
89   0.494995  8.142521  0.494507  8.150065
90   0.494873  8.143878  0.496826  8.112130
91   0.495972  8.125672  0.499023  8.076266
92   0.504395  7.989509  0.512939  7.851607
93   0.506226  7.959674  0.500488  8.052012
94   0.495850  8.126663  0.500000  8.059660
95   0.502686  8.016285  0.495483  8.132288
96   0.501465  8.035811  0.476685  8.435160
97   0.503174  8.008154  0.498657  8.080907
98   0.500854  8.045455  0.506348  7.956882
99   0.492798  8.175253  0.488159  8.249996
100  0.503784  7.998133  0.508545  7.921382
101  0.500610  8.049260  0.500854  8.045314
102  0.498779  8.078754  0.505249  7.974468
103  0.493286  8.167281  0.502075  8.025614
104  0.496338  8.118085  0.487183  8.265648
105  0.501709  8.031509  0.494873  8.141689
106  0.498535  8.082662  0.504150  7.992154
107  0.496582  8.114141  0.494507  8.147588
108  0.494995  8.118198  0.493408  8.120505
109  0.505859  7.920058  0.506226  7.912357
110  0.505737  7.918426  0.501709  7.980968
111  0.502563  7.965764  0.496948  8.053728
112  0.500244  7.999707  0.497437  8.043012
113  0.502197  7.965730  0.497559  8.038316
114  0.496582  8.052587  0.504761  7.920918
115  0.497070  8.042308  0.502197  7.959375
116  0.497803  8.028302  0.502930  7.945452
117  0.499756  7.995000  0.504395  7.920018
118  0.499268  8.000786  0.501831  7.958970
119  0.505737  7.895808  0.498657  8.007814
120  0.501221  7.966139  0.503174  7.934214
121  0.500610  7.974350  0.503662  7.924985
122  0.498413  8.008007  0.502319  7.945089
123  0.488281  8.168294  0.505249  7.897208
124  0.496094  8.042628  0.497314  8.022644
125  0.494629  8.064972  0.502441  7.939949
126  0.495483  8.050435  0.498535  8.001352
127  0.500366  7.971757  0.507202  7.862381

2018-02-13 11:13:52.259136 Finish.
Total elapsed time: 15:13:56.26.
