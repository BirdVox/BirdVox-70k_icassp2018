2018-02-12 20:00:44.158312: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:00:44.159182: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:00:44.159196: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 20:00:20.115363 Start.
Training NTT-like convnet on BirdVox-70k with PCEN input. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 16)            1168        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 4)             68          bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_dense2[0][0]                  
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
flatten (Flatten)                (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             65          flatten[0][0]                    
====================================================================================================
Total params: 678,577
Trainable params: 678,575
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.977539  0.112740  0.823486  0.870116
1    0.973389  0.124563  0.843994  0.745500
2    0.979370  0.105490  0.856812  0.574321
3    0.979126  0.104205  0.856934  0.471894
4    0.978027  0.098319  0.831665  0.635642
5    0.977905  0.102672  0.861084  0.490276
6    0.978516  0.093845  0.869751  0.493597
7    0.980713  0.092912  0.851685  0.603836
8    0.981201  0.081475  0.855347  0.702867
9    0.977905  0.100272  0.871704  0.545750
10   0.981323  0.086722  0.885864  0.380270
11   0.980225  0.087570  0.885498  0.462307
12   0.980469  0.089613  0.858154  0.594551
13   0.982788  0.087924  0.886108  0.420789
14   0.982788  0.091091  0.873169  0.476010
15   0.978516  0.118807  0.869629  0.510108
16   0.983398  0.087605  0.879761  0.597919
17   0.979370  0.090178  0.877808  0.889987
18   0.981079  0.096876  0.898071  0.524341
19   0.981689  0.085438  0.876099  0.540241
20   0.981934  0.094242  0.879761  0.445844
21   0.965576  0.171967  0.839966  0.601141
22   0.959106  0.403299  0.498291  8.123983
23   0.493652  8.194332  0.492920  8.201087
24   0.513550  7.864728  0.503418  8.024595
25   0.492676  8.195004  0.501343  8.052825
26   0.497925  8.105900  0.503296  8.017485
27   0.500122  8.067130  0.497559  8.107063
28   0.497070  8.113792  0.509644  7.910088
29   0.498779  8.084337  0.496704  8.116993
30   0.493652  8.165529  0.505371  7.976047
31   0.498169  8.091641  0.498779  8.081354
32   0.499878  8.063278  0.498779  8.080649
33   0.496094  8.123660  0.504028  7.995519
34   0.502563  8.018926  0.492554  8.180078
35   0.512451  7.859219  0.483276  8.329325
36   0.502686  8.016376  0.507690  7.935607
37   0.492798  8.175567  0.502808  8.013939
38   0.502808  7.994799  0.498901  8.104347
39   0.500366  8.072636  0.498657  8.095147
40   0.506348  7.968619  0.500610  8.059048
41   0.495483  8.140321  0.497070  8.113579
42   0.499512  8.073368  0.505005  7.984067
43   0.502563  8.022823  0.494385  8.154110
44   0.497437  8.104486  0.508911  7.919137
45   0.500000  8.062435  0.504150  7.995231
46   0.501953  8.030387  0.499390  8.071462
47   0.508057  7.920350  0.500000  7.987223
48   0.501709  7.954734  0.497559  8.018614
49   0.506836  7.869613  0.510620  7.808336
50   0.514648  7.743362  0.505249  7.892518
51   0.511719  7.788793  0.507324  7.858310
52   0.501343  7.953209  0.508911  7.832123
53   0.505737  7.882359  0.501221  7.954027
54   0.504761  7.897306  0.494507  8.060514
55   0.496460  8.029156  0.497192  8.017276
56   0.496704  8.024891  0.500000  7.972191
57   0.505615  7.882544  0.498047  8.003084
58   0.505249  7.888169  0.495605  8.041822
59   0.502441  7.932770  0.500854  7.958005
60   0.495117  8.049418  0.496948  8.020179
61   0.505737  7.880021  0.501343  7.950045
62   0.498413  7.996722  0.494995  8.051186
63   0.508423  7.912107  0.510254  7.935760
64   0.500366  8.091709  0.499023  8.110225
65   0.497070  8.139015  0.495972  8.154184
66   0.502930  8.039817  0.503052  8.035751
67   0.502686  8.039811  0.497070  8.128569
68   0.504883  8.001102  0.498291  8.105879
69   0.493530  8.181305  0.506470  7.971496
70   0.502319  8.037271  0.495483  8.146377
71   0.498291  8.100151  0.499756  8.075602
72   0.498169  8.100324  0.502563  8.028664
73   0.508301  7.935430  0.496216  8.129478
74   0.492432  8.189792  0.491821  8.198968
75   0.494873  8.149168  0.490845  8.213502
76   0.503418  8.010296  0.502686  8.021567
77   0.504517  7.991563  0.502930  8.016664
78   0.503540  8.006390  0.499268  8.074831
79   0.498901  8.080349  0.507690  7.938316
80   0.499756  8.065872  0.491821  8.193442
81   0.506836  7.951148  0.501099  8.043350
82   0.504639  7.986050  0.509644  7.905152
83   0.513550  7.841992  0.494629  8.146773
84   0.497437  8.101359  0.511597  7.872974
85   0.498047  8.091245  0.505859  7.965204
86   0.504639  7.984783  0.507324  7.941408
87   0.493408  8.143366  0.501709  8.006281
88   0.500732  8.019741  0.492432  8.149956
89   0.514893  7.789775  0.496948  8.073735
90   0.503174  7.972391  0.488770  8.199924
91   0.499146  8.032430  0.489746  8.180197
92   0.505249  7.931003  0.508423  7.878364
93   0.493286  8.117693  0.486572  8.222749
94   0.499268  8.018444  0.504272  7.936757
95   0.498047  8.034191  0.504761  7.925360
96   0.500488  7.991769  0.488159  8.186647
97   0.508789  7.856179  0.497803  8.029782
98   0.506348  7.892114  0.492920  8.104779
99   0.500977  7.975038  0.496216  8.049674
100  0.494995  8.067979  0.510010  7.827491
101  0.499512  7.993835  0.495850  8.051235
102  0.505737  7.892711  0.491211  8.123438
103  0.504639  7.908593  0.500244  7.977907
104  0.500244  7.977233  0.500366  7.974639
105  0.502197  7.944861  0.501709  7.952079
106  0.499756  7.982702  0.504883  7.900468
107  0.499390  7.987586  0.494385  8.066934
108  0.498779  7.996467  0.510620  7.807300
109  0.506714  7.869208  0.500244  7.971993
110  0.506958  7.864626  0.499634  7.981068
111  0.503052  7.926275  0.501831  7.945442
112  0.494263  8.065826  0.506104  7.876788
113  0.491089  8.115910  0.495239  8.049503
114  0.497559  8.012306  0.487793  8.167779
115  0.499878  7.974921  0.502686  7.929972
116  0.506714  7.865580  0.494141  8.065863
117  0.504517  7.900299  0.496582  8.026655
118  0.494263  8.063515  0.500732  7.960248
119  0.503906  7.909548  0.500854  7.958105
120  0.491821  8.102034  0.498413  7.996870
121  0.495972  8.035729  0.495239  8.047348
122  0.495117  8.049251  0.500610  7.961632
123  0.509155  7.922900  0.494019  8.219347
124  0.498535  8.145380  0.496582  8.175659
125  0.494507  8.207877  0.505371  8.031510
126  0.496582  8.171892  0.504150  8.048597
127  0.504883  8.035464  0.499878  8.114781

2018-02-13 13:29:27.238613 Finish.
Total elapsed time: 17:29:07.24.
