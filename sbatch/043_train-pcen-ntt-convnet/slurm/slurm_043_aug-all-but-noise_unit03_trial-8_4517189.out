2018-02-12 20:00:40.022798: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:00:40.023305: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:00:40.023320: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 20:00:18.433880 Start.
Training NTT-like convnet on BirdVox-70k with PCEN input. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 16)            1168        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 4)             68          bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_dense2[0][0]                  
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
flatten (Flatten)                (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             65          flatten[0][0]                    
====================================================================================================
Total params: 678,577
Trainable params: 678,575
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.971436  0.114629  0.849121  0.452750
1    0.970947  0.121315  0.858154  0.598964
2    0.972046  0.111802  0.864136  0.374337
3    0.977661  0.098551  0.882568  0.450823
4    0.977417  0.100165  0.860352  0.551962
5    0.973999  0.109149  0.868774  0.602431
6    0.974487  0.113692  0.855835  0.599264
7    0.979492  0.092243  0.874878  0.468267
8    0.975708  0.101320  0.834595  0.527765
9    0.976562  0.101364  0.873047  0.449686
10   0.976562  0.109767  0.855957  0.566363
11   0.976562  0.101039  0.839478  0.556751
12   0.976318  0.114036  0.870239  0.368627
13   0.977661  0.097043  0.852173  0.544839
14   0.979370  0.092082  0.878906  0.397447
15   0.978760  0.110777  0.857544  0.825467
16   0.977417  0.100442  0.858032  0.577543
17   0.977905  0.095042  0.875244  0.495496
18   0.978760  0.099933  0.873413  0.537330
19   0.973877  0.139216  0.863770  0.746923
20   0.973999  0.120890  0.853638  0.545098
21   0.979858  0.103677  0.863892  0.561470
22   0.979492  0.092294  0.869019  0.608638
23   0.979736  0.094715  0.869751  0.542022
24   0.980713  0.089097  0.866699  0.634886
25   0.980957  0.088892  0.865723  0.457277
26   0.982300  0.079708  0.884277  0.565214
27   0.970703  0.154977  0.864380  0.514869
28   0.979980  0.114608  0.875854  0.431476
29   0.981445  0.100549  0.873169  0.415043
30   0.981445  0.091955  0.875854  0.430962
31   0.980713  0.095579  0.871338  0.358460
32   0.981201  0.083995  0.863892  0.590863
33   0.829712  2.607320  0.567383  7.003640
34   0.520630  7.691849  0.490112  8.161921
35   0.492188  8.125950  0.501221  7.979434
36   0.491577  8.131219  0.491211  8.135270
37   0.498291  8.020903  0.503906  7.929988
38   0.493164  8.100039  0.507690  7.867315
39   0.495483  8.060918  0.509399  7.838105
40   0.490723  8.134998  0.500977  7.970702
41   0.501465  7.962171  0.492554  8.103516
42   0.492676  8.100913  0.487061  8.189797
43   0.501465  7.959572  0.500610  7.972626
44   0.505127  7.900094  0.500732  7.969640
45   0.502075  7.947756  0.500977  7.964805
46   0.497925  8.013023  0.500732  7.967837
47   0.505981  7.883757  0.494141  8.072137
48   0.491821  8.108746  0.493164  8.086979
49   0.503174  7.927061  0.497925  8.010411
50   0.503906  7.914739  0.492798  8.091526
51   0.501099  7.958902  0.495605  8.046191
52   0.507202  7.861044  0.495605  8.045659
53   0.498291  8.002596  0.505493  7.887532
54   0.508545  7.838650  0.497925  8.007734
55   0.492920  8.087311  0.493652  8.075426
56   0.508545  7.837807  0.497314  8.016656
57   0.504639  7.899711  0.497192  8.018248
58   0.507812  7.848774  0.500977  7.957596
59   0.499268  7.984693  0.503540  7.916436
60   0.509277  7.824837  0.495483  8.044616
61   0.500732  7.960816  0.495972  8.036599
62   0.492920  8.085146  0.500610  7.962442
63   0.501221  7.952620  0.502075  7.938910
64   0.505249  7.888232  0.490601  8.121688
65   0.493164  8.080753  0.494385  8.061228
66   0.494141  8.065064  0.503662  7.913215
67   0.499268  7.983227  0.498047  8.002644
68   0.501953  7.940331  0.505981  7.876074
69   0.510376  7.805983  0.493042  8.082300
70   0.489990  8.130927  0.509155  7.825368
71   0.499390  7.981037  0.506226  7.872038
72   0.500000  7.971274  0.506470  7.868118
73   0.490234  8.126937  0.507080  7.858367
74   0.505005  7.891442  0.506836  7.862244
75   0.508423  7.836939  0.494995  8.050517
76   0.499634  7.977080  0.515381  7.726033
77   0.490845  8.117164  0.503052  7.922549
78   0.507812  7.846650  0.489990  8.130234
79   0.503784  7.911487  0.497314  8.014312
80   0.492676  8.088088  0.504517  7.899234
81   0.493042  8.082145  0.517090  7.698752
82   0.497681  8.008175  0.508423  7.836916
83   0.500610  7.961464  0.491211  8.111313
84   0.495605  8.041266  0.498047  8.002331
85   0.497437  8.106761  0.508179  7.971808
86   0.503418  8.042693  0.491333  8.233523
87   0.500000  8.091394  0.496948  8.138446
88   0.493408  8.193762  0.489502  8.255088
89   0.500854  8.070656  0.492798  8.199117
90   0.500122  8.079788  0.499268  8.092318
91   0.496826  8.130516  0.508789  7.936571
92   0.494629  8.163755  0.500977  8.060415
93   0.505737  7.982718  0.507080  7.960134
94   0.497437  8.114688  0.498779  8.092183
95   0.503418  8.016609  0.502319  8.033527
96   0.499268  8.081979  0.498169  8.098965
97   0.510620  7.897603  0.492432  8.190110
98   0.498413  8.093088  0.502686  8.023626
99   0.509521  7.912889  0.500854  8.052043
100  0.498413  8.090894  0.496338  8.123854
101  0.499390  8.074217  0.496948  8.113132
102  0.503662  8.004516  0.494995  8.143824
103  0.509766  7.905398  0.498413  8.088038
104  0.494019  8.158560  0.499756  8.065788
105  0.500244  8.057650  0.499878  8.063296
106  0.508301  7.927307  0.496216  8.121875
107  0.505371  8.000006  0.499634  8.137437
108  0.494751  8.204240  0.495850  8.177791
109  0.500977  8.089489  0.492676  8.215881
110  0.509521  7.942378  0.507202  7.974728
111  0.509155  7.939563  0.500244  8.077799
112  0.493042  8.189206  0.498047  8.106142
113  0.491577  8.206320  0.491577  8.203457
114  0.501587  8.041255  0.493042  8.174935
115  0.496948  8.110306  0.490601  8.209206
116  0.498779  8.076677  0.497070  8.101825
117  0.503418  7.998657  0.490112  8.208843
118  0.496094  8.111649  0.503418  7.993072
119  0.483887  8.302719  0.500488  8.036340
120  0.495728  8.110594  0.504272  7.972736
121  0.495972  8.103493  0.496704  8.090245
122  0.486938  8.244406  0.500854  8.021027
123  0.497681  8.070138  0.494751  8.115356
124  0.502197  7.995186  0.493042  8.139682
125  0.504028  7.963100  0.496948  8.074534
126  0.501465  8.001115  0.499268  8.034725
127  0.504517  7.949648  0.500732  8.008576

2018-02-13 13:28:03.259487 Finish.
Total elapsed time: 17:27:45.26.
