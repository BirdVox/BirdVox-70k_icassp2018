2018-02-12 20:00:39.819075: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:00:39.819370: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:00:39.819383: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 19:59:58.193564 Start.
Training NTT-like convnet on BirdVox-70k with PCEN input. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 16)            1168        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 4)             68          bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_dense2[0][0]                  
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
flatten (Flatten)                (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             65          flatten[0][0]                    
====================================================================================================
Total params: 678,577
Trainable params: 678,575
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.970093  0.108667  0.820679  0.856971
1    0.974121  0.105706  0.797241  0.650252
2    0.974609  0.100106  0.824585  0.556910
3    0.972412  0.100913  0.784668  0.717190
4    0.974854  0.093628  0.811279  0.788134
5    0.974365  0.092137  0.776855  0.632711
6    0.970825  0.107065  0.820312  0.680591
7    0.977051  0.082134  0.791504  0.682022
8    0.977417  0.084932  0.823364  0.857261
9    0.977783  0.085302  0.805054  0.582979
10   0.977539  0.090216  0.807373  0.814181
11   0.978394  0.097627  0.809082  0.586113
12   0.979614  0.086887  0.817993  0.662801
13   0.977539  0.093995  0.810181  0.610883
14   0.980957  0.079012  0.834839  0.614864
15   0.976685  0.100201  0.832520  0.536298
16   0.982788  0.078985  0.830078  0.722801
17   0.978516  0.088917  0.819946  0.859439
18   0.980103  0.087795  0.791260  0.720931
19   0.980713  0.091281  0.816895  0.540917
20   0.979126  0.094510  0.815674  0.521278
21   0.974487  0.095014  0.815918  0.652615
22   0.979370  0.082385  0.819580  0.645285
23   0.975586  0.098664  0.817261  0.590217
24   0.981323  0.077321  0.832275  0.541032
25   0.978271  0.088541  0.784424  0.709714
26   0.979614  0.089651  0.807861  0.718764
27   0.982788  0.076361  0.835449  0.539958
28   0.980713  0.075252  0.819824  0.557261
29   0.981567  0.072731  0.802490  0.729327
30   0.980835  0.074200  0.804688  0.841454
31   0.977905  0.090078  0.811157  0.612506
32   0.974854  0.112659  0.821411  1.149609
33   0.978760  0.099042  0.829346  0.778621
34   0.968506  0.126157  0.786865  0.735753
35   0.976807  0.113604  0.798706  1.039897
36   0.945312  0.414657  0.698486  4.835546
37   0.586670  6.635304  0.512451  7.811800
38   0.492920  8.121063  0.494385  8.095851
39   0.501465  7.981452  0.499634  8.009212
40   0.505615  7.912593  0.500488  7.993122
41   0.508911  7.857747  0.500488  7.990968
42   0.497925  8.030860  0.500732  7.985149
43   0.504883  7.918096  0.497803  8.030103
44   0.498169  8.023451  0.491577  8.127742
45   0.495361  8.066659  0.497314  8.034780
46   0.495605  8.061323  0.499634  7.996409
47   0.495361  8.063864  0.496704  8.041806
48   0.503906  7.926367  0.506104  7.890725
49   0.496704  8.039989  0.488037  8.177583
50   0.509766  7.830626  0.504150  7.919599
51   0.504272  7.917129  0.502197  7.949693
52   0.502563  7.943358  0.499023  7.999303
53   0.499512  7.991047  0.500244  7.978904
54   0.499634  7.988186  0.505249  7.898222
55   0.504883  7.903636  0.498779  8.000519
56   0.497681  8.017631  0.503052  7.931604
57   0.498779  7.999334  0.506470  7.876352
58   0.491333  8.117305  0.501221  7.959313
59   0.500244  7.974540  0.494751  8.061775
60   0.491333  8.115942  0.488159  8.166220
61   0.500732  7.965468  0.497803  8.011873
62   0.499390  7.986286  0.501831  7.947081
63   0.498779  7.995464  0.505615  7.886218
64   0.498169  8.004677  0.507080  7.862365
65   0.492310  8.097607  0.501465  7.951419
66   0.494873  8.056290  0.502319  7.937365
67   0.491577  8.108420  0.504028  7.909721
68   0.503662  7.915374  0.506104  7.876272
69   0.493774  8.072658  0.500732  7.961566
70   0.503418  7.918599  0.498047  8.004078
71   0.495483  8.044808  0.509644  7.818928
72   0.492676  8.089312  0.496338  8.030811
73   0.496216  8.032649  0.501221  7.952754
74   0.500610  7.962390  0.490356  8.125771
75   0.523315  7.600245  0.495972  8.036091
76   0.501221  7.952340  0.504517  7.899729
77   0.516235  7.712845  0.503662  7.913237
78   0.498535  7.994924  0.487671  8.168080
79   0.489868  8.133011  0.498047  8.002585
80   0.497803  8.006445  0.494629  8.057013
81   0.500977  7.955791  0.501343  7.949928
82   0.492554  8.090027  0.497681  8.008272
83   0.500366  7.965443  0.496826  8.021865
84   0.498047  8.002393  0.502075  7.938161
85   0.501953  7.940098  0.496094  8.033503
86   0.511597  7.817310  0.492188  8.190936
87   0.500366  8.057917  0.492676  8.181157
88   0.490234  8.220078  0.497437  8.103619
89   0.511719  7.873125  0.500732  8.049938
90   0.494629  8.148092  0.500244  8.057377
91   0.501099  8.043424  0.506958  7.948811
92   0.493652  8.163123  0.496216  8.121661
93   0.501099  8.042832  0.504761  7.983684
94   0.508423  7.924548  0.500610  8.050366
95   0.499634  8.066011  0.501709  8.032473
96   0.495239  8.136671  0.502930  8.012638
97   0.499512  8.067658  0.503418  8.004630
98   0.505615  7.969153  0.507568  7.937614
99   0.508911  7.915919  0.508911  7.915869
100  0.505981  8.044242  0.507080  8.009777
101  0.499512  8.114767  0.491699  8.228438
102  0.482178  8.374983  0.501099  8.064485
103  0.502197  8.043138  0.502075  8.042040
104  0.500122  8.071323  0.503296  8.018255
105  0.498413  8.095513  0.495483  8.141453
106  0.494019  8.164063  0.503906  8.003793
107  0.496338  8.125062  0.500122  8.063415
108  0.498047  8.096332  0.494629  8.150936
109  0.502930  8.016741  0.490479  8.217058
110  0.498535  8.086888  0.501221  8.043311
111  0.502563  8.021419  0.507935  7.934614
112  0.500244  8.058365  0.493652  8.164419
113  0.503296  8.008812  0.494873  8.144408
114  0.490845  8.209190  0.502319  8.024098
115  0.500732  8.049546  0.498413  8.086802
116  0.505737  7.968633  0.513062  7.850466
117  0.497314  8.104172  0.495117  8.139483
118  0.505127  7.978047  0.494263  8.153063
119  0.503540  8.003440  0.503662  8.001384
120  0.505249  7.975724  0.504272  7.991383
121  0.503052  8.010984  0.500488  8.052228
122  0.503296  8.006906  0.503906  7.996956
123  0.511353  7.960787  0.485474  8.388540
124  0.503784  8.083615  0.503174  8.081883
125  0.503540  8.066926  0.509155  7.969024
126  0.504517  8.035893  0.501221  8.081784
127  0.497803  8.130442  0.489868  8.251375

2018-02-13 11:04:25.758058 Finish.
Total elapsed time: 15:04:27.76.
