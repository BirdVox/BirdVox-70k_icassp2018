2018-02-12 20:00:42.106673: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:00:42.106978: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:00:42.106991: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 20:00:02.414800 Start.
Training NTT-like convnet on BirdVox-70k with PCEN input. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 16)            1168        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 4)             68          bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_dense2[0][0]                  
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
flatten (Flatten)                (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             65          flatten[0][0]                    
====================================================================================================
Total params: 678,577
Trainable params: 678,575
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.974365  0.107575  0.871338  0.563166
1    0.972778  0.110648  0.866699  0.514851
2    0.977173  0.099191  0.873779  0.375123
3    0.975220  0.102008  0.848267  0.396650
4    0.972656  0.110554  0.865601  0.464486
5    0.976929  0.097893  0.880859  0.437915
6    0.974609  0.128075  0.866821  0.429297
7    0.976685  0.104015  0.850464  0.579925
8    0.977173  0.104519  0.850708  0.466059
9    0.979248  0.090412  0.854736  0.546896
10   0.979126  0.092859  0.847534  0.464417
11   0.976807  0.097768  0.862549  0.522891
12   0.974121  0.112562  0.857300  0.617401
13   0.979614  0.099650  0.819702  0.719468
14   0.972778  0.124963  0.860962  0.551653
15   0.976929  0.113031  0.856812  0.497508
16   0.979004  0.096265  0.859863  0.478702
17   0.977661  0.093625  0.865845  0.345848
18   0.978027  0.110013  0.850342  0.715664
19   0.978027  0.098945  0.868896  0.454924
20   0.980225  0.084839  0.879761  0.378869
21   0.978760  0.090959  0.847778  0.441704
22   0.929932  0.474385  0.674194  0.671435
23   0.956665  0.246821  0.860596  0.438680
24   0.972534  0.165274  0.853149  0.560645
25   0.977783  0.132673  0.838501  0.431847
26   0.976562  0.162120  0.844116  0.511405
27   0.842163  2.361590  0.500488  8.115917
28   0.499634  8.120261  0.490356  8.262216
29   0.506348  7.998997  0.505859  8.001983
30   0.487915  8.287278  0.502686  8.045591
31   0.508545  7.948121  0.506348  7.980717
32   0.508545  7.942896  0.497192  8.123622
33   0.497803  8.111841  0.502686  8.031312
34   0.513184  7.860523  0.497559  8.110879
35   0.502197  8.034823  0.498657  8.090666
36   0.508301  7.934178  0.492188  8.192902
37   0.502930  8.018902  0.498657  8.086960
38   0.498535  8.088234  0.504150  7.997076
39   0.500732  8.051607  0.490601  8.214389
40   0.494141  8.156883  0.499634  8.067925
41   0.502197  8.026250  0.497314  8.104620
42   0.496704  8.114176  0.498413  8.086370
43   0.494507  8.149111  0.499268  8.072173
44   0.496338  8.119223  0.493774  8.160383
45   0.498657  8.081550  0.505005  7.979117
46   0.503784  7.998692  0.501343  8.037950
47   0.488770  8.240531  0.503540  8.002388
48   0.506836  7.949208  0.497192  8.104591
49   0.491821  8.191120  0.480469  8.374063
50   0.496948  8.108414  0.499023  8.074937
51   0.501099  8.041466  0.497681  8.096537
52   0.495972  8.124065  0.510010  7.897783
53   0.501831  8.029596  0.503174  8.007942
54   0.492554  8.179110  0.501343  8.037440
55   0.492188  8.184999  0.495117  8.137773
56   0.499268  8.070873  0.497803  8.094479
57   0.504883  7.980359  0.506714  7.950844
58   0.500732  8.047251  0.501343  8.037412
59   0.503540  8.001995  0.509277  7.909519
60   0.503296  8.005928  0.503784  7.998057
61   0.499634  8.064953  0.500000  8.059050
62   0.511108  7.880004  0.502075  8.025602
63   0.510620  7.875705  0.507935  7.902577
64   0.498779  8.039647  0.500000  8.013227
65   0.501587  7.983168  0.496338  8.062692
66   0.499756  8.004972  0.494019  8.093513
67   0.510620  7.826453  0.508179  7.863169
68   0.498535  8.015054  0.494385  8.079490
69   0.504639  7.914535  0.494385  8.076612
70   0.490845  8.131839  0.488770  8.163779
71   0.506958  7.872809  0.498047  8.013923
72   0.496582  8.036436  0.502930  7.934440
73   0.500488  7.972652  0.503296  7.927215
74   0.495117  8.056999  0.509766  7.822890
75   0.492432  8.098718  0.496582  8.032055
76   0.501099  7.959604  0.502930  7.929985
77   0.496826  8.026905  0.496704  8.028480
78   0.495483  8.047607  0.489136  8.148482
79   0.505005  7.895199  0.504272  7.906595
80   0.499268  7.986131  0.494263  8.065675
81   0.498291  8.001232  0.499878  7.975718
82   0.500732  7.961900  0.500610  7.963658
83   0.500366  7.967378  0.492554  8.091763
84   0.507690  7.850297  0.502197  7.937726
85   0.499390  7.982355  0.498413  7.997796
86   0.496582  8.026873  0.499634  7.978110
87   0.501099  7.954657  0.504761  7.896179
88   0.500732  7.960314  0.492188  8.096457
89   0.490112  8.129467  0.500488  7.963979
90   0.501221  7.952240  0.503052  7.922990
91   0.513428  7.757520  0.499146  7.985164
92   0.493408  8.076587  0.499390  7.981189
93   0.506470  7.868281  0.500854  7.957769
94   0.505249  7.887682  0.487427  8.021963
95   0.500977  7.956186  0.494263  8.062771
96   0.500854  7.957660  0.492920  8.084140
97   0.503418  7.971874  0.491333  8.346099
98   0.500244  8.163191  0.507935  8.012123
99   0.497925  8.158970  0.491699  8.248004
100  0.495239  8.183863  0.499878  8.103273
101  0.499512  8.105176  0.493164  8.204058
102  0.499878  8.093281  0.500122  8.087057
103  0.498413  8.112753  0.494263  8.177937
104  0.504272  8.015124  0.490967  8.228184
105  0.503906  8.018364  0.490845  8.227672
106  0.516113  7.819268  0.498169  8.107401
107  0.507690  7.952911  0.496948  8.125053
108  0.505859  7.980484  0.496338  8.133031
109  0.489502  8.242350  0.503296  8.019172
110  0.498901  8.089211  0.506592  7.964480
111  0.502930  8.022782  0.499878  8.071263
112  0.503540  8.011579  0.496094  8.130957
113  0.496094  8.130363  0.493042  8.178974
114  0.504395  7.995459  0.498047  8.097254
115  0.509644  7.909861  0.499390  8.074674
116  0.500244  8.060479  0.503418  8.008915
117  0.506226  7.963288  0.499756  8.067207
118  0.505005  7.982275  0.504517  7.989828
119  0.506592  7.956092  0.501343  8.040418
120  0.501099  8.044100  0.489380  8.232741
121  0.504517  7.988545  0.503662  8.002105
122  0.498047  8.092419  0.501465  8.037142
123  0.488403  8.247502  0.500366  8.054523
124  0.493286  8.168496  0.502319  8.022760
125  0.505615  7.969514  0.499146  8.073677
126  0.503296  8.006677  0.510254  7.894430
127  0.497437  8.100936  0.496948  8.108726

2018-02-13 11:04:42.323309 Finish.
Total elapsed time: 15:04:40.32.
