2018-02-12 20:00:35.573363: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:00:35.573676: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:00:35.573689: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 20:00:02.487320 Start.
Training NTT-like convnet on BirdVox-70k with PCEN input. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 16)            1168        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 4)             68          bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_dense2[0][0]                  
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
flatten (Flatten)                (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             65          flatten[0][0]                    
====================================================================================================
Total params: 678,577
Trainable params: 678,575
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.972900  0.126618  0.794922  0.755906
1    0.973267  0.116136  0.827026  0.873186
2    0.978271  0.108905  0.812500  0.647778
3    0.974121  0.110853  0.836548  0.573386
4    0.976074  0.111514  0.827026  0.665299
5    0.976196  0.109214  0.822876  0.685472
6    0.976807  0.106073  0.821533  0.821196
7    0.979004  0.097962  0.839722  0.740943
8    0.978638  0.093933  0.814819  0.745282
9    0.977295  0.112295  0.819214  0.667007
10   0.963013  0.181999  0.816162  1.091021
11   0.970459  0.160838  0.812500  0.645099
12   0.976318  0.134287  0.839355  0.824552
13   0.734985  3.677659  0.494385  8.135018
14   0.489136  8.208658  0.502808  7.982542
15   0.503784  7.960999  0.491089  8.158039
16   0.505005  7.931850  0.497925  8.040734
17   0.501221  7.984836  0.500366  7.995331
18   0.494263  8.089953  0.503174  7.945368
19   0.499756  7.997672  0.487305  8.194108
20   0.495850  8.056077  0.498169  8.017393
21   0.499512  7.994487  0.498413  8.010581
22   0.497070  8.030738  0.495728  8.050958
23   0.506836  7.872820  0.503540  7.924374
24   0.492188  8.104489  0.501221  7.959651
25   0.502319  7.941410  0.502563  7.936829
26   0.501343  7.955686  0.494141  8.069933
27   0.499390  7.985749  0.505493  7.887969
28   0.498901  7.992644  0.494629  8.060365
29   0.502197  7.939365  0.501343  7.952664
30   0.511841  7.785019  0.494751  8.057206
31   0.495117  8.051137  0.505859  7.879663
32   0.506348  7.871690  0.498291  7.999954
33   0.505493  7.884980  0.497559  8.011331
34   0.499390  7.982014  0.490479  8.123961
35   0.497803  8.007094  0.503296  7.919424
36   0.499390  7.981617  0.502075  7.938725
37   0.502808  7.926982  0.493774  8.070931
38   0.495972  8.035847  0.488770  8.150616
39   0.500244  7.967641  0.510254  7.808022
40   0.501953  7.940322  0.502686  7.928613
41   0.501221  7.951940  0.490967  8.115386
42   0.502197  7.936325  0.515259  7.728073
43   0.499268  7.982994  0.501953  7.940165
44   0.499390  7.981020  0.491455  8.107504
45   0.499268  7.995335  0.499634  8.119916
46   0.500244  8.094038  0.502441  8.047633
47   0.503296  8.028813  0.495605  8.148807
48   0.503784  8.014381  0.503052  8.023976
49   0.505737  7.979063  0.495239  8.146831
50   0.493896  8.167344  0.515869  7.812160
51   0.493530  8.171379  0.501099  8.048615
52   0.493530  8.169948  0.495850  8.131954
53   0.490723  8.214066  0.511108  7.884993
54   0.494019  8.160019  0.502930  8.015982
55   0.509277  7.913313  0.509033  7.916910
56   0.505249  7.977604  0.502808  8.016671
57   0.503174  8.010515  0.495117  8.140131
58   0.506348  7.958903  0.495972  8.125939
59   0.498413  8.086405  0.502075  8.027204
60   0.502686  8.004113  0.490479  8.169956
61   0.496338  8.062785  0.502075  7.962811
62   0.488770  8.170454  0.494629  8.073392
63   0.499634  7.991044  0.505981  7.887615
64   0.501953  7.950126  0.502441  7.940808
65   0.490967  8.122521  0.501953  7.946267
66   0.503052  7.927858  0.502197  7.940666
67   0.497681  8.012009  0.505249  7.890747
68   0.501343  7.952532  0.508545  7.837265
69   0.506104  7.927727  0.505371  8.008996
70   0.509277  7.927653  0.493774  8.161910
71   0.502441  8.017414  0.510864  7.877927
72   0.498413  8.072680  0.497437  8.084935
73   0.500610  8.031719  0.497925  8.072146
74   0.501465  8.013731  0.488525  8.218179
75   0.501831  8.004478  0.495239  8.108078
76   0.496216  8.091196  0.495605  8.099669
77   0.510986  7.853323  0.497681  8.064344
78   0.507812  7.901797  0.498901  8.042861
79   0.494019  8.119761  0.496582  8.077962
80   0.497070  8.069286  0.497192  8.066454
81   0.490967  8.164847  0.495728  8.088095
82   0.509399  7.869300  0.495850  8.084483
83   0.490112  8.175133  0.501343  7.995276
84   0.508423  7.881600  0.498291  8.042321
85   0.499878  8.016231  0.497314  8.056305
86   0.500977  7.997141  0.500244  8.008034
87   0.498413  8.036453  0.498535  8.033730
88   0.502808  7.964852  0.491333  8.147016
89   0.502686  7.965270  0.491821  8.137708
90   0.502319  7.969589  0.490234  8.161490
91   0.489502  8.172413  0.503784  7.943959
92   0.498291  8.030777  0.510132  7.841244
93   0.503540  7.945573  0.504883  7.923398
94   0.500977  7.984907  0.508545  7.863474
95   0.510376  7.833509  0.506470  7.895002
96   0.505737  7.905897  0.512573  7.796126
97   0.497314  8.038599  0.493896  8.092293
98   0.500610  7.984466  0.500610  7.983666
99   0.500854  7.978980  0.490601  8.141653
100  0.496338  8.049398  0.494629  8.075852
101  0.501343  7.968038  0.500244  7.984773
102  0.500854  7.974282  0.495483  8.059151
103  0.501465  7.963055  0.495728  8.053788
104  0.506348  7.883772  0.496460  8.040706
105  0.504028  7.919378  0.499390  7.992668
106  0.497314  8.025124  0.499146  7.995315
107  0.505371  7.895481  0.497437  8.021405
108  0.487671  8.176556  0.503052  7.930825
109  0.496826  8.029587  0.494629  8.064140
110  0.500244  7.974177  0.501953  7.946501
111  0.503540  7.920803  0.507324  7.860087
112  0.507446  7.857782  0.500732  7.964469
113  0.502197  7.940794  0.492676  8.092276
114  0.499390  7.984952  0.493286  8.081974
115  0.500366  7.968840  0.504150  7.908257
116  0.492310  8.096793  0.500000  7.973960
117  0.503174  7.923149  0.502563  7.932672
118  0.498169  8.002541  0.497925  8.006247
119  0.501343  7.951585  0.496582  8.027316
120  0.505127  7.890938  0.507324  7.855761
121  0.498535  7.995746  0.498291  7.999510
122  0.498535  7.995502  0.499146  7.985661
123  0.498291  7.999185  0.498657  7.993253
124  0.494263  8.063230  0.505859  7.878274
125  0.497559  8.010541  0.499878  7.973502
126  0.497925  8.004586  0.505981  7.876095
127  0.495972  8.035633  0.495361  8.045325

2018-02-13 11:02:13.044428 Finish.
Total elapsed time: 15:02:11.04.
