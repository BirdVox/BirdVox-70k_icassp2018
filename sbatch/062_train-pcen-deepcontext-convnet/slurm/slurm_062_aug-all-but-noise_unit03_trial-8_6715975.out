2018-06-07 10:35:04.918064: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:04.918260: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:04.918271: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:04.918276: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:04.918281: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:35:02.140843 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.495850  8.037424  0.504028  7.906971
1    0.495972  8.035564  0.501465  7.947839
2    0.501221  7.951839  0.505005  7.891403
3    0.496826  8.021877  0.501831  7.942001
4    0.499756  7.975235  0.495361  8.045144
5    0.511963  7.780626  0.496948  8.019845
6    0.500000  7.971300  0.501221  7.951731
7    0.504395  7.901283  0.495605  8.041252
8    0.491333  8.109408  0.497437  8.012060
9    0.501831  7.954639  0.504883  7.980346
10   0.499390  8.058525  0.515747  7.805235
11   0.497803  8.083953  0.496216  8.120042
12   0.501221  8.029141  0.492432  8.181035
13   0.502686  8.006024  0.507568  7.937060
14   0.499390  8.058375  0.500122  8.057080
15   0.498169  8.078673  0.499634  8.064950
16   0.505615  7.958245  0.494629  8.145620
17   0.507446  7.928882  0.503296  8.005924
18   0.495117  8.126467  0.494995  8.139717
19   0.503662  7.989254  0.496948  8.108236
20   0.500122  8.046463  0.497803  8.094464
21   0.499878  8.050870  0.504028  7.994119
22   0.495728  8.117595  0.505981  7.962638
23   0.500000  8.048044  0.496094  8.122009
24   0.497925  8.081664  0.507324  7.940995
25   0.495117  8.127690  0.503784  7.998054
26   0.500610  8.038013  0.494995  8.139717
27   0.497437  8.090027  0.496338  8.118074
28   0.501953  8.016821  0.495361  8.133814
29   0.506470  7.943915  0.503418  8.003957
30   0.497559  8.087760  0.500366  8.053145
31   0.497070  8.095287  0.505005  7.978379
32   0.509277  7.899391  0.493042  8.171198
33   0.503906  7.985791  0.503296  8.005924
34   0.491821  8.180170  0.490601  8.210548
35   0.501587  8.023024  0.493042  8.171198
36   0.510132  7.885875  0.494019  8.155457
37   0.505615  7.957452  0.496460  8.116106
38   0.502686  8.005788  0.489380  8.230224
39   0.498779  8.068213  0.500488  8.051178
40   0.503296  7.995650  0.495483  8.131847
41   0.505859  7.953817  0.508789  7.917385
42   0.496948  8.096203  0.509766  7.901645
43   0.498291  8.075697  0.500854  8.045275
44   0.500488  8.040496  0.507202  7.942963
45   0.505249  7.963483  0.500366  8.053145
46   0.501099  8.031087  0.505005  7.978379
47   0.498291  8.075912  0.503052  8.009859
48   0.489624  8.215156  0.502686  8.015762
49   0.508301  7.914552  0.493530  8.163327
50   0.504150  7.982134  0.499023  8.074788
51   0.495117  8.127711  0.504883  7.980346
52   0.499268  8.060107  0.496094  8.122009
53   0.501099  8.030765  0.507568  7.937060
54   0.498901  8.066245  0.505615  7.968541
55   0.498413  8.073687  0.497070  8.106269
56   0.503174  7.997446  0.501465  8.035437
57   0.494263  8.141677  0.496582  8.114139
58   0.505615  7.957773  0.506714  7.950833
59   0.498535  8.071333  0.502930  8.011827
60   0.499146  8.062804  0.496948  8.108236
61   0.489136  8.223756  0.500854  8.045275
62   0.496460  8.105489  0.505127  7.976411
63   0.506592  7.942655  0.504150  7.992151
64   0.494507  8.136819  0.496948  8.108236
65   0.509155  7.901594  0.504028  7.994119
66   0.500854  8.034936  0.506348  7.956736
67   0.505615  7.957645  0.506226  7.958703
68   0.508545  7.910273  0.501831  8.029535
69   0.503784  7.987093  0.505493  7.970508
70   0.500366  8.041755  0.490234  8.216451
71   0.497559  8.087416  0.503052  8.009859
72   0.504395  7.978242  0.505737  7.966573
73   0.501953  8.017228  0.498169  8.088561
74   0.502197  8.012843  0.494995  8.139717
75   0.504272  7.980167  0.492798  8.175133
76   0.511353  7.865985  0.501709  8.031502
77   0.492065  8.174948  0.489502  8.228256
78   0.496948  8.096890  0.504639  7.984281
79   0.509521  7.894812  0.499268  8.070853
80   0.488281  8.238065  0.498291  8.086593
81   0.507935  7.920626  0.501465  8.035437
82   0.496216  8.109381  0.498901  8.076756
83   0.505005  7.967890  0.502441  8.019697
84   0.503784  7.987801  0.496094  8.122009
85   0.502441  8.009184  0.506348  7.956736
86   0.492310  8.183003  0.501953  8.027567
87   0.500122  8.057080  0.497070  8.106269
88   0.501343  8.037405  0.510742  7.885904
89   0.497070  8.106269  0.509888  7.899677
90   0.502563  8.017729  0.503662  8.000022
91   0.502563  8.017729  0.502197  8.023632
92   0.495361  8.133814  0.496704  8.112171
93   0.507324  7.940995  0.504639  7.984281
94   0.489868  8.222354  0.489380  8.230224
95   0.495239  8.135782  0.501099  8.041340
96   0.496826  8.110204  0.500000  8.059048
97   0.498657  8.080691  0.492554  8.179068
98   0.500610  8.049210  0.497803  8.094464
99   0.503906  7.996086  0.508057  7.929190
100  0.503418  8.003957  0.502808  8.013794
101  0.497437  8.100366  0.494141  8.153490
102  0.500488  8.051178  0.502197  8.023632
103  0.501831  8.029535  0.502930  8.011827
104  0.499756  8.062983  0.497314  8.102334
105  0.497070  8.106269  0.497559  8.098399
106  0.489502  8.228256  0.499512  8.066918
107  0.507080  7.944930  0.487671  8.257769
108  0.498657  8.080691  0.488525  8.243997
109  0.495972  8.123977  0.502075  8.025600
110  0.496948  8.108236  0.496338  8.118074
111  0.497070  8.106269  0.517090  7.783592
112  0.496826  8.110204  0.502686  8.015762
113  0.494629  8.145620  0.492188  8.184970
114  0.494141  8.153490  0.501709  8.031502
115  0.492432  8.181035  0.498535  8.082658
116  0.499634  8.064950  0.494751  8.143652
117  0.500244  8.055113  0.498047  8.090528
118  0.503052  8.009859  0.496216  8.120042
119  0.495972  8.123977  0.507690  7.935093
120  0.494873  8.141685  0.494995  8.139717
121  0.503540  8.001989  0.501221  8.039372
122  0.495605  8.129879  0.499390  8.068885
123  0.500000  8.059048  0.505737  7.966573
124  0.500000  8.059048  0.496094  8.122009
125  0.498169  8.088561  0.499023  8.074788
126  0.494263  8.151522  0.500122  8.057080
127  0.496582  8.114139  0.506226  7.958703

2018-06-08 17:58:58.310371 Finish.
Total elapsed time: 31:23:56.31.
