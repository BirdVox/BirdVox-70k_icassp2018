2018-06-07 10:35:04.865520: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:04.865767: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:04.865779: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:35:02.044941 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit02, unit03, unit05.
Validation set: unit07, unit10.
Test set: unit01.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.499023  7.986761  0.498901  7.988707
1    0.512817  7.766153  0.503052  7.922540
2    0.503052  7.922540  0.503174  7.920594
3    0.504272  7.903079  0.503662  7.912810
4    0.503174  7.920594  0.492310  8.093796
5    0.494873  8.052928  0.499878  7.973138
6    0.494507  8.058766  0.495117  8.049036
7    0.493774  8.070464  0.496826  8.021791
8    0.498535  7.994545  0.494385  8.060713
9    0.494141  8.064605  0.500122  7.969246
10   0.494873  8.052928  0.500000  7.971192
11   0.503540  7.914756  0.505493  7.883618
12   0.496582  8.025683  0.509644  7.817451
13   0.499390  7.980923  0.503784  7.910864
14   0.496948  8.019845  0.498047  8.002330
15   0.503296  7.918648  0.506592  7.866103
16   0.505005  7.891403  0.492310  8.093796
17   0.495972  8.035413  0.508057  7.842750
18   0.508789  7.831074  0.499146  7.984815
19   0.497559  8.010136  0.492432  8.091850
20   0.505859  7.877780  0.502075  7.938109
21   0.492432  8.091850  0.497314  8.014006
22   0.492188  8.095742  0.500366  7.965354
23   0.514038  7.747392  0.500854  7.957570
24   0.499146  7.984815  0.500488  7.963408
25   0.494995  8.050982  0.514526  7.739607
26   0.505127  7.889478  0.499268  7.982869
27   0.503052  7.922540  0.506348  7.869996
28   0.490967  8.115203  0.494873  8.052928
29   0.504028  7.906971  0.505615  7.881672
30   0.498047  8.002330  0.506348  7.869996
31   0.500977  7.955624  0.499512  7.978977
32   0.497925  8.004297  0.498413  7.996492
33   0.508179  7.840804  0.507202  7.856373
34   0.500244  7.967300  0.499756  7.975085
35   0.497925  8.004276  0.489868  8.132718
36   0.503662  7.912810  0.517944  7.685117
37   0.491089  8.113257  0.509521  7.819397
38   0.503418  7.916702  0.503418  7.916702
39   0.502930  7.924486  0.496826  8.021791
40   0.507812  7.846642  0.497925  8.004276
41   0.505249  7.887510  0.492798  8.086012
42   0.502197  7.936163  0.503418  7.916702
43   0.491211  8.111311  0.500732  7.959516
44   0.503296  7.918648  0.497559  8.010114
45   0.502319  7.934217  0.501709  7.943947
46   0.492920  8.084087  0.502686  7.928378
47   0.504028  7.906971  0.487427  8.171640
48   0.500610  7.961462  0.498535  7.994545
49   0.516113  7.714308  0.495361  8.045144
50   0.502563  7.930324  0.493530  8.074335
51   0.505005  7.891403  0.499878  7.973138
52   0.504517  7.899187  0.506592  7.866103
53   0.505615  7.881672  0.494507  8.058766
54   0.492920  8.084066  0.503296  7.918648
55   0.494751  8.054874  0.497803  8.006222
56   0.504639  7.897241  0.503052  7.922540
57   0.502686  7.928378  0.512085  7.778529
58   0.499878  7.973138  0.503540  7.914756
59   0.498413  7.996492  0.507935  7.844696
60   0.498657  7.992599  0.500854  7.957570
61   0.497803  8.006222  0.502441  7.932271
62   0.498291  7.998438  0.495728  8.039306
63   0.501953  7.940055  0.491699  8.103527
64   0.500732  7.959516  0.507690  7.848589
65   0.496216  8.031521  0.489502  8.138556
66   0.496826  8.021791  0.494751  8.054874
67   0.498047  8.002330  0.491333  8.109365
68   0.491821  8.101580  0.503784  7.910864
69   0.514526  7.739607  0.500122  7.969246
70   0.491455  8.107419  0.505249  7.887510
71   0.500366  7.965354  0.509766  7.815505
72   0.507080  7.858319  0.504395  7.901133
73   0.511230  7.792152  0.508179  7.840804
74   0.494019  8.066551  0.493774  8.070443
75   0.500854  7.957570  0.502075  7.938109
76   0.505737  7.879726  0.495361  8.045144
77   0.495850  8.037359  0.507324  7.854427
78   0.500000  7.971192  0.496338  8.029575
79   0.502441  7.932271  0.508545  7.834966
80   0.498413  7.996492  0.500244  7.967300
81   0.503418  7.916702  0.507568  7.850535
82   0.504761  7.895295  0.496704  8.023737
83   0.495361  8.045144  0.486816  8.181370
84   0.498657  7.992621  0.501709  7.943947
85   0.502319  7.934217  0.494629  8.056820
86   0.496338  8.029575  0.509155  7.825235
87   0.506714  7.864157  0.505737  7.879726
88   0.502075  7.938109  0.498657  7.992599
89   0.495117  8.049036  0.496704  8.023737
90   0.505615  7.881672  0.502808  7.926432
91   0.496094  8.033467  0.506714  7.864157
92   0.497803  8.006222  0.498169  8.000384
93   0.505859  7.877780  0.506592  7.866103
94   0.501465  7.947839  0.497070  8.017899
95   0.499878  7.973138  0.503174  7.920594
96   0.498901  7.988707  0.492798  8.086012
97   0.503296  7.918648  0.502563  7.930324
98   0.496094  8.033467  0.496826  8.021791
99   0.509644  7.817451  0.512939  7.764907
100  0.505005  7.891403  0.498291  7.998438
101  0.506836  7.862211  0.493164  8.080173
102  0.496582  8.025683  0.504272  7.903079
103  0.503418  7.916702  0.505615  7.881672
104  0.496704  8.023737  0.500854  7.957570
105  0.501099  7.953678  0.496216  8.031521
106  0.499023  7.986783  0.495605  8.041252
107  0.504150  7.905025  0.500732  7.959516
108  0.506104  7.873888  0.500122  7.969246
109  0.502075  7.938109  0.496704  8.023737
110  0.498779  7.990653  0.495728  8.039306
111  0.503174  7.920594  0.508179  7.840804
112  0.503906  7.908917  0.506348  7.869996
113  0.503662  7.912810  0.500244  7.967300
114  0.503052  7.922540  0.504028  7.906971
115  0.502197  7.936163  0.508789  7.831074
116  0.497437  8.012060  0.491577  8.105473
117  0.505371  7.885564  0.499878  7.973138
118  0.504639  7.897241  0.495117  8.049036
119  0.497437  8.012060  0.499512  7.978977
120  0.498169  8.000384  0.496582  8.025683
121  0.491699  8.103527  0.501343  7.949785
122  0.499146  7.984815  0.500244  7.967300
123  0.506592  7.866103  0.499268  7.982869
124  0.498779  7.990653  0.501465  7.947839
125  0.496948  8.019845  0.495972  8.035413
126  0.490723  8.119095  0.496948  8.019845
127  0.501709  7.943947  0.495850  8.037359

2018-06-08 16:53:44.397084 Finish.
Total elapsed time: 30:18:42.40.
