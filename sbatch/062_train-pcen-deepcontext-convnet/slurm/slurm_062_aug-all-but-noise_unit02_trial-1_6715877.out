2018-06-07 10:34:58.050866: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:34:58.051072: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:34:58.051084: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:34:52.198368 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.506470  7.869015  0.506592  7.866103
1    0.510498  7.804729  0.493408  8.076281
2    0.515137  7.730971  0.496094  8.033467
3    0.513062  7.764033  0.500732  7.959516
4    0.502197  7.936999  0.500610  7.961462
5    0.511108  7.795085  0.493652  8.072389
6    0.503662  7.913603  0.503296  7.918648
7    0.506714  7.865294  0.501465  7.947839
8    0.507446  7.853382  0.498169  8.000384
9    0.507568  7.851650  0.501709  7.943947
10   0.523193  7.602143  0.499023  7.986761
11   0.512573  7.771646  0.501099  7.953678
12   0.505371  7.886551  0.501587  7.945893
13   0.503174  7.921623  0.500366  7.965354
14   0.517822  7.687921  0.512573  7.770745
15   0.508545  7.835953  0.494751  8.054874
16   0.508057  7.843651  0.500732  7.959516
17   0.513062  7.763797  0.496826  8.021791
18   0.501831  7.942773  0.502075  7.938109
19   0.508667  7.833942  0.494385  8.060713
20   0.501465  7.948697  0.499512  7.978977
21   0.511475  7.788860  0.498901  7.988707
22   0.510986  7.797095  0.495117  8.049036
23   0.511963  7.781548  0.499268  7.982869
24   0.515015  7.732681  0.503418  7.916702
25   0.514893  7.734691  0.504883  7.893349
26   0.505615  7.882680  0.500366  7.965354
27   0.505005  7.892668  0.511719  7.784368
28   0.505981  7.876756  0.500488  7.963408
29   0.514160  7.746432  0.509888  7.813559
30   0.517090  7.699726  0.490112  8.128826
31   0.501343  7.950836  0.503906  7.908917
32   0.509521  7.820641  0.499756  7.975085
33   0.515137  7.730971  0.495605  8.041252
34   0.516724  7.705629  0.502563  7.930324
35   0.504639  7.898292  0.497803  8.006222
36   0.517212  7.697759  0.520020  7.652033
37   0.510742  7.800901  0.488647  8.152179
38   0.499756  7.976050  0.505493  7.883618
39   0.504272  7.904087  0.508057  7.842750
40   0.506104  7.874767  0.501953  7.940055
41   0.504883  7.894164  0.493042  8.082120
42   0.504639  7.898056  0.489258  8.142448
43   0.503784  7.911679  0.494385  8.060713
44   0.516724  7.705414  0.503784  7.910864
45   0.513428  7.758173  0.497437  8.012060
46   0.500977  7.956632  0.492432  8.091850
47   0.508179  7.841748  0.504395  7.901133
48   0.507446  7.853682  0.505859  7.877780
49   0.517212  7.697844  0.497925  8.004276
50   0.505127  7.890486  0.492188  8.095742
51   0.514526  7.740551  0.499390  7.980923
52   0.522827  7.608153  0.498657  7.992599
53   0.505249  7.888583  0.507690  7.848589
54   0.504883  7.894593  0.494995  8.050982
55   0.503906  7.909883  0.492920  8.084066
56   0.501465  7.948697  0.501465  7.947839
57   0.511841  7.783537  0.503418  7.916702
58   0.508667  7.833899  0.505371  7.885564
59   0.509766  7.816599  0.493408  8.076281
60   0.512329  7.775559  0.501465  7.947839
61   0.503906  7.909990  0.499634  7.977031
62   0.503662  7.913861  0.501465  7.947839
63   0.503540  7.915871  0.508057  7.842750
64   0.511475  7.789118  0.506714  7.864157
65   0.504150  7.905669  0.496704  8.023737
66   0.502319  7.935053  0.497437  8.012060
67   0.510498  7.804708  0.505615  7.881672
68   0.508179  7.841769  0.494873  8.052928
69   0.518555  7.676352  0.499268  7.982869
70   0.509644  7.818266  0.507446  7.852481
71   0.513916  7.750217  0.497314  8.014006
72   0.502563  7.931225  0.500366  7.965354
73   0.501709  7.944848  0.511230  7.792152
74   0.507568  7.851822  0.503418  7.916702
75   0.518555  7.676116  0.507324  7.854427
76   0.512939  7.766065  0.498413  7.996492
77   0.505371  7.886680  0.505615  7.881672
78   0.503906  7.909797  0.504639  7.897241
79   0.513306  7.760012  0.504395  7.901133
80   0.507202  7.857167  0.499634  7.977031
81   0.509644  7.818674  0.498047  8.002330
82   0.513306  7.759969  0.502808  7.926432
83   0.505981  7.876756  0.502808  7.926432
84   0.508179  7.841920  0.498291  7.998438
85   0.505615  7.882830  0.496704  8.023737
86   0.510376  7.806826  0.506958  7.860265
87   0.506104  7.874853  0.501099  7.953678
88   0.502441  7.933386  0.504395  7.901133
89   0.494019  8.067495  0.497803  8.006222
90   0.504517  7.900174  0.501587  7.945893
91   0.513428  7.758045  0.490723  8.119095
92   0.505249  7.888540  0.495117  8.049036
93   0.511963  7.781419  0.495972  8.035413
94   0.508179  7.841919  0.506836  7.862211
95   0.506714  7.864929  0.498901  7.988707
96   0.511719  7.785418  0.504150  7.905025
97   0.517212  7.697544  0.500366  7.965354
98   0.503662  7.913903  0.498413  7.996492
99   0.506714  7.864801  0.499878  7.973138
100  0.501221  7.952739  0.489502  8.138556
101  0.508423  7.837770  0.504395  7.901133
102  0.505859  7.878788  0.494873  8.052928
103  0.504272  7.904152  0.501709  7.943947
104  0.500366  7.966276  0.491577  8.105473
105  0.507690  7.849682  0.497314  8.014006
106  0.508911  7.830179  0.489258  8.142448
107  0.511353  7.791042  0.505249  7.887510
108  0.501953  7.941149  0.486450  8.187209
109  0.500000  7.972115  0.502930  7.924486
110  0.505493  7.884841  0.493164  8.080173
111  0.513672  7.754367  0.498535  7.994545
112  0.511353  7.790956  0.501221  7.951731
113  0.511230  7.793053  0.512085  7.778529
114  0.508545  7.835845  0.487427  8.171640
115  0.503662  7.913796  0.493286  8.078227
116  0.510132  7.810610  0.503052  7.922540
117  0.505493  7.884648  0.502686  7.928378
118  0.500244  7.967595  0.489502  8.138556
119  0.503174  7.920594  0.495728  8.039306
120  0.506470  7.868071  0.498535  7.994545
121  0.501099  7.953699  0.500610  7.961462
122  0.495850  8.037359  0.497437  8.012060
123  0.500488  7.963408  0.500854  7.957570
124  0.501831  7.942022  0.498535  7.994545
125  0.515503  7.724039  0.490479  8.122988
126  0.500122  7.969246  0.497070  8.017899
127  0.495728  8.039306  0.504028  7.906971

2018-06-08 17:20:21.434569 Finish.
Total elapsed time: 30:45:29.43.
