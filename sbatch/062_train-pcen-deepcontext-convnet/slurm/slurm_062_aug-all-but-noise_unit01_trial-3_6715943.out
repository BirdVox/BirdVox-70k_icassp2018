2018-06-07 10:35:13.368880: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:13.369119: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:13.369131: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:35:02.650991 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit02, unit03, unit05.
Validation set: unit07, unit10.
Test set: unit01.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.488770  8.240062  0.494141  8.153490
1    0.496948  8.108236  0.503662  8.000022
2    0.505371  7.972476  0.511230  7.878034
3    0.492432  8.181035  0.498291  8.086593
4    0.498291  8.086593  0.504272  7.990184
5    0.488159  8.249899  0.500366  8.053145
6    0.512573  7.856391  0.494751  8.143652
7    0.491089  8.202678  0.494995  8.139717
8    0.503418  8.003957  0.492188  8.184970
9    0.502686  8.015762  0.492310  8.183003
10   0.497314  8.102334  0.504883  7.980346
11   0.494019  8.155457  0.499146  8.072821
12   0.503052  8.009859  0.493774  8.159392
13   0.502319  8.021665  0.499268  8.070853
14   0.502197  8.023632  0.501953  8.027567
15   0.504028  7.994119  0.498779  8.078723
16   0.501099  8.041340  0.501343  8.037405
17   0.506470  7.954768  0.500732  8.047243
18   0.507202  7.942963  0.495728  8.127912
19   0.498169  8.088561  0.509155  7.911482
20   0.498291  8.086593  0.503662  8.000022
21   0.498779  8.078702  0.496704  8.112171
22   0.502197  8.023632  0.495117  8.137749
23   0.507324  7.940995  0.503296  8.005924
24   0.502686  8.015762  0.507324  7.940995
25   0.494141  8.153468  0.495605  8.129879
26   0.491577  8.194808  0.497314  8.102334
27   0.504761  7.982314  0.494263  8.151522
28   0.500000  8.059048  0.500610  8.049210
29   0.502197  8.023632  0.495117  8.137749
30   0.503784  7.998054  0.500488  8.051178
31   0.494995  8.139717  0.493408  8.165295
32   0.495239  8.135782  0.503296  8.005924
33   0.497681  8.096431  0.502563  8.017729
34   0.508667  7.919352  0.499878  8.061015
35   0.500977  8.043307  0.503418  8.003957
36   0.503296  8.005924  0.501953  8.027567
37   0.506348  7.956736  0.502075  8.025600
38   0.489258  8.232191  0.501465  8.035437
39   0.496826  8.110204  0.495972  8.123977
40   0.503052  8.009859  0.495850  8.125944
41   0.490112  8.218419  0.492798  8.175133
42   0.495117  8.137749  0.500488  8.051178
43   0.500977  8.043307  0.489136  8.234159
44   0.505493  7.970508  0.500366  8.053145
45   0.507812  7.933125  0.496826  8.110204
46   0.495605  8.129879  0.501709  8.031502
47   0.496094  8.122009  0.503784  7.998054
48   0.500366  8.053145  0.492554  8.179068
49   0.504028  7.994119  0.509888  7.899677
50   0.501221  8.039372  0.501587  8.033470
51   0.503052  8.009859  0.503296  8.005924
52   0.497681  8.096431  0.495483  8.131847
53   0.500366  8.053145  0.512451  7.858359
54   0.492065  8.186938  0.500854  8.045275
55   0.496948  8.108215  0.497803  8.094464
56   0.493042  8.171133  0.494995  8.139717
57   0.502075  8.025578  0.490845  8.206613
58   0.498291  8.086550  0.492798  8.175133
59   0.497559  8.098399  0.499390  8.068886
60   0.504028  7.994097  0.494995  8.139717
61   0.496216  8.120020  0.490967  8.204646
62   0.494263  8.151501  0.503540  8.001989
63   0.504639  7.984260  0.499023  8.074788
64   0.492065  8.186917  0.498901  8.076756
65   0.492188  8.184949  0.497192  8.104301
66   0.498779  8.078702  0.497803  8.094464
67   0.493408  8.165252  0.503174  8.007892
68   0.494141  8.153383  0.498901  8.076756
69   0.496094  8.121966  0.501099  8.041340
70   0.487915  8.253834  0.502319  8.021664
71   0.499756  8.062983  0.494873  8.141685
72   0.499146  8.072799  0.508667  7.919352
73   0.501587  8.033448  0.501831  8.029535
74   0.504639  7.984238  0.508179  7.927223
75   0.498535  8.082658  0.499023  8.074788
76   0.501953  8.027567  0.504883  7.980346
77   0.506104  7.960585  0.499390  8.068885
78   0.501343  8.037405  0.493652  8.161360
79   0.501343  8.037383  0.500244  8.055113
80   0.506104  7.960649  0.501587  8.033470
81   0.494507  8.113655  0.500366  7.965354
82   0.504272  7.909299  0.504639  7.897241
83   0.499146  7.991056  0.502930  7.924486
84   0.495850  8.044051  0.495972  8.035413
85   0.504761  7.901601  0.497681  8.008168
86   0.509033  7.832565  0.501465  7.947839
87   0.510742  7.806242  0.501343  7.949785
88   0.494629  8.062547  0.492554  8.089904
89   0.509644  7.823821  0.508423  7.836912
90   0.506836  7.868431  0.504639  7.897241
91   0.506104  7.880258  0.501709  7.943947
92   0.507690  7.854680  0.498535  7.994545
93   0.498779  7.996058  0.496582  8.025683
94   0.506226  7.878033  0.493652  8.072389
95   0.496582  8.032396  0.491577  8.105473
96   0.503906  7.914794  0.507324  7.854427
97   0.500000  7.977563  0.504517  7.899187
98   0.497070  8.024612  0.499878  7.973138
99   0.499146  7.991228  0.509033  7.827182
100  0.501709  7.950339  0.501221  7.951731
101  0.506348  7.875937  0.498657  7.992599
102  0.498413  8.002519  0.493408  8.076281
103  0.506592  7.872238  0.504639  7.897241
104  0.494995  8.056945  0.489502  8.138556
105  0.499146  7.990863  0.497925  8.004276
106  0.508545  7.840843  0.500610  7.961462
107  0.508667  7.838339  0.504028  7.906971
108  0.512817  7.773008  0.495972  8.035413
109  0.506592  7.872323  0.494751  8.054874
110  0.501953  7.945953  0.503296  7.918648
111  0.498169  8.005896  0.503174  7.920594
112  0.502197  7.942747  0.506470  7.868049
113  0.513184  7.766805  0.502563  7.930324
114  0.505615  7.887635  0.487549  8.169694
115  0.498657  7.998648  0.499023  7.986761
116  0.506714  7.870141  0.499878  7.973138
117  0.506470  7.874184  0.494263  8.062659
118  0.502808  7.932502  0.495728  8.039306
119  0.505615  7.887742  0.500122  7.969246
120  0.508911  7.835476  0.496338  8.029575
121  0.502686  7.934748  0.493042  8.082120
122  0.508545  7.840414  0.496460  8.027629
123  0.510498  7.809770  0.503540  7.914756
124  0.511230  7.798908  0.504395  7.901133
125  0.506104  7.879958  0.500000  7.971192
126  0.502319  7.940158  0.502319  7.934217
127  0.502441  7.938791  0.507690  7.848589

2018-06-08 16:43:45.146824 Finish.
Total elapsed time: 30:08:43.15.
