2018-06-07 10:35:04.697453: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:04.697635: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:04.697646: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:04.697650: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:04.697655: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:35:02.140954 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit10, unit01, unit02.
Validation set: unit03, unit05.
Test set: unit07.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.932373  0.191693  0.920288  0.307639
1    0.932739  0.198954  0.922119  0.254170
2    0.932251  0.184156  0.920532  0.257969
3    0.935059  0.177612  0.923706  0.245071
4    0.939941  0.173474  0.915894  0.237518
5    0.945557  0.156969  0.935913  0.203603
6    0.946045  0.161220  0.921021  0.303435
7    0.949707  0.146503  0.939575  0.210551
8    0.951416  0.152327  0.941528  0.220285
9    0.947266  0.153663  0.926758  0.236573
10   0.950562  0.143525  0.960938  0.136195
11   0.955933  0.119812  0.939697  0.193357
12   0.952759  0.126452  0.942749  0.181976
13   0.952881  0.154014  0.955811  0.144017
14   0.954346  0.135301  0.961304  0.130384
15   0.957031  0.123034  0.965576  0.119740
16   0.959961  0.136825  0.946411  0.210760
17   0.955811  0.142920  0.963379  0.117731
18   0.961304  0.122889  0.967041  0.110135
19   0.961304  0.114067  0.956299  0.156840
20   0.956543  0.129045  0.929565  0.200454
21   0.958008  0.133239  0.959229  0.132102
22   0.957886  0.132432  0.957397  0.128781
23   0.953247  0.148006  0.945068  0.159413
24   0.955444  0.136661  0.961914  0.143718
25   0.958740  0.121059  0.961792  0.118663
26   0.960571  0.125555  0.953735  0.190280
27   0.957153  0.136682  0.962036  0.147556
28   0.950562  0.154869  0.961670  0.129412
29   0.954712  0.126945  0.970703  0.108055
30   0.953979  0.142725  0.952759  0.155487
31   0.959717  0.130637  0.966675  0.121496
32   0.962402  0.124479  0.962280  0.174082
33   0.946167  0.194151  0.911011  0.281860
34   0.946899  0.159924  0.958862  0.141512
35   0.954346  0.126106  0.955322  0.142827
36   0.956909  0.131201  0.958862  0.144572
37   0.956543  0.131664  0.955933  0.149089
38   0.950439  0.162107  0.945190  0.173692
39   0.957153  0.130176  0.958374  0.176889
40   0.957764  0.123070  0.956787  0.181808
41   0.961670  0.120025  0.960083  0.143837
42   0.958008  0.123093  0.961182  0.134848
43   0.959473  0.131906  0.965820  0.159600
44   0.952759  0.158127  0.957642  0.204386
45   0.956787  0.126714  0.954590  0.142809
46   0.957031  0.127005  0.970947  0.102838
47   0.962036  0.111371  0.965820  0.119290
48   0.945923  0.302991  0.909912  1.005449
49   0.539429  7.390319  0.504028  7.994119
50   0.533447  7.489879  0.535156  7.410718
51   0.556152  7.079551  0.533691  7.434071
52   0.555664  7.087142  0.542480  7.293952
53   0.554688  7.102453  0.543823  7.272545
54   0.558472  7.042253  0.545532  7.245300
55   0.566040  6.921703  0.530884  7.478831
56   0.554443  7.106474  0.535767  7.400988
57   0.556763  7.069327  0.538574  7.356227
58   0.554810  7.101279  0.536133  7.395149
59   0.560913  7.003803  0.538696  7.353825
60   0.555542  7.089109  0.537720  7.369850
61   0.548462  7.201489  0.527832  7.527483
62   0.557739  7.054209  0.535156  7.410719
63   0.558350  7.044478  0.538330  7.360120
64   0.550659  7.167017  0.530518  7.484669
65   0.552856  7.131902  0.530396  7.486616
66   0.574219  6.791615  0.636719  5.791570
67   0.593994  6.479008  0.631348  5.877219
68   0.604492  6.311365  0.619507  6.066033
69   0.593628  6.485189  0.618530  6.081559
70   0.594727  6.467267  0.627441  5.939494
71   0.596069  6.445924  0.626221  5.958934
72   0.598389  6.409206  0.618896  6.075699
73   0.591309  6.522058  0.619507  6.065969
74   0.605347  6.297849  0.629272  5.910281
75   0.584351  6.633564  0.623535  6.001769
76   0.590942  6.528368  0.621826  6.029014
77   0.592529  6.502704  0.629395  5.908335
78   0.604004  6.319471  0.624390  5.988125
79   0.579956  6.735284  0.498901  8.076756
80   0.572510  6.879925  0.507324  7.940995
81   0.532349  7.527638  0.501587  8.033427
82   0.544434  7.326497  0.514160  7.830813
83   0.535400  7.471280  0.505615  7.968541
84   0.544800  7.320315  0.505005  7.978336
85   0.543823  7.336270  0.497681  8.096431
86   0.545166  7.314177  0.499878  8.060994
87   0.553833  7.174996  0.493774  8.159349
88   0.554077  7.171126  0.502563  8.017729
89   0.538208  7.426606  0.495361  8.133814
90   0.543579  7.340205  0.500366  8.053102
91   0.534790  7.482212  0.501831  8.029470
92   0.555908  7.132294  0.741333  4.151223
93   0.589478  6.580870  0.742432  4.133601
94   0.587769  6.609509  0.744141  4.106635
95   0.578735  6.753327  0.741455  4.149256
96   0.584229  6.665839  0.747070  4.059736
97   0.583984  6.670203  0.740845  4.160380
98   0.586060  6.635983  0.749634  4.018374
99   0.586060  6.636755  0.751099  3.995129
100  0.590820  6.558498  0.746094  4.074446
101  0.583740  6.673258  0.752319  3.975475
102  0.580322  6.728564  0.749023  4.029134
103  0.581665  6.707243  0.742432  4.133666
104  0.587769  6.608995  0.748901  4.030008
105  0.588257  6.600374  0.735718  4.242159
106  0.581543  6.708588  0.749146  4.026416
107  0.585693  6.643279  0.740479  4.165425
108  0.587891  6.607327  0.755615  3.922566
109  0.587402  6.614833  0.740356  4.167672
110  0.572754  6.850380  0.748169  4.041920
111  0.587280  6.617229  0.744507  4.100775
112  0.591553  6.548623  0.738281  4.200691
113  0.584595  6.660215  0.746582  4.066984
114  0.594971  6.493489  0.741699  4.146651
115  0.579346  6.743661  0.742676  4.129688
116  0.584717  6.657990  0.745239  4.089485
117  0.592896  6.526873  0.742432  4.133451
118  0.584717  6.657625  0.749023  4.028148
119  0.585327  6.648882  0.732910  4.286705
120  0.592041  6.540924  0.741211  4.152805
121  0.587524  6.613509  0.743774  4.112409
122  0.579834  6.719476  0.505981  7.875834
123  0.533325  7.443341  0.507568  7.850535
124  0.552368  7.139408  0.499634  7.977031
125  0.536377  7.394689  0.510742  7.799936
126  0.550171  7.174587  0.500488  7.963408
127  0.544800  7.260065  0.501465  7.947839

2018-06-08 17:56:09.031212 Finish.
Total elapsed time: 31:21:07.03.
