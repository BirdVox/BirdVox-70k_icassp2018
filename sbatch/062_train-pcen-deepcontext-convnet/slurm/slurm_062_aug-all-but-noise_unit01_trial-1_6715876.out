2018-06-07 10:34:59.125219: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:34:59.125418: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:34:59.125430: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:34:52.099192 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit02, unit03, unit05.
Validation set: unit07, unit10.
Test set: unit01.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.502808  7.926432  0.498657  7.992599
1    0.499023  7.986761  0.504395  7.901133
2    0.495972  8.035413  0.498291  7.998438
3    0.494263  8.062659  0.500488  7.963408
4    0.503784  7.910864  0.499878  7.973138
5    0.506714  7.864157  0.503540  7.914756
6    0.508667  7.833020  0.495850  8.037359
7    0.503174  7.920594  0.504028  7.906971
8    0.496704  8.023758  0.504883  7.893349
9    0.505005  7.891403  0.492676  8.087958
10   0.501221  7.951731  0.494873  8.052928
11   0.507690  7.848589  0.498413  7.996492
12   0.495605  8.041252  0.501709  7.943947
13   0.493774  8.070443  0.499390  7.980923
14   0.500488  7.963408  0.506348  7.869996
15   0.501953  7.940055  0.501099  7.953678
16   0.504395  7.901133  0.500977  7.955624
17   0.490356  8.124934  0.516968  7.700686
18   0.499146  7.984815  0.506592  7.866103
19   0.502319  7.934217  0.500488  7.963408
20   0.503418  7.916702  0.496338  8.029575
21   0.502197  7.936163  0.504639  7.897241
22   0.503540  7.914756  0.501465  7.947839
23   0.497559  8.010114  0.502686  7.928378
24   0.500366  7.965354  0.504639  7.897241
25   0.501221  7.951731  0.505127  7.889457
26   0.495483  8.043198  0.500854  7.957570
27   0.500122  7.969246  0.508789  7.831074
28   0.502319  7.934217  0.505859  7.877780
29   0.496094  8.033467  0.501343  7.949785
30   0.494995  8.050982  0.500488  7.963408
31   0.496216  8.031521  0.493408  8.076281
32   0.498291  7.998438  0.505005  7.891403
33   0.497192  8.015952  0.500000  7.971192
34   0.502075  7.938109  0.502319  7.934217
35   0.500244  7.967300  0.507568  7.850535
36   0.505981  7.875834  0.510620  7.801882
37   0.505005  7.891403  0.495728  8.039306
38   0.504517  7.899187  0.500610  7.961462
39   0.496948  8.019845  0.491577  8.105473
40   0.504639  7.897241  0.505981  7.875834
41   0.503540  7.914756  0.504761  7.895295
42   0.498169  8.000384  0.501831  7.942001
43   0.499756  7.975085  0.498901  7.988707
44   0.515015  7.731823  0.504761  7.895295
45   0.501343  7.949785  0.506470  7.868049
46   0.502197  7.936163  0.490723  8.119095
47   0.501465  7.947839  0.499512  7.978977
48   0.497803  8.006243  0.492188  8.095742
49   0.494141  8.064605  0.496338  8.029575
50   0.493042  8.082120  0.504639  7.897241
51   0.497314  8.014006  0.500244  7.967300
52   0.503418  7.916702  0.501343  7.949785
53   0.503052  7.922540  0.507324  7.854427
54   0.498291  7.998438  0.497559  8.010114
55   0.497559  8.010114  0.503296  7.918648
56   0.488647  8.152179  0.498047  8.002330
57   0.506958  7.860265  0.505371  7.885564
58   0.504028  7.906993  0.490723  8.119095
59   0.500977  7.955624  0.495850  8.037359
60   0.500366  7.965354  0.503052  7.922540
61   0.490723  8.119095  0.498291  7.998438
62   0.488770  8.150233  0.498413  7.996492
63   0.496704  8.023737  0.509644  7.817451
64   0.506348  7.869996  0.509155  7.825235
65   0.492798  8.086012  0.503784  7.910864
66   0.500732  7.959516  0.495972  8.035413
67   0.498413  7.996492  0.498169  8.000384
68   0.492432  8.091850  0.505981  7.875834
69   0.496582  8.025683  0.498413  7.996492
70   0.498047  8.002330  0.497559  8.010114
71   0.495117  8.049036  0.505005  7.891403
72   0.498657  7.992599  0.502441  7.932271
73   0.497681  8.008168  0.494751  8.054874
74   0.507202  7.910189  0.497437  8.100366
75   0.501953  8.026816  0.497070  8.106269
76   0.495972  8.123183  0.500732  8.047243
77   0.500000  8.058040  0.512451  7.858359
78   0.498169  8.087874  0.495605  8.129879
79   0.497559  8.097562  0.505737  7.966573
80   0.494751  8.142944  0.495361  8.133814
81   0.503662  7.999314  0.495850  8.125944
82   0.504639  7.983595  0.498657  8.080691
83   0.496582  8.113174  0.498901  8.076756
84   0.490112  8.217625  0.503906  7.996086
85   0.499512  8.065910  0.488403  8.245964
86   0.493164  8.168093  0.496338  8.118074
87   0.492310  8.182016  0.497314  8.102334
88   0.503296  8.005173  0.503418  8.003957
89   0.506714  7.950104  0.492310  8.183003
90   0.503418  8.002970  0.494873  8.141684
91   0.507446  7.938213  0.520142  7.734404
92   0.498413  8.083939  0.504272  7.990184
93   0.497314  8.101519  0.508057  7.929190
94   0.489624  8.225517  0.509277  7.909515
95   0.510620  7.887100  0.500854  8.045275
96   0.491821  8.189929  0.503174  8.007892
97   0.503784  7.997282  0.494995  8.139717
98   0.497559  8.097626  0.493896  8.157425
99   0.491821  8.190037  0.517578  7.775722
100  0.496704  8.111335  0.511841  7.868196
101  0.498169  8.087703  0.503540  8.001989
102  0.495361  8.133042  0.508057  7.929190
103  0.511230  7.877369  0.502441  8.019697
104  0.514038  7.831987  0.500977  8.043307
105  0.493286  8.166276  0.500610  8.049210
106  0.496460  8.115034  0.490479  8.212516
107  0.504272  7.989240  0.510132  7.895742
108  0.505981  7.961845  0.501343  8.037405
109  0.498779  8.077887  0.499634  8.064950
110  0.504761  7.981520  0.494141  8.153490
111  0.500854  8.044288  0.500244  8.055113
112  0.495483  8.130903  0.495117  8.137749
113  0.497314  8.101540  0.501953  8.027567
114  0.505371  7.971597  0.492432  8.181035
115  0.514893  7.818000  0.499268  8.070853
116  0.507202  7.942191  0.503174  8.007892
117  0.501953  8.026709  0.506104  7.960671
118  0.503052  8.009237  0.492798  8.175133
119  0.501709  8.030859  0.504761  7.982314
120  0.498779  8.077801  0.495483  8.131847
121  0.497925  8.091895  0.509155  7.911482
122  0.515503  7.808355  0.503052  8.009859
123  0.496704  8.111399  0.488159  8.249899
124  0.498169  8.087724  0.496826  8.110204
125  0.503662  7.999228  0.498779  8.078723
126  0.496216  8.119184  0.503174  8.007892
127  0.494019  8.154514  0.500977  8.043307

2018-06-08 18:18:41.370578 Finish.
Total elapsed time: 31:43:49.37.
