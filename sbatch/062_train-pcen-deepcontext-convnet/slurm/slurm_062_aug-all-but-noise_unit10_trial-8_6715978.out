2018-06-07 10:35:03.707041: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:03.707233: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:03.707243: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:03.707247: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:03.707251: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:35:01.752360 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit01, unit02, unit03.
Validation set: unit05, unit07.
Test set: unit10.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.508057  7.842750  0.500488  7.963408
1    0.499268  7.982869  0.507202  7.856373
2    0.503418  7.916702  0.508545  7.834966
3    0.501465  7.947839  0.502075  7.938109
4    0.487061  8.177478  0.501831  7.942001
5    0.495483  8.043198  0.503418  7.916702
6    0.500732  7.959516  0.511963  7.780475
7    0.495117  8.049036  0.501221  7.951731
8    0.486450  8.187209  0.500488  7.963408
9    0.493774  8.070443  0.494751  8.054874
10   0.493774  8.070443  0.496338  8.029575
11   0.513062  7.762961  0.495605  8.041252
12   0.504272  7.903079  0.503540  7.914756
13   0.506348  7.869996  0.503052  7.922540
14   0.500977  7.955624  0.510254  7.807721
15   0.492798  8.086012  0.494995  8.050982
16   0.506348  7.869996  0.511475  7.788260
17   0.504639  7.897241  0.500122  7.969246
18   0.502075  7.938109  0.500244  7.967300
19   0.501465  7.947839  0.502197  7.936163
20   0.517090  7.698740  0.492310  8.093796
21   0.508179  7.840804  0.502808  7.926432
22   0.490967  8.115203  0.505005  7.891403
23   0.505493  7.883618  0.500000  7.971192
24   0.503662  7.912810  0.494995  8.050982
25   0.490479  8.122988  0.488281  8.158017
26   0.508423  7.836912  0.497681  8.008168
27   0.498169  8.000384  0.503052  7.922540
28   0.494873  8.052928  0.491089  8.113257
29   0.496094  8.033467  0.495972  8.035413
30   0.497681  8.008168  0.492676  8.087958
31   0.495483  8.043198  0.494263  8.062659
32   0.501343  7.949785  0.506348  7.869996
33   0.495850  8.037359  0.500854  7.957570
34   0.501831  7.942001  0.499146  7.984815
35   0.502197  7.936163  0.502075  7.938109
36   0.507446  7.852481  0.503418  7.916702
37   0.490112  8.128826  0.492920  8.084066
38   0.496948  8.019845  0.501343  7.949785
39   0.499512  7.978977  0.502197  7.936163
40   0.499146  7.984815  0.497314  8.014006
41   0.498169  8.000384  0.510742  7.799936
42   0.493652  8.072389  0.494263  8.062659
43   0.496582  8.025683  0.510132  7.809667
44   0.506592  7.866103  0.500732  7.959516
45   0.503418  7.916702  0.511353  7.790206
46   0.497559  8.010114  0.503418  7.916702
47   0.500610  7.961462  0.497559  8.010114
48   0.497192  8.015952  0.493164  8.080173
49   0.489258  8.142448  0.498291  7.998438
50   0.498169  8.000384  0.503052  7.922540
51   0.500488  7.963408  0.494019  8.066551
52   0.502075  7.938109  0.503174  7.920594
53   0.506348  7.869996  0.504639  7.897241
54   0.501709  7.943947  0.505371  7.885564
55   0.495483  8.043198  0.496826  8.021791
56   0.501343  7.949785  0.490723  8.119095
57   0.510986  7.796044  0.499023  7.986761
58   0.500610  7.961462  0.494873  8.052928
59   0.500244  7.967300  0.500732  7.959516
60   0.499634  7.977031  0.496094  8.033467
61   0.502075  7.938109  0.493164  8.080173
62   0.496704  8.023737  0.490479  8.122988
63   0.497681  8.008168  0.496460  8.027629
64   0.505615  7.881672  0.509277  7.823289
65   0.508423  7.836912  0.497437  8.012060
66   0.498413  7.996492  0.514526  7.739607
67   0.495361  8.045144  0.499512  7.978977
68   0.499634  7.977031  0.499878  7.973138
69   0.503662  7.912810  0.502808  7.926432
70   0.500732  7.959516  0.496826  8.021791
71   0.496338  8.029575  0.512451  7.772691
72   0.496216  8.031521  0.510864  7.797990
73   0.496704  8.023737  0.495850  8.037359
74   0.499268  7.982869  0.502930  7.924486
75   0.496460  8.027629  0.492798  8.086012
76   0.496582  8.025683  0.492554  8.089904
77   0.496948  8.019845  0.495117  8.049036
78   0.490601  8.121041  0.494995  8.050982
79   0.487549  8.169694  0.501465  7.947839
80   0.500488  7.963408  0.499878  7.973138
81   0.494385  8.060713  0.497314  8.014006
82   0.491211  8.111311  0.501221  7.951731
83   0.494385  8.060713  0.498047  8.002330
84   0.493042  8.082120  0.500244  7.967300
85   0.501465  7.947839  0.496216  8.031521
86   0.503052  7.922540  0.488892  8.148287
87   0.500610  7.961462  0.494629  8.056820
88   0.499512  7.978977  0.491333  8.109365
89   0.505859  7.877780  0.497192  8.015952
90   0.492432  8.091850  0.497925  8.004276
91   0.505127  7.889457  0.491455  8.107419
92   0.501221  7.951731  0.488037  8.161909
93   0.506470  7.868050  0.501709  7.943947
94   0.504028  7.906971  0.499756  7.975085
95   0.495361  8.045144  0.506714  7.864157
96   0.504761  7.895295  0.502441  7.932271
97   0.510132  7.809667  0.498169  8.000384
98   0.498291  7.998438  0.505249  7.887510
99   0.503052  7.922540  0.500244  7.967300
100  0.495361  8.045144  0.506958  7.860265
101  0.497559  8.010114  0.503052  7.922540
102  0.502197  7.936163  0.508911  7.829128
103  0.489868  8.132718  0.500732  7.959516
104  0.494385  8.060713  0.507202  7.856373
105  0.492065  8.097688  0.506226  7.871942
106  0.506470  7.868049  0.503906  7.908917
107  0.492310  8.093796  0.503174  7.920594
108  0.502075  7.938109  0.501465  7.947839
109  0.491699  8.103527  0.493286  8.078227
110  0.503418  7.916702  0.492310  8.093796
111  0.509766  7.815505  0.493286  8.078227
112  0.500854  7.957570  0.501953  7.940055
113  0.495239  8.047090  0.498291  7.998438
114  0.503784  7.910864  0.507812  7.846642
115  0.503540  7.914756  0.503296  7.918648
116  0.504028  7.906971  0.491821  8.101580
117  0.499268  7.982869  0.495361  8.045144
118  0.495605  8.041252  0.513916  7.749338
119  0.504028  7.906971  0.497803  8.006222
120  0.494873  8.052928  0.502686  7.928378
121  0.501343  7.949785  0.493652  8.072389
122  0.492065  8.097688  0.499268  7.982869
123  0.486328  8.189155  0.507446  7.852481
124  0.493042  8.082120  0.503052  7.922540
125  0.498169  8.000384  0.517456  7.692901
126  0.505249  7.887510  0.495361  8.045144
127  0.497803  8.006222  0.504517  7.899187

2018-06-08 18:04:10.968886 Finish.
Total elapsed time: 31:29:09.97.
