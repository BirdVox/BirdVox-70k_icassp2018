2018-06-07 10:35:05.087499: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.087717: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.087730: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:35:02.106977 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.485474  8.202777  0.505249  7.887510
1    0.500488  7.963408  0.515991  7.716254
2    0.502686  7.928400  0.498657  7.992599
3    0.493530  8.074357  0.504639  7.897241
4    0.494141  8.064605  0.499878  7.973138
5    0.492310  8.093796  0.494995  8.050982
6    0.499634  7.977031  0.489868  8.132718
7    0.506226  7.871942  0.496094  8.033467
8    0.506592  7.866103  0.507812  7.846642
9    0.502930  7.924486  0.504272  7.903079
10   0.507690  7.848589  0.490356  8.124934
11   0.494629  8.056820  0.507446  7.852481
12   0.506836  7.868391  0.498657  8.080691
13   0.502075  8.001469  0.495239  8.135782
14   0.502563  7.991561  0.506348  7.956736
15   0.503662  7.975312  0.497559  8.098399
16   0.504639  7.958199  0.499390  8.068885
17   0.501343  8.012245  0.504517  7.986249
18   0.497559  8.013962  0.495728  8.039306
19   0.505615  7.882873  0.503906  7.908917
20   0.501221  7.952654  0.501953  7.940055
21   0.485596  8.201946  0.490845  8.117149
22   0.507324  7.855413  0.499634  7.977031
23   0.502808  7.927548  0.496704  8.023737
24   0.491577  8.106352  0.501953  7.940055
25   0.496704  8.024852  0.504395  7.901133
26   0.508911  7.830050  0.498047  8.002330
27   0.499268  7.983856  0.491821  8.101580
28   0.502197  7.937171  0.496094  8.033467
29   0.502441  7.933429  0.493530  8.074335
30   0.498901  7.989458  0.490112  8.128826
31   0.500122  7.970469  0.504761  7.895295
32   0.507446  7.853510  0.503906  7.908917
33   0.499023  7.987619  0.501465  7.947839
34   0.495728  8.040421  0.494629  8.056820
35   0.489380  8.141618  0.500000  7.971192
36   0.501953  7.941063  0.495483  8.043198
37   0.505371  7.886572  0.502563  7.930324
38   0.491089  8.114415  0.500000  7.971192
39   0.502197  7.937149  0.498291  7.998438
40   0.498535  7.995725  0.499878  7.973138
41   0.510742  7.801094  0.495972  8.035413
42   0.492798  8.086934  0.503052  7.922540
43   0.498047  8.003123  0.503662  7.912810
44   0.484863  8.213494  0.500610  7.961462
45   0.493042  8.083171  0.497192  8.015952
46   0.506836  7.863069  0.500244  7.967300
47   0.502808  7.927612  0.506714  7.864157
48   0.494629  8.057743  0.507568  7.850535
49   0.499268  7.983920  0.498779  7.990653
50   0.505371  7.886358  0.497925  8.004276
51   0.502197  7.937321  0.495850  8.037359
52   0.493530  8.075343  0.496216  8.031521
53   0.502808  7.927569  0.494873  8.052928
54   0.499634  7.978060  0.496826  8.021791
55   0.513184  7.762130  0.505371  7.885564
56   0.497559  8.011144  0.496094  8.033467
57   0.504761  7.896324  0.509277  7.823289
58   0.505981  7.876735  0.505371  7.885564
59   0.497681  8.009090  0.497925  8.004276
60   0.494751  8.055925  0.503418  7.916702
61   0.505493  7.884841  0.504150  7.905025
62   0.500366  7.966148  0.487915  8.163855
63   0.506836  7.863284  0.504639  7.897241
64   0.500977  7.956825  0.516968  7.700686
65   0.497803  8.007273  0.500000  7.971192
66   0.500977  7.956696  0.492554  8.089904
67   0.515625  7.722886  0.502441  7.932271
68   0.504517  7.900388  0.507202  7.856373
69   0.496704  8.024809  0.494507  8.058766
70   0.500732  7.960631  0.494507  8.058766
71   0.500122  7.970383  0.504761  7.895295
72   0.492676  8.089159  0.496948  8.019845
73   0.492920  8.085267  0.492432  8.091850
74   0.488647  8.153294  0.501099  7.953678
75   0.497803  8.007251  0.495239  8.047090
76   0.498779  7.991533  0.497803  8.006222
77   0.493896  8.069698  0.508057  7.842750
78   0.499756  7.976243  0.498169  8.000384
79   0.513428  7.758302  0.501343  7.949785
80   0.496582  8.026755  0.492310  8.093796
81   0.506958  7.861187  0.489014  8.146341
82   0.496216  8.032529  0.490723  8.119095
83   0.497192  8.017347  0.505371  7.885564
84   0.487427  8.172691  0.503418  7.916702
85   0.503784  7.911957  0.499756  7.975085
86   0.491089  8.114094  0.499634  7.977031
87   0.500366  7.966169  0.500244  7.967300
88   0.494995  8.052162  0.497437  8.012060
89   0.499634  7.978017  0.506958  7.860265
90   0.499023  7.987941  0.503418  7.916702
91   0.506958  7.861316  0.509888  7.813559
92   0.509644  7.818416  0.493774  8.070443
93   0.498535  7.995747  0.509155  7.825235
94   0.501099  7.955007  0.507080  7.858319
95   0.492188  8.096750  0.497803  8.006222
96   0.500488  7.964416  0.496826  8.021791
97   0.510498  7.805008  0.486816  8.181370
98   0.503662  7.913946  0.499756  7.975085
99   0.502441  7.933300  0.500610  7.961462
100  0.489990  8.131608  0.504883  7.893349
101  0.503662  7.913796  0.496704  8.023737
102  0.494995  8.052183  0.499146  7.984815
103  0.508423  7.837620  0.494629  8.056820
104  0.502808  7.927676  0.489868  8.132718
105  0.495117  8.050387  0.495117  8.049036
106  0.506958  7.861338  0.499512  7.978977
107  0.502441  7.933193  0.496582  8.025683
108  0.503906  7.909775  0.494995  8.050982
109  0.501831  7.943138  0.499146  7.984815
110  0.506714  7.865015  0.491211  8.111311
111  0.498291  7.999531  0.493652  8.072389
112  0.497437  8.013133  0.496460  8.027629
113  0.509155  7.826244  0.505615  7.881672
114  0.498291  7.999639  0.503296  7.918648
115  0.496094  8.034733  0.495361  8.045144
116  0.493774  8.071623  0.501465  7.947839
117  0.498047  8.003488  0.492554  8.089904
118  0.510254  7.808621  0.509521  7.819397
119  0.496094  8.034432  0.487671  8.167748
120  0.513916  7.750367  0.495605  8.041252
121  0.494995  8.051926  0.507568  7.850535
122  0.496826  8.022970  0.497437  8.012060
123  0.495239  8.047948  0.501099  7.953678
124  0.495728  8.040314  0.491089  8.113257
125  0.501709  7.945084  0.498291  7.998438
126  0.505005  7.892647  0.504639  7.897241
127  0.503418  7.917924  0.494873  8.052928

2018-06-08 16:52:22.392121 Finish.
Total elapsed time: 30:17:20.39.
