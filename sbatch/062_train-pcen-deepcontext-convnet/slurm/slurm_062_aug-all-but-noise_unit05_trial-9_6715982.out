2018-06-07 10:35:06.080489: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:06.080675: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:06.080687: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:06.080692: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:06.080697: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:35:03.381449 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit07, unit10, unit01.
Validation set: unit02, unit03.
Test set: unit05.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.495728  8.127804  0.505859  7.964606
1    0.498413  8.084466  0.499023  8.074788
2    0.496216  8.120042  0.499878  8.061015
3    0.506714  7.950833  0.510132  7.895742
4    0.500610  8.049210  0.486206  8.281380
5    0.507812  7.933125  0.506104  7.960671
6    0.489380  8.230224  0.506104  7.960671
7    0.498413  8.084626  0.500610  8.049210
8    0.493530  8.163327  0.509155  7.911482
9    0.505127  7.976411  0.501099  8.041340
10   0.500732  8.047243  0.504395  7.988216
11   0.498047  8.090507  0.490967  8.204646
12   0.501465  8.035437  0.490479  8.212516
13   0.508057  7.929190  0.502930  8.011827
14   0.508301  7.925255  0.490479  8.212516
15   0.504028  7.994119  0.503296  8.005924
16   0.493896  8.157425  0.498291  8.086593
17   0.500000  8.059026  0.502197  8.023632
18   0.500122  8.057059  0.510864  7.883937
19   0.501953  8.027567  0.497437  8.100366
20   0.501831  8.029535  0.497070  8.106269
21   0.509155  7.911482  0.488037  8.251867
22   0.504028  7.994119  0.500488  8.051178
23   0.504028  7.994119  0.509155  7.911482
24   0.498657  8.080691  0.497437  8.100366
25   0.503540  8.001989  0.496704  8.112171
26   0.500854  8.045275  0.495728  8.127912
27   0.502930  8.011827  0.503784  7.998054
28   0.504028  7.994119  0.495117  8.137749
29   0.505615  7.968541  0.502808  8.013794
30   0.498779  8.078723  0.508911  7.915417
31   0.502441  8.019697  0.506714  7.950833
32   0.499023  8.074788  0.499512  8.066918
33   0.488770  8.240062  0.503296  8.005924
34   0.507568  7.937039  0.499390  8.068885
35   0.492798  8.175133  0.496338  8.118074
36   0.504150  7.992151  0.505493  7.970508
37   0.499634  8.064950  0.495605  8.129879
38   0.499268  8.070853  0.501465  8.035437
39   0.496582  8.114139  0.498047  8.090528
40   0.510132  7.895742  0.489380  8.230224
41   0.490112  8.218419  0.506958  7.946898
42   0.501343  8.037405  0.508911  7.915417
43   0.497437  8.100366  0.506592  7.952801
44   0.496704  8.112171  0.504639  7.984281
45   0.497070  8.106247  0.507812  7.933125
46   0.495361  8.133793  0.484863  8.303023
47   0.508545  7.921320  0.505127  7.976411
48   0.489014  8.236126  0.500000  8.059048
49   0.510010  7.897709  0.511841  7.868196
50   0.493042  8.171198  0.501343  8.037405
51   0.501465  8.035437  0.504883  7.980346
52   0.496094  8.122009  0.496338  8.118074
53   0.509033  7.913450  0.499146  8.072821
54   0.496338  8.118053  0.510254  7.893774
55   0.500610  8.049210  0.505005  7.978379
56   0.491333  8.198743  0.506470  7.954768
57   0.504028  7.994097  0.484863  8.303023
58   0.505249  7.974444  0.496216  8.120042
59   0.497314  8.102334  0.494873  8.141685
60   0.495361  8.133814  0.491333  8.198743
61   0.496948  8.108215  0.490234  8.216451
62   0.513428  7.842618  0.498413  8.084626
63   0.500732  8.047243  0.504883  7.980346
64   0.501831  8.029535  0.492065  8.186938
65   0.488770  8.240062  0.505005  7.978379
66   0.507324  7.940995  0.492065  8.186938
67   0.505615  7.968519  0.504761  7.982314
68   0.500977  8.043307  0.494507  8.147587
69   0.500977  8.043307  0.509155  7.911482
70   0.497559  8.098399  0.499634  8.064950
71   0.496216  8.120042  0.499023  8.074788
72   0.501099  8.041340  0.505493  7.970508
73   0.500854  8.045275  0.510742  7.885904
74   0.510010  7.897709  0.509033  7.913450
75   0.498047  8.090528  0.499268  8.070853
76   0.498047  8.090528  0.502686  8.015762
77   0.498779  8.078723  0.509033  7.913450
78   0.505493  7.970508  0.500000  8.059048
79   0.500000  8.059026  0.503784  7.998054
80   0.499268  8.070853  0.500488  8.051178
81   0.503174  8.007892  0.492920  8.173165
82   0.504395  7.988195  0.509033  7.913450
83   0.502930  8.011827  0.506714  7.950833
84   0.500366  8.053145  0.498413  8.084626
85   0.497559  8.098399  0.504639  7.984281
86   0.508179  7.927223  0.496460  8.116106
87   0.498901  8.076756  0.494263  8.151522
88   0.503174  8.007892  0.496704  8.112171
89   0.497314  8.102334  0.502441  8.019697
90   0.504395  7.988216  0.515503  7.809170
91   0.497681  8.096431  0.506226  7.958703
92   0.495117  8.137749  0.500244  8.055113
93   0.498657  8.080691  0.498413  8.084626
94   0.502808  8.013794  0.499512  8.066918
95   0.500000  8.059048  0.495850  8.125944
96   0.503052  8.009859  0.499512  8.066918
97   0.502808  8.013794  0.502319  8.021665
98   0.495728  8.127912  0.502441  8.019697
99   0.497070  8.106269  0.500122  8.057080
100  0.509155  7.911482  0.503784  7.998054
101  0.497070  8.106269  0.513794  7.836716
102  0.502075  8.025600  0.494141  8.153490
103  0.499634  8.064950  0.501465  8.035437
104  0.504517  7.986249  0.500366  8.053145
105  0.491089  8.202678  0.496948  8.108236
106  0.506958  7.946898  0.494385  8.149555
107  0.492065  8.186938  0.500244  8.055113
108  0.494507  8.147566  0.500122  8.057080
109  0.505005  7.978379  0.496460  8.116106
110  0.511108  7.880002  0.498779  8.078723
111  0.500244  8.055113  0.497925  8.092496
112  0.502686  8.015762  0.507324  7.940995
113  0.499512  8.066918  0.499878  8.061015
114  0.505737  7.966552  0.492676  8.177100
115  0.498169  8.088561  0.500000  8.059048
116  0.505493  7.970508  0.497070  8.106269
117  0.505371  7.972476  0.506104  7.960671
118  0.504272  7.990184  0.501099  8.041340
119  0.496460  8.116085  0.494507  8.147587
120  0.503174  8.007892  0.499756  8.062983
121  0.498169  8.088561  0.508179  7.927223
122  0.506592  7.952801  0.498169  8.088561
123  0.499512  8.066918  0.493286  8.167263
124  0.501831  8.029535  0.500610  8.049210
125  0.501221  8.039372  0.499390  8.068885
126  0.505005  7.978379  0.498291  8.086593
127  0.493530  8.163327  0.501831  8.029535

2018-06-08 18:19:27.220519 Finish.
Total elapsed time: 31:44:24.22.
