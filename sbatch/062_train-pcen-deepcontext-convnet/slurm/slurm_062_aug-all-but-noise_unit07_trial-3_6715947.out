2018-06-07 10:35:05.505443: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.505654: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.505666: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:35:02.529793 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit10, unit01, unit02.
Validation set: unit03, unit05.
Test set: unit07.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.923096  0.219181  0.974121  0.120718
1    0.927490  0.205776  0.961060  0.108533
2    0.924438  0.204123  0.929565  0.185813
3    0.936035  0.181517  0.965820  0.096127
4    0.921631  0.217161  0.967407  0.110772
5    0.933594  0.186082  0.972290  0.084812
6    0.941040  0.166395  0.968628  0.107101
7    0.937622  0.174165  0.967896  0.122955
8    0.938721  0.174914  0.939331  0.184399
9    0.937378  0.172062  0.979004  0.075550
10   0.942627  0.171306  0.980225  0.060410
11   0.943481  0.164163  0.976196  0.082406
12   0.936523  0.183112  0.984375  0.055727
13   0.942261  0.174446  0.967163  0.109583
14   0.941162  0.161212  0.975708  0.075511
15   0.947021  0.152305  0.976318  0.084917
16   0.951172  0.143971  0.977905  0.087057
17   0.949951  0.135510  0.980713  0.068618
18   0.946411  0.149676  0.979736  0.090025
19   0.951416  0.137846  0.969482  0.099903
20   0.942261  0.183015  0.975830  0.110513
21   0.953491  0.140521  0.980713  0.069405
22   0.944824  0.151505  0.977661  0.079539
23   0.948608  0.154224  0.968018  0.105126
24   0.946167  0.161396  0.973633  0.090577
25   0.945557  0.157567  0.972778  0.091903
26   0.944824  0.159291  0.963623  0.107246
27   0.944702  0.169317  0.976440  0.083984
28   0.951294  0.145334  0.963257  0.128387
29   0.942749  0.163067  0.975586  0.088871
30   0.948730  0.147212  0.977539  0.075535
31   0.946045  0.154113  0.979004  0.082924
32   0.950806  0.156676  0.973877  0.080004
33   0.951538  0.145938  0.973267  0.095808
34   0.944580  0.161276  0.973389  0.093265
35   0.952637  0.139405  0.980835  0.082536
36   0.946167  0.154255  0.973145  0.095391
37   0.950928  0.143812  0.977905  0.093262
38   0.952393  0.153623  0.967407  0.116722
39   0.946655  0.164254  0.972046  0.118198
40   0.947876  0.165343  0.977173  0.089464
41   0.953247  0.134604  0.980103  0.079308
42   0.945435  0.168177  0.981934  0.071147
43   0.948853  0.147013  0.974243  0.105161
44   0.952759  0.144593  0.973022  0.107416
45   0.948120  0.162069  0.978882  0.078336
46   0.948975  0.152326  0.978516  0.091749
47   0.949829  0.143265  0.979492  0.084382
48   0.953491  0.135596  0.967651  0.124957
49   0.950195  0.133902  0.980469  0.069005
50   0.951538  0.140501  0.978149  0.097538
51   0.950562  0.140572  0.963867  0.132494
52   0.954102  0.137594  0.974365  0.095391
53   0.948853  0.145407  0.981323  0.089097
54   0.953735  0.147425  0.974854  0.090086
55   0.947510  0.166443  0.971558  0.139887
56   0.948486  0.167453  0.969116  0.125284
57   0.837646  2.158935  0.490234  8.126880
58   0.504150  7.905025  0.509644  7.817451
59   0.504761  7.895295  0.504761  7.895295
60   0.493042  8.082120  0.508667  7.833020
61   0.498901  7.988707  0.505249  7.887510
62   0.499390  7.980923  0.507202  7.856373
63   0.483521  8.233915  0.496460  8.027629
64   0.500488  7.963408  0.496338  8.029575
65   0.503662  7.912810  0.488647  8.152179
66   0.510010  7.811613  0.502930  7.924486
67   0.488892  8.148287  0.501587  7.945893
68   0.505493  7.883618  0.501465  7.947839
69   0.500244  7.967300  0.496826  8.021791
70   0.495972  8.035413  0.496948  8.019845
71   0.496216  8.031521  0.521118  7.634519
72   0.497070  8.017899  0.500610  7.961462
73   0.489868  8.132718  0.507080  7.858319
74   0.493530  8.074335  0.502686  7.928378
75   0.501587  7.945893  0.496216  8.031521
76   0.507202  7.856373  0.494873  8.052928
77   0.513062  7.762961  0.495728  8.039306
78   0.495605  8.041252  0.502319  7.934217
79   0.495117  8.049036  0.501709  7.943947
80   0.492554  8.089904  0.498047  8.002330
81   0.503174  7.920594  0.494873  8.052928
82   0.496460  8.027629  0.507202  7.856373
83   0.489258  8.142448  0.505615  7.881672
84   0.498047  8.002330  0.499634  7.977031
85   0.498169  8.000384  0.508545  7.834966
86   0.495361  8.045144  0.504150  7.905025
87   0.491943  8.099634  0.497681  8.008168
88   0.489014  8.146341  0.504883  7.893349
89   0.503784  7.910864  0.498169  8.000384
90   0.513184  7.761014  0.505371  7.885564
91   0.503662  7.912810  0.499390  7.980923
92   0.499146  7.984815  0.495850  8.037359
93   0.494385  8.060713  0.500244  7.967300
94   0.505981  7.875834  0.509277  7.823289
95   0.504517  7.899187  0.507812  7.846642
96   0.503418  7.916702  0.495972  8.035413
97   0.496216  8.031521  0.494141  8.064605
98   0.491821  8.101580  0.497925  8.004276
99   0.501343  7.949785  0.498535  7.994545
100  0.492920  8.084066  0.513550  7.755176
101  0.496582  8.025683  0.495117  8.049036
102  0.498047  8.002330  0.505005  7.891403
103  0.510986  7.796044  0.494873  8.052928
104  0.502686  7.928378  0.492065  8.097688
105  0.502930  7.924486  0.501465  7.947839
106  0.493408  8.076281  0.496704  8.023737
107  0.492676  8.087958  0.501221  7.951731
108  0.500488  7.963408  0.498657  7.992599
109  0.502319  7.934217  0.509521  7.819397
110  0.498169  8.000384  0.502319  7.934217
111  0.509888  7.813559  0.507202  7.856373
112  0.500000  7.971192  0.503662  7.912810
113  0.497192  8.015952  0.508789  7.831074
114  0.503052  7.922540  0.503662  7.912810
115  0.502441  7.932271  0.502441  7.932271
116  0.492920  8.084066  0.500366  7.965354
117  0.502930  7.924486  0.512207  7.776583
118  0.513794  7.751284  0.500244  7.967300
119  0.494507  8.058766  0.512085  7.778529
120  0.498169  8.000384  0.498413  7.996492
121  0.494507  8.058766  0.499634  7.977031
122  0.491943  8.099634  0.505127  7.889457
123  0.498901  7.988707  0.507568  7.850535
124  0.494385  8.060713  0.501465  7.947839
125  0.508667  7.833020  0.500244  7.967300
126  0.500000  7.971192  0.503906  7.908917
127  0.502563  7.930324  0.494263  8.062659

2018-06-08 16:52:47.874043 Finish.
Total elapsed time: 30:17:45.87.
