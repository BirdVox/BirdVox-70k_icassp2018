2018-06-07 10:35:08.864552: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:08.864748: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:08.864760: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:08.864764: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:08.864769: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:35:06.286989 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit10, unit01, unit02.
Validation set: unit03, unit05.
Test set: unit07.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.513550  7.797928  0.500488  8.051178
1    0.501587  8.033362  0.497559  8.098399
2    0.500732  8.047221  0.497925  8.092496
3    0.496216  8.119977  0.497925  8.092496
4    0.506104  7.960585  0.502197  8.023632
5    0.493042  8.171155  0.506714  7.950833
6    0.511475  7.874035  0.501099  8.041340
7    0.501465  8.035437  0.517090  7.783592
8    0.504395  7.988195  0.502319  8.021664
9    0.502563  8.017644  0.488525  8.243997
10   0.500977  8.043265  0.500854  8.045275
11   0.493042  8.171133  0.496338  8.118074
12   0.493286  8.167241  0.494385  8.149555
13   0.491211  8.200711  0.504517  7.986249
14   0.501221  8.039308  0.506104  7.960671
15   0.498169  8.088539  0.492188  8.184970
16   0.499023  8.074745  0.509766  7.901645
17   0.506836  7.948844  0.504883  7.980346
18   0.496582  8.114096  0.501587  8.033470
19   0.501465  8.035394  0.498901  8.076756
20   0.495483  8.131804  0.500488  8.051178
21   0.498413  8.084583  0.502930  8.011827
22   0.510010  7.897709  0.501221  8.039372
23   0.501343  8.037340  0.500366  8.053145
24   0.493286  8.167241  0.502197  8.023632
25   0.502808  8.013773  0.512329  7.860326
26   0.496338  8.118053  0.508301  7.925255
27   0.503296  8.005903  0.496826  8.110204
28   0.502930  8.011762  0.494263  8.151522
29   0.495605  8.129836  0.499146  8.072821
30   0.500244  8.055070  0.492920  8.173165
31   0.504883  7.980282  0.498413  8.084626
32   0.498779  8.078702  0.499390  8.068885
33   0.502075  8.025578  0.497314  8.102334
34   0.495361  8.133771  0.497559  8.098399
35   0.495605  8.129836  0.510864  7.883937
36   0.502319  8.021665  0.503418  8.003957
37   0.496704  8.112107  0.498535  8.082658
38   0.501953  8.027546  0.506958  7.946898
39   0.504517  7.986206  0.504272  7.990184
40   0.499512  8.066875  0.507446  7.939028
41   0.500732  8.047200  0.497192  8.104301
42   0.507446  7.938985  0.501587  8.033470
43   0.496094  8.121880  0.500732  8.047243
44   0.494751  8.143652  0.506958  7.946898
45   0.504028  7.994119  0.495605  8.129879
46   0.502808  8.013794  0.507446  7.939028
47   0.500977  8.043307  0.499146  8.072821
48   0.498291  8.086508  0.506104  7.960671
49   0.509277  7.909472  0.512451  7.858359
50   0.491089  8.202678  0.500000  8.059048
51   0.490479  8.212473  0.497437  8.100366
52   0.494873  8.141684  0.504028  7.994119
53   0.502808  8.013751  0.498535  8.082658
54   0.497803  8.094399  0.498169  8.088561
55   0.486938  8.269553  0.493286  8.167263
56   0.497925  8.092475  0.501709  8.031502
57   0.500977  8.043265  0.497681  8.096431
58   0.501831  8.029535  0.497803  8.094464
59   0.492310  8.182960  0.507812  7.933125
60   0.493286  8.167198  0.494507  8.147587
61   0.505859  7.964563  0.500122  8.057080
62   0.499390  8.068864  0.494751  8.143652
63   0.499878  8.060930  0.492554  8.179068
64   0.503662  8.000000  0.496094  8.122009
65   0.493896  8.157360  0.501709  8.031502
66   0.495728  8.127890  0.507446  7.939028
67   0.498169  8.088497  0.498169  8.088561
68   0.497192  8.104301  0.494629  8.145620
69   0.507812  7.933104  0.514648  7.822943
70   0.492065  8.186916  0.493286  8.167263
71   0.501343  8.037340  0.502319  8.021664
72   0.509155  7.911439  0.502686  8.015762
73   0.509644  7.903591  0.509277  7.909515
74   0.488647  8.242008  0.508423  7.923287
75   0.497925  8.092410  0.504761  7.982314
76   0.499512  8.066896  0.491455  8.196776
77   0.504150  7.992151  0.506226  7.958703
78   0.500610  8.049210  0.502075  8.025600
79   0.496826  8.110118  0.492188  8.184970
80   0.507080  7.944887  0.503174  8.007892
81   0.496338  8.118074  0.502686  8.015762
82   0.497803  8.094399  0.502930  8.011827
83   0.506958  7.946855  0.491089  8.202678
84   0.504150  7.992108  0.492188  8.184970
85   0.495361  8.133814  0.501587  8.033470
86   0.490601  8.210506  0.497314  8.102334
87   0.499634  8.064886  0.500854  8.045275
88   0.497314  8.102312  0.502197  8.023632
89   0.495728  8.127869  0.499268  8.070853
90   0.504150  7.992130  0.493530  8.163327
91   0.496826  8.110139  0.495483  8.131847
92   0.501831  8.029492  0.503174  8.007892
93   0.497803  8.094421  0.513306  7.844586
94   0.503784  7.998033  0.495239  8.135782
95   0.499146  8.072778  0.496582  8.114139
96   0.499634  8.064908  0.492798  8.175133
97   0.499390  8.068843  0.494995  8.139717
98   0.499878  8.060994  0.494019  8.155457
99   0.495850  8.125923  0.507324  7.940995
100  0.499268  8.070789  0.500000  8.059048
101  0.509888  7.899634  0.499634  8.064950
102  0.496948  8.108236  0.503052  8.009859
103  0.511353  7.876024  0.493896  8.157425
104  0.500000  8.059026  0.491211  8.200711
105  0.501099  8.041211  0.500488  8.051178
106  0.504395  7.988173  0.503784  7.998054
107  0.506592  7.952779  0.511230  7.878034
108  0.508301  7.925148  0.503906  7.996086
109  0.492432  8.180971  0.493652  8.161360
110  0.498535  8.082637  0.497192  8.104301
111  0.508423  7.923245  0.492920  8.173165
112  0.496948  8.108193  0.505859  7.964606
113  0.491943  8.188884  0.494873  8.141685
114  0.498779  8.078702  0.503662  8.000022
115  0.501343  8.037341  0.495361  8.133814
116  0.486938  8.269553  0.493042  8.171198
117  0.496094  8.122009  0.497559  8.098399
118  0.498413  8.084583  0.497437  8.100366
119  0.504272  7.990162  0.487549  8.259737
120  0.510376  7.891785  0.501343  8.037405
121  0.501343  8.037340  0.482788  8.336471
122  0.498047  8.090486  0.509277  7.909515
123  0.497437  8.100345  0.504395  7.988216
124  0.504028  7.994055  0.495117  8.137749
125  0.498535  8.082637  0.497803  8.094464
126  0.499146  8.072756  0.487427  8.261705
127  0.504517  7.986206  0.505615  7.968541

2018-06-08 17:31:42.324378 Finish.
Total elapsed time: 30:56:36.32.
