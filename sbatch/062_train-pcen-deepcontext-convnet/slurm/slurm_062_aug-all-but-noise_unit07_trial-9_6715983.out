2018-06-07 10:35:05.162279: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.162517: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.162537: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.162545: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.162553: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:35:01.533398 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit10, unit01, unit02.
Validation set: unit03, unit05.
Test set: unit07.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.486084  8.193175  0.505005  7.891403
1    0.498169  8.000598  0.502197  7.936163
2    0.498535  7.994631  0.486450  8.187209
3    0.499512  7.979127  0.500977  7.955624
4    0.501831  7.942194  0.495605  8.041252
5    0.508667  7.833149  0.497314  8.014006
6    0.492798  8.086183  0.500854  7.957570
7    0.502930  7.924679  0.502930  7.924486
8    0.504272  7.903251  0.503174  7.920594
9    0.493408  8.076603  0.498413  7.996492
10   0.493286  8.078335  0.500000  7.971192
11   0.498291  7.998588  0.500366  7.965354
12   0.500366  7.965654  0.498413  7.996492
13   0.500244  7.967407  0.495361  8.045144
14   0.486694  8.183424  0.499390  7.980923
15   0.492065  8.097774  0.499756  7.975085
16   0.493042  8.082291  0.508789  7.831074
17   0.501099  7.953699  0.503296  7.918648
18   0.507812  7.846707  0.503418  7.916702
19   0.507080  7.858448  0.511597  7.786314
20   0.495850  8.037381  0.492920  8.084066
21   0.512817  7.840231  0.496094  8.122009
22   0.515259  7.809266  0.498779  8.078723
23   0.511963  7.862282  0.490356  8.214484
24   0.517456  7.741372  0.503174  7.920766
25   0.518677  7.704799  0.500366  7.965418
26   0.519653  7.696073  0.508545  7.921320
27   0.498657  8.079854  0.499023  8.074788
28   0.506470  7.953889  0.504272  7.990184
29   0.501221  8.038579  0.494751  8.143652
30   0.493774  8.158556  0.494263  8.151522
31   0.510498  7.888960  0.484985  8.301055
32   0.502075  8.024720  0.498535  8.082658
33   0.495483  8.130882  0.513184  7.846553
34   0.506714  7.949632  0.494629  8.145620
35   0.494751  8.142773  0.500122  8.057080
36   0.505737  7.965694  0.503296  8.005924
37   0.505615  7.967661  0.491821  8.190873
38   0.498535  8.081564  0.496216  8.120042
39   0.491943  8.187919  0.493286  8.167263
40   0.492920  8.172414  0.494995  8.139717
41   0.501587  8.032805  0.499146  8.072821
42   0.502075  8.024634  0.500732  8.047243
43   0.500366  8.052373  0.496948  8.108236
44   0.502686  8.014647  0.498535  8.082658
45   0.502686  8.014861  0.500610  8.049210
46   0.493408  8.164373  0.499512  8.066918
47   0.510620  7.886885  0.505981  7.962638
48   0.501465  8.034622  0.504517  7.986249
49   0.495850  8.124829  0.500366  8.053145
50   0.509277  7.908893  0.496338  8.118074
51   0.503784  7.997303  0.499268  8.070853
52   0.497925  8.091702  0.504028  7.994119
53   0.502075  8.024849  0.496948  8.108236
54   0.501587  8.032397  0.502808  8.013794
55   0.502197  8.022774  0.505493  7.970508
56   0.504028  7.993046  0.489380  8.230224
57   0.500122  8.055986  0.498413  8.084626
58   0.492920  8.172050  0.489136  8.234159
59   0.507568  7.936224  0.499756  8.062983
60   0.498413  8.083939  0.501099  8.041340
61   0.502441  8.018839  0.495239  8.135782
62   0.502686  8.015097  0.498291  8.086593
63   0.499634  8.064028  0.496826  8.110204
64   0.496216  8.119012  0.510132  7.895742
65   0.490356  8.213583  0.503784  7.998054
66   0.507568  7.936374  0.502075  8.025600
67   0.502441  8.019096  0.504028  7.994119
68   0.494019  8.154299  0.503418  8.003957
69   0.495972  8.122904  0.503174  8.007892
70   0.499268  8.069823  0.503296  8.005924
71   0.501709  8.030816  0.499146  8.072821
72   0.503052  8.008937  0.493286  8.167263
73   0.506836  7.948029  0.498901  8.076756
74   0.499146  8.072156  0.511108  7.880002
75   0.491821  8.190101  0.501831  8.029535
76   0.500610  8.048181  0.506592  7.952801
77   0.492798  8.174446  0.507446  7.939028
78   0.496460  8.115356  0.508057  7.929190
79   0.510620  7.887057  0.493896  8.157425
80   0.499634  8.064135  0.496582  8.114139
81   0.499512  8.066189  0.502808  8.013794
82   0.501587  8.032526  0.498657  8.080691
83   0.499146  8.071984  0.489136  8.234159
84   0.494141  8.152739  0.506226  7.958703
85   0.502808  8.013044  0.507324  7.940995
86   0.507202  7.942148  0.500366  8.053145
87   0.499878  8.060050  0.503418  8.003957
88   0.495728  8.126989  0.508911  7.915417
89   0.495361  8.133021  0.502441  8.019697
90   0.502808  8.012979  0.495117  8.137749
91   0.497437  8.099465  0.494141  8.153490
92   0.496094  8.120915  0.508423  7.923287
93   0.506836  7.947922  0.493530  8.163327
94   0.507812  7.932332  0.496582  8.114139
95   0.498047  8.089670  0.512695  7.854424
96   0.506470  7.954189  0.492920  8.173165
97   0.505737  7.965608  0.500977  8.043307
98   0.508667  7.918580  0.497803  8.094463
99   0.509888  7.898669  0.497925  8.092496
100  0.492188  8.184027  0.501709  8.031502
101  0.501953  8.026752  0.494141  8.153490
102  0.504883  7.979681  0.493774  8.159392
103  0.489746  8.223399  0.505981  7.962638
104  0.488281  8.247117  0.505493  7.970508
105  0.500977  8.042492  0.506714  7.950833
106  0.502075  8.024742  0.503784  7.998054
107  0.508423  7.922387  0.496582  8.114139
108  0.497437  8.099766  0.499878  8.061015
109  0.499023  8.074037  0.497070  8.106269
110  0.509033  7.912871  0.503540  8.001989
111  0.504517  7.985133  0.504272  7.990184
112  0.511353  7.875209  0.500977  8.043307
113  0.506714  7.949911  0.490967  8.204646
114  0.490845  8.205584  0.493408  8.165295
115  0.491577  8.194122  0.493042  8.171198
116  0.495972  8.123119  0.509277  7.909515
117  0.501831  8.028848  0.503662  8.000022
118  0.491333  8.197971  0.502563  8.017729
119  0.507690  7.934235  0.505615  7.968541
120  0.502563  8.017064  0.497437  8.100366
121  0.492798  8.174146  0.490112  8.218419
122  0.497803  8.093563  0.499023  8.074788
123  0.494507  8.146622  0.508057  7.929190
124  0.496338  8.117237  0.499878  8.061015
125  0.498657  8.079704  0.501221  8.039372
126  0.504517  7.985284  0.506226  7.958703
127  0.489258  8.231248  0.496094  8.122009

2018-06-08 14:50:31.838248 Finish.
Total elapsed time: 28:15:30.84.
