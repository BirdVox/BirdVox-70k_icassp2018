2018-06-07 10:35:05.233510: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.233693: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.233705: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:35:02.660385 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit07, unit10, unit01.
Validation set: unit02, unit03.
Test set: unit05.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.495483  8.131847  0.498657  8.080691
1    0.506104  7.960671  0.494507  8.147587
2    0.502441  8.019697  0.499512  8.066918
3    0.492920  8.173165  0.498291  8.086593
4    0.499268  8.070853  0.501099  8.041340
5    0.505005  7.978379  0.488281  8.247932
6    0.497070  8.106269  0.508545  7.921320
7    0.498779  8.078723  0.506592  7.952801
8    0.503052  8.009859  0.504272  7.990184
9    0.500732  8.047243  0.501221  8.039372
10   0.497437  8.100366  0.502319  8.021664
11   0.497803  8.094464  0.503906  7.996086
12   0.498779  8.078723  0.498779  8.078723
13   0.503662  8.000022  0.497192  8.104301
14   0.496338  8.118074  0.493408  8.165295
15   0.488647  8.242029  0.496826  8.110204
16   0.494995  8.139717  0.492554  8.179068
17   0.500366  8.053145  0.499390  8.068885
18   0.514160  7.830813  0.504395  7.988216
19   0.509399  7.907547  0.498657  8.080691
20   0.497314  8.102334  0.494019  8.155457
21   0.495361  8.133814  0.498169  8.088561
22   0.494019  8.155457  0.503418  8.003957
23   0.488037  8.251867  0.498047  8.090528
24   0.489624  8.226289  0.491089  8.202678
25   0.505981  7.962638  0.497437  8.100366
26   0.497314  8.102334  0.501709  8.031502
27   0.503906  7.996086  0.505615  7.968541
28   0.498535  8.082658  0.489624  8.226289
29   0.495239  8.135782  0.495361  8.133814
30   0.507812  7.933125  0.499634  8.064950
31   0.506714  7.950833  0.497314  8.102334
32   0.490479  8.212516  0.500610  8.049210
33   0.500122  8.057080  0.500122  8.057080
34   0.504150  7.992151  0.503906  7.996086
35   0.493286  8.167263  0.502197  8.023632
36   0.504395  7.988216  0.496948  8.108236
37   0.505127  7.976411  0.500732  8.047243
38   0.501953  8.027567  0.493774  8.159392
39   0.505981  7.962638  0.507446  7.939028
40   0.497437  8.100366  0.496826  8.110204
41   0.502563  8.017729  0.501953  8.027567
42   0.501953  8.027567  0.500610  8.049210
43   0.503296  8.005924  0.502930  8.011827
44   0.498413  8.084626  0.501709  8.031502
45   0.507568  7.937060  0.496216  8.120042
46   0.496704  8.112171  0.495850  8.125944
47   0.508301  7.925255  0.493408  8.165295
48   0.503662  8.000022  0.502563  8.017729
49   0.490479  8.212516  0.502808  8.013794
50   0.500977  8.043307  0.486816  8.271542
51   0.506714  7.950833  0.498779  8.078723
52   0.499756  8.062983  0.498169  8.088561
53   0.497681  8.096431  0.496094  8.122009
54   0.492188  8.184970  0.496216  8.120042
55   0.496338  8.118074  0.499023  8.074788
56   0.489136  8.234159  0.500488  8.051178
57   0.500122  8.057080  0.494995  8.139717
58   0.500732  8.047243  0.503540  8.001989
59   0.497803  8.094464  0.505981  7.962638
60   0.496094  8.122009  0.495850  8.125944
61   0.497070  8.106269  0.493408  8.165295
62   0.494019  8.155457  0.500732  8.047243
63   0.494263  8.151522  0.499634  8.064950
64   0.497314  8.102334  0.505371  7.972476
65   0.502808  8.013794  0.500610  8.049210
66   0.506958  7.946898  0.503784  7.998054
67   0.494263  8.151522  0.507690  7.935093
68   0.497314  8.102334  0.499390  8.068885
69   0.502686  8.015762  0.499268  8.070853
70   0.497681  8.096431  0.509644  7.903612
71   0.487427  8.261705  0.497314  8.102334
72   0.499268  8.070853  0.510986  7.881969
73   0.507690  7.935093  0.498413  8.084626
74   0.498291  8.086593  0.508301  7.925255
75   0.512573  7.856391  0.505249  7.974444
76   0.500854  8.045275  0.496216  8.120042
77   0.500000  8.059048  0.507446  7.939028
78   0.501709  8.031502  0.498901  8.076756
79   0.502930  8.011827  0.496582  8.114139
80   0.493774  8.159392  0.502319  8.021665
81   0.496460  8.116106  0.503174  8.007892
82   0.497314  8.102334  0.505127  7.976411
83   0.501831  8.029535  0.501465  8.035437
84   0.500977  8.043307  0.496216  8.120042
85   0.502808  8.013794  0.498535  8.082658
86   0.491577  8.194808  0.501099  8.041340
87   0.500854  8.045275  0.500610  8.049210
88   0.494873  8.141685  0.509155  7.911482
89   0.505493  7.970508  0.504639  7.984281
90   0.501831  8.029535  0.504028  7.994119
91   0.499756  8.062983  0.499268  8.070853
92   0.505737  7.966573  0.502808  8.013794
93   0.493408  8.165295  0.489868  8.222354
94   0.499756  8.062983  0.494629  8.145620
95   0.510132  7.895742  0.504517  7.986249
96   0.493286  8.167263  0.499878  8.061015
97   0.505127  7.976411  0.496094  8.122009
98   0.498291  8.086593  0.502197  8.023632
99   0.498413  8.084626  0.501831  8.029535
100  0.492798  8.175133  0.500977  8.043307
101  0.491577  8.194808  0.491699  8.192841
102  0.503662  8.000022  0.497803  8.094464
103  0.505615  7.968541  0.507080  7.944930
104  0.502686  8.015762  0.506714  7.950833
105  0.491943  8.188906  0.507812  7.933125
106  0.494873  8.141684  0.498901  8.076756
107  0.501099  8.041340  0.496094  8.122009
108  0.509033  7.913450  0.501709  8.031502
109  0.504639  7.984281  0.502563  8.017729
110  0.498413  8.084626  0.494629  8.145620
111  0.503296  8.005924  0.491333  8.198743
112  0.501221  8.039372  0.497925  8.092496
113  0.499756  8.062983  0.503174  8.007892
114  0.496826  8.110204  0.494629  8.145620
115  0.508545  7.921320  0.497314  8.102334
116  0.510376  7.891807  0.497803  8.094464
117  0.500000  8.059048  0.509766  7.901645
118  0.503052  8.009859  0.501587  8.033470
119  0.495972  8.123977  0.501709  8.031502
120  0.496948  8.108236  0.496826  8.110204
121  0.502075  8.025600  0.506470  7.954768
122  0.485107  8.299088  0.498779  8.078723
123  0.495239  8.135782  0.494873  8.141685
124  0.501953  8.027567  0.494385  8.149555
125  0.504395  7.988216  0.494751  8.143652
126  0.495728  8.127912  0.503296  8.005924
127  0.503540  8.001989  0.507935  7.931158

2018-06-08 16:43:35.123222 Finish.
Total elapsed time: 30:08:33.12.
