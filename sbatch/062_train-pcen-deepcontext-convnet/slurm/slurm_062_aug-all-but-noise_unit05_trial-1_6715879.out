2018-06-07 10:35:01.380044: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:01.380308: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:01.380321: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:34:52.098159 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit07, unit10, unit01.
Validation set: unit02, unit03.
Test set: unit05.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.929443  0.204172  0.925659  0.235557
1    0.932129  0.193887  0.910645  0.303306
2    0.937134  0.185235  0.911377  0.263776
3    0.940430  0.175317  0.913818  0.302456
4    0.945923  0.156736  0.894897  0.310056
5    0.947266  0.164864  0.921265  0.300704
6    0.946655  0.163532  0.887207  0.352208
7    0.948242  0.153925  0.924438  0.250372
8    0.949707  0.156412  0.912964  0.246876
9    0.945435  0.166488  0.900269  0.378913
10   0.948608  0.144876  0.881470  0.386058
11   0.947998  0.137412  0.893555  0.404605
12   0.953735  0.136137  0.896606  0.375370
13   0.946533  0.162251  0.904053  0.275339
14   0.948120  0.150805  0.912231  0.343502
15   0.953491  0.146245  0.919800  0.300919
16   0.958252  0.125998  0.912842  0.273581
17   0.954346  0.142825  0.924561  0.337728
18   0.949097  0.150593  0.911011  0.263517
19   0.954834  0.142977  0.867676  0.380710
20   0.953735  0.139348  0.796265  0.752319
21   0.949707  0.154377  0.932861  0.207971
22   0.957764  0.131913  0.924561  0.232820
23   0.958862  0.127792  0.927979  0.213472
24   0.952881  0.141829  0.939819  0.212321
25   0.956665  0.136520  0.919556  0.248050
26   0.953857  0.140931  0.815430  0.763414
27   0.950073  0.151738  0.894165  0.362243
28   0.957031  0.134685  0.730713  1.532080
29   0.943970  0.180430  0.913086  0.256763
30   0.953979  0.144547  0.898560  0.432864
31   0.951538  0.160121  0.836792  0.591863
32   0.953735  0.141958  0.918823  0.390869
33   0.958984  0.132575  0.944702  0.208431
34   0.958008  0.125213  0.943970  0.245280
35   0.955933  0.136301  0.753296  1.460406
36   0.960815  0.127182  0.909546  0.291624
37   0.953369  0.139669  0.913330  0.482124
38   0.956055  0.132313  0.849609  0.541493
39   0.956787  0.134557  0.905518  0.393811
40   0.955078  0.133739  0.862305  0.605088
41   0.958252  0.136473  0.861816  1.186895
42   0.957642  0.137983  0.887817  0.802123
43   0.953979  0.133262  0.859741  1.071274
44   0.957153  0.131159  0.842773  1.134044
45   0.928345  0.479378  0.889526  1.756037
46   0.555542  7.085806  0.530029  7.490367
47   0.668335  5.307051  0.493652  8.161360
48   0.509888  7.899055  0.507202  7.942963
49   0.509277  7.908893  0.504395  7.988216
50   0.503418  8.003506  0.505127  7.976411
51   0.505615  7.966942  0.496582  8.114139
52   0.500000  8.059026  0.499634  8.064950
53   0.505371  7.972476  0.496582  8.114139
54   0.505493  7.970508  0.503174  8.007892
55   0.505371  7.972476  0.494141  8.153490
56   0.512329  7.860326  0.498047  8.090528
57   0.499756  8.062983  0.499634  8.064950
58   0.500000  8.059048  0.500488  8.051178
59   0.498901  8.076756  0.509033  7.913450
60   0.489258  8.232191  0.502563  8.017729
61   0.505005  7.978357  0.511230  7.878034
62   0.498169  8.088561  0.512939  7.850488
63   0.498779  8.078723  0.499146  8.072821
64   0.500488  8.051178  0.505615  7.968541
65   0.497437  8.100366  0.499512  8.066918
66   0.493652  8.161360  0.501343  8.037405
67   0.497681  8.096431  0.499146  8.072821
68   0.489990  8.220365  0.496338  8.118074
69   0.501343  8.037383  0.502930  8.011827
70   0.500488  8.051178  0.497070  8.106269
71   0.488770  8.240062  0.501587  8.033470
72   0.486938  8.269575  0.494629  8.145620
73   0.501099  8.041340  0.509521  7.905580
74   0.507202  7.942963  0.503662  8.000022
75   0.501343  8.037405  0.506348  7.956736
76   0.503784  7.998033  0.507080  7.944930
77   0.501465  8.035437  0.500610  8.049210
78   0.496338  8.118074  0.499512  8.066918
79   0.498901  8.076756  0.494141  8.153490
80   0.489624  8.226289  0.505005  7.978379
81   0.496094  8.122009  0.496826  8.110204
82   0.489502  8.228256  0.491577  8.194808
83   0.500732  8.047221  0.505493  7.970508
84   0.501953  8.027567  0.507080  7.944930
85   0.501831  8.029535  0.500488  8.051178
86   0.500244  8.055113  0.497314  8.102334
87   0.497803  8.094464  0.499756  8.062983
88   0.493042  8.171198  0.504028  7.994119
89   0.500977  8.043307  0.494751  8.143652
90   0.503296  8.005924  0.500244  8.055113
91   0.494263  8.151522  0.493042  8.171198
92   0.500854  8.045275  0.494141  8.153490
93   0.500366  8.053145  0.496338  8.118074
94   0.495117  8.137749  0.498413  8.084626
95   0.497070  8.106269  0.502686  8.015762
96   0.492676  8.177100  0.503906  7.996086
97   0.503174  8.007892  0.507446  7.939028
98   0.504395  7.988216  0.509521  7.905580
99   0.509033  7.913450  0.504395  7.988216
100  0.496216  8.120042  0.506470  7.954768
101  0.507935  7.931158  0.508423  7.923287
102  0.501343  8.037405  0.507812  7.933125
103  0.496826  8.110204  0.498779  8.078723
104  0.491455  8.196776  0.507446  7.939028
105  0.501831  8.029535  0.495483  8.131847
106  0.505981  7.962638  0.495728  8.127912
107  0.502319  8.021643  0.493042  8.171198
108  0.508667  7.919352  0.491821  8.190873
109  0.512451  7.858359  0.490112  8.218419
110  0.494751  8.143652  0.496704  8.112171
111  0.490112  8.218419  0.512085  7.864261
112  0.504395  7.988216  0.494263  8.151522
113  0.506470  7.954768  0.494263  8.151522
114  0.505005  7.978379  0.500488  8.051178
115  0.492798  8.175133  0.505005  7.978379
116  0.494385  8.149555  0.512451  7.858359
117  0.495850  8.125944  0.506226  7.958703
118  0.498047  8.090507  0.500122  8.057080
119  0.494263  8.151522  0.501587  8.033470
120  0.491943  8.188905  0.497559  8.098399
121  0.501709  8.031502  0.501465  8.035437
122  0.500977  8.043307  0.502686  8.015762
123  0.492554  8.179068  0.502808  8.013794
124  0.502441  8.019676  0.495728  8.127912
125  0.491821  8.190873  0.503418  8.003957
126  0.503174  8.007870  0.499512  8.066918
127  0.501831  8.029513  0.498901  8.076756

2018-06-08 16:46:30.821244 Finish.
Total elapsed time: 30:11:38.82.
