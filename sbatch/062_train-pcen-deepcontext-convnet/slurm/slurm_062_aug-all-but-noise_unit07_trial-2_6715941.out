2018-06-07 10:35:05.130203: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.130424: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.130437: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:35:02.573628 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit10, unit01, unit02.
Validation set: unit03, unit05.
Test set: unit07.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.506714  7.950833  0.497925  8.092496
1    0.498657  8.080691  0.496338  8.118074
2    0.514038  7.832781  0.505371  7.972476
3    0.497070  8.106247  0.498169  8.088561
4    0.500977  8.043286  0.513550  7.840651
5    0.495117  8.137707  0.507568  7.937060
6    0.506836  7.948865  0.502197  8.023632
7    0.498291  8.086593  0.500244  8.055113
8    0.504761  7.982292  0.500610  8.049210
9    0.502808  8.013794  0.503296  8.005924
10   0.491699  8.192841  0.496094  8.122009
11   0.501709  8.031502  0.495117  8.137749
12   0.507202  7.942963  0.515991  7.801300
13   0.489746  8.224321  0.505859  7.964606
14   0.500488  8.051156  0.489502  8.228256
15   0.503052  8.009859  0.501099  8.041340
16   0.507568  7.937060  0.503418  8.003957
17   0.501831  8.029535  0.500122  8.057080
18   0.497681  8.096431  0.498047  8.090528
19   0.500488  8.051178  0.498047  8.090528
20   0.502319  8.021664  0.498291  8.086593
21   0.514038  7.832781  0.489746  8.224321
22   0.503052  8.009859  0.501343  8.037405
23   0.498779  8.078723  0.495239  8.135782
24   0.511475  7.874099  0.506348  7.956736
25   0.502686  8.015762  0.503418  8.003957
26   0.501831  8.029535  0.492676  8.177100
27   0.502441  8.019697  0.493286  8.167263
28   0.504028  7.994119  0.505493  7.970508
29   0.490845  8.206613  0.495239  8.135782
30   0.496826  8.110182  0.508423  7.923287
31   0.492188  8.184949  0.495483  8.131847
32   0.494019  8.155457  0.499023  8.074788
33   0.499390  8.068885  0.506226  7.958703
34   0.498657  8.080691  0.511719  7.870164
35   0.501465  8.035437  0.503784  7.998054
36   0.504883  7.980346  0.498413  8.084626
37   0.497803  8.094442  0.496582  8.114139
38   0.500610  8.049210  0.511719  7.870164
39   0.499390  8.068864  0.483398  8.326633
40   0.496826  8.110204  0.502075  8.025600
41   0.493042  8.171198  0.500122  8.057080
42   0.513672  7.838683  0.501953  8.027567
43   0.507812  7.933125  0.496826  8.110204
44   0.503296  8.005903  0.494995  8.139717
45   0.507446  7.939028  0.495483  8.131847
46   0.504761  7.982314  0.508911  7.915417
47   0.496948  8.108236  0.506958  7.946898
48   0.500244  8.055113  0.500610  8.049210
49   0.506348  7.956736  0.499756  8.062983
50   0.500244  8.055113  0.491821  8.190873
51   0.503052  8.009859  0.495117  8.137749
52   0.492554  8.179068  0.491577  8.194808
53   0.499146  8.072821  0.499634  8.064950
54   0.501709  8.031481  0.489502  8.228256
55   0.496948  8.108215  0.500000  8.059048
56   0.501343  8.037405  0.504883  7.980346
57   0.491577  8.194787  0.490112  8.218419
58   0.485840  8.287261  0.504517  7.986249
59   0.502197  8.023632  0.486572  8.275477
60   0.508057  7.929190  0.495850  8.125944
61   0.501221  8.039372  0.512207  7.862294
62   0.500854  8.045275  0.501221  8.039372
63   0.495728  8.127890  0.501221  8.039372
64   0.498169  8.088561  0.504272  7.990184
65   0.505127  7.976411  0.497803  8.094464
66   0.496704  8.112171  0.494263  8.151522
67   0.506348  7.956736  0.491943  8.188905
68   0.499756  8.062983  0.502563  8.017729
69   0.503052  8.009859  0.502686  8.015762
70   0.501343  8.037405  0.497681  8.096431
71   0.504517  7.986249  0.502441  8.019697
72   0.500610  8.049210  0.498779  8.078723
73   0.498169  8.088561  0.500610  8.049210
74   0.491333  8.198743  0.499878  8.061015
75   0.513062  7.848521  0.500488  8.051178
76   0.492554  8.179068  0.501221  8.039372
77   0.497070  8.106226  0.492798  8.175133
78   0.496460  8.116106  0.492920  8.173165
79   0.501709  8.031502  0.490234  8.216451
80   0.496094  8.122009  0.500244  8.055113
81   0.504761  7.982314  0.484985  8.301055
82   0.502808  8.013773  0.495483  8.131847
83   0.497803  8.094464  0.505249  7.974444
84   0.504639  7.984281  0.495483  8.131847
85   0.498901  8.076756  0.496704  8.112171
86   0.508667  7.919352  0.502808  8.013794
87   0.491821  8.190873  0.499634  8.064950
88   0.496216  8.120020  0.501099  8.041340
89   0.495972  8.123977  0.497803  8.094464
90   0.501831  8.029535  0.493164  8.169230
91   0.500488  8.051178  0.503662  8.000022
92   0.501221  8.039372  0.499268  8.070853
93   0.504028  7.994119  0.492432  8.181035
94   0.495483  8.131847  0.501099  8.041340
95   0.487061  8.267607  0.500610  8.049210
96   0.504395  7.988216  0.493042  8.171198
97   0.506226  7.958682  0.500854  8.045275
98   0.504761  7.982314  0.501465  8.035437
99   0.499512  8.066918  0.499146  8.072821
100  0.495972  8.123977  0.494873  8.141684
101  0.503174  8.007892  0.492554  8.179068
102  0.501343  8.037405  0.510254  7.893774
103  0.509888  7.899656  0.508179  7.927223
104  0.501709  8.031502  0.504517  7.986249
105  0.496582  8.114139  0.503540  8.001989
106  0.500732  8.047200  0.498901  8.076756
107  0.507080  7.944930  0.496338  8.118074
108  0.504272  7.990184  0.508301  7.925255
109  0.495850  8.125944  0.497070  8.106269
110  0.491699  8.192841  0.494385  8.149555
111  0.507446  7.939028  0.493774  8.159392
112  0.507690  7.935093  0.491577  8.194808
113  0.495483  8.131847  0.507935  7.931158
114  0.497192  8.104301  0.504761  7.982314
115  0.502808  8.013794  0.494263  8.151522
116  0.497070  8.106247  0.499023  8.074788
117  0.501099  8.041340  0.501831  8.029535
118  0.505493  7.970508  0.496216  8.120042
119  0.490967  8.204646  0.514282  7.828845
120  0.500244  8.055091  0.500244  8.055113
121  0.500488  8.051156  0.503296  8.005924
122  0.505981  7.962638  0.496460  8.116106
123  0.504028  7.994119  0.498901  8.076756
124  0.503418  8.003957  0.501465  8.035437
125  0.505371  7.972476  0.495728  8.127912
126  0.502563  8.017729  0.499390  8.068886
127  0.505615  7.968541  0.504761  7.982314

2018-06-08 16:37:10.451710 Finish.
Total elapsed time: 30:02:08.45.
