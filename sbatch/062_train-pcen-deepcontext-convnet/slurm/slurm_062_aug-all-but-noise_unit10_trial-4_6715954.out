2018-06-07 10:35:12.173736: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:12.173956: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:12.173969: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:35:02.963435 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit01, unit02, unit03.
Validation set: unit05, unit07.
Test set: unit10.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.488159  8.160006  0.498779  7.990653
1    0.500977  7.955624  0.506592  7.866103
2    0.497070  8.017941  0.495972  8.035413
3    0.514160  7.745467  0.496460  8.027629
4    0.498779  7.990675  0.502808  7.926432
5    0.500977  7.955667  0.494995  8.050982
6    0.496216  8.031521  0.489258  8.142448
7    0.499634  7.977052  0.499390  7.980923
8    0.507080  7.858319  0.493652  8.072389
9    0.500488  7.963494  0.500244  7.967300
10   0.503662  7.912853  0.502319  7.934217
11   0.505981  7.875877  0.490479  8.122988
12   0.497559  8.010179  0.500732  7.959516
13   0.496338  8.029575  0.500122  7.969246
14   0.501587  7.944069  0.503784  7.910864
15   0.496582  8.025683  0.499634  7.977031
16   0.498047  8.002330  0.491577  8.105473
17   0.497437  8.012060  0.494751  8.054874
18   0.506714  7.864157  0.514771  7.735715
19   0.500610  7.961483  0.506958  7.860265
20   0.504883  7.893349  0.500244  7.967300
21   0.503052  7.922540  0.501099  7.953678
22   0.500244  7.967300  0.502197  7.936163
23   0.497925  8.004297  0.499512  7.978977
24   0.503418  7.916723  0.498291  7.998438
25   0.493164  8.080195  0.493896  8.068497
26   0.501343  7.949785  0.493164  8.080173
27   0.495483  8.043219  0.491943  8.099634
28   0.503662  7.912810  0.484375  8.220292
29   0.494751  8.054874  0.491821  8.101580
30   0.508179  7.840847  0.494995  8.050982
31   0.502441  7.932271  0.504395  7.901133
32   0.490112  8.128826  0.502075  7.938109
33   0.505005  7.891424  0.493286  8.078227
34   0.503418  7.916702  0.495117  8.049036
35   0.508179  7.840804  0.507080  7.858319
36   0.502075  7.938130  0.497192  8.015952
37   0.503662  7.912831  0.507446  7.852481
38   0.501587  7.945936  0.501831  7.942001
39   0.492310  8.093818  0.503662  7.912810
40   0.510376  7.805775  0.506348  7.869996
41   0.492310  8.093796  0.497681  8.008168
42   0.501831  7.942022  0.500977  7.955624
43   0.490845  8.117149  0.503662  7.912810
44   0.497925  8.004276  0.502686  7.928378
45   0.494507  8.058766  0.505859  7.877780
46   0.497681  8.008168  0.507202  7.856373
47   0.493164  8.080195  0.498047  8.002330
48   0.499390  7.980944  0.500610  7.961462
49   0.501587  7.945936  0.495361  8.045144
50   0.493408  8.076303  0.488647  8.152179
51   0.506958  7.860265  0.506958  7.860265
52   0.495972  8.035413  0.501831  7.942001
53   0.503174  7.920615  0.505005  7.891403
54   0.508667  7.833020  0.500854  7.957570
55   0.505859  7.877801  0.501221  7.951731
56   0.497314  8.014006  0.500366  7.965354
57   0.497070  8.017899  0.503784  7.910864
58   0.494141  8.064605  0.503662  7.912810
59   0.505737  7.879726  0.508423  7.836912
60   0.501099  7.953678  0.509155  7.825235
61   0.496948  8.019845  0.497070  8.017899
62   0.507202  7.856373  0.496094  8.033467
63   0.496094  8.033467  0.497681  8.008168
64   0.502319  7.934217  0.500610  7.961462
65   0.496582  8.025683  0.506226  7.871942
66   0.496460  8.027629  0.503784  7.910864
67   0.507935  7.844718  0.506958  7.860265
68   0.507080  7.858319  0.504028  7.906971
69   0.511597  7.786314  0.507202  7.856373
70   0.499390  7.980944  0.497681  8.008168
71   0.507812  7.846642  0.503174  7.920594
72   0.500244  7.967322  0.499390  7.980923
73   0.498779  7.990653  0.497803  8.006222
74   0.495117  8.049036  0.502808  7.926432
75   0.501465  7.947861  0.500000  7.971192
76   0.505005  7.891403  0.499146  7.984815
77   0.496338  8.029575  0.502319  7.934217
78   0.506958  7.860265  0.500000  7.971192
79   0.501831  7.942001  0.497070  8.017899
80   0.500366  7.965376  0.498413  7.996492
81   0.511108  7.794098  0.491211  8.111311
82   0.494995  8.050982  0.502563  7.930324
83   0.508911  7.829128  0.503540  7.914756
84   0.495117  8.049079  0.503906  7.908917
85   0.509277  7.823289  0.499878  7.973138
86   0.496948  8.019845  0.503906  7.908917
87   0.500610  7.961483  0.503418  7.916702
88   0.498779  7.990739  0.495361  8.045144
89   0.504517  7.899187  0.490723  8.119095
90   0.503418  7.916702  0.502808  7.926432
91   0.503906  7.908917  0.507080  7.858319
92   0.496704  8.023737  0.494019  8.066551
93   0.494263  8.062659  0.494507  8.058766
94   0.498413  7.996513  0.501221  7.951731
95   0.493408  8.076281  0.501587  7.945893
96   0.497681  8.008211  0.496216  8.031521
97   0.502441  7.932292  0.485352  8.204723
98   0.495483  8.043241  0.493652  8.072389
99   0.496338  8.029597  0.499146  7.984815
100  0.494751  8.054874  0.499634  7.977031
101  0.498169  8.000405  0.497925  8.004276
102  0.498901  7.988707  0.495117  8.049036
103  0.497681  8.008211  0.508911  7.829128
104  0.497192  8.015995  0.497192  8.015952
105  0.494873  8.052928  0.497192  8.015952
106  0.499390  7.980944  0.494873  8.052928
107  0.494385  8.060713  0.502197  7.936163
108  0.505005  7.891424  0.500488  7.963408
109  0.503662  7.912810  0.489136  8.144395
110  0.500732  7.959516  0.497803  8.006222
111  0.499634  7.977031  0.486084  8.193047
112  0.498535  7.994545  0.498779  7.990653
113  0.492798  8.086012  0.508789  7.831074
114  0.498901  7.988729  0.498169  8.000384
115  0.493164  8.080173  0.503662  7.912810
116  0.495605  8.041252  0.503784  7.910864
117  0.498779  7.990653  0.502197  7.936163
118  0.504883  7.893370  0.490112  8.128826
119  0.501465  7.947861  0.501221  7.951731
120  0.489502  8.138599  0.495483  8.043198
121  0.492188  8.095764  0.500488  7.963408
122  0.511230  7.792152  0.503906  7.908917
123  0.499878  7.973138  0.492798  8.086012
124  0.493774  8.070464  0.506836  7.862211
125  0.497192  8.015952  0.501465  7.947839
126  0.513062  7.762982  0.500610  7.961462
127  0.495972  8.035413  0.506592  7.866103

2018-06-08 17:01:28.960085 Finish.
Total elapsed time: 30:26:26.96.
