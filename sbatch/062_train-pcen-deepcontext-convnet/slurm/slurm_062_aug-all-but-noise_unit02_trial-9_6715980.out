2018-06-07 10:35:05.078386: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.079139: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.079148: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.079152: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.079157: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:35:02.442073 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.493652  8.072389  0.512573  7.770745
1    0.501465  7.947839  0.494019  8.066551
2    0.497559  8.010114  0.503662  7.912810
3    0.490845  8.117149  0.509277  7.823289
4    0.489258  8.142448  0.494141  8.064605
5    0.502808  7.926432  0.493164  8.080173
6    0.500977  7.955624  0.497925  8.004276
7    0.498169  8.000384  0.502930  7.924486
8    0.502808  7.926454  0.496704  8.023737
9    0.502808  7.926432  0.504883  7.893349
10   0.500366  7.965354  0.501099  7.953678
11   0.502930  7.924486  0.502686  7.928378
12   0.490479  8.122988  0.498657  7.992599
13   0.500610  7.961462  0.502441  7.932271
14   0.497925  8.004276  0.502197  7.936163
15   0.508301  7.838858  0.504395  7.901133
16   0.495728  8.039327  0.504883  7.893349
17   0.500854  7.957570  0.493530  8.074335
18   0.508911  7.829149  0.499756  7.975085
19   0.497803  8.006222  0.497559  8.010114
20   0.490356  8.124934  0.501953  7.940055
21   0.493774  8.070443  0.498901  7.988707
22   0.495483  8.043198  0.504883  7.893349
23   0.497681  8.008168  0.493652  8.072389
24   0.491821  8.101580  0.490601  8.121041
25   0.504272  7.903101  0.513062  7.762961
26   0.495605  8.041252  0.503418  7.916702
27   0.501587  7.945893  0.505981  7.875834
28   0.502808  7.926432  0.494263  8.062659
29   0.490723  8.119095  0.503296  7.918648
30   0.500000  7.971192  0.496460  8.027629
31   0.512329  7.774637  0.496704  8.023737
32   0.511353  7.790206  0.500610  7.961462
33   0.510986  7.796044  0.505005  7.891403
34   0.495239  8.047090  0.503662  7.912810
35   0.497925  8.004276  0.493652  8.072389
36   0.497314  8.014006  0.508545  7.834966
37   0.506104  7.873888  0.497192  8.015952
38   0.510498  7.803828  0.507080  7.858319
39   0.502930  7.924486  0.492920  8.084066
40   0.509521  7.819419  0.502930  7.924486
41   0.506958  7.860265  0.497070  8.017899
42   0.500977  7.955645  0.490356  8.124934
43   0.495605  8.041252  0.504883  7.893349
44   0.488525  8.154125  0.498901  7.988707
45   0.492798  8.086012  0.499634  7.977031
46   0.494507  8.058766  0.490234  8.126880
47   0.497925  8.004276  0.493286  8.078227
48   0.510864  7.798012  0.502808  7.926432
49   0.499756  7.975085  0.503296  7.918648
50   0.495483  8.043198  0.505371  7.885564
51   0.497803  8.006222  0.498657  7.992599
52   0.496094  8.033489  0.508545  7.834966
53   0.503052  7.922540  0.504883  7.893349
54   0.496948  8.019845  0.497192  8.015952
55   0.492188  8.095764  0.508423  7.836912
56   0.502808  7.926432  0.498901  7.988707
57   0.492432  8.091850  0.502075  7.938109
58   0.500854  7.957570  0.504639  7.897241
59   0.488159  8.159963  0.505127  7.889457
60   0.487305  8.173586  0.493286  8.078227
61   0.502075  7.938109  0.497437  8.012060
62   0.501831  7.942001  0.492554  8.089904
63   0.498047  8.002330  0.514282  7.743500
64   0.505127  7.889457  0.508301  7.838858
65   0.501587  7.945893  0.496216  8.031521
66   0.501587  7.945893  0.494873  8.052928
67   0.498901  7.988707  0.501587  7.945893
68   0.500000  7.971192  0.500977  7.955624
69   0.502930  7.924486  0.503052  7.922540
70   0.512817  7.766853  0.494141  8.064605
71   0.501465  7.947839  0.513062  7.762961
72   0.497070  8.017899  0.505127  7.889456
73   0.503784  7.910864  0.500977  7.955624
74   0.492798  8.086012  0.509888  7.813559
75   0.488281  8.158017  0.502930  7.924486
76   0.492065  8.097688  0.504028  7.906971
77   0.502808  7.926432  0.502930  7.924486
78   0.499878  7.973138  0.506592  7.866103
79   0.500610  7.961462  0.505615  7.881672
80   0.496582  8.025683  0.496704  8.023737
81   0.504272  7.903079  0.504639  7.897241
82   0.503052  7.922540  0.484375  8.220292
83   0.505371  7.885564  0.488403  8.156071
84   0.498047  8.002330  0.493164  8.080173
85   0.498169  8.000384  0.505371  7.885564
86   0.501709  7.943947  0.504272  7.903079
87   0.494019  8.066551  0.497437  8.012060
88   0.507568  7.850535  0.494507  8.058766
89   0.495361  8.045144  0.493896  8.068497
90   0.500122  7.969246  0.498901  7.988707
91   0.499756  7.975085  0.501587  7.945893
92   0.494019  8.066551  0.499268  7.982869
93   0.506592  7.866103  0.493530  8.074335
94   0.505737  7.879726  0.500000  7.971192
95   0.489380  8.140502  0.492310  8.093796
96   0.495972  8.035413  0.502808  7.926432
97   0.488281  8.158017  0.503174  7.920594
98   0.507690  7.848589  0.498779  7.990653
99   0.494019  8.066551  0.500732  7.959516
100  0.496826  8.021791  0.489624  8.136610
101  0.497314  8.014006  0.497803  8.006222
102  0.498535  7.994545  0.503296  7.918648
103  0.499512  7.978977  0.510010  7.811613
104  0.497070  8.017899  0.499512  7.978977
105  0.507324  7.854427  0.495605  8.041252
106  0.505127  7.889457  0.490356  8.124934
107  0.509277  7.823289  0.500977  7.955624
108  0.506592  7.866103  0.500122  7.969246
109  0.501831  7.942001  0.511719  7.784368
110  0.496338  8.029575  0.498291  7.998438
111  0.489990  8.130772  0.504761  7.895295
112  0.504639  7.897241  0.500854  7.957570
113  0.492310  8.093796  0.503174  7.920594
114  0.482666  8.247537  0.493286  8.078227
115  0.505005  7.891403  0.499023  7.986761
116  0.501953  7.940055  0.503052  7.922540
117  0.514038  7.747392  0.502808  7.926432
118  0.506958  7.860265  0.499268  7.982869
119  0.497314  8.014006  0.505127  7.889457
120  0.495605  8.041252  0.496338  8.029575
121  0.495972  8.035413  0.504395  7.901133
122  0.496460  8.027629  0.498047  8.002330
123  0.497559  8.010114  0.488770  8.150233
124  0.503540  7.914756  0.493652  8.072389
125  0.496338  8.029575  0.500366  7.965354
126  0.502075  7.938109  0.498047  8.002330
127  0.502930  7.924486  0.495605  8.041252

2018-06-08 17:51:07.864129 Finish.
Total elapsed time: 31:16:05.86.
