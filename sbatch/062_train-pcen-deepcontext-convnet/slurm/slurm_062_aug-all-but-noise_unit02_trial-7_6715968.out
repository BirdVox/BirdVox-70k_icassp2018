2018-06-07 10:35:06.301612: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:06.301881: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:06.301893: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:35:03.149095 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.512451  7.807090  0.502930  8.011827
1    0.502686  8.013638  0.499512  8.066918
2    0.506592  7.950934  0.505737  7.966573
3    0.499512  8.064516  0.503662  8.000022
4    0.509155  7.909359  0.495850  8.125944
5    0.503906  7.994306  0.506226  7.958703
6    0.501099  8.039002  0.506836  7.948866
7    0.512207  7.858691  0.494141  8.153490
8    0.498291  8.084255  0.498535  8.082658
9    0.510254  7.891329  0.499146  8.072821
10   0.507446  7.936733  0.498291  8.086593
11   0.508789  7.914897  0.498535  8.082658
12   0.509399  7.904759  0.493286  8.167263
13   0.507202  7.940282  0.502197  8.023632
14   0.520386  7.728023  0.488037  8.251867
15   0.507935  7.928519  0.499146  8.072821
16   0.504150  7.989535  0.506592  7.952801
17   0.511719  7.867418  0.496704  8.112171
18   0.517090  7.781383  0.505249  7.974444
19   0.504883  7.977601  0.505737  7.966573
20   0.516724  7.787264  0.503418  8.003957
21   0.510132  7.892911  0.506348  7.956736
22   0.514526  7.822315  0.502563  8.017729
23   0.517822  7.769813  0.495239  8.135782
24   0.508545  7.918789  0.497925  8.092496
25   0.510010  7.895371  0.489136  8.234159
26   0.513306  7.842183  0.502197  8.023632
27   0.498413  8.082481  0.499146  8.072821
28   0.511230  7.875267  0.503052  8.009859
29   0.514648  7.820412  0.507202  7.942963
30   0.508301  7.922745  0.504761  7.982314
31   0.501343  8.034809  0.497314  8.102334
32   0.502319  8.019412  0.494019  8.155457
33   0.511597  7.870072  0.505249  7.974444
34   0.514282  7.826465  0.506104  7.960671
35   0.516846  7.784996  0.498413  8.084626
36   0.508911  7.912865  0.498657  8.080691
37   0.512939  7.848108  0.501831  8.029535
38   0.513062  7.846333  0.505005  7.978379
39   0.510742  7.883523  0.502930  8.011827
40   0.512207  7.859419  0.503784  7.998054
41   0.513184  7.844172  0.503296  8.005924
42   0.511597  7.868122  0.556519  7.070259
43   0.528931  7.530409  0.491577  8.105473
44   0.527466  7.551918  0.502808  7.926432
45   0.520020  7.672667  0.504517  7.899187
46   0.535645  7.420479  0.504517  7.899187
47   0.534424  7.440326  0.490356  8.124934
48   0.524048  7.607030  0.505249  7.887510
49   0.530396  7.505340  0.508179  7.840804
50   0.529663  7.517789  0.503418  7.916702
51   0.533325  7.458570  0.501831  7.942001
52   0.529175  7.524908  0.507568  7.850535
53   0.532593  7.470740  0.499146  7.984815
54   0.533691  7.452924  0.506836  7.862211
55   0.523438  7.616546  0.498779  7.990653
56   0.524292  7.602774  0.501831  7.942001
57   0.519165  7.684595  0.501465  7.947839
58   0.528442  7.536349  0.506592  7.866103
59   0.532593  7.469603  0.487305  8.173586
60   0.530029  7.511221  0.501709  7.943947
61   0.529541  7.519370  0.499023  7.986761
62   0.525269  7.587183  0.500122  7.969246
63   0.530640  7.501234  0.499268  7.982869
64   0.532837  7.467062  0.486816  8.181370
65   0.531982  7.480492  0.501221  7.951731
66   0.527344  7.555258  0.504272  7.903079
67   0.534668  7.437356  0.499878  7.973138
68   0.529785  7.515821  0.506714  7.864157
69   0.516357  7.729527  0.498657  7.992599
70   0.533203  7.460709  0.502319  7.934217
71   0.528564  7.534382  0.501831  7.942001
72   0.520264  7.668131  0.505737  7.879726
73   0.533203  7.460258  0.502808  7.926432
74   0.528931  7.528693  0.497925  8.004276
75   0.534424  7.441398  0.500610  7.961462
76   0.530396  7.504504  0.503052  7.922540
77   0.516968  7.719389  0.507568  7.850535
78   0.517700  7.708527  0.505249  7.887510
79   0.528687  7.532993  0.498657  7.992599
80   0.523438  7.617083  0.510132  7.809667
81   0.528076  7.542595  0.507812  7.846642
82   0.529541  7.519435  0.501587  7.945893
83   0.520630  7.661693  0.508789  7.831074
84   0.529053  7.527069  0.509033  7.827182
85   0.519043  7.687828  0.503906  7.908917
86   0.527588  7.550529  0.495972  8.035413
87   0.528564  7.535604  0.493896  8.068497
88   0.529907  7.513017  0.496094  8.033467
89   0.532227  7.477093  0.494873  8.052928
90   0.529419  7.521445  0.496826  8.021791
91   0.538330  7.378308  0.515015  7.731823
92   0.544067  7.287292  0.496704  8.023737
93   0.522217  7.636972  0.509399  7.821343
94   0.517944  7.704356  0.493286  8.078227
95   0.522339  7.634619  0.508423  7.836912
96   0.530884  7.497749  0.500122  7.969246
97   0.535278  7.428526  0.500732  7.959516
98   0.529053  7.527648  0.502075  7.938109
99   0.526855  7.561627  0.499146  7.984815
100  0.526245  7.571700  0.504761  7.895295
101  0.527100  7.558979  0.497314  8.014006
102  0.530884  7.497856  0.501709  7.943947
103  0.525757  7.579270  0.505371  7.885564
104  0.523682  7.612740  0.498535  7.994545
105  0.520752  7.659532  0.501953  7.940055
106  0.525879  7.578590  0.498413  7.996492
107  0.524048  7.607073  0.502197  7.936163
108  0.526123  7.573368  0.502075  7.938109
109  0.529297  7.522555  0.498291  7.998438
110  0.543457  7.297087  0.508789  7.831074
111  0.531250  7.492447  0.499634  7.977031
112  0.534668  7.437141  0.497925  8.004276
113  0.520142  7.669563  0.512573  7.770745
114  0.531128  7.494221  0.499756  7.975085
115  0.530762  7.500296  0.500122  7.969246
116  0.534180  7.444990  0.508789  7.831074
117  0.528809  7.531219  0.500977  7.955624
118  0.528931  7.529702  0.503174  7.920594
119  0.538574  7.373944  0.502319  7.934217
120  0.529907  7.513790  0.495483  8.043198
121  0.523438  7.617040  0.490601  8.121041
122  0.536865  7.401275  0.498047  8.002330
123  0.518555  7.694755  0.495117  8.049036
124  0.531006  7.495996  0.496826  8.021791
125  0.529663  7.518690  0.505249  7.887510
126  0.533936  7.448775  0.488159  8.159963
127  0.521362  7.649737  0.495117  8.049036

2018-06-08 16:39:48.864209 Finish.
Total elapsed time: 30:04:45.86.
