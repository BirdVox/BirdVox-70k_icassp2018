2018-06-07 10:35:05.660437: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.660580: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.660592: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:35:02.850632 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit07, unit10, unit01.
Validation set: unit02, unit03.
Test set: unit05.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.507202  7.859118  0.504395  7.901133
1    0.500610  7.964293  0.507324  7.854427
2    0.504639  7.899900  0.497559  8.010114
3    0.497314  8.016945  0.507690  7.848589
4    0.507568  7.853044  0.494873  8.052928
5    0.501465  7.950906  0.507690  7.848589
6    0.498901  7.991495  0.507446  7.852481
7    0.501953  7.943058  0.489014  8.146341
8    0.494019  8.068760  0.502319  7.934217
9    0.507446  7.855612  0.493774  8.070443
10   0.497559  8.012902  0.496094  8.033467
11   0.507812  7.849581  0.498901  7.988707
12   0.498535  7.997548  0.500000  7.971192
13   0.499634  7.979797  0.494141  8.064605
14   0.504517  7.902211  0.492065  8.097688
15   0.512207  7.778857  0.499268  7.982869
16   0.499390  7.983497  0.506470  7.868049
17   0.502319  7.937241  0.492798  8.086012
18   0.497192  8.018505  0.497192  8.015952
19   0.510010  7.814744  0.501953  7.940055
20   0.508057  7.845710  0.501099  7.953678
21   0.502319  7.936876  0.500122  7.969246
22   0.500122  7.971949  0.506226  7.871942
23   0.495605  8.044169  0.489136  8.144395
24   0.507202  7.859054  0.502808  7.926432
25   0.504272  7.905760  0.500854  7.957570
26   0.504639  7.900051  0.497070  8.017899
27   0.498047  8.004711  0.491577  8.105473
28   0.501465  7.950349  0.496826  8.021791
29   0.497070  8.020644  0.491455  8.107419
30   0.504028  7.909609  0.503662  7.912810
31   0.511230  7.795133  0.493164  8.080173
32   0.496582  8.028171  0.503052  7.922540
33   0.503906  7.911620  0.495605  8.041252
34   0.508423  7.839743  0.507080  7.858319
35   0.502930  7.927382  0.501465  7.947839
36   0.508301  7.842011  0.499390  7.980923
37   0.498657  7.995452  0.499878  7.973138
38   0.493774  8.073896  0.504272  7.903079
39   0.510010  7.814551  0.496338  8.029575
40   0.504028  7.909910  0.505615  7.881672
41   0.500122  7.972142  0.489990  8.130772
42   0.500732  7.962025  0.492920  8.084066
43   0.488647  8.154946  0.500244  7.967300
44   0.508667  7.836044  0.499268  7.982869
45   0.497314  8.016623  0.499512  7.978977
46   0.500000  7.973830  0.499634  7.977031
47   0.505737  7.882343  0.508057  7.842750
48   0.506470  7.870537  0.505005  7.891403
49   0.498657  7.995495  0.506714  7.864157
50   0.494263  8.065490  0.505005  7.891403
51   0.506104  7.876998  0.485718  8.198885
52   0.508301  7.841625  0.491699  8.103527
53   0.500977  7.958476  0.498047  8.002330
54   0.503662  7.915598  0.501221  7.951731
55   0.511475  7.790748  0.491699  8.103527
56   0.503296  7.921522  0.504028  7.906971
57   0.500732  7.962154  0.499390  7.980923
58   0.510254  7.810380  0.494019  8.066551
59   0.498291  8.001097  0.505737  7.879726
60   0.505981  7.878022  0.503418  7.916702
61   0.492432  8.094402  0.499268  7.982869
62   0.507812  7.849624  0.507812  7.846642
63   0.500610  7.963714  0.502441  7.932271
64   0.505371  7.888117  0.502319  7.934217
65   0.497681  8.011021  0.503662  7.912810
66   0.491821  8.104240  0.507690  7.848589
67   0.496338  8.032492  0.505615  7.881672
68   0.507446  7.855333  0.501465  7.947839
69   0.505981  7.878708  0.503296  7.918648
70   0.508423  7.839486  0.498169  8.000384
71   0.498535  7.997806  0.494385  8.060713
72   0.507568  7.853001  0.504395  7.901133
73   0.496826  8.024536  0.500366  7.965354
74   0.497803  8.008667  0.484619  8.216400
75   0.495239  8.049749  0.498047  8.002330
76   0.501099  7.956208  0.494629  8.056820
77   0.511475  7.791177  0.494141  8.064605
78   0.500610  7.964400  0.505249  7.887510
79   0.499756  7.977315  0.495483  8.043198
80   0.497437  8.014441  0.492065  8.097688
81   0.492432  8.094552  0.504761  7.895295
82   0.506836  7.864699  0.503540  7.914756
83   0.499756  7.977487  0.503418  7.916702
84   0.503784  7.913287  0.498413  7.996492
85   0.501465  7.950327  0.503784  7.910864
86   0.501831  7.944918  0.496460  8.027629
87   0.501221  7.954777  0.516602  7.706524
88   0.508789  7.834012  0.501831  7.942001
89   0.500854  7.960229  0.504028  7.906971
90   0.494507  8.061490  0.497437  8.012060
91   0.491455  8.110615  0.499268  7.982869
92   0.492798  8.088929  0.490967  8.115203
93   0.500610  7.963907  0.502686  7.928378
94   0.501587  7.948424  0.498779  7.990653
95   0.507446  7.855205  0.496948  8.019845
96   0.502197  7.939015  0.511841  7.782421
97   0.508423  7.839657  0.497559  8.010114
98   0.504517  7.901889  0.496704  8.023737
99   0.503662  7.915748  0.505371  7.885564
100  0.494629  8.059887  0.498413  7.996492
101  0.503906  7.912242  0.500244  7.967300
102  0.504639  7.899836  0.507812  7.846642
103  0.505249  7.890342  0.496094  8.033467
104  0.502808  7.929564  0.499268  7.982869
105  0.498535  7.997441  0.502319  7.934217
106  0.504517  7.901954  0.498901  7.988707
107  0.503540  7.917608  0.507935  7.844696
108  0.499756  7.977873  0.499878  7.973138
109  0.508911  7.832130  0.505249  7.887510
110  0.507568  7.853237  0.497559  8.010114
111  0.503784  7.913566  0.507690  7.848589
112  0.488159  8.163009  0.503540  7.914756
113  0.507935  7.847077  0.505371  7.885564
114  0.496338  8.032964  0.496094  8.033467
115  0.498291  8.001612  0.496338  8.029575
116  0.495972  8.038116  0.498291  7.998438
117  0.502319  7.936833  0.490234  8.126880
118  0.496704  8.027104  0.491943  8.099634
119  0.492676  8.090553  0.491577  8.105473
120  0.495117  8.051653  0.501343  7.949785
121  0.495972  8.038588  0.499756  7.975085
122  0.502075  7.940790  0.493652  8.072389
123  0.497192  8.018526  0.498291  7.998438
124  0.492432  8.094402  0.497925  8.004276
125  0.502441  7.935402  0.496094  8.033467
126  0.506592  7.868999  0.495361  8.045144
127  0.497925  8.007171  0.499634  7.977031

2018-06-08 16:48:54.983380 Finish.
Total elapsed time: 30:13:52.98.
