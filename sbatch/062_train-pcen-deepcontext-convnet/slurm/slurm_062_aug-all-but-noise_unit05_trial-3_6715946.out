2018-06-07 10:35:05.326014: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.326219: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.326231: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:35:02.468487 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit07, unit10, unit01.
Validation set: unit02, unit03.
Test set: unit05.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.927979  0.224791  0.899658  0.318320
1    0.941772  0.187395  0.898071  0.382429
2    0.937988  0.206276  0.913818  0.280764
3    0.943237  0.194426  0.911255  0.322664
4    0.942261  0.200384  0.902710  0.496455
5    0.945435  0.191710  0.926025  0.259126
6    0.950317  0.163298  0.914307  0.333431
7    0.947021  0.163827  0.898315  0.489217
8    0.954712  0.155336  0.913086  0.444502
9    0.953369  0.157858  0.921875  0.414371
10   0.953369  0.152588  0.931885  0.291266
11   0.950806  0.165943  0.922852  0.344117
12   0.958740  0.133210  0.923584  0.272828
13   0.956909  0.147003  0.903931  0.352244
14   0.954956  0.142053  0.937256  0.225069
15   0.954956  0.146992  0.895874  0.378785
16   0.958740  0.135047  0.938965  0.332791
17   0.962402  0.130469  0.937866  0.405953
18   0.963501  0.133820  0.934937  0.402244
19   0.953491  0.144505  0.944458  0.284333
20   0.957275  0.137682  0.911621  0.481664
21   0.960571  0.139072  0.941162  0.270613
22   0.960205  0.132821  0.937134  0.301263
23   0.961060  0.132571  0.948853  0.267792
24   0.961792  0.123754  0.932983  0.324569
25   0.963989  0.124334  0.939575  0.308840
26   0.956787  0.152379  0.925903  0.346781
27   0.958374  0.125657  0.932129  0.288682
28   0.964478  0.123352  0.933838  0.360944
29   0.960815  0.141312  0.942017  0.315643
30   0.960449  0.129603  0.954224  0.197166
31   0.963989  0.118007  0.953491  0.221982
32   0.966431  0.115496  0.916748  0.477990
33   0.953979  0.155069  0.891724  0.676384
34   0.952271  0.163517  0.897339  0.698164
35   0.955444  0.150070  0.940430  0.294527
36   0.964233  0.127029  0.947754  0.285037
37   0.961304  0.124849  0.943481  0.264008
38   0.962158  0.117484  0.943970  0.369342
39   0.961670  0.134233  0.945312  0.202568
40   0.960938  0.134457  0.919312  0.561328
41   0.963379  0.119504  0.932007  0.424089
42   0.962402  0.136200  0.914795  0.452161
43   0.962646  0.116670  0.949585  0.235323
44   0.967896  0.102651  0.944824  0.289445
45   0.960938  0.128888  0.910767  0.305531
46   0.959717  0.132493  0.953613  0.249762
47   0.962280  0.122769  0.949097  0.262929
48   0.965210  0.114611  0.948120  0.299593
49   0.968506  0.097451  0.943237  0.314011
50   0.965332  0.107943  0.933838  0.369998
51   0.969116  0.104016  0.952026  0.210619
52   0.962769  0.132488  0.939087  0.288368
53   0.961548  0.132025  0.944824  0.270647
54   0.968018  0.104610  0.945923  0.315611
55   0.961304  0.135762  0.949707  0.227614
56   0.964478  0.115735  0.933228  0.306781
57   0.959106  0.138597  0.939819  0.309985
58   0.965210  0.108989  0.933960  0.371562
59   0.965576  0.115075  0.953857  0.289226
60   0.969482  0.105658  0.930908  0.372203
61   0.966675  0.106920  0.939941  0.335452
62   0.967529  0.114457  0.949097  0.230930
63   0.965820  0.116898  0.939331  0.342468
64   0.963013  0.109809  0.937622  0.431256
65   0.968994  0.100915  0.946289  0.297855
66   0.968506  0.104252  0.955322  0.246693
67   0.966553  0.103868  0.931885  0.429889
68   0.967896  0.114790  0.957031  0.217826
69   0.967773  0.114763  0.949829  0.278057
70   0.969604  0.104180  0.948975  0.260730
71   0.955322  0.137817  0.959961  0.180630
72   0.953125  0.132470  0.951050  0.209546
73   0.955322  0.132082  0.952759  0.285179
74   0.957397  0.129496  0.947632  0.166417
75   0.958374  0.125087  0.955200  0.189742
76   0.964233  0.117700  0.936157  0.380037
77   0.963989  0.112830  0.952515  0.236829
78   0.954712  0.137428  0.954712  0.199303
79   0.960571  0.122976  0.954224  0.192370
80   0.961792  0.121377  0.950806  0.311001
81   0.960815  0.123345  0.950684  0.264054
82   0.967529  0.107890  0.949707  0.226713
83   0.961792  0.115459  0.948486  0.277539
84   0.963745  0.113781  0.945679  0.297865
85   0.960205  0.126557  0.946045  0.263122
86   0.962158  0.119820  0.953369  0.185152
87   0.963989  0.111803  0.950684  0.309328
88   0.952271  0.146402  0.955444  0.205877
89   0.965576  0.097111  0.950806  0.249279
90   0.965820  0.115774  0.936157  0.365716
91   0.962646  0.125991  0.956299  0.197469
92   0.957153  0.141603  0.929321  0.318667
93   0.963379  0.111764  0.946899  0.265264
94   0.963257  0.120382  0.953735  0.205981
95   0.962280  0.115491  0.946777  0.257691
96   0.964844  0.116529  0.954590  0.259835
97   0.966064  0.116993  0.950073  0.272715
98   0.963013  0.125249  0.960205  0.152285
99   0.964478  0.111529  0.958740  0.233890
100  0.958862  0.135386  0.935669  0.238455
101  0.962891  0.117381  0.962524  0.190908
102  0.963135  0.126381  0.951050  0.268159
103  0.962036  0.120181  0.948364  0.218627
104  0.963989  0.110687  0.947021  0.184790
105  0.963013  0.116628  0.951416  0.180132
106  0.964233  0.112357  0.959351  0.165589
107  0.967041  0.112690  0.949829  0.171351
108  0.960083  0.125202  0.960327  0.227111
109  0.966187  0.103005  0.954346  0.184483
110  0.966309  0.120255  0.958862  0.191619
111  0.969727  0.106352  0.956787  0.199944
112  0.965820  0.111946  0.954346  0.161228
113  0.961426  0.135306  0.947632  0.242845
114  0.962158  0.125165  0.956177  0.241085
115  0.963745  0.118626  0.950317  0.322890
116  0.963501  0.117498  0.956787  0.245087
117  0.963379  0.107389  0.954346  0.188617
118  0.964600  0.115846  0.964111  0.160712
119  0.967529  0.108069  0.962769  0.188299
120  0.969849  0.104826  0.950562  0.215165
121  0.971558  0.094771  0.956055  0.238037
122  0.968262  0.103994  0.952393  0.186475
123  0.967529  0.108635  0.948486  0.197448
124  0.968994  0.096280  0.944580  0.251491
125  0.968872  0.103614  0.956299  0.204675
126  0.970337  0.096548  0.953979  0.238591
127  0.963501  0.124381  0.941284  0.379268

2018-06-08 16:51:03.034368 Finish.
Total elapsed time: 30:16:01.03.
