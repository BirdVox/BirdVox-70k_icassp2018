2018-06-07 10:35:05.555499: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.555713: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.555736: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:35:03.227619 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit10, unit01, unit02.
Validation set: unit03, unit05.
Test set: unit07.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.510864  7.883937  0.505981  7.962638
1    0.494873  8.141685  0.502197  8.023632
2    0.502319  8.021665  0.488525  8.243997
3    0.505493  7.970508  0.502930  8.011827
4    0.496582  8.114139  0.490356  8.214484
5    0.494263  8.151522  0.505981  7.962638
6    0.505371  7.972476  0.497070  8.106269
7    0.511719  7.870164  0.497070  8.106269
8    0.507446  7.939028  0.498779  8.078723
9    0.500366  8.053145  0.498901  8.076756
10   0.503662  8.000022  0.500488  8.051178
11   0.506592  7.952801  0.485474  8.293185
12   0.500366  8.053145  0.500977  8.043307
13   0.504395  7.988216  0.502075  8.025600
14   0.511597  7.872131  0.505981  7.962638
15   0.495605  8.129879  0.505371  7.972476
16   0.503662  8.000022  0.499512  8.066918
17   0.506592  7.952801  0.499268  8.070853
18   0.504639  7.984281  0.502197  8.023632
19   0.505737  7.966573  0.484131  8.314828
20   0.497559  8.098399  0.500244  8.055113
21   0.500610  8.049210  0.497314  8.102334
22   0.497559  8.098399  0.495972  8.123977
23   0.494263  8.151522  0.498291  8.086593
24   0.506104  7.960671  0.493286  8.167263
25   0.502319  8.021665  0.506714  7.950833
26   0.501953  8.027567  0.501953  8.027567
27   0.509033  7.913450  0.507568  7.937060
28   0.495361  8.133814  0.494873  8.141685
29   0.506470  7.954768  0.495972  8.123977
30   0.499512  8.066918  0.501831  8.029535
31   0.488525  8.243997  0.504028  7.994119
32   0.491699  8.192841  0.499023  8.074788
33   0.493896  8.157425  0.504272  7.990184
34   0.494873  8.141685  0.501587  8.033470
35   0.501587  8.033470  0.502808  8.013794
36   0.496582  8.114139  0.494385  8.149555
37   0.508667  7.919352  0.502319  8.021665
38   0.515015  7.817040  0.503296  8.005924
39   0.495728  8.127912  0.500732  8.047243
40   0.488159  8.249899  0.503296  8.005924
41   0.505493  7.970508  0.488037  8.251867
42   0.504761  7.982314  0.497437  8.100366
43   0.491943  8.188905  0.507202  7.942963
44   0.501343  8.037405  0.502075  8.025600
45   0.507446  7.939028  0.503418  8.003957
46   0.501831  8.029535  0.499146  8.072821
47   0.503784  7.998054  0.496460  8.116106
48   0.500854  8.045275  0.494629  8.145620
49   0.507446  7.939028  0.495483  8.131847
50   0.494751  8.143652  0.498657  8.080691
51   0.498657  8.080691  0.497314  8.102334
52   0.492920  8.173165  0.497925  8.092496
53   0.497681  8.096431  0.500732  8.047243
54   0.496216  8.120042  0.507446  7.939028
55   0.510742  7.885904  0.497070  8.106269
56   0.502930  8.011827  0.497437  8.100366
57   0.502686  8.015762  0.507690  7.935093
58   0.506226  7.958703  0.503418  8.003957
59   0.497559  8.098399  0.508911  7.915417
60   0.496582  8.114139  0.500366  8.053145
61   0.496216  8.120042  0.490845  8.206613
62   0.501587  8.033470  0.490601  8.210548
63   0.495239  8.135782  0.495117  8.137749
64   0.500488  8.051178  0.505249  7.974444
65   0.500122  8.057080  0.501099  8.041340
66   0.503906  7.996086  0.498535  8.082658
67   0.496582  8.114139  0.506104  7.960671
68   0.499878  8.061015  0.504517  7.986249
69   0.500854  8.045275  0.488281  8.247932
70   0.501587  8.033470  0.504150  7.992151
71   0.500732  8.047243  0.495117  8.137749
72   0.513306  7.844586  0.497314  8.102334
73   0.493042  8.171198  0.503174  8.007892
74   0.500732  8.047243  0.498901  8.076756
75   0.504028  7.994119  0.499268  8.070853
76   0.500732  8.047243  0.490234  8.216451
77   0.497192  8.104301  0.507568  7.937060
78   0.502563  8.017729  0.496704  8.112171
79   0.497437  8.100366  0.502075  8.025600
80   0.497070  8.106269  0.506104  7.960671
81   0.511230  7.878034  0.507080  7.944930
82   0.490723  8.208581  0.490234  8.216451
83   0.500122  8.057080  0.509399  7.907547
84   0.503662  8.000022  0.495239  8.135782
85   0.506714  7.950833  0.499756  8.062983
86   0.488770  8.240062  0.493164  8.169230
87   0.485962  8.285315  0.502930  8.011827
88   0.504395  7.988216  0.504883  7.980346
89   0.491821  8.190873  0.496704  8.112171
90   0.502319  8.021665  0.496338  8.118074
91   0.501953  8.027567  0.499146  8.072821
92   0.504272  7.990184  0.488525  8.243997
93   0.502075  8.025600  0.492676  8.177100
94   0.498901  8.076756  0.503540  8.001989
95   0.505249  7.974444  0.502441  8.019697
96   0.498779  8.078723  0.499634  8.064950
97   0.497437  8.100366  0.496460  8.116106
98   0.501465  8.035437  0.499512  8.066918
99   0.504028  7.994119  0.511353  7.876066
100  0.498413  8.084626  0.505615  7.968541
101  0.487793  8.255802  0.499512  8.066918
102  0.492920  8.173165  0.495361  8.133814
103  0.507080  7.944930  0.496460  8.116106
104  0.501465  8.035437  0.497314  8.102334
105  0.494019  8.155457  0.498413  8.084626
106  0.490967  8.204646  0.486206  8.281380
107  0.497192  8.104301  0.505371  7.972476
108  0.505615  7.968541  0.494873  8.141685
109  0.504639  7.984281  0.496704  8.112171
110  0.491577  8.194808  0.494629  8.145620
111  0.500732  8.047243  0.498901  8.076756
112  0.493896  8.157425  0.494019  8.155457
113  0.497070  8.106269  0.503906  7.996086
114  0.503418  8.003957  0.504639  7.984281
115  0.505005  7.978379  0.499268  8.070853
116  0.487793  8.255802  0.504883  7.980346
117  0.503662  8.000022  0.510620  7.887872
118  0.506226  7.958703  0.496338  8.118074
119  0.501709  8.031502  0.499756  8.062983
120  0.491943  8.188905  0.505981  7.962638
121  0.493652  8.161360  0.492920  8.173165
122  0.495850  8.125944  0.503418  8.003957
123  0.493530  8.163327  0.504150  7.992151
124  0.488892  8.238094  0.506836  7.948865
125  0.496338  8.118074  0.496094  8.122009
126  0.505249  7.974444  0.509399  7.907547
127  0.494507  8.147587  0.498901  8.076756

2018-06-08 16:47:06.086828 Finish.
Total elapsed time: 30:12:03.09.
