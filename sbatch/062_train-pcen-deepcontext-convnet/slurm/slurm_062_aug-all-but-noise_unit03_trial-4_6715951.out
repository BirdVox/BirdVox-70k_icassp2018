2018-06-07 10:35:05.865049: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.865269: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.865282: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:35:02.850558 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.518066  7.764120  0.497925  8.092496
1    0.525146  7.650152  0.497559  8.098399
2    0.523560  7.675966  0.491455  8.196776
3    0.525391  7.645874  0.495117  8.137749
4    0.516113  7.795429  0.495728  8.127912
5    0.515869  7.799728  0.495972  8.123977
6    0.519653  7.738970  0.500244  8.055113
7    0.522827  7.688394  0.503052  8.009859
8    0.514404  7.823682  0.504395  7.988216
9    0.527710  7.609906  0.503174  8.007892
10   0.515137  7.811662  0.513550  7.840651
11   0.518066  7.764763  0.499878  8.061015
12   0.520996  7.717478  0.510498  7.889839
13   0.511963  7.862475  0.510986  7.881969
14   0.519653  7.738970  0.508545  7.921320
15   0.527588  7.611252  0.497070  8.106269
16   0.521240  7.714143  0.502686  8.015762
17   0.525513  7.644614  0.505249  7.974443
18   0.524902  7.654881  0.506470  7.954768
19   0.521240  7.713779  0.500000  8.059048
20   0.522705  7.689546  0.504272  7.990184
21   0.524902  7.654173  0.508179  7.927223
22   0.518677  7.755011  0.493774  8.159392
23   0.518677  7.754582  0.499756  8.062983
24   0.509399  7.904201  0.499268  8.070853
25   0.522949  7.685976  0.500244  8.055113
26   0.524536  7.660226  0.507324  7.940995
27   0.528320  7.599275  0.490967  8.204646
28   0.519653  7.738627  0.484253  8.312861
29   0.516968  7.782042  0.502319  8.021665
30   0.531616  7.546001  0.499512  8.066918
31   0.516602  7.788159  0.505493  7.970508
32   0.515503  7.805674  0.492554  8.179068
33   0.532471  7.531928  0.501709  8.031502
34   0.521729  7.705994  0.493652  8.161360
35   0.531494  7.547754  0.501465  8.035437
36   0.515625  7.804178  0.509888  7.899677
37   0.527344  7.615166  0.502441  8.019697
38   0.525269  7.648850  0.498779  8.078723
39   0.518066  7.764398  0.500854  8.045275
40   0.515625  7.803706  0.496826  8.110204
41   0.521118  7.715145  0.499512  8.066918
42   0.530396  7.566041  0.495117  8.137749
43   0.516602  7.788309  0.497681  8.096431
44   0.518921  7.750861  0.498779  8.078723
45   0.519287  7.745409  0.495605  8.129879
46   0.521729  7.705865  0.491577  8.194808
47   0.524170  7.666043  0.495605  8.129879
48   0.513184  7.843250  0.502197  8.023632
49   0.507202  7.939960  0.496826  8.110204
50   0.520752  7.721241  0.497925  8.092496
51   0.521118  7.715274  0.498779  8.078723
52   0.517456  7.774064  0.494873  8.141684
53   0.526123  7.634197  0.494141  8.153490
54   0.515503  7.806017  0.502563  8.017729
55   0.519409  7.743184  0.499268  8.070853
56   0.511353  7.872463  0.495605  8.129879
57   0.524048  7.668396  0.501953  8.027567
58   0.523804  7.672267  0.497681  8.096431
59   0.521606  7.707597  0.501953  8.027567
60   0.522339  7.695470  0.502563  8.017729
61   0.522583  7.691814  0.494141  8.153490
62   0.521484  7.709650  0.497803  8.094463
63   0.522217  7.697588  0.497437  8.100366
64   0.516113  7.796051  0.495728  8.127912
65   0.522705  7.690254  0.509033  7.913450
66   0.515991  7.798104  0.498291  8.086593
67   0.520386  7.727251  0.497070  8.106269
68   0.518799  7.752915  0.508911  7.915417
69   0.510864  7.880462  0.500000  8.059048
70   0.526001  7.636530  0.502808  8.013794
71   0.520020  7.732832  0.496216  8.120042
72   0.504517  7.983160  0.502075  8.025600
73   0.525879  7.601333  0.499390  7.980923
74   0.522949  7.624845  0.498657  7.992599
75   0.525513  7.583420  0.496704  8.023737
76   0.522949  7.624502  0.502686  7.928378
77   0.531738  7.484019  0.507080  7.858319
78   0.516724  7.724375  0.508667  7.833020
79   0.516479  7.727838  0.492676  8.087958
80   0.514282  7.763704  0.496216  8.031521
81   0.524536  7.598195  0.504028  7.906971
82   0.521729  7.643277  0.510620  7.801882
83   0.521729  7.645014  0.499146  7.984815
84   0.526733  7.563444  0.498535  7.994545
85   0.518799  7.691098  0.493652  8.072389
86   0.521362  7.649115  0.500122  7.969246
87   0.523438  7.617319  0.500610  7.961462
88   0.517090  7.718623  0.499512  7.978977
89   0.516479  7.727817  0.506714  7.864157
90   0.514038  7.765924  0.499634  7.977031
91   0.531982  7.480685  0.488281  8.158017
92   0.526001  7.575593  0.504395  7.901133
93   0.531860  7.481901  0.507446  7.852481
94   0.515991  7.735601  0.501099  7.953678
95   0.515137  7.749546  0.491943  8.099634
96   0.530518  7.504317  0.494995  8.050982
97   0.513306  7.778544  0.500854  7.957570
98   0.519409  7.681325  0.508911  7.829128
99   0.519287  7.683078  0.504639  7.897241
100  0.511963  7.799887  0.506104  7.873888
101  0.519409  7.680960  0.485840  8.196939
102  0.523438  7.616804  0.502197  7.936163
103  0.521729  7.645550  0.504272  7.903079
104  0.524536  7.598581  0.503662  7.912810
105  0.522339  7.634254  0.508545  7.834966
106  0.529175  7.525402  0.490601  8.121041
107  0.527954  7.544520  0.497803  8.006222
108  0.523560  7.614364  0.501709  7.943947
109  0.518677  7.692916  0.494751  8.054874
110  0.520874  7.657929  0.505371  7.885564
111  0.527954  7.545099  0.506592  7.866103
112  0.525391  7.587039  0.497437  8.012060
113  0.529785  7.515392  0.501465  7.947839
114  0.522339  7.635112  0.495483  8.043198
115  0.524902  7.593408  0.499634  7.977031
116  0.524048  7.607095  0.502686  7.928378
117  0.520630  7.661950  0.496826  8.021791
118  0.517700  7.709149  0.502808  7.926432
119  0.525513  7.583870  0.498901  7.988707
120  0.520264  7.666930  0.506958  7.860265
121  0.528442  7.536714  0.495605  8.041252
122  0.526245  7.571593  0.497681  8.008168
123  0.519653  7.677433  0.500366  7.965354
124  0.514282  7.764026  0.494873  8.052928
125  0.526123  7.573153  0.484863  8.212508
126  0.519653  7.677025  0.495239  8.047090
127  0.518188  7.700636  0.504517  7.899187

2018-06-08 16:48:37.928296 Finish.
Total elapsed time: 30:13:35.93.
