2018-06-07 10:35:00.504561: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:00.504798: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:00.504810: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:34:52.198300 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.495483  8.131847  0.505371  7.972476
1    0.499390  8.068885  0.492065  8.186938
2    0.494873  8.131123  0.510986  7.796044
3    0.504517  7.907466  0.502441  7.932271
4    0.502197  7.944571  0.509277  7.823289
5    0.508545  7.843202  0.502197  7.936163
6    0.496460  8.031051  0.497192  8.015952
7    0.488037  8.162017  0.507202  7.856373
8    0.498169  8.000534  0.491333  8.109365
9    0.519775  7.656033  0.498657  7.992599
10   0.493896  8.068690  0.507812  7.846642
11   0.500732  7.959602  0.496094  8.033467
12   0.488281  8.158103  0.503052  7.922540
13   0.502441  7.932313  0.503418  7.916702
14   0.511475  7.788367  0.493896  8.068497
15   0.490356  8.125062  0.499634  7.977031
16   0.506348  7.870167  0.510010  7.811613
17   0.499146  7.984901  0.494629  8.056820
18   0.500610  7.961591  0.505981  7.875834
19   0.511353  7.790292  0.506104  7.873888
20   0.511475  7.788388  0.498901  7.988707
21   0.502686  7.928528  0.505127  7.889457
22   0.494629  8.056885  0.499268  7.982869
23   0.500366  7.965504  0.495850  8.037359
24   0.501221  7.951903  0.502075  7.938109
25   0.502075  7.938173  0.500977  7.955624
26   0.513428  7.757294  0.507812  7.846642
27   0.488037  8.162059  0.492065  8.097688
28   0.497681  8.008318  0.497925  8.004276
29   0.499390  7.981009  0.501587  7.945893
30   0.500610  7.961633  0.497681  8.008168
31   0.496338  8.029597  0.498291  7.998438
32   0.489014  8.146469  0.497192  8.015952
33   0.499390  7.981073  0.504883  7.893349
34   0.495117  8.049100  0.507202  7.856373
35   0.500000  7.971321  0.498047  8.002330
36   0.492310  8.093946  0.500610  7.961462
37   0.494141  8.064712  0.491577  8.105473
38   0.502197  7.936248  0.506104  7.873888
39   0.497559  8.010221  0.500488  7.963408
40   0.509399  7.821472  0.504517  7.899187
41   0.505127  7.889607  0.500122  7.969246
42   0.497803  8.006329  0.506836  7.862211
43   0.508301  7.838880  0.499878  7.973138
44   0.507812  7.846771  0.497803  8.006222
45   0.502075  7.938195  0.512451  7.772691
46   0.501587  7.946043  0.492310  8.093796
47   0.496582  8.025726  0.496216  8.031521
48   0.503906  7.908960  0.499268  7.982869
49   0.505859  7.877909  0.503174  7.920594
50   0.496826  8.021898  0.495361  8.045144
51   0.507568  7.850620  0.494629  8.056820
52   0.497925  8.004447  0.508545  7.834966
53   0.501465  7.947904  0.492798  8.086012
54   0.501587  7.945979  0.497925  8.004276
55   0.495117  8.049250  0.501465  7.947839
56   0.511841  7.782507  0.506592  7.866103
57   0.497070  8.018027  0.493652  8.072389
58   0.504517  7.899337  0.498169  8.000384
59   0.498535  7.994738  0.493652  8.072389
60   0.499390  7.981116  0.504639  7.897241
61   0.498779  7.990803  0.496826  8.021791
62   0.505493  7.883704  0.496826  8.021791
63   0.492920  8.084130  0.512573  7.770745
64   0.506592  7.866232  0.505981  7.875834
65   0.504639  7.897391  0.492188  8.095742
66   0.502197  7.936356  0.501221  7.951731
67   0.487061  8.177778  0.506958  7.860265
68   0.503418  7.916830  0.490601  8.121041
69   0.500122  7.969311  0.498779  7.990653
70   0.505127  7.889521  0.496826  8.021791
71   0.511230  7.792238  0.501831  7.942001
72   0.504639  7.897348  0.507812  7.846642
73   0.500854  7.957677  0.490234  8.126880
74   0.495361  8.045294  0.494751  8.054874
75   0.491943  8.099720  0.491089  8.113257
76   0.500854  7.957806  0.510376  7.805775
77   0.493408  8.076367  0.489746  8.134664
78   0.502686  7.928507  0.503296  7.918648
79   0.492310  8.093946  0.488037  8.161909
80   0.489624  8.136739  0.500732  7.959516
81   0.501831  7.942151  0.506104  7.873888
82   0.496826  8.021855  0.498047  8.002330
83   0.506714  7.864200  0.502441  7.932271
84   0.497925  8.004340  0.503662  7.912810
85   0.499268  7.982976  0.500244  7.967300
86   0.501221  7.951924  0.495850  8.037359
87   0.501953  7.940162  0.503174  7.920594
88   0.504761  7.895381  0.508057  7.842750
89   0.492310  8.093989  0.501953  7.940055
90   0.504395  7.901348  0.497437  8.012060
91   0.503174  7.920723  0.499390  7.980923
92   0.500977  7.955774  0.493652  8.072389
93   0.506226  7.872049  0.493530  8.074335
94   0.504639  7.897370  0.498535  7.994545
95   0.508911  7.829256  0.507935  7.844696
96   0.494019  8.066722  0.500610  7.961462
97   0.485229  8.206755  0.502563  7.930324
98   0.502563  7.930410  0.507202  7.856373
99   0.507202  7.856523  0.490845  8.117149
100  0.507690  7.848717  0.508423  7.836912
101  0.502197  7.936248  0.496582  8.025683
102  0.494141  8.064755  0.507690  7.848589
103  0.491089  8.113429  0.493896  8.068497
104  0.494873  8.053143  0.496338  8.029575
105  0.503540  7.914820  0.503906  7.908917
106  0.497559  8.010264  0.507080  7.858319
107  0.501343  7.949850  0.498291  7.998438
108  0.490601  8.121192  0.505615  7.881672
109  0.501221  7.951882  0.503540  7.914756
110  0.500122  7.969375  0.504761  7.895295
111  0.508301  7.838922  0.503906  7.908917
112  0.504761  7.895423  0.501099  7.953678
113  0.500122  7.969418  0.509277  7.823289
114  0.504150  7.905197  0.489014  8.146341
115  0.503418  7.916830  0.505737  7.879726
116  0.499634  7.977181  0.494751  8.054874
117  0.503906  7.908982  0.504517  7.899187
118  0.493286  8.078249  0.502197  7.936163
119  0.502686  7.928464  0.494507  8.058766
120  0.500854  7.957677  0.508057  7.842750
121  0.509399  7.821472  0.493530  8.074335
122  0.496460  8.027779  0.501953  7.940055
123  0.497803  8.006329  0.502808  7.926432
124  0.507690  7.848631  0.499268  7.982869
125  0.494751  8.055046  0.498413  7.996492
126  0.494141  8.064755  0.493164  8.080173
127  0.494629  8.056949  0.488892  8.148287

2018-06-08 15:40:25.925122 Finish.
Total elapsed time: 29:05:33.93.
