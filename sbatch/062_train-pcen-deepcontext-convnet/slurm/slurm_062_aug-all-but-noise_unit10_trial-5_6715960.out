2018-06-07 10:35:12.262376: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:12.262595: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:12.262607: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:35:02.142988 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit01, unit02, unit03.
Validation set: unit05, unit07.
Test set: unit10.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.502808  8.012550  0.497925  8.092496
1    0.499390  8.067963  0.501953  8.027567
2    0.504395  7.987401  0.495117  8.137749
3    0.502808  8.012765  0.489624  8.226289
4    0.498047  8.088528  0.497803  8.094464
5    0.524658  7.600346  0.508667  7.833020
6    0.525757  7.581587  0.507568  7.850556
7    0.522583  7.633172  0.499268  7.982912
8    0.523560  7.616724  0.500000  7.971235
9    0.518066  7.704984  0.499512  7.978977
10   0.520752  7.661999  0.508057  7.842750
11   0.517090  7.719974  0.507446  7.852481
12   0.526978  7.562190  0.509399  7.821343
13   0.518921  7.691083  0.500122  7.969246
14   0.515747  7.741917  0.502319  7.934217
15   0.518188  7.702781  0.506226  7.871942
16   0.515747  7.742003  0.507446  7.852481
17   0.507568  7.870868  0.508423  7.836912
18   0.526733  7.565696  0.508545  7.834966
19   0.519897  7.674935  0.511230  7.792173
20   0.518677  7.694181  0.509033  7.827182
21   0.519897  7.676479  0.505859  7.877780
22   0.514893  7.755218  0.504272  7.903079
23   0.511108  7.796278  0.512817  7.766853
24   0.499268  7.982912  0.492554  8.089904
25   0.504028  7.906993  0.500366  7.965354
26   0.497559  8.010243  0.498901  7.988707
27   0.507202  7.856373  0.497559  8.010114
28   0.497559  8.010264  0.503540  7.914756
29   0.501465  7.947968  0.493042  8.082120
30   0.505127  7.889521  0.493286  8.078227
31   0.505249  7.887661  0.511108  7.794098
32   0.499268  7.983062  0.500122  7.969246
33   0.495239  8.047154  0.499634  7.977031
34   0.495239  8.047133  0.497314  8.014006
35   0.496460  8.027715  0.504150  7.905025
36   0.500732  7.959602  0.504639  7.897241
37   0.506592  7.866232  0.495361  8.045144
38   0.495361  8.045230  0.497070  8.017899
39   0.509888  7.813709  0.500244  7.967300
40   0.499390  7.981051  0.501465  7.947839
41   0.506470  7.868178  0.493530  8.074335
42   0.504639  7.897391  0.503174  7.920594
43   0.508057  7.842815  0.497437  8.012060
44   0.500977  7.955752  0.501221  7.951731
45   0.500977  7.955667  0.498169  8.000384
46   0.497192  8.016038  0.492676  8.087958
47   0.500610  7.961526  0.496216  8.031521
48   0.492676  8.088087  0.497314  8.014006
49   0.498657  7.992685  0.500488  7.963408
50   0.504395  7.901326  0.504395  7.901133
51   0.504395  7.901197  0.503296  7.918648
52   0.506836  7.862340  0.498901  7.988707
53   0.507690  7.848674  0.500732  7.959516
54   0.497192  8.015995  0.504883  7.893349
55   0.496460  8.027715  0.496826  8.021791
56   0.507690  7.848674  0.500244  7.967300
57   0.494629  8.056906  0.495605  8.041252
58   0.499512  7.979084  0.500122  7.969246
59   0.501343  7.949850  0.500122  7.969246
60   0.497681  8.008275  0.486694  8.183316
61   0.507080  7.858383  0.495239  8.047090
62   0.499023  7.986847  0.497192  8.015952
63   0.504150  7.905132  0.503418  7.916702
64   0.490112  8.128912  0.504517  7.899187
65   0.498535  7.994653  0.496216  8.031521
66   0.491333  8.109429  0.495728  8.039306
67   0.498169  8.000534  0.498779  7.990653
68   0.505005  7.891467  0.501831  7.942001
69   0.495361  8.045294  0.502441  7.932271
70   0.496704  8.023758  0.501465  7.947839
71   0.490112  8.128890  0.501465  7.947839
72   0.507202  7.856523  0.500854  7.957570
73   0.495117  8.049143  0.504883  7.893349
74   0.502930  7.924529  0.493408  8.076281
75   0.491455  8.107483  0.513306  7.759068
76   0.499756  7.975170  0.500977  7.955624
77   0.507935  7.844889  0.496582  8.025683
78   0.495483  8.043305  0.492676  8.087958
79   0.503662  7.912938  0.493896  8.068497
80   0.507324  7.854620  0.503296  7.918648
81   0.488403  8.156157  0.495972  8.035413
82   0.496826  8.021855  0.510376  7.805775
83   0.507446  7.852609  0.500610  7.961462
84   0.505615  7.881801  0.490234  8.126880
85   0.506958  7.860372  0.495483  8.043198
86   0.501587  7.946022  0.493896  8.068497
87   0.502319  7.934367  0.496216  8.031521
88   0.488770  8.150254  0.503174  7.920594
89   0.507324  7.854491  0.515869  7.718200
90   0.500366  7.965504  0.501953  7.940055
91   0.500732  7.959537  0.497925  8.004276
92   0.503540  7.914820  0.496338  8.029575
93   0.508789  7.831138  0.503784  7.910864
94   0.506470  7.868092  0.509155  7.825235
95   0.494751  8.054960  0.505249  7.887510
96   0.498901  7.988857  0.500122  7.969246
97   0.497925  8.004297  0.502075  7.938109
98   0.502930  7.924593  0.494873  8.052928
99   0.494385  8.060841  0.506348  7.869996
100  0.498779  7.990803  0.508301  7.838858
101  0.494507  8.058852  0.500488  7.963408
102  0.498779  7.990739  0.484863  8.212508
103  0.496704  8.023801  0.504272  7.903079
104  0.492920  8.084087  0.506104  7.873888
105  0.507324  7.854513  0.498901  7.988707
106  0.493164  8.080281  0.502197  7.936163
107  0.501099  7.953806  0.498901  7.988707
108  0.507690  7.848653  0.506226  7.871942
109  0.498535  7.994674  0.508667  7.833020
110  0.505493  7.883704  0.504639  7.897241
111  0.504272  7.903186  0.497559  8.010114
112  0.507690  7.848696  0.492310  8.093796
113  0.498657  7.992685  0.495483  8.043198
114  0.503296  7.918712  0.492798  8.086012
115  0.502197  7.936313  0.496460  8.027629
116  0.496826  8.021898  0.498901  7.988707
117  0.500732  7.959602  0.490601  8.121041
118  0.500732  7.959580  0.499756  7.975085
119  0.493896  8.068669  0.502930  7.924486
120  0.509766  7.815634  0.492798  8.086012
121  0.499390  7.981030  0.504272  7.903079
122  0.495850  8.037488  0.494019  8.066551
123  0.512207  7.776712  0.499146  7.984815
124  0.497925  8.004340  0.505127  7.889457
125  0.490845  8.117214  0.510132  7.809667
126  0.508301  7.838987  0.499268  7.982869
127  0.493042  8.082141  0.494263  8.062659

2018-06-08 17:22:37.283867 Finish.
Total elapsed time: 30:47:35.28.
