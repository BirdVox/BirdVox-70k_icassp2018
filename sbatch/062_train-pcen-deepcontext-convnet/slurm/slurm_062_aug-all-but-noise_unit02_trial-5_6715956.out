2018-06-07 10:35:05.935480: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.935724: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.935737: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:35:02.805081 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.967529  0.113866  0.788574  1.671760
1    0.967407  0.118932  0.765137  2.068527
2    0.964722  0.121126  0.849976  0.815575
3    0.958740  0.254096  0.496338  8.116712
4    0.499390  8.068885  0.498169  8.088561
5    0.495728  8.127912  0.504639  7.984281
6    0.510010  7.897709  0.504395  7.988216
7    0.498901  8.076756  0.504761  7.982314
8    0.500854  8.045275  0.493530  8.163327
9    0.499390  8.068885  0.512939  7.850488
10   0.500977  8.043307  0.496094  8.122009
11   0.494629  8.145620  0.505249  7.974444
12   0.494141  8.153490  0.503174  8.007892
13   0.498657  8.080691  0.505005  7.978379
14   0.504272  7.990184  0.510498  7.889839
15   0.496582  8.114139  0.503052  8.009859
16   0.499634  8.064950  0.502563  8.017729
17   0.489990  8.220386  0.510864  7.883937
18   0.489258  8.232191  0.497437  8.100366
19   0.494385  8.149555  0.499023  8.074788
20   0.501465  8.035437  0.491577  8.194808
21   0.501587  8.033470  0.503296  8.005924
22   0.492310  8.183003  0.493408  8.165295
23   0.496460  8.116106  0.501221  8.039372
24   0.501709  8.031502  0.505127  7.976411
25   0.494995  8.139717  0.493164  8.169230
26   0.498291  8.086593  0.504761  7.982314
27   0.489990  8.220386  0.498535  8.082658
28   0.499268  8.070853  0.504272  7.990184
29   0.499634  8.064950  0.490601  8.210548
30   0.487671  8.257769  0.495972  8.123977
31   0.502197  8.023632  0.498779  8.078723
32   0.496582  8.114117  0.491089  8.202678
33   0.504150  7.992151  0.506592  7.952801
34   0.502686  8.015762  0.497803  8.094464
35   0.502930  8.011805  0.497437  8.100366
36   0.502808  8.013794  0.504761  7.982314
37   0.498047  8.090507  0.493286  8.167263
38   0.507324  7.940995  0.503906  7.996086
39   0.491699  8.192841  0.500122  8.057080
40   0.507202  7.942963  0.502197  8.023632
41   0.504883  7.980325  0.503174  8.007892
42   0.513062  7.848521  0.502319  8.021665
43   0.504883  7.980346  0.508789  7.917385
44   0.492432  8.181035  0.505005  7.978379
45   0.496460  8.116106  0.482422  8.342374
46   0.505371  7.972476  0.503906  7.996087
47   0.501831  8.029535  0.494019  8.155457
48   0.490112  8.218419  0.502808  8.013794
49   0.508179  7.927223  0.501343  8.037405
50   0.521484  7.703246  0.502563  7.930324
51   0.509399  7.821429  0.506226  7.871942
52   0.508423  7.837105  0.490723  8.119095
53   0.503784  7.911099  0.507935  7.844696
54   0.503052  7.922690  0.494751  8.054874
55   0.503906  7.908982  0.502075  7.938109
56   0.502563  7.930453  0.500610  7.961462
57   0.506958  7.860437  0.498047  8.002330
58   0.509888  7.813645  0.495972  8.035413
59   0.510986  7.796259  0.503418  7.916702
60   0.514771  7.735994  0.503662  7.912810
61   0.513428  7.757251  0.507202  7.856373
62   0.505005  7.891488  0.506470  7.868049
63   0.502930  7.924615  0.496338  8.029575
64   0.505981  7.875984  0.495605  8.041252
65   0.506348  7.870060  0.494751  8.054874
66   0.516357  7.710502  0.492676  8.087958
67   0.511963  7.780711  0.494873  8.052928
68   0.503906  7.909068  0.507690  7.848589
69   0.502075  7.938195  0.497070  8.017899
70   0.506958  7.860501  0.499512  7.978977
71   0.497070  8.018070  0.501709  7.943947
72   0.513306  7.759176  0.500122  7.969246
73   0.503296  7.918819  0.503540  7.914756
74   0.505981  7.875877  0.503784  7.910864
75   0.509399  7.821472  0.500854  7.957570
76   0.506958  7.860308  0.509888  7.813559
77   0.512207  7.776669  0.499023  7.986761
78   0.509399  7.821493  0.505005  7.891403
79   0.514648  7.737919  0.508179  7.840804
80   0.500610  7.961655  0.502197  7.936163
81   0.498657  7.992685  0.494995  8.050982
82   0.502563  7.930432  0.504761  7.895295
83   0.507812  7.846878  0.500610  7.961462
84   0.505615  7.881865  0.502319  7.934217
85   0.513184  7.761250  0.498901  7.988707
86   0.507324  7.854556  0.506714  7.864157
87   0.518188  7.681546  0.497925  8.004276
88   0.509277  7.823461  0.504883  7.893349
89   0.507568  7.850771  0.504883  7.893349
90   0.498901  7.988879  0.503174  7.920594
91   0.508301  7.839094  0.501221  7.951731
92   0.496460  8.027758  0.498657  7.992599
93   0.503174  7.920744  0.507080  7.858319
94   0.506226  7.872135  0.501465  7.947839
95   0.511841  7.782550  0.506836  7.862211
96   0.491943  8.099763  0.502930  7.924486
97   0.508179  7.841019  0.498291  7.998438
98   0.494751  8.054960  0.500366  7.965354
99   0.513794  7.751391  0.502930  7.924486
100  0.503784  7.911035  0.497681  8.008168
101  0.504395  7.901305  0.493652  8.072389
102  0.511841  7.782507  0.507202  7.856373
103  0.505127  7.889692  0.495483  8.043198
104  0.511963  7.780604  0.492798  8.086012
105  0.513428  7.757272  0.490601  8.121041
106  0.509521  7.819590  0.494873  8.052928
107  0.512573  7.770852  0.494995  8.050982
108  0.505859  7.960370  0.504517  7.986249
109  0.501709  8.030108  0.488647  8.242029
110  0.506592  7.951621  0.494263  8.151522
111  0.505005  7.977263  0.491943  8.188905
112  0.511230  7.877069  0.495972  8.123977
113  0.495728  8.126603  0.488403  8.245964
114  0.500610  8.048223  0.504883  7.980346
115  0.515137  7.814065  0.489380  8.230224
116  0.511353  7.874951  0.493042  8.171198
117  0.517578  7.774521  0.509033  7.913450
118  0.503906  7.994778  0.500000  8.059048
119  0.513550  7.839171  0.496826  8.110204
120  0.505981  7.961523  0.495728  8.127912
121  0.507812  7.932010  0.499756  8.062983
122  0.510864  7.882843  0.498047  8.090528
123  0.496460  8.114862  0.505615  7.968541
124  0.508057  7.928182  0.505737  7.966573
125  0.499878  8.059943  0.504028  7.994119
126  0.497437  8.099079  0.489136  8.234159
127  0.504517  7.985069  0.501709  8.031502

2018-06-08 16:57:57.272658 Finish.
Total elapsed time: 30:22:55.27.
