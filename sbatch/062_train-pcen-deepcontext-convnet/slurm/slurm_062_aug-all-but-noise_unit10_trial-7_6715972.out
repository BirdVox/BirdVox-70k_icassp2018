2018-06-07 10:35:12.999103: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:12.999292: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:12.999303: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:12.999308: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:12.999313: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:35:02.101690 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit01, unit02, unit03.
Validation set: unit05, unit07.
Test set: unit10.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.553833  7.174031  0.495483  8.131847
1    0.553101  7.186673  0.489380  8.230117
2    0.541992  7.365998  0.502930  8.011827
3    0.544312  7.328936  0.506592  7.952715
4    0.550781  7.224078  0.506714  7.950790
5    0.555664  7.146341  0.506470  7.954682
6    0.549683  7.241592  0.502930  8.011762
7    0.549927  7.238430  0.500732  8.047200
8    0.554688  7.161846  0.500488  8.051135
9    0.544678  7.322862  0.496948  8.108193
10   0.557129  7.123031  0.503540  8.001968
11   0.551514  7.213152  0.504028  7.994055
12   0.555420  7.150040  0.495361  8.133750
13   0.545532  7.309175  0.503906  7.996044
14   0.552734  7.194120  0.500000  8.058983
15   0.559692  7.082463  0.502197  8.023589
16   0.536865  7.449664  0.505859  7.964541
17   0.538452  7.423507  0.500854  8.045232
18   0.547852  7.272907  0.508301  7.925212
19   0.540527  7.391153  0.509399  7.907504
20   0.561157  7.059260  0.491211  8.200603
21   0.543701  7.340125  0.496216  8.120020
22   0.542358  7.362455  0.487061  8.267521
23   0.553345  7.184904  0.487915  8.253813
24   0.544678  7.324449  0.500122  8.057037
25   0.546997  7.288053  0.498047  8.090464
26   0.546021  7.302528  0.502441  8.019676
27   0.541382  7.377294  0.500488  8.051178
28   0.550415  7.231911  0.494019  8.155372
29   0.542114  7.365081  0.498291  8.086550
30   0.547485  7.280161  0.507324  7.940952
31   0.555542  7.148416  0.501343  8.037405
32   0.555054  7.157251  0.504272  7.990120
33   0.542603  7.332379  0.499756  7.975085
34   0.513062  7.767979  0.496338  8.029575
35   0.525391  7.571231  0.498535  7.994545
36   0.516724  7.709275  0.490356  8.124934
37   0.526611  7.551985  0.499878  7.973138
38   0.507812  7.850996  0.502319  7.934217
39   0.522583  7.615991  0.507080  7.858319
40   0.515747  7.724801  0.504883  7.893349
41   0.527954  7.530428  0.502808  7.926432
42   0.513306  7.764216  0.500122  7.969246
43   0.514038  7.752432  0.501953  7.940055
44   0.535400  7.411308  0.500610  7.961462
45   0.523926  7.594820  0.504028  7.906971
46   0.515381  7.730875  0.502075  7.938109
47   0.515259  7.732864  0.512085  7.778529
48   0.520508  7.648689  0.492310  8.093796
49   0.524048  7.592574  0.491211  8.111311
50   0.521851  7.627561  0.500732  7.959516
51   0.513794  7.756560  0.502319  7.934217
52   0.505737  7.885324  0.508179  7.840804
53   0.518433  7.682480  0.499878  7.973138
54   0.523071  7.608164  0.492065  8.097688
55   0.517822  7.691674  0.509033  7.827182
56   0.523315  7.603950  0.500122  7.969246
57   0.527100  7.543879  0.496338  8.029575
58   0.516357  7.715328  0.502686  7.928378
59   0.518188  7.685536  0.493896  8.068497
60   0.519531  7.664665  0.507812  7.846642
61   0.525757  7.565757  0.496094  8.033467
62   0.520142  7.654656  0.503540  7.914756
63   0.529785  7.501494  0.505981  7.875834
64   0.522827  7.611413  0.493164  8.080173
65   0.520630  7.647172  0.502441  7.932271
66   0.518311  7.684147  0.497314  8.014006
67   0.502563  7.935064  0.496460  8.027629
68   0.516479  7.713360  0.495728  8.039306
69   0.520508  7.648817  0.496704  8.023737
70   0.533569  7.440629  0.502319  7.934217
71   0.519653  7.662504  0.492310  8.093796
72   0.516968  7.705490  0.502686  7.928378
73   0.515625  7.726854  0.504883  7.893349
74   0.518799  7.676556  0.496948  8.019845
75   0.515747  7.725101  0.500366  7.965354
76   0.525879  7.563318  0.493042  8.082120
77   0.518555  7.680320  0.493652  8.072389
78   0.523804  7.596723  0.499146  7.984815
79   0.520874  7.643108  0.507324  7.854427
80   0.511719  7.789150  0.499268  7.982869
81   0.528198  7.526814  0.495483  8.043198
82   0.513550  7.760345  0.490723  8.119095
83   0.522339  7.619583  0.492188  8.095742
84   0.519043  7.672471  0.498047  8.002330
85   0.509766  7.820245  0.494873  8.052928
86   0.529785  7.500850  0.494019  8.066551
87   0.514404  7.746808  0.495239  8.047090
88   0.520020  7.657095  0.502686  7.928378
89   0.515015  7.736606  0.499146  7.984815
90   0.519775  7.660666  0.490112  8.128826
91   0.518188  7.685686  0.498169  8.000384
92   0.518066  7.687975  0.500732  7.959516
93   0.522583  7.615691  0.508789  7.831074
94   0.524292  7.588338  0.502319  7.934217
95   0.528442  7.522729  0.493774  8.070443
96   0.523315  7.604293  0.505127  7.889457
97   0.527100  7.543750  0.507935  7.844696
98   0.520264  7.652924  0.496582  8.025683
99   0.520874  7.643237  0.502197  7.936163
100  0.524170  7.590692  0.504028  7.906971
101  0.523438  7.602862  0.506104  7.873888
102  0.521240  7.637463  0.498535  7.994545
103  0.520630  7.646979  0.494995  8.050982
104  0.519409  7.666654  0.500122  7.969246
105  0.512451  7.777131  0.503052  7.922540
106  0.530029  7.496808  0.494141  8.064605
107  0.524780  7.581069  0.506714  7.864157
108  0.529663  7.503290  0.501831  7.942001
109  0.521240  7.637098  0.498535  7.994545
110  0.521118  7.639173  0.497803  8.006222
111  0.516724  7.708867  0.500977  7.955624
112  0.521729  7.629228  0.493652  8.072389
113  0.519165  7.669988  0.495239  8.047090
114  0.520752  7.644904  0.510132  7.809667
115  0.516113  7.719284  0.503174  7.920594
116  0.517822  7.692318  0.506348  7.869996
117  0.519775  7.660966  0.506104  7.873888
118  0.520996  7.640690  0.500244  7.967300
119  0.523315  7.604722  0.503906  7.908917
120  0.528931  7.514687  0.497437  8.012060
121  0.533813  7.436350  0.506714  7.864157
122  0.524780  7.580254  0.499634  7.977031
123  0.516357  7.715714  0.497681  8.008168
124  0.525879  7.563447  0.504150  7.905025
125  0.517090  7.703866  0.506104  7.873888
126  0.513672  7.757777  0.503540  7.914756
127  0.526245  7.557609  0.505371  7.885564

2018-06-08 17:08:43.867042 Finish.
Total elapsed time: 30:33:41.87.
