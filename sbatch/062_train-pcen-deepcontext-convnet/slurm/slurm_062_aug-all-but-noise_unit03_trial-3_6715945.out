2018-06-07 10:35:05.514602: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.514776: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.514788: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:35:02.468538 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.958252  0.163920  0.719727  1.971031
1    0.961792  0.150502  0.774048  1.865913
2    0.967896  0.119318  0.730957  2.315374
3    0.967773  0.118285  0.731079  2.484938
4    0.966431  0.120494  0.735107  2.445482
5    0.970459  0.106020  0.724609  2.276325
6    0.965698  0.123478  0.762207  2.043261
7    0.965942  0.118594  0.760742  2.220999
8    0.971680  0.094968  0.754517  2.162555
9    0.968140  0.107446  0.767578  2.155619
10   0.967163  0.110665  0.764648  2.148197
11   0.968384  0.100416  0.766357  2.062699
12   0.962646  0.172121  0.738037  2.684216
13   0.965820  0.130768  0.758789  2.268389
14   0.969360  0.106492  0.736206  2.602437
15   0.972046  0.105510  0.752930  2.221570
16   0.967651  0.110149  0.743652  2.442563
17   0.971680  0.098633  0.726685  2.658849
18   0.976074  0.085156  0.742676  2.484468
19   0.976318  0.092467  0.725098  2.615783
20   0.970581  0.106834  0.723267  2.948957
21   0.971313  0.103145  0.717407  2.941224
22   0.973145  0.099144  0.733154  2.715671
23   0.972168  0.097214  0.732788  2.738370
24   0.974121  0.088514  0.725952  2.940910
25   0.972412  0.096983  0.718140  2.892654
26   0.970215  0.107534  0.723511  3.101039
27   0.967651  0.142472  0.734009  2.821112
28   0.964844  0.160954  0.755249  2.528084
29   0.967896  0.132885  0.759155  2.712742
30   0.964233  0.133863  0.718750  2.604132
31   0.966797  0.136080  0.729248  2.611377
32   0.962891  0.141306  0.717651  2.813716
33   0.969116  0.102150  0.731812  2.705060
34   0.972534  0.096018  0.732910  2.620417
35   0.968018  0.107634  0.723633  2.904485
36   0.971191  0.108276  0.707764  2.901988
37   0.967529  0.106722  0.712524  3.155114
38   0.974976  0.097768  0.726074  2.960426
39   0.972534  0.122236  0.733154  3.156260
40   0.967529  0.118410  0.751099  2.674295
41   0.961060  0.179817  0.735352  2.588981
42   0.958008  0.183422  0.762085  2.385905
43   0.969238  0.114103  0.745483  2.534832
44   0.965210  0.138483  0.708740  3.024667
45   0.969727  0.123863  0.729370  2.929610
46   0.963989  0.121195  0.703369  3.310396
47   0.966553  0.143364  0.732788  2.980804
48   0.974487  0.091387  0.722046  3.143220
49   0.973633  0.102713  0.730225  2.792238
50   0.965332  0.136897  0.719482  2.957962
51   0.966675  0.123484  0.708252  3.124369
52   0.965942  0.147755  0.724976  3.057303
53   0.972412  0.120313  0.723389  3.177957
54   0.966187  0.162087  0.818726  2.105950
55   0.941040  0.266739  0.686890  3.616706
56   0.961060  0.158279  0.704102  3.490432
57   0.961060  0.165583  0.692261  3.639629
58   0.956177  0.169839  0.723511  3.326780
59   0.956543  0.167131  0.713623  3.026105
60   0.944946  0.421011  0.734009  3.346586
61   0.773193  2.718691  0.517822  7.699546
62   0.682739  4.258144  0.526367  7.563016
63   0.646240  4.133274  0.501953  7.940545
64   0.617310  4.311462  0.507935  7.845726
65   0.576660  4.297702  0.507568  7.851221
66   0.606934  4.863743  0.490479  8.128907
67   0.616333  4.935485  0.489258  8.150792
68   0.624023  4.028500  0.486084  8.201369
69   0.616455  4.152993  0.487183  8.185012
70   0.592041  4.292403  0.486938  8.189183
71   0.608887  4.215131  0.493042  8.091259
72   0.614868  4.144434  0.486938  8.189484
73   0.615112  4.053360  0.492676  8.097288
74   0.608154  4.195684  0.489136  8.153210
75   0.607666  4.213510  0.488647  8.161445
76   0.611206  4.199396  0.494995  8.060355
77   0.615967  4.068209  0.486938  8.187896
78   0.561768  6.134202  0.509155  7.834180
79   0.584717  5.998869  0.491089  8.121064
80   0.579956  5.895671  0.499023  7.994568
81   0.573120  6.309857  0.510620  7.808746
82   0.588501  6.238779  0.503174  7.926814
83   0.592407  6.124351  0.503540  7.918573
84   0.584839  5.847619  0.501587  7.949432
85   0.552979  6.656783  0.521240  7.634546
86   0.531982  7.459983  0.527100  7.539267
87   0.542358  7.329030  0.533081  7.444574
88   0.546753  7.257541  0.534790  7.417264
89   0.572021  6.839002  0.518188  7.681268
90   0.573730  6.809866  0.529419  7.502184
91   0.537842  7.372916  0.501587  7.945893
92   0.506470  7.868135  0.498291  7.998438
93   0.507202  7.856437  0.493652  8.072389
94   0.491089  8.113300  0.496216  8.031521
95   0.498901  7.988750  0.503418  7.916702
96   0.498657  7.992728  0.496826  8.021791
97   0.500366  7.965440  0.503174  7.920594
98   0.498779  7.990675  0.501221  7.951731
99   0.492188  8.095828  0.501465  7.947839
100  0.501953  7.940141  0.494629  8.056820
101  0.503296  7.918691  0.500610  7.961462
102  0.500854  7.957613  0.501343  7.949785
103  0.498901  7.988772  0.502075  7.938109
104  0.496338  8.029597  0.498413  7.996492
105  0.494873  8.053057  0.502197  7.936163
106  0.505859  7.877844  0.499146  7.984815
107  0.507812  7.846728  0.512207  7.776583
108  0.495605  8.041380  0.498169  8.000384
109  0.495361  8.045144  0.508911  7.829128
110  0.505615  7.881715  0.501221  7.951731
111  0.508301  7.838965  0.505005  7.891403
112  0.504639  7.897284  0.502441  7.932271
113  0.505981  7.875855  0.506104  7.873888
114  0.495483  8.043241  0.484741  8.214454
115  0.501465  7.947925  0.497803  8.006222
116  0.503662  7.912852  0.500977  7.955624
117  0.499268  7.982912  0.508789  7.831074
118  0.498779  7.990696  0.489868  8.132718
119  0.496704  8.023823  0.499268  7.982869
120  0.493286  8.078313  0.496948  8.019845
121  0.492920  8.084130  0.504883  7.893349
122  0.495239  8.047154  0.498779  7.990653
123  0.499878  7.973224  0.500488  7.963408
124  0.503418  7.916745  0.499756  7.975085
125  0.497437  8.012168  0.503906  7.908917
126  0.489624  8.136696  0.501709  7.943947
127  0.499268  7.982955  0.500366  7.965354

2018-06-08 16:51:02.230182 Finish.
Total elapsed time: 30:16:00.23.
