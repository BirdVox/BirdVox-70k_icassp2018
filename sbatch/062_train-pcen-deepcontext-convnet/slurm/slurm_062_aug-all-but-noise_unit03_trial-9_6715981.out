2018-06-07 10:35:05.524154: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.524324: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.524334: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.524339: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.524344: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:35:02.591784 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.498169  8.000405  0.493530  8.074335
1    0.491943  8.099634  0.490234  8.126880
2    0.498779  7.990653  0.505615  7.881672
3    0.493042  8.082141  0.495605  8.041252
4    0.497925  8.004319  0.503906  7.908917
5    0.498901  7.988707  0.496460  8.027629
6    0.497681  8.008168  0.498047  8.002330
7    0.500488  7.963429  0.512817  7.766853
8    0.494385  8.060713  0.501953  7.940055
9    0.498535  7.994588  0.501099  7.953678
10   0.490356  8.124976  0.501953  7.940055
11   0.493286  8.078227  0.490479  8.122987
12   0.505371  7.885586  0.496948  8.019845
13   0.487915  8.163877  0.503052  7.922540
14   0.504272  7.903079  0.502563  7.930324
15   0.499512  7.978977  0.505127  7.889457
16   0.494263  8.062680  0.495361  8.045144
17   0.497681  8.008168  0.494751  8.054874
18   0.501831  7.942022  0.498779  7.990653
19   0.494751  8.054896  0.499756  7.975085
20   0.498657  7.992599  0.491821  8.101580
21   0.487305  8.173607  0.501831  7.942001
22   0.499512  7.978998  0.510864  7.797990
23   0.499512  7.978998  0.505737  7.879726
24   0.502808  7.926432  0.508789  7.831074
25   0.496216  8.031543  0.499878  7.973138
26   0.508911  7.829150  0.501953  7.940055
27   0.494507  8.058766  0.498169  8.000384
28   0.502930  7.924486  0.495972  8.035413
29   0.498901  7.988707  0.495361  8.045144
30   0.492798  8.086012  0.494873  8.052928
31   0.499023  7.986761  0.501953  7.940055
32   0.496216  8.031521  0.507690  7.848589
33   0.508057  7.842750  0.492676  8.087958
34   0.500244  7.967300  0.492554  8.089904
35   0.497192  8.015952  0.511230  7.792152
36   0.502075  7.938109  0.487915  8.163855
37   0.501343  7.949785  0.501831  7.942001
38   0.499634  7.977031  0.506592  7.866103
39   0.498779  7.990653  0.500488  7.963408
40   0.496948  8.019845  0.512085  7.778529
41   0.496826  8.021791  0.505005  7.891403
42   0.492920  8.084066  0.492310  8.093796
43   0.505371  7.885564  0.493530  8.074335
44   0.498413  7.996492  0.496094  8.033467
45   0.494873  8.052928  0.494507  8.058766
46   0.492920  8.084066  0.506470  7.868049
47   0.500732  7.959516  0.508545  7.834966
48   0.500854  7.957570  0.492798  8.086012
49   0.493774  8.070443  0.501709  7.943947
50   0.493042  8.082120  0.488647  8.152179
51   0.501221  7.951731  0.498901  7.988707
52   0.499634  7.977031  0.502441  7.932271
53   0.499634  7.977031  0.498291  7.998438
54   0.497681  8.008168  0.498535  7.994545
55   0.494629  8.056820  0.503174  7.920594
56   0.503052  7.922540  0.500488  7.963408
57   0.498657  7.992599  0.500000  7.971192
58   0.508423  7.836912  0.494263  8.062659
59   0.497314  8.014006  0.502563  7.930324
60   0.494507  8.058766  0.500854  7.957570
61   0.497925  8.004276  0.502686  7.928378
62   0.499878  7.973138  0.500000  7.971192
63   0.510376  7.805796  0.505127  7.889457
64   0.491943  8.099634  0.497681  8.008168
65   0.503906  7.908917  0.505005  7.891403
66   0.496826  8.021791  0.493530  8.074335
67   0.506226  7.871942  0.505249  7.887510
68   0.490967  8.115203  0.499268  7.982869
69   0.503784  7.910864  0.497070  8.017899
70   0.507080  7.858319  0.509155  7.825235
71   0.496704  8.023737  0.508911  7.829128
72   0.498535  7.994545  0.503540  7.914756
73   0.503662  7.912810  0.493164  8.080173
74   0.500977  7.955624  0.502808  7.926432
75   0.503174  7.920594  0.501099  7.953678
76   0.496704  8.023737  0.501709  7.943947
77   0.502930  7.924486  0.498657  7.992599
78   0.504639  7.897241  0.504028  7.906971
79   0.497437  8.012060  0.493652  8.072389
80   0.498169  8.000405  0.505249  7.887510
81   0.493530  8.074335  0.513306  7.759068
82   0.505249  7.887532  0.505859  7.877780
83   0.495483  8.043198  0.497192  8.015952
84   0.502197  7.936163  0.507690  7.848589
85   0.500732  7.959516  0.498657  7.992599
86   0.500732  7.959516  0.499390  7.980923
87   0.497314  8.014006  0.506836  7.862211
88   0.497314  8.014006  0.507080  7.858319
89   0.499146  7.984815  0.500488  7.963408
90   0.491455  8.107440  0.501221  7.951731
91   0.490723  8.119095  0.501221  7.951731
92   0.492554  8.089904  0.494629  8.056820
93   0.510132  7.809667  0.509399  7.821343
94   0.507080  7.858319  0.504639  7.897241
95   0.501343  7.949785  0.503662  7.912810
96   0.498413  7.996492  0.507935  7.844696
97   0.493896  8.068497  0.496826  8.021791
98   0.514404  7.741554  0.491821  8.101580
99   0.495850  8.037359  0.494751  8.054874
100  0.489014  8.146341  0.502686  7.928378
101  0.499146  7.984815  0.496216  8.031521
102  0.495605  8.041252  0.494995  8.050982
103  0.497070  8.017899  0.494385  8.060713
104  0.491943  8.099634  0.508423  7.836912
105  0.500366  7.965354  0.502197  7.936163
106  0.500610  7.961462  0.495117  8.049036
107  0.509644  7.817451  0.494263  8.062659
108  0.496094  8.033467  0.501099  7.953678
109  0.498779  7.990675  0.505005  7.891403
110  0.498657  7.992599  0.493774  8.070443
111  0.505249  7.887532  0.503540  7.914756
112  0.509766  7.815505  0.507812  7.846642
113  0.511475  7.788260  0.497192  8.015952
114  0.496704  8.023737  0.500732  7.959516
115  0.500366  7.965354  0.493286  8.078227
116  0.501099  7.953678  0.495728  8.039306
117  0.500854  7.957570  0.500732  7.959516
118  0.492798  8.086012  0.489746  8.134664
119  0.493164  8.080173  0.502197  7.936163
120  0.504639  7.897241  0.499268  7.982869
121  0.498901  7.988707  0.492920  8.084066
122  0.485474  8.202777  0.495605  8.041252
123  0.495239  8.047090  0.500488  7.963408
124  0.495850  8.037359  0.503174  7.920594
125  0.501587  7.945893  0.499023  7.986761
126  0.494019  8.066551  0.496704  8.023737
127  0.500122  7.969246  0.496094  8.033467

2018-06-08 18:13:13.846745 Finish.
Total elapsed time: 31:38:11.85.
