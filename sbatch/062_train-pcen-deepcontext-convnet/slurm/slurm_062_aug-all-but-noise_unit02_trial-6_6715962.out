2018-06-07 10:35:13.450798: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:13.452327: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:13.452341: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:35:03.317514 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.504883  7.970930  0.508911  7.915417
1    0.506592  7.943577  0.500732  8.047243
2    0.502075  8.016333  0.502441  8.019697
3    0.495117  8.056950  0.498291  7.998438
4    0.500732  7.963569  0.497314  8.014006
5    0.504028  7.911368  0.497559  8.010114
6    0.500977  7.960235  0.507812  7.846642
7    0.501709  7.948108  0.500122  7.969246
8    0.490112  8.132965  0.495605  8.041252
9    0.501099  7.957581  0.484253  8.222238
10   0.502686  7.932947  0.493530  8.074335
11   0.499390  7.984633  0.497925  8.004276
12   0.505981  7.880552  0.490601  8.121041
13   0.504761  7.899456  0.500732  7.959516
14   0.496704  8.027640  0.496094  8.033467
15   0.498169  8.004673  0.509644  7.817451
16   0.498657  7.997254  0.498657  7.992599
17   0.487427  8.176101  0.496704  8.023737
18   0.502808  7.930722  0.490356  8.124934
19   0.497681  8.012308  0.505615  7.881672
20   0.487305  8.178047  0.493652  8.072389
21   0.504883  7.897338  0.502441  7.932271
22   0.502563  7.934528  0.503662  7.912810
23   0.502441  7.935917  0.505859  7.877780
24   0.495361  8.049326  0.501831  7.942001
25   0.504028  7.910982  0.491211  8.111311
26   0.501099  7.957903  0.505493  7.883618
27   0.498413  8.000781  0.504028  7.906971
28   0.513062  7.766714  0.503784  7.910864
29   0.507568  7.855017  0.498779  7.990653
30   0.499512  7.983009  0.499390  7.980923
31   0.505737  7.883501  0.499268  7.982869
32   0.494507  8.062949  0.496460  8.027629
33   0.509766  7.819323  0.493774  8.070443
34   0.501709  7.948215  0.506592  7.866103
35   0.500488  7.967247  0.504883  7.893349
36   0.496704  8.027426  0.503052  7.922540
37   0.497803  8.010233  0.502075  7.938109
38   0.506104  7.877727  0.506348  7.869996
39   0.513306  7.762886  0.490845  8.117149
40   0.497681  8.012372  0.489502  8.138556
41   0.501343  7.954054  0.497925  8.004276
42   0.512817  7.770906  0.503418  7.916702
43   0.495117  8.053047  0.493774  8.070443
44   0.493774  8.074561  0.494629  8.056820
45   0.496948  8.024027  0.498901  7.988707
46   0.493408  8.080228  0.501099  7.953678
47   0.495728  8.043831  0.491699  8.103527
48   0.495239  8.050822  0.500000  7.971192
49   0.498901  7.992718  0.495850  8.037359
50   0.495483  8.047144  0.497681  8.008168
51   0.500244  7.971290  0.498901  7.988707
52   0.509888  7.817505  0.504761  7.895295
53   0.498291  8.002491  0.507690  7.848589
54   0.503662  7.916713  0.502441  7.932271
55   0.500244  7.971697  0.499756  7.975085
56   0.495728  8.043488  0.490723  8.119095
57   0.495850  8.042014  0.486694  8.183316
58   0.499756  7.978945  0.516968  7.700686
59   0.492310  8.097399  0.490112  8.128826
60   0.501343  7.953539  0.503662  7.912810
61   0.507690  7.852471  0.495361  8.045144
62   0.502197  7.939916  0.503906  7.908917
63   0.501099  7.957903  0.500977  7.955624
64   0.495605  8.045670  0.495972  8.035413
65   0.501587  7.949218  0.495117  8.049036
66   0.495483  8.047058  0.510010  7.811613
67   0.499512  7.983202  0.504517  7.899187
68   0.511719  7.788057  0.502686  7.928378
69   0.499390  7.985191  0.493164  8.080173
70   0.507812  7.850803  0.502075  7.938109
71   0.496704  8.027726  0.496826  8.021791
72   0.505249  7.891628  0.498901  7.988707
73   0.487549  8.173790  0.494141  8.064605
74   0.497559  8.014168  0.492554  8.089904
75   0.499634  7.981513  0.499878  7.973138
76   0.494995  8.055486  0.494019  8.066551
77   0.497681  8.012522  0.507080  7.858319
78   0.499756  7.979374  0.500610  7.961462
79   0.501587  7.950397  0.500977  7.955624
80   0.494873  8.056789  0.499878  7.973138
81   0.499512  7.982666  0.490967  8.115203
82   0.509766  7.819366  0.501099  7.953678
83   0.494507  8.062863  0.498779  7.990653
84   0.507690  7.852514  0.497192  8.015952
85   0.503906  7.912671  0.496948  8.019845
86   0.501709  7.948172  0.505859  7.877780
87   0.501221  7.955785  0.502319  7.934217
88   0.493530  8.078217  0.500854  7.957570
89   0.495728  8.043617  0.502686  7.928378
90   0.508911  7.833632  0.497925  8.004276
91   0.498413  8.000459  0.498779  7.990653
92   0.500488  7.967247  0.501953  7.940055
93   0.504395  7.905315  0.497803  8.006222
94   0.498047  8.006791  0.491699  8.103527
95   0.500122  7.973021  0.497559  8.010114
96   0.502319  7.938313  0.501099  7.953678
97   0.503052  7.927109  0.508911  7.829128
98   0.498169  8.004051  0.504639  7.897241
99   0.493286  8.082667  0.502686  7.928378
100  0.506470  7.872039  0.492798  8.086012
101  0.492554  8.094515  0.502197  7.936163
102  0.500488  7.967869  0.507690  7.848589
103  0.493408  8.080228  0.510986  7.796044
104  0.500610  7.965644  0.494019  8.066551
105  0.508301  7.842998  0.491333  8.109365
106  0.496582  8.029951  0.502319  7.934217
107  0.496216  8.035682  0.507935  7.844696
108  0.508301  7.842955  0.505981  7.875834
109  0.499634  7.981470  0.505371  7.885564
110  0.507568  7.854181  0.506226  7.871942
111  0.502563  7.934614  0.497192  8.015952
112  0.504761  7.899327  0.502441  7.932271
113  0.504639  7.901681  0.506348  7.869996
114  0.490723  8.123192  0.496216  8.031521
115  0.502930  7.928476  0.495117  8.049036
116  0.503296  7.922680  0.494385  8.060713
117  0.499146  7.989233  0.494019  8.066551
118  0.492920  8.088119  0.494873  8.052928
119  0.498169  8.004931  0.500366  7.965354
120  0.499634  7.981084  0.501221  7.951731
121  0.501099  7.957924  0.494873  8.052928
122  0.495483  8.047402  0.498291  7.998438
123  0.496216  8.035875  0.502319  7.934217
124  0.505981  7.879737  0.488770  8.150233
125  0.501465  7.951979  0.495117  8.049036
126  0.499756  7.979696  0.507568  7.850535
127  0.494995  8.054736  0.507812  7.846642

2018-06-08 17:16:05.231205 Finish.
Total elapsed time: 30:41:02.23.
