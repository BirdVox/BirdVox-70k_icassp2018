2018-06-07 10:35:01.222443: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:01.222687: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:01.222700: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:34:52.094266 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit01, unit02, unit03.
Validation set: unit05, unit07.
Test set: unit10.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.506592  7.950977  0.493652  8.161360
1    0.503906  7.994156  0.494629  8.145620
2    0.501465  8.033571  0.497559  8.098399
3    0.504395  7.985814  0.500854  8.045275
4    0.509766  7.899636  0.498169  8.088561
5    0.495483  8.131847  0.505005  7.978379
6    0.488647  8.242029  0.489502  8.228256
7    0.504272  7.990184  0.496460  8.116106
8    0.503418  8.003957  0.494751  8.143652
9    0.503540  8.001989  0.499878  8.061015
10   0.496094  8.122009  0.493530  8.163327
11   0.500000  8.059048  0.499634  8.064950
12   0.497070  8.106269  0.510742  7.885904
13   0.500732  8.047243  0.501831  8.029535
14   0.493652  8.161360  0.513672  7.838683
15   0.495483  8.131847  0.501953  8.027567
16   0.498779  8.078723  0.500122  8.057080
17   0.504028  7.994119  0.500244  8.055113
18   0.500244  8.055113  0.502808  8.013794
19   0.504761  7.982314  0.504028  7.994119
20   0.493896  8.157425  0.492065  8.186938
21   0.508667  7.919352  0.503540  8.001989
22   0.497070  8.106269  0.492554  8.179068
23   0.499512  8.066918  0.495728  8.127912
24   0.498535  8.082658  0.500610  8.049210
25   0.495117  8.137749  0.500732  8.047243
26   0.501831  8.029535  0.506714  7.950833
27   0.501831  8.029535  0.504517  7.986249
28   0.499878  8.061015  0.500000  8.059048
29   0.499023  8.074788  0.501953  8.027567
30   0.489380  8.230224  0.487915  8.253834
31   0.494141  8.153490  0.491211  8.200711
32   0.503662  8.000022  0.502808  8.013794
33   0.495728  8.127912  0.501343  8.037405
34   0.492065  8.186938  0.494629  8.145620
35   0.496460  8.116106  0.497559  8.098399
36   0.496704  8.112171  0.494873  8.141684
37   0.499023  8.074788  0.494141  8.153490
38   0.506226  7.958703  0.492310  8.183003
39   0.506592  7.952801  0.500122  8.057080
40   0.498413  8.084626  0.484253  8.312861
41   0.499268  8.070853  0.510376  7.891807
42   0.495483  8.131847  0.499756  8.062983
43   0.492188  8.184970  0.494995  8.139717
44   0.489990  8.220386  0.512573  7.856391
45   0.489868  8.222354  0.504272  7.990184
46   0.504761  7.982314  0.507568  7.937060
47   0.489990  8.220386  0.486938  8.269575
48   0.497803  8.094464  0.502319  8.021664
49   0.482544  8.340406  0.492432  8.181035
50   0.486816  8.271542  0.502563  8.017729
51   0.495605  8.129879  0.501221  8.039372
52   0.508423  7.923287  0.504639  7.984281
53   0.489990  8.220386  0.502075  8.025600
54   0.503418  8.003957  0.499146  8.072821
55   0.503906  7.996086  0.502441  8.019697
56   0.500854  8.045275  0.495483  8.131847
57   0.495728  8.127912  0.502197  8.023632
58   0.497070  8.106269  0.502686  8.015762
59   0.501099  8.041340  0.502075  8.025600
60   0.498291  8.086593  0.495972  8.123977
61   0.510010  7.897709  0.513916  7.834748
62   0.502930  8.011827  0.497803  8.094464
63   0.504517  7.986227  0.501343  8.037405
64   0.494995  8.139717  0.497681  8.096431
65   0.503540  8.001989  0.499878  8.061015
66   0.507935  7.931158  0.492065  8.186938
67   0.511230  7.878034  0.503296  8.005924
68   0.502197  8.023632  0.505493  7.970508
69   0.496216  8.120020  0.495728  8.127912
70   0.500488  8.051178  0.490845  8.206613
71   0.497070  8.106269  0.502441  8.019697
72   0.497803  8.094464  0.498901  8.076756
73   0.495728  8.127912  0.501587  8.033470
74   0.493286  8.167263  0.498413  8.084626
75   0.502563  8.017729  0.503174  8.007892
76   0.496094  8.122009  0.500977  8.043307
77   0.490723  8.208581  0.502319  8.021664
78   0.507812  7.933125  0.498535  8.082658
79   0.497925  8.092496  0.504150  7.992151
80   0.494019  8.155457  0.502686  8.015762
81   0.494873  8.141684  0.505859  7.964606
82   0.497925  8.092496  0.515503  7.809170
83   0.494995  8.139717  0.499512  8.066918
84   0.491089  8.202678  0.490479  8.212516
85   0.495972  8.123977  0.503418  8.003957
86   0.505249  7.974444  0.485840  8.287283
87   0.505859  7.964606  0.496582  8.114139
88   0.507446  7.939006  0.500122  8.057080
89   0.495972  8.123977  0.498413  8.084626
90   0.505859  7.964584  0.506226  7.958703
91   0.501709  8.031502  0.506592  7.952801
92   0.501343  8.037405  0.496582  8.114139
93   0.499146  8.072821  0.497803  8.094463
94   0.499756  8.062983  0.493652  8.161360
95   0.501099  8.041340  0.500244  8.055113
96   0.497559  8.098399  0.500122  8.057080
97   0.508179  7.927223  0.487061  8.267607
98   0.502075  8.025600  0.503662  8.000022
99   0.497559  8.098399  0.499146  8.072821
100  0.508301  7.925234  0.505005  7.978379
101  0.503662  8.000022  0.501831  8.029535
102  0.506470  7.954768  0.517822  7.771787
103  0.499634  8.064950  0.502319  8.021665
104  0.502686  8.015762  0.497070  8.106269
105  0.497559  8.098399  0.505127  7.976411
106  0.510742  7.885904  0.496094  8.122009
107  0.504150  7.992151  0.500244  8.055113
108  0.499390  8.068885  0.500977  8.043307
109  0.490112  8.218419  0.498413  8.084626
110  0.501953  8.027567  0.506470  7.954768
111  0.509521  7.905580  0.503418  8.003957
112  0.495361  8.133814  0.499023  8.074788
113  0.507935  7.931158  0.503906  7.996086
114  0.508423  7.923287  0.505615  7.968541
115  0.507568  7.937060  0.501221  8.039372
116  0.502441  8.019697  0.489258  8.232191
117  0.490479  8.212516  0.495483  8.131847
118  0.501953  8.027546  0.504395  7.988216
119  0.500610  8.049210  0.497681  8.096431
120  0.491211  8.200711  0.500732  8.047243
121  0.499878  8.061015  0.512329  7.860326
122  0.504883  7.980346  0.500488  8.051178
123  0.500610  8.049210  0.508789  7.917385
124  0.498779  8.078723  0.491455  8.196776
125  0.500854  8.045275  0.501099  8.041340
126  0.494873  8.141685  0.505005  7.978379
127  0.507446  7.939028  0.508545  7.921320

2018-06-08 16:48:37.264815 Finish.
Total elapsed time: 30:13:45.26.
