2018-06-07 10:34:58.463554: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:34:58.463775: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:34:58.463787: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:34:52.097065 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit10, unit01, unit02.
Validation set: unit03, unit05.
Test set: unit07.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.497314  8.102312  0.501099  8.041340
1    0.506348  7.956736  0.497925  8.092496
2    0.495361  8.133814  0.504028  7.994119
3    0.510132  7.895742  0.505127  7.976411
4    0.504883  7.980346  0.499756  8.062983
5    0.510132  7.895742  0.504639  7.984281
6    0.504272  7.990184  0.502197  8.023632
7    0.504150  7.992151  0.500488  8.051178
8    0.500610  8.049210  0.495972  8.123977
9    0.497070  8.106269  0.507324  7.940995
10   0.504517  7.986227  0.501221  8.039372
11   0.498779  8.078723  0.494995  8.139717
12   0.496582  8.114139  0.501343  8.037405
13   0.493164  8.169230  0.501343  8.037405
14   0.503296  8.005924  0.506226  7.958703
15   0.499023  8.074788  0.501831  8.029535
16   0.497559  8.098399  0.507324  7.940995
17   0.491699  8.192841  0.499878  8.061015
18   0.495972  8.123977  0.502197  8.023632
19   0.506958  7.946898  0.507446  7.939028
20   0.501343  8.037405  0.500732  8.047243
21   0.502319  8.021665  0.496582  8.114139
22   0.500244  8.055113  0.504395  7.988216
23   0.499756  8.062983  0.500977  8.043307
24   0.505249  7.974444  0.503662  8.000022
25   0.497437  8.100366  0.492554  8.179068
26   0.499268  8.070853  0.501343  8.037405
27   0.506592  7.952801  0.514282  7.828846
28   0.496582  8.114139  0.502319  8.021665
29   0.501099  8.041340  0.501587  8.033470
30   0.494263  8.151522  0.503052  8.009859
31   0.495972  8.123977  0.494263  8.151522
32   0.501343  8.037383  0.489258  8.232191
33   0.498169  8.088561  0.508179  7.927223
34   0.508301  7.925255  0.510986  7.881969
35   0.497681  8.096410  0.497314  8.102334
36   0.497925  8.092496  0.497559  8.098399
37   0.494995  8.139717  0.509399  7.907547
38   0.489746  8.224321  0.497803  8.094464
39   0.499268  8.070853  0.502319  8.021665
40   0.500488  8.051178  0.508301  7.925255
41   0.498413  8.084626  0.497070  8.106269
42   0.489014  8.236126  0.495117  8.137749
43   0.493286  8.167263  0.508423  7.923287
44   0.504639  7.984281  0.505615  7.968541
45   0.502197  8.023632  0.497681  8.096431
46   0.503784  7.998054  0.501953  8.027567
47   0.497437  8.100366  0.497192  8.104301
48   0.505615  7.968541  0.492920  8.173165
49   0.503906  7.996087  0.508423  7.923287
50   0.488892  8.238094  0.504395  7.988216
51   0.502075  8.025600  0.502686  8.015762
52   0.494141  8.153490  0.507324  7.940995
53   0.503052  8.009859  0.498291  8.086593
54   0.505005  7.978379  0.496704  8.112171
55   0.511230  7.878034  0.504395  7.988216
56   0.504517  7.986249  0.501831  8.029535
57   0.499268  8.070853  0.492676  8.177100
58   0.506104  7.960671  0.502930  8.011827
59   0.503296  8.005924  0.498169  8.088561
60   0.496582  8.114139  0.504028  7.994119
61   0.499023  8.074788  0.493652  8.161360
62   0.496948  8.108215  0.501831  8.029535
63   0.494385  8.149555  0.503784  7.998054
64   0.508057  7.929190  0.508423  7.923287
65   0.510498  7.889839  0.496582  8.114139
66   0.502808  8.013794  0.494507  8.147587
67   0.486206  8.281380  0.500000  8.059048
68   0.499146  8.072821  0.493530  8.163327
69   0.492065  8.186938  0.511108  7.880002
70   0.506104  7.960671  0.493896  8.157425
71   0.502441  8.019697  0.498901  8.076756
72   0.499512  8.066918  0.509277  7.909515
73   0.492676  8.177100  0.487427  8.261705
74   0.502563  8.017729  0.496094  8.122009
75   0.494507  8.147587  0.494995  8.139717
76   0.503296  8.005924  0.500000  8.059048
77   0.482178  8.346287  0.498535  8.082658
78   0.508911  7.915417  0.491699  8.192841
79   0.501099  8.041340  0.497437  8.100366
80   0.493042  8.171198  0.499512  8.066918
81   0.498413  8.084626  0.503174  8.007892
82   0.498901  8.076756  0.502808  8.013794
83   0.492065  8.186602  0.493042  8.171198
84   0.501221  8.039372  0.506714  7.950833
85   0.499878  8.061015  0.502686  8.015762
86   0.502686  8.015762  0.495361  8.133814
87   0.498047  8.090528  0.500366  8.053145
88   0.498047  8.090528  0.491699  8.192841
89   0.496094  8.122009  0.504272  7.990184
90   0.501953  8.027567  0.488647  8.242029
91   0.503418  8.003957  0.499878  8.061015
92   0.510742  7.885904  0.501343  8.037405
93   0.502808  8.013794  0.502563  8.017729
94   0.496948  8.108236  0.500122  8.057080
95   0.495117  8.137749  0.492920  8.173165
96   0.500854  8.045275  0.495972  8.123977
97   0.502319  8.021643  0.498535  8.082658
98   0.497314  8.102334  0.504517  7.986249
99   0.488892  8.238094  0.492554  8.179068
100  0.504639  7.984281  0.493530  8.163327
101  0.506592  7.952801  0.501099  8.041340
102  0.507812  7.933125  0.500732  8.047243
103  0.498047  8.090528  0.507202  7.942963
104  0.501831  8.029535  0.501709  8.031502
105  0.503784  7.998054  0.490601  8.210548
106  0.505371  7.972476  0.502197  8.023632
107  0.494751  8.143652  0.499268  8.070853
108  0.498657  8.080691  0.490723  8.208581
109  0.496460  8.116106  0.500122  8.057080
110  0.494019  8.155457  0.498779  8.078723
111  0.512573  7.856391  0.498047  8.090528
112  0.504883  7.980346  0.502197  8.023632
113  0.506348  7.956736  0.512695  7.854424
114  0.501343  8.037405  0.495972  8.123977
115  0.503296  8.005924  0.501099  8.041340
116  0.497925  8.092496  0.492554  8.179068
117  0.502930  8.011827  0.498413  8.084626
118  0.494263  8.151522  0.503052  8.009859
119  0.491089  8.202678  0.503174  8.007892
120  0.503906  7.996086  0.496094  8.122009
121  0.496704  8.112171  0.501709  8.031502
122  0.505249  7.974444  0.503662  8.000022
123  0.499634  8.064950  0.499756  8.062983
124  0.493774  8.159392  0.503906  7.996086
125  0.503296  8.005924  0.493530  8.163327
126  0.503662  8.000022  0.500000  8.059048
127  0.509888  7.899677  0.491455  8.196776

2018-06-08 16:46:30.785148 Finish.
Total elapsed time: 30:11:38.79.
