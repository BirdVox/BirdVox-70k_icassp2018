2018-06-07 10:35:13.927825: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:13.928047: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:13.928059: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:35:02.804991 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit02, unit03, unit05.
Validation set: unit07, unit10.
Test set: unit01.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.517578  7.774035  0.498413  8.084626
1    0.495361  8.133793  0.511597  7.872131
2    0.504395  7.988152  0.497437  8.100366
3    0.497437  8.100366  0.510742  7.885904
4    0.505981  7.962617  0.497925  8.092496
5    0.493896  8.157425  0.489258  8.232191
6    0.507202  7.942941  0.496704  8.112171
7    0.491089  8.202657  0.501587  8.033470
8    0.495483  8.131825  0.503296  8.005924
9    0.496582  8.114096  0.503540  8.001989
10   0.498535  8.082637  0.492798  8.175133
11   0.489990  8.220386  0.500366  8.053145
12   0.498657  8.080691  0.500122  8.057080
13   0.489014  8.236126  0.497070  8.106269
14   0.510254  7.893753  0.506104  7.960671
15   0.480469  8.373854  0.500000  8.059048
16   0.499023  8.074788  0.497681  8.096431
17   0.508789  7.917385  0.502686  8.015762
18   0.501831  8.029513  0.499512  8.066918
19   0.502319  8.021665  0.501221  8.039372
20   0.492798  8.175133  0.490723  8.208581
21   0.498779  8.078702  0.496582  8.114139
22   0.494751  8.143652  0.499268  8.070853
23   0.500366  8.053145  0.503296  8.005924
24   0.503784  7.998011  0.504028  7.994119
25   0.503906  7.996086  0.498413  8.084626
26   0.501465  8.035437  0.498291  8.086593
27   0.503174  8.007849  0.500732  8.047243
28   0.495361  8.133814  0.502075  8.025600
29   0.495239  8.135782  0.496094  8.122009
30   0.497559  8.098377  0.487061  8.267607
31   0.502930  8.011827  0.494263  8.151522
32   0.499268  8.070853  0.493164  8.169230
33   0.495972  8.123934  0.505859  7.964606
34   0.500488  8.051156  0.497925  8.092496
35   0.500488  8.051178  0.502319  8.021665
36   0.502075  8.025578  0.499023  8.074788
37   0.496216  8.120042  0.505859  7.964606
38   0.498779  8.078723  0.510620  7.887872
39   0.497192  8.104301  0.496704  8.112171
40   0.495728  8.127912  0.499634  8.064950
41   0.499756  8.062983  0.505859  7.964606
42   0.497437  8.100366  0.492554  8.179068
43   0.503418  8.003935  0.502075  8.025600
44   0.499756  8.062961  0.511719  7.870164
45   0.502441  8.019697  0.502686  8.015762
46   0.495361  8.133771  0.492188  8.184970
47   0.500977  8.043307  0.501221  8.039372
48   0.500610  8.049189  0.501953  8.027567
49   0.497559  8.098399  0.508789  7.917385
50   0.512939  7.850488  0.496460  8.116106
51   0.500610  8.049210  0.496948  8.108236
52   0.498657  8.080691  0.494141  8.153490
53   0.499390  8.068864  0.494751  8.143652
54   0.504395  7.988216  0.504883  7.980346
55   0.504028  7.994097  0.506958  7.946898
56   0.499146  8.072799  0.505615  7.968541
57   0.492676  8.177100  0.511963  7.866229
58   0.501831  8.029513  0.503174  8.007892
59   0.503906  7.996086  0.489136  8.234159
60   0.494263  8.151501  0.498169  8.088561
61   0.510254  7.893774  0.504395  7.988216
62   0.497314  8.102334  0.497437  8.100366
63   0.490723  8.208559  0.500977  8.043307
64   0.492310  8.182960  0.502319  8.021665
65   0.507080  7.944930  0.488403  8.245964
66   0.509766  7.901623  0.498901  8.076756
67   0.494263  8.151522  0.500610  8.049210
68   0.504883  7.980346  0.507080  7.944930
69   0.503906  7.996086  0.497314  8.102334
70   0.498291  8.086593  0.501831  8.029535
71   0.507446  7.939006  0.508911  7.915417
72   0.492554  8.179068  0.497559  8.098399
73   0.496582  8.114139  0.490601  8.210548
74   0.506836  7.948866  0.497070  8.106269
75   0.498169  8.088561  0.512329  7.860326
76   0.499634  8.064950  0.490112  8.218419
77   0.488037  8.251867  0.499512  8.066918
78   0.493408  8.165295  0.506958  7.946898
79   0.500366  8.053145  0.494141  8.153490
80   0.502075  8.025578  0.504517  7.986249
81   0.500610  8.049210  0.503418  8.003957
82   0.503906  7.996086  0.494507  8.147587
83   0.508911  7.915417  0.502197  8.023632
84   0.495239  8.135760  0.495483  8.131847
85   0.495605  8.129879  0.493530  8.163327
86   0.501465  8.035437  0.502075  8.025600
87   0.498169  8.088539  0.497681  8.096431
88   0.500000  8.059048  0.498169  8.088561
89   0.488525  8.243975  0.496460  8.116106
90   0.499390  8.068885  0.504272  7.990184
91   0.510742  7.885904  0.497192  8.104301
92   0.505615  7.968541  0.508179  7.927223
93   0.501465  8.035416  0.495605  8.129879
94   0.503296  8.005924  0.502075  8.025600
95   0.502197  8.023611  0.504028  7.994119
96   0.492798  8.175111  0.499268  8.070853
97   0.497803  8.094421  0.494263  8.151522
98   0.497925  8.092475  0.501831  8.029535
99   0.488403  8.245964  0.500977  8.043307
100  0.508667  7.919331  0.506104  7.960671
101  0.498779  8.078723  0.498413  8.084626
102  0.497803  8.094464  0.493164  8.169230
103  0.491943  8.188905  0.489258  8.232191
104  0.507935  7.931136  0.499023  8.074788
105  0.503906  7.996065  0.505615  7.968541
106  0.493408  8.165295  0.495605  8.129879
107  0.499023  8.074788  0.504150  7.992151
108  0.501343  8.037405  0.498047  8.090528
109  0.497070  8.106247  0.496460  8.116106
110  0.503662  8.000022  0.501221  8.039372
111  0.491455  8.196776  0.493286  8.167263
112  0.497314  8.102334  0.485352  8.295153
113  0.503906  7.996086  0.490479  8.212516
114  0.494629  8.145620  0.495850  8.125944
115  0.502075  8.025600  0.501709  8.031502
116  0.494629  8.145598  0.501587  8.033470
117  0.491943  8.188906  0.499512  8.066918
118  0.505737  7.966573  0.495850  8.125944
119  0.507080  7.944909  0.502075  8.025600
120  0.501221  8.039351  0.492310  8.183003
121  0.498657  8.080691  0.494873  8.141685
122  0.498047  8.090507  0.498047  8.090528
123  0.497559  8.098399  0.500732  8.047243
124  0.510620  7.887850  0.503784  7.998054
125  0.497437  8.100323  0.500610  8.049210
126  0.498291  8.086593  0.496338  8.118074
127  0.501953  8.027546  0.507812  7.933125

2018-06-08 16:58:09.595058 Finish.
Total elapsed time: 30:23:07.60.
