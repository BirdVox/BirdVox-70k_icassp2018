2018-06-07 10:35:04.219097: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:04.219308: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:04.219317: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:04.219323: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:04.219328: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:35:01.535256 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit07, unit10, unit01.
Validation set: unit02, unit03.
Test set: unit05.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.512451  7.855506  0.509399  7.907547
1    0.503540  7.998557  0.500610  8.049210
2    0.502075  8.021932  0.505249  7.974444
3    0.507324  7.937070  0.514771  7.820975
4    0.495728  8.124415  0.508667  7.919352
5    0.509888  7.896395  0.498779  8.078723
6    0.505249  7.971162  0.499023  8.074788
7    0.513672  7.835466  0.502441  8.019697
8    0.508911  7.911599  0.504150  7.992151
9    0.500122  8.053670  0.499634  8.064950
10   0.501587  8.029931  0.502319  8.021665
11   0.504883  7.976764  0.504639  7.984281
12   0.500366  8.049692  0.500366  8.053145
13   0.504517  7.982559  0.499146  8.072821
14   0.501953  8.023856  0.507202  7.942963
15   0.504639  7.980764  0.504639  7.984281
16   0.509766  7.898298  0.498169  8.088561
17   0.510620  7.884719  0.508179  7.927223
18   0.499268  8.066949  0.504761  7.982314
19   0.505493  7.967012  0.494995  8.139717
20   0.510132  7.892653  0.499634  8.064950
21   0.501465  8.032027  0.498535  8.082658
22   0.491577  8.191076  0.503784  7.998054
23   0.506714  7.947187  0.493164  8.169230
24   0.504761  7.978775  0.500610  8.049210
25   0.504639  7.980635  0.494385  8.149555
26   0.495728  8.123886  0.502686  8.015762
27   0.494629  8.145469  0.501221  8.039372
28   0.508301  7.925148  0.495117  8.137749
29   0.486084  8.283154  0.490601  8.210548
30   0.503418  8.003635  0.498169  8.088561
31   0.504150  7.992066  0.504028  7.994119
32   0.494873  8.141513  0.504272  7.990184
33   0.499512  8.066768  0.500977  8.043307
34   0.498535  8.082530  0.502197  8.023632
35   0.488403  8.245685  0.503906  7.996086
36   0.509399  7.907376  0.503052  8.009859
37   0.494385  8.149319  0.491455  8.196776
38   0.494629  8.145469  0.501099  8.041340
39   0.505493  7.970444  0.500854  8.045275
40   0.492065  8.186788  0.503662  8.000022
41   0.493896  8.157275  0.499634  8.064950
42   0.504761  7.982206  0.506470  7.954768
43   0.493774  8.159199  0.486694  8.273510
44   0.496704  8.112021  0.498413  8.084626
45   0.501587  8.033341  0.504150  7.992151
46   0.503662  7.999914  0.510376  7.891807
47   0.495361  8.133536  0.510132  7.895742
48   0.504761  7.982206  0.513062  7.848521
49   0.496704  8.112086  0.502075  8.025600
50   0.506714  7.950726  0.490967  8.204646
51   0.499390  8.068821  0.501465  8.035437
52   0.504272  7.989948  0.509888  7.899677
53   0.505249  7.974358  0.497681  8.096431
54   0.491699  8.192626  0.496826  8.110204
55   0.507568  7.936974  0.494019  8.155457
56   0.503418  8.003828  0.502686  8.015762
57   0.504639  7.984045  0.505005  7.978379
58   0.499634  8.064822  0.497681  8.096431
59   0.502075  8.025385  0.494751  8.143652
60   0.495605  8.129686  0.491455  8.196776
61   0.497559  8.098227  0.495239  8.135782
62   0.506592  7.952629  0.501343  8.037405
63   0.499512  8.066725  0.502075  8.025600
64   0.499023  8.074681  0.494873  8.141684
65   0.506348  7.956628  0.500244  8.055113
66   0.497681  8.096302  0.501831  8.029535
67   0.502197  8.023503  0.486816  8.271542
68   0.508301  7.925126  0.490967  8.204646
69   0.495850  8.125858  0.505127  7.976411
70   0.499756  8.062833  0.502075  8.025600
71   0.500977  8.043029  0.502441  8.019697
72   0.494507  8.147458  0.503418  8.003957
73   0.497803  8.094335  0.483154  8.330568
74   0.500122  8.056952  0.497192  8.104301
75   0.500977  8.043200  0.501953  8.027567
76   0.499634  8.064822  0.486938  8.269575
77   0.500000  8.058876  0.499390  8.068885
78   0.505371  7.972369  0.495605  8.129879
79   0.499756  8.062833  0.496216  8.120042
80   0.489014  8.235976  0.498291  8.086593
81   0.502441  8.019611  0.497192  8.104301
82   0.496948  8.108129  0.508911  7.915417
83   0.503052  8.009838  0.502930  8.011827
84   0.498047  8.090357  0.498413  8.084626
85   0.502441  8.019547  0.501709  8.031502
86   0.493774  8.159307  0.503662  8.000022
87   0.499146  8.072649  0.502197  8.023632
88   0.498657  8.080583  0.496460  8.116106
89   0.499756  8.062854  0.500366  8.053145
90   0.489502  8.228128  0.494995  8.139717
91   0.499268  8.070746  0.500000  8.059048
92   0.498413  8.084433  0.501953  8.027567
93   0.502319  8.021493  0.509521  7.905580
94   0.510010  7.897645  0.502075  8.025600
95   0.499756  8.062747  0.502563  8.017729
96   0.501465  8.035351  0.500366  8.053145
97   0.493042  8.171112  0.499634  8.064950
98   0.499512  8.066703  0.498413  8.084626
99   0.498047  8.090357  0.500732  8.047243
100  0.506836  7.948801  0.508057  7.929190
101  0.512817  7.852263  0.509399  7.907547
102  0.505859  7.964499  0.504272  7.990184
103  0.488159  8.249599  0.498657  8.080691
104  0.503174  8.007827  0.496948  8.108236
105  0.496826  8.110054  0.508423  7.923287
106  0.502563  8.017536  0.507446  7.939028
107  0.502930  8.011720  0.493042  8.171198
108  0.504883  7.980110  0.501099  8.041340
109  0.506836  7.948694  0.505005  7.978379
110  0.501831  8.029299  0.503418  8.003957
111  0.512573  7.856091  0.497070  8.106269
112  0.505249  7.974315  0.496704  8.112171
113  0.508301  7.925083  0.512695  7.854424
114  0.489990  8.220322  0.502808  8.013794
115  0.496094  8.121955  0.489136  8.234159
116  0.494019  8.155457  0.500732  8.047243
117  0.498169  8.088561  0.491577  8.194808
118  0.486816  8.271542  0.509277  7.909515
119  0.504272  7.990184  0.505371  7.972476
120  0.506592  7.952801  0.503296  8.005924
121  0.512329  7.860326  0.502930  8.011827
122  0.493164  8.169209  0.507568  7.937060
123  0.496094  8.122009  0.499023  8.074788
124  0.507202  7.942963  0.506592  7.952801
125  0.498657  8.080691  0.498657  8.080691
126  0.507202  7.942963  0.505981  7.962638
127  0.495850  8.125923  0.501831  8.029535

2018-06-08 17:25:27.088708 Finish.
Total elapsed time: 30:50:26.09.
