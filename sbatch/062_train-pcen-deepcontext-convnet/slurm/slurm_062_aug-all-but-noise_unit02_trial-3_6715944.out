2018-06-07 10:35:05.519046: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.519201: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.519213: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:35:02.483925 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.968384  0.114265  0.818115  1.159412
1    0.959595  0.131723  0.819092  1.213709
2    0.970825  0.102552  0.842285  1.121850
3    0.970337  0.101320  0.804932  1.498200
4    0.966431  0.111706  0.830200  1.228061
5    0.970825  0.093890  0.839478  1.013478
6    0.971191  0.095544  0.836548  1.115690
7    0.972290  0.085950  0.852661  0.939213
8    0.970947  0.090950  0.836060  1.094056
9    0.974854  0.080052  0.777710  1.631226
10   0.975342  0.086222  0.843506  0.747386
11   0.973267  0.095210  0.826294  1.199889
12   0.975342  0.086470  0.838135  0.784539
13   0.974487  0.098030  0.789917  1.351255
14   0.973389  0.083614  0.790161  1.435816
15   0.976440  0.073628  0.814941  1.356530
16   0.976562  0.077329  0.827026  1.265994
17   0.979248  0.076462  0.836670  0.976531
18   0.981445  0.063755  0.803589  1.442322
19   0.979248  0.078686  0.830933  1.101045
20   0.980225  0.065901  0.860596  0.853741
21   0.977661  0.074433  0.828979  0.925851
22   0.979370  0.074720  0.847900  0.957883
23   0.979492  0.069200  0.814941  1.192558
24   0.976562  0.082295  0.848389  1.129043
25   0.980957  0.066223  0.836792  1.078782
26   0.982300  0.063997  0.844604  1.076104
27   0.979126  0.080703  0.857666  0.862894
28   0.980347  0.073564  0.811890  1.316060
29   0.980713  0.081536  0.824341  1.417638
30   0.978516  0.082819  0.842163  1.160580
31   0.980835  0.071304  0.853882  1.189645
32   0.977661  0.083235  0.825806  1.223043
33   0.982666  0.057761  0.842285  1.137237
34   0.982544  0.060062  0.835693  1.126491
35   0.976318  0.078114  0.845215  0.951195
36   0.980591  0.066058  0.826416  1.323865
37   0.982300  0.059801  0.842407  1.243505
38   0.982178  0.069084  0.828735  1.051498
39   0.980347  0.070902  0.849854  0.889592
40   0.981934  0.068762  0.841675  1.103080
41   0.985229  0.053371  0.825806  1.155067
42   0.978638  0.075641  0.836548  0.956912
43   0.979126  0.069922  0.853760  0.749483
44   0.981445  0.065115  0.831543  1.129481
45   0.979126  0.064829  0.850464  0.839966
46   0.982788  0.054071  0.832153  1.123870
47   0.979736  0.066360  0.832520  1.212559
48   0.981201  0.060151  0.825195  1.216652
49   0.977905  0.071566  0.840942  0.999786
50   0.980469  0.067365  0.834717  1.066014
51   0.979248  0.070124  0.849365  0.990444
52   0.980713  0.064505  0.833496  0.937806
53   0.979126  0.066149  0.840820  0.968174
54   0.981201  0.060469  0.814941  1.234010
55   0.981201  0.067024  0.803955  1.424083
56   0.978149  0.067060  0.857788  0.793058
57   0.978760  0.067828  0.867676  0.726538
58   0.980713  0.064389  0.833740  1.263843
59   0.981689  0.056125  0.846802  1.082984
60   0.980713  0.062062  0.819092  1.410670
61   0.979980  0.064659  0.841553  1.259135
62   0.984619  0.054448  0.825562  1.213077
63   0.982666  0.060805  0.820068  1.419429
64   0.981689  0.061336  0.835449  0.987041
65   0.983398  0.056819  0.840698  1.103736
66   0.982910  0.062814  0.812134  1.345227
67   0.983887  0.060159  0.807617  1.581855
68   0.984131  0.058689  0.822388  1.652098
69   0.981689  0.065056  0.828735  1.280877
70   0.984985  0.049371  0.840088  1.165192
71   0.981812  0.063654  0.838989  1.151780
72   0.983032  0.063174  0.844482  0.809299
73   0.983887  0.060354  0.829956  1.281879
74   0.979004  0.072689  0.867676  0.766617
75   0.982544  0.057865  0.834229  1.256206
76   0.983643  0.056903  0.840088  0.913701
77   0.980591  0.064162  0.826660  1.370317
78   0.983765  0.054135  0.860107  0.862493
79   0.982910  0.060227  0.856812  0.843605
80   0.984253  0.054331  0.849609  1.057551
81   0.981812  0.062222  0.836182  1.249101
82   0.981934  0.066734  0.830444  1.387363
83   0.980225  0.061308  0.820435  1.594836
84   0.983765  0.060008  0.845337  1.185100
85   0.980225  0.063988  0.851929  0.861210
86   0.980713  0.064323  0.817627  1.785018
87   0.981567  0.063616  0.818726  1.181007
88   0.983887  0.066274  0.810669  1.531955
89   0.987427  0.042572  0.839111  1.430615
90   0.980103  0.070708  0.803223  2.037663
91   0.979004  0.075318  0.863892  1.018859
92   0.980225  0.070061  0.839844  1.185819
93   0.983765  0.052383  0.849243  1.110274
94   0.984009  0.061210  0.828979  1.195396
95   0.981812  0.064992  0.825562  1.468363
96   0.984131  0.059982  0.846924  0.923766
97   0.983643  0.065251  0.852417  0.815456
98   0.980713  0.067830  0.812866  1.637598
99   0.981445  0.078867  0.857300  1.073641
100  0.982910  0.064257  0.856812  1.028821
101  0.982788  0.062913  0.828735  1.430149
102  0.983521  0.059441  0.818970  1.471592
103  0.983887  0.051633  0.797607  1.670337
104  0.983032  0.060435  0.814209  1.687177
105  0.981812  0.073207  0.826904  1.591042
106  0.984741  0.054178  0.840454  1.460221
107  0.980469  0.069816  0.852417  0.850007
108  0.978027  0.104318  0.819214  2.064339
109  0.968750  0.150080  0.851196  1.271300
110  0.973267  0.123891  0.859375  1.152436
111  0.973877  0.121946  0.847656  1.304170
112  0.979370  0.086469  0.840210  1.632024
113  0.978149  0.085426  0.838501  1.442027
114  0.978882  0.092967  0.791138  2.235269
115  0.972412  0.126748  0.837158  1.601635
116  0.974365  0.164613  0.838867  1.658128
117  0.978271  0.128874  0.776611  2.990247
118  0.974487  0.154878  0.844238  1.262342
119  0.970459  0.225031  0.790894  3.261454
120  0.842773  2.444190  0.554932  7.050206
121  0.928955  0.997953  0.825684  2.783262
122  0.925659  1.122389  0.832031  2.683087
123  0.841797  2.469574  0.489868  8.222354
124  0.713623  4.561953  0.500244  8.055113
125  0.720703  4.432301  0.494629  8.145602
126  0.755981  3.826343  0.503540  8.001753
127  0.763672  3.684398  0.499146  8.072542

2018-06-08 16:44:23.701803 Finish.
Total elapsed time: 30:09:21.70.
