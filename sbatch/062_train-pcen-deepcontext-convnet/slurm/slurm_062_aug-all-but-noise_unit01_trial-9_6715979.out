2018-06-07 10:35:12.258558: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:12.258762: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:12.258777: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:12.258784: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:12.258790: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:35:01.939285 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit02, unit03, unit05.
Validation set: unit07, unit10.
Test set: unit01.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.519653  7.734573  0.491577  8.194808
1    0.520752  7.716458  0.495483  8.131847
2    0.513184  7.838810  0.508789  7.917385
3    0.504395  7.979980  0.494507  8.147587
4    0.513794  7.827664  0.498413  8.084626
5    0.517212  7.773988  0.506470  7.954768
6    0.511597  7.863594  0.498901  8.076756
7    0.501465  8.026664  0.502686  8.015762
8    0.508301  7.917190  0.490845  8.206613
9    0.511353  7.867916  0.495972  8.123977
10   0.516602  7.783097  0.495239  8.135782
11   0.515747  7.796612  0.501465  8.035437
12   0.510986  7.873454  0.511353  7.876066
13   0.511353  7.867465  0.507446  7.939028
14   0.501953  8.019030  0.503174  8.007892
15   0.511353  7.868152  0.508301  7.925255
16   0.506226  7.950338  0.492310  8.183003
17   0.514160  7.822877  0.505371  7.972476
18   0.512451  7.850851  0.501953  8.027567
19   0.512451  7.850765  0.498535  8.082658
20   0.505737  7.958144  0.504883  7.980346
21   0.509888  7.891419  0.486206  8.281380
22   0.515015  7.809082  0.497681  8.096431
23   0.512451  7.850122  0.506348  7.956736
24   0.508301  7.916975  0.494507  8.147587
25   0.520752  7.716823  0.499268  8.070853
26   0.518799  7.748067  0.491821  8.190873
27   0.514282  7.820223  0.490479  8.212516
28   0.522217  7.696430  0.489746  8.224321
29   0.508423  7.923180  0.494629  8.145620
30   0.488037  8.251781  0.498047  8.090528
31   0.503906  7.996022  0.502686  8.015762
32   0.493408  8.165295  0.502930  8.011827
33   0.492798  8.175047  0.501587  8.033470
34   0.503784  7.998033  0.493408  8.165295
35   0.503174  8.007870  0.493286  8.167263
36   0.498779  8.078616  0.492065  8.186938
37   0.497437  8.100302  0.490479  8.212516
38   0.499878  8.060887  0.493896  8.157425
39   0.494141  8.153340  0.500000  8.059048
40   0.506226  7.958617  0.503296  8.005924
41   0.497314  8.102269  0.491089  8.202678
42   0.494141  8.153447  0.500732  8.047243
43   0.498657  8.080669  0.499878  8.061015
44   0.501953  8.027524  0.504272  7.990184
45   0.503052  8.009816  0.496338  8.118074
46   0.501099  8.041297  0.501587  8.033470
47   0.497437  8.100280  0.498047  8.090528
48   0.500732  8.047092  0.505249  7.974443
49   0.505737  7.966466  0.496582  8.114139
50   0.508667  7.919267  0.500977  8.043307
51   0.499268  8.070767  0.503174  8.007892
52   0.500244  8.055005  0.496948  8.108236
53   0.502441  8.019590  0.501709  8.031502
54   0.501831  8.029492  0.496094  8.122009
55   0.495728  8.127847  0.501587  8.033470
56   0.499268  8.070789  0.501099  8.041340
57   0.495361  8.133771  0.489868  8.222354
58   0.495728  8.127804  0.503418  8.003957
59   0.498657  8.080648  0.497803  8.094464
60   0.497803  8.094292  0.507324  7.940995
61   0.506348  7.956650  0.503662  8.000022
62   0.503052  8.009795  0.495605  8.129879
63   0.501709  8.031416  0.498535  8.082658
64   0.492188  8.184928  0.507690  7.935093
65   0.505249  7.974358  0.499878  8.061015
66   0.492554  8.178939  0.492065  8.186938
67   0.492554  8.179025  0.504761  7.982314
68   0.499268  8.070789  0.498169  8.088561
69   0.505615  7.968348  0.498779  8.078723
70   0.494873  8.141663  0.496094  8.122009
71   0.500488  8.051113  0.506226  7.958703
72   0.502563  8.017665  0.500122  8.057080
73   0.491821  8.190809  0.496826  8.110204
74   0.498779  8.078637  0.505371  7.972476
75   0.494263  8.151479  0.498047  8.090528
76   0.498901  8.076713  0.502808  8.013794
77   0.504272  7.990120  0.506470  7.954768
78   0.490479  8.212495  0.504883  7.980346
79   0.502563  8.017622  0.499146  8.072821
80   0.505127  7.976347  0.501465  8.035437
81   0.509521  7.905537  0.495117  8.137749
82   0.507935  7.931136  0.494019  8.155457
83   0.499634  8.064886  0.500488  8.051178
84   0.509888  7.899634  0.494873  8.141685
85   0.490356  8.214419  0.497192  8.104301
86   0.496826  8.110161  0.499756  8.062983
87   0.505127  7.976282  0.496704  8.112171
88   0.505493  7.970358  0.502197  8.023632
89   0.501099  8.041254  0.509399  7.907547
90   0.498047  8.090507  0.504395  7.988216
91   0.495117  8.137621  0.493164  8.169230
92   0.500610  8.049146  0.495239  8.135782
93   0.498413  8.084561  0.503296  8.005924
94   0.498413  8.084561  0.494995  8.139717
95   0.492188  8.184906  0.501953  8.027567
96   0.502808  8.013730  0.504395  7.988216
97   0.496582  8.114010  0.498169  8.088561
98   0.496460  8.116085  0.497803  8.094464
99   0.493530  8.163220  0.503418  8.003957
100  0.495850  8.125794  0.507568  7.937060
101  0.494019  8.155371  0.500977  8.043307
102  0.503906  7.996044  0.499512  8.066918
103  0.497925  8.092389  0.507446  7.939028
104  0.498291  8.086593  0.498657  8.080691
105  0.498291  8.086572  0.508667  7.919352
106  0.493896  8.157360  0.489746  8.224321
107  0.500366  8.053081  0.501953  8.027567
108  0.491089  8.202635  0.491699  8.192841
109  0.500244  8.055070  0.495239  8.135782
110  0.505737  7.966552  0.505249  7.974443
111  0.494995  8.139631  0.500732  8.047243
112  0.505371  7.972390  0.502808  8.013794
113  0.491455  8.196647  0.499756  8.062983
114  0.502686  8.015676  0.496582  8.114139
115  0.505371  7.972412  0.490356  8.214484
116  0.499512  8.066875  0.503052  8.009859
117  0.497925  8.092432  0.507080  7.944930
118  0.497070  8.106226  0.494263  8.151522
119  0.500000  8.058983  0.504761  7.982314
120  0.500244  8.055027  0.494141  8.153490
121  0.491211  8.200646  0.499023  8.074788
122  0.503418  8.003871  0.498291  8.086593
123  0.495728  8.127869  0.498779  8.078723
124  0.499023  8.074702  0.495605  8.129879
125  0.502930  8.011805  0.505737  7.966573
126  0.504395  7.988152  0.507446  7.939028
127  0.514893  7.818965  0.498413  8.084626

2018-06-08 16:15:52.685802 Finish.
Total elapsed time: 29:40:51.69.
