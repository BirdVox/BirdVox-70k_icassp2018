2018-06-07 10:35:12.784341: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:12.784526: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:12.784539: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:35:02.850567 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.494873  8.141642  0.489014  8.236126
1    0.499878  8.061015  0.512207  7.862294
2    0.496948  8.108215  0.498291  8.086593
3    0.507690  7.935093  0.495117  8.137749
4    0.506714  7.950833  0.485352  8.295153
5    0.504395  7.988216  0.496094  8.122009
6    0.506592  7.952801  0.498169  8.088561
7    0.487915  8.253813  0.502808  8.013794
8    0.500366  8.053102  0.491943  8.188906
9    0.496216  8.120042  0.492798  8.175133
10   0.497437  8.100345  0.500610  8.049210
11   0.496094  8.122009  0.498413  8.084626
12   0.512817  7.852435  0.499756  8.062983
13   0.504028  7.994119  0.491943  8.188905
14   0.494019  8.155436  0.495483  8.131847
15   0.500366  8.053145  0.503906  7.996086
16   0.503540  8.001989  0.502319  8.021665
17   0.500610  8.049210  0.499634  8.064950
18   0.510620  7.887850  0.488281  8.247932
19   0.489258  8.232191  0.491699  8.192841
20   0.502930  8.011805  0.498047  8.090528
21   0.501099  8.041340  0.500977  8.043307
22   0.492554  8.179068  0.505737  7.966573
23   0.494629  8.145620  0.498779  8.078723
24   0.491211  8.200711  0.504272  7.990184
25   0.498901  8.076734  0.499023  8.074788
26   0.495117  8.137749  0.493774  8.159392
27   0.501953  8.027524  0.505371  7.972476
28   0.500977  8.043265  0.498291  8.086593
29   0.500122  8.057080  0.501221  8.039372
30   0.496094  8.121988  0.493652  8.161360
31   0.501099  8.041340  0.505127  7.976411
32   0.505371  7.972433  0.503662  8.000022
33   0.498901  8.076756  0.508057  7.929190
34   0.497070  8.106269  0.505615  7.968541
35   0.504150  7.992130  0.494019  8.155457
36   0.507324  7.940952  0.499146  8.072821
37   0.508423  7.923266  0.500244  8.055113
38   0.506714  7.950833  0.492920  8.173165
39   0.486206  8.281380  0.494141  8.153490
40   0.502197  8.023632  0.503418  8.003957
41   0.502075  8.025600  0.489868  8.222354
42   0.508667  7.919331  0.500488  8.051178
43   0.502686  8.015740  0.502075  8.025600
44   0.501709  8.031502  0.489014  8.236126
45   0.504395  7.988173  0.488892  8.238094
46   0.499146  8.072821  0.498047  8.090528
47   0.501831  8.029535  0.499390  8.068885
48   0.499878  8.060994  0.507446  7.939028
49   0.503784  7.998054  0.505005  7.978379
50   0.504395  7.988195  0.507324  7.940995
51   0.495361  8.133814  0.491821  8.190873
52   0.481567  8.356146  0.503784  7.998054
53   0.499023  8.074788  0.502808  8.013794
54   0.505615  7.968541  0.502563  8.017729
55   0.506470  7.954768  0.500488  8.051178
56   0.492676  8.177079  0.494751  8.143652
57   0.490356  8.214462  0.498657  8.080691
58   0.495972  8.123977  0.507080  7.944930
59   0.503174  8.007827  0.502930  8.011827
60   0.496338  8.118053  0.493530  8.163327
61   0.502441  8.019697  0.502197  8.023632
62   0.496094  8.121988  0.493652  8.161360
63   0.493896  8.157425  0.501831  8.029535
64   0.495239  8.135782  0.494385  8.149555
65   0.502686  8.015762  0.502808  8.013794
66   0.501709  8.031502  0.501343  8.037405
67   0.508545  7.921298  0.503784  7.998054
68   0.491089  8.202657  0.506836  7.948865
69   0.493164  8.169230  0.503540  8.001989
70   0.503540  8.001968  0.495483  8.131847
71   0.499756  8.062983  0.492432  8.181035
72   0.501587  8.033448  0.499390  8.068886
73   0.505249  7.974444  0.497925  8.092496
74   0.496338  8.118074  0.494995  8.139717
75   0.501465  8.035437  0.494751  8.143652
76   0.507202  7.942963  0.498169  8.088561
77   0.500854  8.045275  0.502075  8.025600
78   0.510132  7.895742  0.499634  8.064950
79   0.509399  7.907483  0.492065  8.186938
80   0.506714  7.950833  0.497681  8.096431
81   0.500000  8.059048  0.501343  8.037405
82   0.498047  8.090528  0.510620  7.887872
83   0.501709  8.031502  0.510132  7.895742
84   0.508179  7.927223  0.502808  8.013794
85   0.498657  8.080669  0.488770  8.240062
86   0.494995  8.139717  0.502930  8.011827
87   0.493042  8.171176  0.501709  8.031502
88   0.505005  7.978379  0.501221  8.039372
89   0.504883  7.980346  0.498169  8.088561
90   0.499390  8.068864  0.500854  8.045275
91   0.496094  8.121988  0.495239  8.135782
92   0.505005  7.978379  0.505371  7.972476
93   0.499146  8.072821  0.505005  7.978379
94   0.501221  8.039329  0.502808  8.013794
95   0.501953  8.027524  0.495728  8.127912
96   0.506348  7.956736  0.506592  7.952801
97   0.503418  8.003957  0.495117  8.137749
98   0.501221  8.039351  0.504150  7.992151
99   0.500610  8.049167  0.492676  8.177100
100  0.503906  7.996086  0.495728  8.127912
101  0.495850  8.125923  0.500854  8.045275
102  0.504272  7.990184  0.497437  8.100366
103  0.494019  8.155414  0.507080  7.944930
104  0.500610  8.049189  0.497070  8.106269
105  0.503418  8.003957  0.508545  7.921320
106  0.496216  8.120042  0.498779  8.078723
107  0.493530  8.163327  0.496948  8.108236
108  0.489258  8.232170  0.489258  8.232191
109  0.504150  7.992130  0.506836  7.948865
110  0.493042  8.171198  0.493408  8.165295
111  0.487305  8.263651  0.496094  8.122009
112  0.490112  8.218419  0.509888  7.899677
113  0.497681  8.096431  0.498535  8.082658
114  0.511108  7.880002  0.506470  7.954768
115  0.497437  8.100345  0.508667  7.919352
116  0.501221  8.039372  0.501343  8.037405
117  0.505737  7.966573  0.498657  8.080691
118  0.494263  8.151501  0.501831  8.029535
119  0.500122  8.057080  0.503174  8.007892
120  0.491699  8.192819  0.494751  8.143652
121  0.500366  8.053124  0.497559  8.098399
122  0.500488  8.051156  0.500122  8.057080
123  0.504028  7.994119  0.499878  8.061015
124  0.498047  8.090464  0.504395  7.988216
125  0.498779  8.078702  0.496704  8.112171
126  0.503418  8.003935  0.493042  8.171198
127  0.508667  7.919352  0.506714  7.950833

2018-06-08 16:52:25.943689 Finish.
Total elapsed time: 30:17:23.94.
