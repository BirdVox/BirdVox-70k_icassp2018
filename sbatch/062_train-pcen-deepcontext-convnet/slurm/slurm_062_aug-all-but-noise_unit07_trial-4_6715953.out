2018-06-07 10:35:05.301406: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.301629: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.301641: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:35:02.850558 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit10, unit01, unit02.
Validation set: unit03, unit05.
Test set: unit07.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.919067  0.222674  0.951538  0.140673
1    0.918945  0.242784  0.952271  0.135305
2    0.926514  0.205682  0.942139  0.161752
3    0.927368  0.202518  0.928833  0.195315
4    0.924683  0.210804  0.948364  0.157553
5    0.928467  0.206549  0.945801  0.164705
6    0.941040  0.170045  0.902588  0.266691
7    0.943726  0.165674  0.955444  0.148589
8    0.937134  0.191750  0.952393  0.148369
9    0.938477  0.192231  0.884644  0.264276
10   0.942993  0.170972  0.949097  0.147788
11   0.942139  0.162966  0.954468  0.129814
12   0.945801  0.166061  0.969849  0.095401
13   0.941650  0.175528  0.967773  0.104159
14   0.944458  0.156944  0.961670  0.128156
15   0.945435  0.160006  0.965332  0.122719
16   0.953613  0.145674  0.955811  0.165704
17   0.949341  0.150276  0.948364  0.157682
18   0.950806  0.148329  0.957642  0.134450
19   0.946411  0.177149  0.971069  0.103916
20   0.951904  0.151229  0.933960  0.175050
21   0.946899  0.160450  0.962280  0.121203
22   0.952393  0.137061  0.962280  0.115293
23   0.948242  0.159276  0.961426  0.129209
24   0.945801  0.161702  0.957642  0.134092
25   0.948730  0.152487  0.969604  0.089753
26   0.954834  0.141684  0.967651  0.100306
27   0.946777  0.173175  0.940552  0.154556
28   0.944458  0.171547  0.955688  0.205508
29   0.948975  0.146403  0.963257  0.109743
30   0.957153  0.134619  0.969849  0.099436
31   0.953857  0.134502  0.965332  0.109996
32   0.950195  0.147366  0.964111  0.116385
33   0.954346  0.134010  0.943359  0.152172
34   0.948120  0.163067  0.952515  0.164102
35   0.950684  0.158181  0.964600  0.111473
36   0.955078  0.131399  0.972900  0.086372
37   0.956055  0.139680  0.950317  0.128496
38   0.954712  0.132254  0.972900  0.089935
39   0.956177  0.137929  0.964600  0.108472
40   0.951294  0.136121  0.957275  0.125230
41   0.956909  0.127248  0.963867  0.111902
42   0.952515  0.146373  0.969360  0.094759
43   0.954102  0.147664  0.965820  0.113464
44   0.953613  0.137478  0.971924  0.083860
45   0.958984  0.118499  0.970337  0.092564
46   0.957886  0.116900  0.971191  0.077514
47   0.957642  0.137171  0.963867  0.100778
48   0.959106  0.121613  0.958130  0.116710
49   0.952637  0.138764  0.950073  0.143790
50   0.953735  0.150201  0.969727  0.094494
51   0.956665  0.138892  0.936157  0.175820
52   0.958984  0.138392  0.969482  0.100763
53   0.950806  0.170235  0.956421  0.122854
54   0.959351  0.129404  0.971436  0.105323
55   0.951050  0.140445  0.947876  0.149191
56   0.954102  0.143236  0.951904  0.121710
57   0.952759  0.138567  0.973511  0.084106
58   0.954834  0.129305  0.974731  0.081902
59   0.957520  0.129300  0.973633  0.085057
60   0.962280  0.128947  0.937378  0.173618
61   0.960205  0.127359  0.966553  0.098430
62   0.958130  0.131226  0.954346  0.127240
63   0.955933  0.138697  0.956787  0.127607
64   0.948608  0.172896  0.970703  0.092000
65   0.957642  0.125922  0.968140  0.104854
66   0.957520  0.138461  0.968384  0.106654
67   0.957520  0.134311  0.955444  0.119221
68   0.958130  0.132193  0.972168  0.088977
69   0.960693  0.129022  0.957886  0.136345
70   0.956909  0.137325  0.972290  0.088940
71   0.957642  0.132215  0.974365  0.090984
72   0.954468  0.152400  0.969238  0.111877
73   0.954590  0.138415  0.960693  0.112295
74   0.956909  0.142520  0.918213  0.222593
75   0.952515  0.180144  0.958740  0.121686
76   0.953613  0.141371  0.962891  0.109213
77   0.958618  0.137322  0.962769  0.112326
78   0.953125  0.150898  0.970215  0.102229
79   0.953491  0.161159  0.961670  0.127921
80   0.956543  0.141324  0.966309  0.109959
81   0.957397  0.134518  0.963867  0.112180
82   0.957642  0.141417  0.968384  0.116454
83   0.956299  0.145398  0.972412  0.092385
84   0.961060  0.118932  0.963867  0.111415
85   0.961060  0.108679  0.970459  0.107843
86   0.962891  0.124767  0.969849  0.094456
87   0.961426  0.128263  0.955444  0.147685
88   0.957031  0.133806  0.964966  0.105851
89   0.948120  0.171989  0.950195  0.139300
90   0.951172  0.168746  0.961670  0.112368
91   0.960449  0.138150  0.935913  0.173872
92   0.940918  0.229752  0.918701  0.199099
93   0.944214  0.184834  0.965210  0.110536
94   0.949219  0.187007  0.961182  0.117322
95   0.953979  0.149763  0.953857  0.119118
96   0.962402  0.137779  0.960693  0.122182
97   0.961792  0.136801  0.964233  0.110989
98   0.961304  0.117936  0.968506  0.110082
99   0.962280  0.125497  0.969116  0.094069
100  0.960083  0.125241  0.973877  0.097638
101  0.965576  0.117785  0.975464  0.108045
102  0.959473  0.138803  0.965698  0.106832
103  0.956055  0.156352  0.921387  0.332536
104  0.949585  0.176626  0.970337  0.117434
105  0.957153  0.137327  0.966309  0.100317
106  0.960449  0.128711  0.965576  0.119459
107  0.962646  0.122684  0.969971  0.096958
108  0.961060  0.124207  0.963501  0.106570
109  0.957886  0.145314  0.975586  0.085372
110  0.963623  0.119520  0.946289  0.166849
111  0.960571  0.131540  0.956787  0.185452
112  0.961426  0.119120  0.945801  0.144552
113  0.958740  0.141751  0.946777  0.148826
114  0.939575  0.230820  0.971924  0.108250
115  0.942383  0.161052  0.969116  0.109603
116  0.942017  0.162509  0.967285  0.112434
117  0.943970  0.153327  0.961304  0.124967
118  0.949097  0.143043  0.971924  0.088568
119  0.947144  0.155548  0.968140  0.095404
120  0.950439  0.133690  0.971558  0.084018
121  0.946655  0.144811  0.969604  0.093313
122  0.946289  0.136389  0.969360  0.097478
123  0.949097  0.131564  0.964844  0.095228
124  0.949829  0.135547  0.967773  0.113011
125  0.944702  0.142309  0.967773  0.096610
126  0.942871  0.145464  0.969971  0.094091
127  0.947021  0.141246  0.966187  0.111568

2018-06-08 16:55:45.546134 Finish.
Total elapsed time: 30:20:43.55.
