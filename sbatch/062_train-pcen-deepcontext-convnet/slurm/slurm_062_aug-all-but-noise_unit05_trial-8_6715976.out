2018-06-07 10:35:04.257882: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:04.258080: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:04.258091: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:04.258096: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:04.258100: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:35:02.140846 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit07, unit10, unit01.
Validation set: unit02, unit03.
Test set: unit05.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.915039  0.247838  0.885620  0.332226
1    0.913330  0.247908  0.915283  0.234087
2    0.924683  0.220358  0.915283  0.261517
3    0.926758  0.202493  0.908081  0.280589
4    0.927002  0.223515  0.916260  0.314486
5    0.908447  0.302028  0.925781  0.278037
6    0.924194  0.233639  0.919678  0.287435
7    0.933594  0.199211  0.930908  0.281652
8    0.927490  0.206220  0.917969  0.307987
9    0.938599  0.179424  0.926270  0.270306
10   0.937134  0.195774  0.938965  0.217808
11   0.938110  0.179107  0.912231  0.329781
12   0.940430  0.185852  0.910400  0.279167
13   0.941772  0.167318  0.925293  0.330730
14   0.930298  0.199906  0.896851  0.294132
15   0.939575  0.193909  0.946167  0.198214
16   0.947998  0.169295  0.933838  0.239749
17   0.947266  0.157175  0.937988  0.270981
18   0.938232  0.182437  0.946167  0.169412
19   0.937988  0.200888  0.918091  0.271012
20   0.943359  0.179729  0.936279  0.223574
21   0.943481  0.175437  0.943604  0.204113
22   0.948364  0.169468  0.931274  0.211649
23   0.935303  0.211299  0.936035  0.220246
24   0.937622  0.189303  0.929443  0.194254
25   0.943726  0.177747  0.931641  0.247984
26   0.939819  0.186059  0.927246  0.313140
27   0.945190  0.170524  0.936768  0.284103
28   0.947632  0.161985  0.935669  0.261266
29   0.939331  0.188365  0.953735  0.170169
30   0.950439  0.162711  0.941772  0.232140
31   0.944946  0.158558  0.933472  0.329022
32   0.947266  0.163423  0.926880  0.355659
33   0.948120  0.175260  0.943359  0.199121
34   0.944702  0.174112  0.951782  0.215855
35   0.948730  0.157416  0.939941  0.222710
36   0.950562  0.169185  0.939087  0.222047
37   0.947632  0.160218  0.930054  0.303191
38   0.942261  0.176910  0.933716  0.240323
39   0.946289  0.157982  0.914185  0.313917
40   0.947876  0.165371  0.948242  0.202700
41   0.949341  0.160667  0.922607  0.310227
42   0.952881  0.139191  0.932251  0.298058
43   0.947388  0.157950  0.950806  0.185521
44   0.950073  0.145174  0.941162  0.195874
45   0.949951  0.147787  0.932495  0.237173
46   0.958984  0.129851  0.938110  0.231632
47   0.949341  0.139563  0.947632  0.174769
48   0.957275  0.137909  0.934326  0.216714
49   0.954834  0.147624  0.943237  0.202715
50   0.953491  0.151988  0.949219  0.201044
51   0.948608  0.169225  0.943359  0.209369
52   0.958862  0.119951  0.927612  0.310272
53   0.950928  0.145955  0.926636  0.275559
54   0.954834  0.135941  0.926636  0.292112
55   0.953979  0.140740  0.905884  0.433389
56   0.953979  0.152384  0.937500  0.203414
57   0.960449  0.131072  0.952637  0.203001
58   0.952515  0.156187  0.925171  0.360497
59   0.949585  0.150323  0.946777  0.179439
60   0.941895  0.177863  0.940308  0.272946
61   0.951782  0.145075  0.944092  0.226805
62   0.936768  0.203721  0.874023  1.119875
63   0.929932  0.209463  0.897461  0.822099
64   0.931030  0.204452  0.874634  1.201754
65   0.936890  0.194462  0.902832  0.879393
66   0.940796  0.161723  0.907104  0.736241
67   0.943604  0.168704  0.881592  0.766928
68   0.943848  0.151350  0.900269  0.680231
69   0.944824  0.150800  0.880981  0.891956
70   0.943481  0.158593  0.891968  0.961347
71   0.942139  0.165387  0.864380  1.036714
72   0.942017  0.162788  0.867432  1.121652
73   0.940430  0.160703  0.881836  1.119996
74   0.943848  0.154406  0.859497  1.269223
75   0.944214  0.163607  0.845459  1.568446
76   0.942139  0.170563  0.867065  0.952424
77   0.944702  0.147361  0.884521  1.014250
78   0.947266  0.146098  0.846558  1.637519
79   0.950928  0.140874  0.841064  1.690957
80   0.944214  0.160146  0.831055  1.667431
81   0.941772  0.165596  0.812378  2.011220
82   0.944092  0.173813  0.882812  1.005114
83   0.945801  0.160142  0.860229  1.298147
84   0.937988  0.177682  0.806641  2.182774
85   0.943970  0.170026  0.821289  1.946711
86   0.947021  0.159114  0.838989  1.757319
87   0.946655  0.160449  0.849976  1.484129
88   0.943359  0.170502  0.821167  1.761494
89   0.945312  0.150432  0.810181  2.000258
90   0.947876  0.145819  0.831421  1.616487
91   0.950806  0.130957  0.826782  2.012384
92   0.936157  0.211780  0.742432  2.797129
93   0.939941  0.161504  0.792603  2.616865
94   0.948242  0.153677  0.804443  2.395576
95   0.941772  0.164884  0.763306  2.653559
96   0.917480  0.652298  0.598145  6.400105
97   0.811646  2.146421  0.835815  1.518445
98   0.772949  1.904282  0.600952  6.361521
99   0.640381  3.992422  0.567871  6.889167
100  0.644897  3.833380  0.587769  6.571953
101  0.654907  3.950768  0.601685  6.347827
102  0.632812  3.952900  0.561768  6.986470
103  0.588745  5.125585  0.498291  8.086593
104  0.551270  6.128045  0.497681  8.096431
105  0.582397  6.122915  0.491699  8.192841
106  0.595703  5.931603  0.501709  8.031502
107  0.584717  6.217877  0.501587  8.033470
108  0.596069  5.738720  0.497437  8.100366
109  0.588989  5.830185  0.501465  8.035437
110  0.596924  5.706878  0.503174  8.007892
111  0.614746  5.866845  0.503784  7.998054
112  0.547607  6.955373  0.494507  8.147587
113  0.550171  6.881057  0.503296  8.005924
114  0.552002  6.332777  0.502930  8.011827
115  0.561890  6.369491  0.507202  7.942963
116  0.571167  6.386362  0.499756  8.062983
117  0.555908  6.405927  0.493408  8.165295
118  0.540771  6.197611  0.504639  7.984281
119  0.537476  6.234750  0.500366  8.053145
120  0.536499  6.309326  0.500610  8.049210
121  0.533936  6.368692  0.502930  8.011827
122  0.537231  6.154871  0.501587  8.033470
123  0.533936  6.295383  0.503906  7.996086
124  0.528931  6.269462  0.505249  7.974444
125  0.532959  6.292460  0.505859  7.964606
126  0.536987  6.223668  0.498657  8.080691
127  0.533569  6.312806  0.500610  8.049210

2018-06-08 18:00:17.162819 Finish.
Total elapsed time: 31:25:15.16.
