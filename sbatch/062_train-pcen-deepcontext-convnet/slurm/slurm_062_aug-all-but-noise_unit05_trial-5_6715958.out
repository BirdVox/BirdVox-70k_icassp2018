2018-06-07 10:35:05.414103: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.414318: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.414330: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:35:02.804990 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit07, unit10, unit01.
Validation set: unit02, unit03.
Test set: unit05.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.927734  0.198197  0.906860  0.331115
1    0.931274  0.192717  0.893799  0.497836
2    0.937744  0.186556  0.881714  0.599217
3    0.929443  0.206293  0.906006  0.449189
4    0.933594  0.186165  0.901123  0.286732
5    0.938110  0.177260  0.921753  0.303787
6    0.934448  0.188644  0.925049  0.333888
7    0.941406  0.165146  0.919800  0.332643
8    0.944702  0.152260  0.928223  0.261120
9    0.937012  0.173635  0.928223  0.219080
10   0.938110  0.166005  0.910889  0.414309
11   0.938232  0.175655  0.928589  0.228090
12   0.949829  0.146500  0.924927  0.250541
13   0.939941  0.161519  0.940796  0.184812
14   0.940308  0.172290  0.939941  0.206695
15   0.945435  0.154839  0.916992  0.301655
16   0.946045  0.153870  0.931885  0.221087
17   0.948730  0.153652  0.908691  0.389745
18   0.952148  0.147651  0.907959  0.413594
19   0.945557  0.164097  0.946533  0.199799
20   0.951172  0.146674  0.930420  0.293009
21   0.948608  0.146462  0.885864  0.573291
22   0.943848  0.172052  0.925415  0.220297
23   0.952759  0.140373  0.912720  0.425510
24   0.954956  0.129180  0.930176  0.311023
25   0.953369  0.147399  0.931030  0.323293
26   0.949341  0.157401  0.943115  0.222677
27   0.949585  0.152703  0.942261  0.218359
28   0.948853  0.146331  0.945679  0.175644
29   0.948242  0.151955  0.940430  0.247540
30   0.952026  0.144085  0.938599  0.259844
31   0.953369  0.140365  0.958984  0.167447
32   0.952271  0.141878  0.935913  0.315382
33   0.950684  0.140396  0.944214  0.285099
34   0.951294  0.146175  0.943115  0.334978
35   0.955444  0.138801  0.943237  0.268006
36   0.953979  0.148969  0.941040  0.243323
37   0.955200  0.142419  0.915771  0.528919
38   0.952515  0.147031  0.937378  0.320713
39   0.955811  0.139245  0.927979  0.379273
40   0.957764  0.135339  0.926880  0.416658
41   0.951538  0.154020  0.944702  0.217231
42   0.958984  0.131178  0.954590  0.198488
43   0.952515  0.142479  0.909668  0.511046
44   0.943481  0.189086  0.941040  0.223048
45   0.959961  0.128418  0.951904  0.201193
46   0.950562  0.162056  0.932373  0.380404
47   0.954590  0.136194  0.940308  0.264223
48   0.953979  0.142011  0.941650  0.235243
49   0.960083  0.121597  0.940552  0.307431
50   0.957275  0.131044  0.942505  0.276421
51   0.959106  0.122765  0.954834  0.217547
52   0.958008  0.134949  0.955200  0.191687
53   0.956299  0.129413  0.950684  0.181502
54   0.953613  0.135580  0.949951  0.291821
55   0.952148  0.148399  0.936646  0.281317
56   0.950073  0.147910  0.943237  0.263889
57   0.955688  0.146618  0.940674  0.270834
58   0.955933  0.126212  0.959961  0.170182
59   0.954590  0.125881  0.949951  0.256291
60   0.958252  0.136074  0.951172  0.223428
61   0.955566  0.153774  0.934326  0.394010
62   0.931030  0.253814  0.934448  0.281181
63   0.903809  0.336991  0.861694  0.344445
64   0.908936  0.271157  0.932251  0.201074
65   0.917725  0.248041  0.937012  0.197092
66   0.927490  0.204518  0.929077  0.212130
67   0.925659  0.207435  0.940430  0.178071
68   0.928467  0.202734  0.932129  0.219269
69   0.934326  0.190504  0.936401  0.239706
70   0.934570  0.196116  0.939941  0.264466
71   0.942139  0.175258  0.943726  0.177407
72   0.940308  0.178126  0.867310  0.314595
73   0.939697  0.180557  0.945435  0.171040
74   0.939087  0.173065  0.945312  0.191617
75   0.940674  0.177980  0.936035  0.208289
76   0.931396  0.196832  0.937744  0.192944
77   0.943359  0.152635  0.950195  0.201395
78   0.947632  0.155788  0.942017  0.213842
79   0.939575  0.183280  0.940063  0.193613
80   0.941895  0.172555  0.941895  0.178925
81   0.944580  0.159008  0.951172  0.168704
82   0.943604  0.166773  0.943359  0.195018
83   0.950562  0.151926  0.939819  0.209100
84   0.942139  0.173547  0.935303  0.227250
85   0.945190  0.169476  0.921021  0.270093
86   0.943237  0.170122  0.940063  0.310788
87   0.948975  0.154200  0.949585  0.228650
88   0.946167  0.179516  0.948608  0.240875
89   0.941650  0.165030  0.947144  0.223532
90   0.952515  0.150451  0.934814  0.204122
91   0.949707  0.151394  0.921021  0.331269
92   0.947021  0.158233  0.958252  0.150805
93   0.947754  0.158307  0.932983  0.218888
94   0.951782  0.140373  0.954834  0.165679
95   0.950073  0.145406  0.935669  0.244461
96   0.944580  0.172225  0.957520  0.243395
97   0.948975  0.159803  0.940796  0.266549
98   0.949341  0.158480  0.949341  0.296273
99   0.945190  0.174816  0.929077  0.311365
100  0.953125  0.143169  0.947510  0.203715
101  0.949585  0.151872  0.960327  0.162183
102  0.954834  0.135950  0.954468  0.205892
103  0.964111  0.108960  0.947021  0.269813
104  0.951050  0.152334  0.936890  0.250532
105  0.952881  0.141726  0.953613  0.152334
106  0.951904  0.156721  0.943848  0.271829
107  0.950317  0.139713  0.945312  0.237159
108  0.954102  0.134455  0.955444  0.170317
109  0.952637  0.137874  0.947998  0.202102
110  0.955811  0.128828  0.949585  0.211183
111  0.961914  0.115319  0.944702  0.220529
112  0.956665  0.124911  0.936646  0.301259
113  0.954834  0.138949  0.947632  0.244628
114  0.956909  0.142659  0.914307  0.293627
115  0.952393  0.153291  0.958252  0.187270
116  0.942383  0.198916  0.944946  0.307181
117  0.941284  0.195163  0.919800  0.244478
118  0.951782  0.164160  0.945435  0.286701
119  0.957153  0.133949  0.952148  0.227114
120  0.955322  0.133987  0.935303  0.296035
121  0.956543  0.140590  0.947144  0.228287
122  0.959473  0.131418  0.938110  0.323340
123  0.959717  0.126884  0.949097  0.201564
124  0.964233  0.111218  0.952271  0.268169
125  0.959106  0.125240  0.937256  0.247941
126  0.939819  0.197500  0.918823  0.245456
127  0.942261  0.174050  0.959473  0.129190

2018-06-08 16:54:20.305211 Finish.
Total elapsed time: 30:19:18.31.
