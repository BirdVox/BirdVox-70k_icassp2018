2018-06-07 10:35:05.057239: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.057497: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.057513: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.057519: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.057527: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:35:02.189221 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.517456  7.771276  0.501221  8.039372
1    0.515381  7.804317  0.500977  8.043307
2    0.522339  7.692467  0.501221  8.039372
3    0.504761  7.980131  0.498169  8.088561
4    0.496704  8.112107  0.496582  8.114139
5    0.494019  8.155457  0.498169  8.088561
6    0.508301  7.925234  0.498535  8.082658
7    0.504028  7.994119  0.506592  7.952801
8    0.500366  8.053145  0.503784  7.998054
9    0.493896  8.157403  0.499268  8.070853
10   0.499512  8.066918  0.501465  8.035437
11   0.504028  7.994098  0.498413  8.084626
12   0.504028  7.994076  0.496582  8.114139
13   0.507202  7.942941  0.497925  8.092496
14   0.490723  8.208581  0.500732  8.047243
15   0.493774  8.159392  0.495850  8.125944
16   0.497559  8.098399  0.499512  8.066918
17   0.511353  7.876045  0.503784  7.998054
18   0.501709  8.031481  0.504639  7.984281
19   0.505493  7.970444  0.498291  8.086593
20   0.500366  8.053081  0.495728  8.127912
21   0.495605  8.129858  0.498657  8.080691
22   0.494995  8.139674  0.500610  8.049210
23   0.498901  8.076713  0.500122  8.057080
24   0.505249  7.974401  0.498535  8.082658
25   0.498413  8.084583  0.502075  8.025600
26   0.504761  7.982292  0.503662  8.000022
27   0.504517  7.986227  0.499146  8.072821
28   0.497437  8.100345  0.492188  8.184970
29   0.501587  8.033448  0.504761  7.982314
30   0.492065  8.186916  0.504517  7.986249
31   0.500977  8.043286  0.490723  8.208581
32   0.499146  8.072756  0.500854  8.045275
33   0.496216  8.119999  0.515137  7.815073
34   0.498413  8.084604  0.500854  8.045275
35   0.501831  8.029535  0.491455  8.196776
36   0.504272  7.990120  0.497314  8.102334
37   0.498291  8.086508  0.499390  8.068885
38   0.503418  8.003914  0.502563  8.017729
39   0.495972  8.123955  0.510986  7.881969
40   0.496094  8.122009  0.497559  8.098399
41   0.493652  8.161338  0.508179  7.927223
42   0.501587  8.033427  0.499023  8.074788
43   0.499634  8.064865  0.499634  8.064950
44   0.497192  8.104301  0.492310  8.183003
45   0.500366  8.053102  0.494629  8.145620
46   0.496704  8.112150  0.506592  7.952801
47   0.500122  8.057059  0.507080  7.944930
48   0.497192  8.104215  0.494873  8.141684
49   0.513062  7.848499  0.495972  8.123977
50   0.502441  8.019654  0.515503  7.809170
51   0.499268  8.070853  0.503540  8.001989
52   0.486328  8.279369  0.498413  8.084626
53   0.500610  8.049124  0.502319  8.021665
54   0.497803  8.094421  0.501587  8.033470
55   0.498657  8.080669  0.499146  8.072821
56   0.499146  8.072799  0.499878  8.061015
57   0.499268  8.070832  0.491089  8.202678
58   0.497070  8.106269  0.489868  8.222354
59   0.499146  8.072799  0.495361  8.133814
60   0.502075  8.025578  0.493652  8.161360
61   0.512573  7.856327  0.500977  8.043307
62   0.499634  8.064929  0.496460  8.116106
63   0.494629  8.145620  0.498535  8.082658
64   0.499634  8.064886  0.507202  7.942963
65   0.502563  8.017729  0.504272  7.990184
66   0.490601  8.210527  0.503052  8.009859
67   0.502441  8.019697  0.497681  8.096431
68   0.498047  8.090486  0.502930  8.011827
69   0.494751  8.143609  0.493042  8.171198
70   0.501343  8.037276  0.500977  8.043307
71   0.507080  7.944909  0.498535  8.082658
72   0.500366  8.053124  0.493164  8.169230
73   0.504150  7.992130  0.504028  7.994119
74   0.497681  8.096410  0.497314  8.102334
75   0.500854  8.045254  0.509888  7.899677
76   0.497925  8.092432  0.496826  8.110204
77   0.510498  7.889796  0.499146  8.072821
78   0.490112  8.218376  0.498291  8.086593
79   0.495239  8.135782  0.501709  8.031502
80   0.508057  7.929190  0.505737  7.966573
81   0.500000  8.059026  0.490723  8.208581
82   0.505981  7.962617  0.502197  8.023632
83   0.506104  7.960628  0.503418  8.003957
84   0.494507  8.147566  0.500122  8.057080
85   0.506592  7.952758  0.500122  8.057080
86   0.506958  7.946855  0.496826  8.110204
87   0.504639  7.984195  0.493652  8.161360
88   0.497437  8.100323  0.496338  8.118074
89   0.511230  7.877970  0.501831  8.029535
90   0.507324  7.940995  0.503906  7.996086
91   0.508057  7.929147  0.500854  8.045275
92   0.498169  8.088539  0.498779  8.078723
93   0.500732  8.047243  0.506348  7.956736
94   0.496338  8.118031  0.506348  7.956736
95   0.501587  8.033470  0.504883  7.980346
96   0.496826  8.110182  0.503540  8.001989
97   0.500488  8.051156  0.489136  8.234159
98   0.497070  8.106247  0.498169  8.088561
99   0.494629  8.145598  0.493530  8.163327
100  0.500977  8.043243  0.506104  7.960671
101  0.505005  7.978336  0.500000  8.059048
102  0.499512  8.066854  0.496338  8.118074
103  0.504761  7.982206  0.512939  7.850488
104  0.509888  7.899656  0.484253  8.312861
105  0.494507  8.147544  0.492188  8.184970
106  0.490723  8.208559  0.501709  8.031502
107  0.516235  7.797343  0.499634  8.064950
108  0.491821  8.190852  0.505005  7.978379
109  0.488770  8.240040  0.502319  8.021665
110  0.502197  8.023632  0.498169  8.088561
111  0.503174  8.007849  0.504761  7.982314
112  0.507935  7.931158  0.493164  8.169230
113  0.492554  8.179025  0.502808  8.013794
114  0.509766  7.901623  0.502075  8.025600
115  0.498047  8.090464  0.499756  8.062983
116  0.491821  8.190873  0.509033  7.913450
117  0.496704  8.112128  0.497314  8.102334
118  0.498169  8.088539  0.499512  8.066918
119  0.499512  8.066897  0.505005  7.978379
120  0.491333  8.198657  0.508911  7.915417
121  0.497559  8.098377  0.497192  8.104301
122  0.493164  8.169187  0.498291  8.086593
123  0.500977  8.043307  0.499268  8.070853
124  0.501953  8.027546  0.505249  7.974444
125  0.497803  8.094442  0.503052  8.009859
126  0.486328  8.279412  0.500732  8.047243
127  0.495239  8.135782  0.501221  8.039372

2018-06-08 17:58:59.772633 Finish.
Total elapsed time: 31:23:57.77.
