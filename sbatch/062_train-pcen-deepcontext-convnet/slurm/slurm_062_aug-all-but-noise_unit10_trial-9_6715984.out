2018-06-07 10:35:12.587040: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:12.587245: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:12.587257: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:12.587262: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:12.587266: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:35:01.533470 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit01, unit02, unit03.
Validation set: unit05, unit07.
Test set: unit10.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.500610  7.961462  0.504395  7.901133
1    0.502686  7.928378  0.497070  8.017899
2    0.496094  8.033467  0.500244  7.967300
3    0.511475  7.788260  0.496460  8.027629
4    0.495728  8.039306  0.504395  7.901133
5    0.500732  7.959516  0.504883  7.893349
6    0.498901  7.988707  0.500732  7.959516
7    0.502075  7.938109  0.503418  7.916702
8    0.490967  8.115203  0.490967  8.115203
9    0.503540  7.914756  0.491455  8.107419
10   0.502319  7.934217  0.500610  7.961462
11   0.498901  7.988707  0.498413  7.996492
12   0.502563  7.930324  0.508301  7.838858
13   0.498657  7.992599  0.494263  8.062659
14   0.494995  8.050982  0.499512  7.978977
15   0.505615  7.881672  0.499878  7.973138
16   0.499512  7.978977  0.493408  8.076281
17   0.500854  7.957570  0.500610  7.961462
18   0.489990  8.130772  0.489380  8.140502
19   0.493530  8.074335  0.492676  8.087958
20   0.491699  8.103527  0.497192  8.015952
21   0.499512  7.978977  0.502319  7.934217
22   0.497559  8.010114  0.503784  7.910864
23   0.501343  7.949785  0.504272  7.903079
24   0.504150  7.905025  0.490845  8.117149
25   0.499390  7.980923  0.497803  8.006222
26   0.483032  8.241699  0.501465  7.947839
27   0.495361  8.045144  0.519165  7.665656
28   0.500610  7.961462  0.513550  7.755176
29   0.498169  8.000384  0.508179  7.840804
30   0.497925  8.004276  0.504761  7.895295
31   0.488892  8.148287  0.497925  8.004276
32   0.504761  7.895295  0.501343  7.949785
33   0.492920  8.084066  0.498169  8.000384
34   0.497437  8.012060  0.502319  7.934217
35   0.503418  7.916702  0.498169  8.000384
36   0.505005  7.891403  0.504761  7.895295
37   0.494629  8.056820  0.500244  7.967300
38   0.498657  7.992599  0.504517  7.899187
39   0.498779  7.990653  0.510132  7.809667
40   0.500854  7.957570  0.489258  8.142448
41   0.492798  8.086012  0.506348  7.869996
42   0.490723  8.119095  0.499146  7.984815
43   0.501221  7.951731  0.490356  8.124934
44   0.500122  7.969246  0.496704  8.023737
45   0.491943  8.099634  0.512085  7.778529
46   0.494141  8.064605  0.503174  7.920594
47   0.495239  8.047090  0.505249  7.887510
48   0.497925  8.004276  0.494141  8.064605
49   0.507812  7.846642  0.500977  7.955624
50   0.487671  8.167748  0.505615  7.881672
51   0.491333  8.109365  0.493896  8.068497
52   0.503784  7.910864  0.506226  7.871942
53   0.491455  8.107419  0.507812  7.846642
54   0.492920  8.084066  0.515137  7.729877
55   0.502319  7.934217  0.508301  7.838858
56   0.494019  8.066551  0.501099  7.953678
57   0.490112  8.128826  0.503296  7.918648
58   0.505005  7.891403  0.506836  7.862211
59   0.496460  8.027629  0.497925  8.004276
60   0.501343  7.949785  0.495117  8.049036
61   0.502930  7.924486  0.497192  8.015952
62   0.501709  7.943947  0.506836  7.862211
63   0.496704  8.023737  0.501099  7.953678
64   0.500610  7.961462  0.505615  7.881672
65   0.507935  7.844696  0.499146  7.984815
66   0.496094  8.033467  0.495361  8.045144
67   0.500000  7.971192  0.504761  7.895295
68   0.507202  7.856373  0.507690  7.848589
69   0.500244  7.967300  0.497925  8.004276
70   0.498169  8.000384  0.501099  7.953678
71   0.500122  7.969246  0.505859  7.877780
72   0.501831  7.942001  0.508301  7.838858
73   0.500488  7.963408  0.495483  8.043198
74   0.503174  7.920594  0.501953  7.940055
75   0.503906  7.908917  0.503296  7.918648
76   0.504639  7.897241  0.501709  7.943947
77   0.501953  7.940055  0.498901  7.988707
78   0.499756  7.975085  0.501343  7.949785
79   0.497070  8.017899  0.502808  7.926432
80   0.499268  7.982869  0.499023  7.986761
81   0.494019  8.066551  0.500732  7.959516
82   0.494629  8.056820  0.499756  7.975085
83   0.498779  7.990653  0.503174  7.920594
84   0.503296  7.918648  0.499756  7.975085
85   0.502441  7.932271  0.490356  8.124934
86   0.493774  8.070443  0.495605  8.041252
87   0.506348  7.869996  0.497559  8.010114
88   0.500244  7.967300  0.500977  7.955624
89   0.491333  8.109365  0.500122  7.969246
90   0.495605  8.041252  0.494629  8.056820
91   0.501465  7.947839  0.497803  8.006222
92   0.501709  7.943947  0.498657  7.992599
93   0.488525  8.154125  0.502319  7.934217
94   0.494995  8.050982  0.504028  7.906971
95   0.502075  7.938109  0.502075  7.938109
96   0.504883  7.893349  0.500488  7.963408
97   0.500854  8.015636  0.492310  8.183003
98   0.493652  8.160867  0.493286  8.167263
99   0.502319  8.021321  0.498535  8.082658
100  0.504761  7.982013  0.499756  8.062983
101  0.498779  8.078316  0.497559  8.098399
102  0.492432  8.180628  0.504883  7.980346
103  0.512695  7.853909  0.504395  7.988216
104  0.511963  7.865778  0.506592  7.952801
105  0.498657  8.080240  0.497559  8.098399
106  0.494873  8.141406  0.503418  8.003957
107  0.503174  8.007377  0.498169  8.088561
108  0.507935  7.930707  0.493652  8.161360
109  0.493896  8.156803  0.505371  7.972476
110  0.495239  8.135353  0.502930  8.011827
111  0.511108  7.879573  0.499146  8.072821
112  0.502441  8.019332  0.493896  8.157425
113  0.495728  8.127633  0.503906  7.996086
114  0.498413  8.084090  0.505981  7.962638
115  0.504517  7.985841  0.501465  8.035437
116  0.500610  8.048738  0.500000  8.059048
117  0.499878  8.060458  0.503296  8.005924
118  0.501099  8.040739  0.491211  8.200711
119  0.500366  8.052781  0.493286  8.167263
120  0.497192  8.103786  0.500122  8.057080
121  0.510376  7.891507  0.496948  8.108236
122  0.504028  7.993668  0.500366  8.053145
123  0.496460  8.115806  0.495728  8.127912
124  0.499023  8.074295  0.494629  8.145620
125  0.494385  8.149126  0.500854  8.045275
126  0.506592  7.952307  0.499756  8.062983
127  0.501465  8.035116  0.502563  8.017729

2018-06-08 14:50:48.551284 Finish.
Total elapsed time: 28:15:47.55.
