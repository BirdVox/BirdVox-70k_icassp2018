2018-06-07 10:35:06.324936: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:06.325116: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:06.325128: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:35:03.563226 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit07, unit10, unit01.
Validation set: unit02, unit03.
Test set: unit05.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.930054  0.212262  0.901855  0.315115
1    0.936401  0.185174  0.877441  0.324150
2    0.932373  0.196690  0.880493  0.308061
3    0.928589  0.212961  0.889160  0.295977
4    0.944336  0.162686  0.900879  0.394431
5    0.938354  0.169232  0.916748  0.233576
6    0.934570  0.184313  0.941406  0.186903
7    0.945557  0.154867  0.939087  0.182856
8    0.942383  0.179875  0.920898  0.274022
9    0.941895  0.160289  0.910645  0.253276
10   0.941040  0.175629  0.921021  0.250813
11   0.942017  0.170226  0.899658  0.317214
12   0.945923  0.162641  0.930542  0.216794
13   0.951050  0.140584  0.932739  0.221317
14   0.951538  0.143117  0.924561  0.258785
15   0.948853  0.155646  0.926025  0.259997
16   0.952026  0.147119  0.927979  0.241078
17   0.956055  0.119636  0.943726  0.183552
18   0.949463  0.142989  0.941650  0.194117
19   0.946167  0.160601  0.938477  0.211817
20   0.952271  0.142184  0.933472  0.256464
21   0.954102  0.132414  0.926636  0.255252
22   0.958862  0.133843  0.931763  0.294650
23   0.957642  0.149215  0.939575  0.182158
24   0.958374  0.139442  0.945923  0.190887
25   0.957275  0.143590  0.935913  0.221427
26   0.955811  0.140056  0.944458  0.192101
27   0.955078  0.135944  0.931763  0.276424
28   0.952759  0.148460  0.950684  0.185739
29   0.951416  0.152187  0.933960  0.304992
30   0.959106  0.122929  0.940186  0.201727
31   0.951416  0.142561  0.938110  0.209766
32   0.958618  0.126689  0.939209  0.247624
33   0.956299  0.137142  0.939453  0.230838
34   0.948975  0.165524  0.943604  0.257949
35   0.957642  0.128663  0.933594  0.294631
36   0.948730  0.171308  0.955200  0.225757
37   0.953369  0.137860  0.937500  0.248901
38   0.958618  0.122945  0.935425  0.322287
39   0.959595  0.126537  0.930908  0.229771
40   0.957764  0.130701  0.923706  0.297872
41   0.957153  0.131036  0.919312  0.354201
42   0.951904  0.140607  0.919067  0.398752
43   0.954712  0.137368  0.920654  0.512664
44   0.958374  0.121663  0.946045  0.204543
45   0.956909  0.137277  0.934937  0.362169
46   0.956909  0.140974  0.949829  0.167807
47   0.958984  0.123435  0.934082  0.337250
48   0.962036  0.117149  0.935791  0.312150
49   0.954956  0.134204  0.943970  0.270425
50   0.960938  0.119221  0.937134  0.383917
51   0.960083  0.128813  0.927979  0.363082
52   0.951904  0.135260  0.938965  0.222787
53   0.958496  0.126446  0.944214  0.310204
54   0.954712  0.131371  0.953613  0.170546
55   0.960571  0.122771  0.945068  0.243112
56   0.961914  0.114092  0.937866  0.332984
57   0.957642  0.133015  0.936157  0.300273
58   0.954712  0.138838  0.937256  0.387625
59   0.962646  0.109361  0.960571  0.161573
60   0.961304  0.123683  0.924438  0.403810
61   0.964111  0.108540  0.932495  0.358318
62   0.958374  0.134026  0.949585  0.293265
63   0.955200  0.138608  0.941040  0.304850
64   0.665527  5.157044  0.497437  8.100366
65   0.491699  8.192841  0.503540  8.001989
66   0.500000  8.059048  0.504028  7.994119
67   0.495972  8.123977  0.490967  8.204646
68   0.503418  8.003957  0.500610  8.049210
69   0.493774  8.159392  0.498169  8.088561
70   0.497314  8.102334  0.513428  7.842618
71   0.502441  8.019697  0.499634  8.064950
72   0.494019  8.155457  0.503662  8.000022
73   0.504150  7.992151  0.499634  8.064950
74   0.497925  8.092496  0.499512  8.066918
75   0.502808  8.013794  0.489868  8.222354
76   0.489990  8.220386  0.498535  8.082658
77   0.499146  8.072821  0.495483  8.131847
78   0.496216  8.120042  0.497437  8.100366
79   0.494995  8.139717  0.488892  8.238094
80   0.504395  7.988216  0.502075  8.025600
81   0.503784  7.998054  0.503174  8.007892
82   0.495850  8.125944  0.501465  8.035437
83   0.502808  8.013794  0.492920  8.173165
84   0.495850  8.125944  0.503906  7.996086
85   0.506714  7.950833  0.496094  8.122009
86   0.491943  8.188905  0.495361  8.133814
87   0.500488  8.051178  0.499023  8.074788
88   0.501831  8.029535  0.502686  8.015762
89   0.499878  8.061015  0.496094  8.122009
90   0.507080  7.944930  0.489624  8.226289
91   0.487305  8.263672  0.500244  8.055113
92   0.494507  8.147587  0.498169  8.088561
93   0.499390  8.068885  0.497925  8.092496
94   0.507690  7.935093  0.497314  8.102334
95   0.497925  8.092496  0.497559  8.098399
96   0.505981  7.962638  0.498535  8.082658
97   0.496948  8.108236  0.507935  7.931158
98   0.506470  7.954768  0.501343  8.037405
99   0.489502  8.228256  0.503174  8.007892
100  0.498413  8.084626  0.502930  8.011827
101  0.502686  8.015762  0.500488  8.051178
102  0.489258  8.232191  0.500000  8.059048
103  0.497314  8.102334  0.503906  7.996086
104  0.499756  8.062983  0.504150  7.992151
105  0.500366  8.053145  0.501465  8.035437
106  0.497559  8.098377  0.504639  7.984281
107  0.504395  7.988216  0.497559  8.098399
108  0.496460  8.116106  0.497314  8.102334
109  0.495117  8.137749  0.494385  8.149555
110  0.499512  8.066918  0.495850  8.125944
111  0.498779  8.078723  0.494629  8.145620
112  0.505249  7.974443  0.507080  7.944930
113  0.502197  8.023632  0.507324  7.940995
114  0.501465  8.035437  0.499390  8.068886
115  0.495361  8.133814  0.506470  7.954768
116  0.500977  8.043307  0.503662  8.000022
117  0.494995  8.139717  0.497437  8.100366
118  0.498169  8.088561  0.501221  8.039372
119  0.500122  8.057080  0.490234  8.216451
120  0.498779  8.078723  0.490723  8.208581
121  0.499756  8.062983  0.501587  8.033470
122  0.490601  8.210527  0.495605  8.129879
123  0.503052  8.009859  0.504639  7.984281
124  0.500122  8.057080  0.496582  8.114139
125  0.496826  8.110204  0.508423  7.923287
126  0.500244  8.055113  0.495361  8.133814
127  0.491455  8.196776  0.506714  7.950833

2018-06-08 17:12:37.888564 Finish.
Total elapsed time: 30:37:34.89.
