2018-06-07 10:35:06.187023: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:06.187216: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:06.187229: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:35:03.149232 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.496826  8.109246  0.495972  8.123977
1    0.501221  8.033744  0.494873  8.052928
2    0.509155  7.825600  0.512207  7.776583
3    0.494995  8.051218  0.495605  8.041252
4    0.504395  7.901305  0.489746  8.134664
5    0.512573  7.771045  0.502197  7.936163
6    0.502197  7.936549  0.495972  8.035413
7    0.500732  7.959773  0.504272  7.903079
8    0.493286  8.078485  0.493530  8.074335
9    0.498169  8.000641  0.495117  8.049036
10   0.497559  8.010436  0.491699  8.103527
11   0.488770  8.150426  0.492920  8.084066
12   0.506470  7.868264  0.490479  8.122988
13   0.502319  7.934495  0.495117  8.049036
14   0.500244  7.967493  0.498779  7.990653
15   0.508423  7.837298  0.506348  7.869996
16   0.502075  7.938280  0.484619  8.216400
17   0.503174  7.920808  0.503906  7.908917
18   0.499878  7.973503  0.494873  8.052928
19   0.493652  8.072689  0.501099  7.953678
20   0.497559  8.010414  0.502197  7.936163
21   0.510986  7.796430  0.493530  8.074335
22   0.510986  7.796280  0.503418  7.916702
23   0.504883  7.893606  0.500854  7.957570
24   0.501587  7.946236  0.498047  8.002330
25   0.497192  8.016167  0.499512  7.978977
26   0.497559  8.010329  0.494263  8.062659
27   0.490967  8.115525  0.498047  8.002330
28   0.492188  8.096107  0.499146  7.984815
29   0.494141  8.064798  0.500122  7.969246
30   0.501587  7.946108  0.503174  7.920594
31   0.498169  8.000620  0.487305  8.173586
32   0.506104  7.874231  0.498291  7.998438
33   0.507202  7.856673  0.506470  7.868049
34   0.505737  7.880026  0.500000  7.971192
35   0.503296  7.918884  0.493164  8.080173
36   0.488159  8.160178  0.491333  8.109365
37   0.505737  7.879941  0.501587  7.945893
38   0.504028  7.907336  0.492432  8.091850
39   0.496704  8.023994  0.503906  7.908917
40   0.505249  7.887832  0.497070  8.017899
41   0.504517  7.899509  0.493652  8.072389
42   0.500732  7.959730  0.500488  7.963408
43   0.502563  7.930646  0.495361  8.045144
44   0.501343  7.949957  0.496460  8.027629
45   0.499634  7.977309  0.500244  7.967300
46   0.502563  7.930475  0.503662  7.912810
47   0.501343  7.950043  0.499268  7.982869
48   0.498779  7.990975  0.495483  8.043198
49   0.502197  7.936484  0.493164  8.080173
50   0.502563  7.930539  0.504028  7.906971
51   0.504761  7.895595  0.503052  7.922540
52   0.507324  7.854598  0.495483  8.043198
53   0.500977  7.955860  0.495239  8.047090
54   0.497803  8.006479  0.498657  7.992599
55   0.499634  7.977331  0.502319  7.934217
56   0.507568  7.850771  0.503906  7.908917
57   0.498413  7.996856  0.500610  7.961462
58   0.500122  7.969568  0.495605  8.041252
59   0.501953  7.940248  0.506104  7.873888
60   0.502686  7.928636  0.502075  7.938109
61   0.498169  8.000663  0.500854  7.957570
62   0.504883  7.893606  0.498291  7.998438
63   0.502686  7.928593  0.499023  7.986761
64   0.485840  8.197282  0.500000  7.971192
65   0.498901  7.988900  0.502075  7.938109
66   0.497070  8.018263  0.508057  7.842750
67   0.498779  7.991018  0.501099  7.953678
68   0.504517  7.899509  0.507568  7.850535
69   0.506226  7.872199  0.503906  7.908917
70   0.509155  7.825579  0.505859  7.877780
71   0.498657  7.992814  0.490356  8.124934
72   0.505127  7.889885  0.499756  7.975085
73   0.495361  8.045423  0.502319  7.934217
74   0.503052  7.922969  0.501831  7.942001
75   0.501709  7.944419  0.497681  8.008168
76   0.510010  7.811870  0.502563  7.930324
77   0.500000  7.971536  0.508179  7.840804
78   0.509766  7.815762  0.501953  7.940055
79   0.504517  7.899423  0.504028  7.906971
80   0.496704  8.024101  0.499390  7.980923
81   0.509766  7.815870  0.510132  7.809667
82   0.497681  8.008404  0.494263  8.062659
83   0.504150  7.905326  0.501831  7.942001
84   0.510742  7.800194  0.495972  8.035413
85   0.510742  7.800365  0.495117  8.049036
86   0.502686  7.928679  0.497559  8.010114
87   0.499268  7.983148  0.503662  7.912810
88   0.493652  8.072711  0.496460  8.027629
89   0.496948  8.020081  0.509155  7.825235
90   0.503540  7.914992  0.504272  7.903079
91   0.505615  7.881865  0.487183  8.175532
92   0.487671  8.168026  0.500122  7.969246
93   0.497314  8.014350  0.506226  7.871942
94   0.496460  8.027886  0.509399  7.821343
95   0.505371  7.885822  0.502563  7.930324
96   0.502808  7.926904  0.512939  7.764907
97   0.498413  7.996813  0.507568  7.850535
98   0.501953  7.940377  0.494141  8.064605
99   0.495483  8.043477  0.506592  7.866103
100  0.497070  8.018349  0.501343  7.949785
101  0.500488  7.963730  0.497437  8.012060
102  0.483887  8.228484  0.490845  8.117149
103  0.499634  7.977159  0.502808  7.926432
104  0.510132  7.810010  0.501221  7.951731
105  0.500488  7.961745  0.500366  7.965354
106  0.502808  7.926432  0.505859  7.877780
107  0.504395  7.901133  0.499146  7.984815
108  0.490479  8.122988  0.496094  8.033467
109  0.504028  7.906971  0.504639  7.897241
110  0.507324  7.854448  0.496826  8.021791
111  0.496948  8.019845  0.508179  7.840804
112  0.503296  7.918648  0.508911  7.829128
113  0.502197  7.936163  0.497437  8.012060
114  0.500488  7.963408  0.497681  8.008168
115  0.502197  7.936163  0.500977  7.955624
116  0.496948  8.019845  0.499512  7.978977
117  0.497314  8.014006  0.500610  7.961462
118  0.498413  7.996492  0.499390  7.980923
119  0.498291  7.998438  0.503784  7.910864
120  0.502808  7.926432  0.501953  7.940055
121  0.500366  7.965354  0.490967  8.115203
122  0.499756  7.975085  0.502930  7.924486
123  0.496582  8.025683  0.489380  8.140502
124  0.505737  7.879726  0.487915  8.163855
125  0.492554  8.089904  0.500244  7.967300
126  0.505737  7.879726  0.503174  7.920594
127  0.498779  7.990653  0.500000  7.971192

2018-06-08 17:11:48.661866 Finish.
Total elapsed time: 30:36:45.66.
