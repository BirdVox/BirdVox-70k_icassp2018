2018-06-07 10:35:13.414317: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:13.414559: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:13.414570: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:35:03.148980 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit01, unit02, unit03.
Validation set: unit05, unit07.
Test set: unit10.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.498291  7.998438  0.507080  7.858319
1    0.492554  8.089904  0.513672  7.753230
2    0.496094  8.033467  0.513306  7.759068
3    0.499390  7.980923  0.501465  7.947839
4    0.503540  7.914777  0.501099  7.953678
5    0.504395  7.901133  0.499023  7.986761
6    0.503540  7.914756  0.502197  7.936163
7    0.503784  7.910885  0.510742  7.799936
8    0.497925  8.004276  0.500244  7.967300
9    0.499878  7.973138  0.491699  8.103527
10   0.507446  7.852481  0.507446  7.852481
11   0.489380  8.140502  0.502319  7.934217
12   0.507202  7.856394  0.505371  7.885564
13   0.499878  7.973138  0.502075  7.938109
14   0.505737  7.879726  0.499146  7.984815
15   0.500854  7.957570  0.505005  7.891403
16   0.502075  7.938109  0.507080  7.858319
17   0.498901  7.988707  0.486816  8.181370
18   0.500977  7.955645  0.510254  7.807721
19   0.501465  7.947839  0.502930  7.924486
20   0.509277  7.823289  0.493042  8.082120
21   0.488525  8.154125  0.494263  8.062659
22   0.505737  7.879726  0.508057  7.842750
23   0.506226  7.871942  0.495605  8.041252
24   0.496948  8.019866  0.503174  7.920594
25   0.505127  7.889457  0.500244  7.967300
26   0.510864  7.797990  0.500977  7.955624
27   0.497314  8.014006  0.518555  7.675386
28   0.503174  7.920594  0.502930  7.924486
29   0.495483  8.043241  0.496338  8.029575
30   0.494019  8.066551  0.500488  7.963408
31   0.502197  7.936163  0.496338  8.029575
32   0.504883  7.893349  0.502808  7.926432
33   0.509399  7.821343  0.500854  7.957570
34   0.503052  7.922562  0.496582  8.025683
35   0.499268  7.982869  0.487549  8.169694
36   0.501465  7.947839  0.495850  8.037359
37   0.504883  7.893349  0.498169  8.000384
38   0.498657  7.992599  0.497192  8.015952
39   0.495117  8.049036  0.511230  7.792152
40   0.501587  7.945893  0.507324  7.854427
41   0.502930  7.924486  0.506714  7.864157
42   0.498291  7.998438  0.503784  7.910864
43   0.499146  7.984815  0.499268  7.982869
44   0.506836  7.862233  0.491699  8.103527
45   0.504150  7.905025  0.502075  7.938109
46   0.501343  7.949785  0.491699  8.103527
47   0.498657  7.992599  0.502075  7.938109
48   0.501587  7.945893  0.493652  8.072389
49   0.494751  8.054874  0.506226  7.871942
50   0.494019  8.066551  0.503662  7.912810
51   0.514771  7.735715  0.505127  7.889457
52   0.493652  8.072389  0.496338  8.029575
53   0.495361  8.045144  0.490601  8.121041
54   0.498047  8.002330  0.507080  7.858319
55   0.505737  7.879747  0.502197  7.936163
56   0.497070  8.017899  0.496460  8.027629
57   0.491455  8.107419  0.509888  7.813559
58   0.496216  8.031521  0.507568  7.850535
59   0.499512  7.978977  0.495117  8.049036
60   0.505127  7.889457  0.499390  7.980923
61   0.505127  7.889457  0.502563  7.930324
62   0.502441  7.932271  0.493774  8.070443
63   0.496094  8.033467  0.501465  7.947839
64   0.498901  7.988707  0.501099  7.953678
65   0.496338  8.029575  0.499023  7.986761
66   0.499146  7.984815  0.489258  8.142448
67   0.500977  7.955624  0.500000  7.971192
68   0.500610  7.961462  0.505005  7.891403
69   0.503906  7.908917  0.499756  7.975085
70   0.505371  7.885586  0.507324  7.854427
71   0.505615  7.881672  0.497559  8.010114
72   0.489746  8.134685  0.507446  7.852481
73   0.495728  8.039306  0.496094  8.033467
74   0.504395  7.901133  0.490479  8.122988
75   0.504761  7.895295  0.490845  8.117149
76   0.497803  8.006222  0.497437  8.012060
77   0.505005  7.891403  0.497681  8.008168
78   0.495117  8.049036  0.501953  7.940055
79   0.497925  8.004276  0.504150  7.905025
80   0.501221  7.951731  0.497559  8.010114
81   0.500244  7.967300  0.509277  7.823289
82   0.496948  8.019845  0.499268  7.982869
83   0.505615  7.881672  0.512085  7.778529
84   0.504150  7.905025  0.496704  8.023737
85   0.499390  7.980923  0.503418  7.916702
86   0.509888  7.813559  0.497314  8.014006
87   0.499878  7.973138  0.497070  8.017899
88   0.491943  8.099656  0.493164  8.080173
89   0.500366  7.965354  0.498169  8.000384
90   0.493408  8.076303  0.510254  7.807721
91   0.498657  7.992599  0.503540  7.914756
92   0.495972  8.035413  0.496582  8.025683
93   0.489014  8.146341  0.492065  8.097688
94   0.497314  8.014006  0.509155  7.825235
95   0.504883  7.893349  0.501709  7.943947
96   0.493530  8.074357  0.503540  7.914756
97   0.493896  8.068497  0.497437  8.012060
98   0.507568  7.850535  0.507812  7.846642
99   0.498291  7.998438  0.496460  8.027629
100  0.501343  7.949785  0.510864  7.797990
101  0.499023  7.986783  0.493286  8.078227
102  0.512329  7.774637  0.505005  7.891403
103  0.502319  7.934217  0.503418  7.916702
104  0.488403  8.156093  0.499512  7.978977
105  0.494019  8.066551  0.496704  8.023737
106  0.492310  8.093796  0.492798  8.086012
107  0.495239  8.047090  0.500244  7.967300
108  0.513672  7.753230  0.503540  7.914756
109  0.501343  7.949785  0.499268  7.982869
110  0.494507  8.058766  0.500732  7.959516
111  0.506714  7.864157  0.495117  8.049036
112  0.507446  7.852481  0.491943  8.099634
113  0.501709  7.943947  0.493896  8.068497
114  0.494019  8.066572  0.505737  7.879726
115  0.497314  8.014006  0.499756  7.975085
116  0.502441  7.932271  0.489380  8.140502
117  0.512573  7.770745  0.502686  7.928378
118  0.507812  7.846642  0.502930  7.924486
119  0.503906  7.908917  0.495972  8.035413
120  0.505127  7.889457  0.499756  7.975085
121  0.501831  7.942001  0.505737  7.879726
122  0.500610  7.961462  0.501099  7.953678
123  0.501953  7.940076  0.500610  7.961462
124  0.493042  8.082120  0.491699  8.103527
125  0.491577  8.105473  0.490967  8.115203
126  0.495239  8.047090  0.492676  8.087958
127  0.494629  8.056820  0.508545  7.834966

2018-06-08 16:36:53.025837 Finish.
Total elapsed time: 30:01:50.03.
