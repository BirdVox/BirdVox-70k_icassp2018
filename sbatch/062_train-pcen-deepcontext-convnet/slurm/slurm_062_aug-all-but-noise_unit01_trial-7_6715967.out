2018-06-07 10:35:12.659925: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:12.660154: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:12.660166: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:35:03.148432 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit02, unit03, unit05.
Validation set: unit07, unit10.
Test set: unit01.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.500122  8.057080  0.504883  7.980346
1    0.497803  8.094464  0.505615  7.968541
2    0.501221  8.039372  0.496582  8.114139
3    0.510620  7.887872  0.488037  8.251867
4    0.501953  8.027567  0.499390  8.068885
5    0.500488  8.051178  0.499146  8.072821
6    0.501343  8.037405  0.514160  7.830813
7    0.500122  8.057080  0.499878  8.061015
8    0.493896  8.157425  0.501709  8.031502
9    0.502319  8.021665  0.506470  7.954768
10   0.500122  8.057080  0.493408  8.165295
11   0.496216  8.120042  0.502197  8.023632
12   0.498413  8.084604  0.506470  7.954768
13   0.505249  7.974444  0.491577  8.194808
14   0.500854  8.045275  0.501221  8.039372
15   0.502441  8.019697  0.491943  8.188905
16   0.495117  8.137749  0.497681  8.096431
17   0.496826  8.110204  0.494629  8.145620
18   0.502930  8.011827  0.498169  8.088561
19   0.507446  7.939006  0.496826  8.110204
20   0.502808  8.013794  0.499023  8.074788
21   0.495605  8.129879  0.505249  7.974444
22   0.503296  8.005924  0.502808  8.013794
23   0.500488  8.051178  0.494751  8.143652
24   0.490234  8.216451  0.498779  8.078723
25   0.497559  8.098399  0.488892  8.238094
26   0.495605  8.129879  0.489624  8.226289
27   0.493408  8.165295  0.493774  8.159392
28   0.510498  7.889839  0.507446  7.939028
29   0.506226  7.958682  0.495728  8.127912
30   0.497070  8.106269  0.494629  8.145620
31   0.493286  8.167263  0.494995  8.139717
32   0.503296  8.005924  0.495361  8.133814
33   0.503052  8.009859  0.500854  8.045275
34   0.506104  7.960671  0.505859  7.964606
35   0.495972  8.123977  0.498779  8.078723
36   0.501709  8.031502  0.504517  7.986249
37   0.497803  8.094464  0.498779  8.078723
38   0.497925  8.092496  0.498169  8.088561
39   0.490601  8.210548  0.498413  8.084626
40   0.499878  8.061015  0.502808  8.013794
41   0.507324  7.940995  0.498047  8.090528
42   0.501221  8.039372  0.496460  8.116106
43   0.499268  8.070853  0.499390  8.068886
44   0.487549  8.259737  0.505249  7.974443
45   0.501587  8.033470  0.507446  7.939028
46   0.497437  8.100366  0.505615  7.968541
47   0.494263  8.151522  0.502319  8.021664
48   0.494629  8.145620  0.506836  7.948865
49   0.505737  7.966573  0.508667  7.919352
50   0.502808  8.013794  0.500122  8.057080
51   0.492310  8.183003  0.497559  8.098399
52   0.502075  8.025600  0.490601  8.210548
53   0.505493  7.970508  0.492188  8.184970
54   0.507935  7.931158  0.503906  7.996086
55   0.498535  8.082637  0.495483  8.131847
56   0.501709  8.031502  0.500488  8.051178
57   0.501465  8.035437  0.505127  7.976411
58   0.509033  7.913450  0.505371  7.972476
59   0.502686  8.015762  0.504639  7.984281
60   0.496460  8.116106  0.499268  8.070853
61   0.502075  8.025557  0.493774  8.159392
62   0.510010  7.897709  0.505615  7.968541
63   0.503052  8.009859  0.510132  7.895742
64   0.501831  8.029535  0.494873  8.141684
65   0.498535  8.082658  0.501465  8.035437
66   0.498901  8.076756  0.498901  8.076756
67   0.499512  8.066918  0.495361  8.133814
68   0.491699  8.192841  0.499390  8.068886
69   0.496582  8.114139  0.499268  8.070853
70   0.500122  8.057080  0.494629  8.145620
71   0.497437  8.100366  0.504517  7.986249
72   0.496704  8.112171  0.491089  8.202678
73   0.491943  8.188905  0.503174  8.007892
74   0.507812  7.933125  0.495850  8.125944
75   0.502686  8.015762  0.500610  8.049210
76   0.501831  8.029513  0.504272  7.990184
77   0.501343  8.037405  0.496094  8.122009
78   0.498901  8.076734  0.497437  8.100366
79   0.504517  7.986249  0.499023  8.074788
80   0.499878  8.061015  0.506348  7.956736
81   0.505249  7.974444  0.492920  8.173165
82   0.494873  8.141685  0.494873  8.141685
83   0.507690  7.935093  0.504517  7.986249
84   0.498413  8.084626  0.504272  7.990184
85   0.500732  8.047243  0.494873  8.141685
86   0.510010  7.897709  0.490479  8.212516
87   0.500366  8.053145  0.506836  7.948866
88   0.510864  7.883937  0.490112  8.218419
89   0.503052  8.009859  0.499634  8.064950
90   0.508057  7.929190  0.498413  8.084626
91   0.491577  8.194808  0.500977  8.043307
92   0.506958  7.946898  0.504028  7.994119
93   0.498413  8.084626  0.496948  8.108236
94   0.493164  8.169230  0.503052  8.009859
95   0.495728  8.127912  0.501953  8.027567
96   0.499390  8.068886  0.496216  8.120042
97   0.497681  8.096431  0.502441  8.019697
98   0.505493  7.970508  0.494629  8.145620
99   0.501587  8.033470  0.500244  8.055113
100  0.486328  8.279412  0.495605  8.129879
101  0.505859  7.964606  0.490845  8.206613
102  0.508789  7.917385  0.507690  7.935093
103  0.506348  7.956736  0.503174  8.007892
104  0.482910  8.334504  0.502808  8.013794
105  0.496826  8.110204  0.493042  8.171198
106  0.498535  8.082637  0.504395  7.988216
107  0.495117  8.137749  0.497803  8.094464
108  0.498169  8.088561  0.492310  8.183003
109  0.506836  7.948865  0.502075  8.025600
110  0.497314  8.102334  0.499146  8.072821
111  0.491577  8.194787  0.499390  8.068885
112  0.504395  7.988216  0.496094  8.122009
113  0.499390  8.068885  0.507202  7.942963
114  0.496460  8.116106  0.504272  7.990184
115  0.502075  8.025600  0.500366  8.053145
116  0.500610  8.049210  0.497192  8.104301
117  0.498779  8.078723  0.504639  7.984281
118  0.494385  8.149555  0.489380  8.230224
119  0.498291  8.086593  0.510742  7.885904
120  0.497070  8.106269  0.501709  8.031502
121  0.499756  8.062983  0.497925  8.092496
122  0.500488  8.051178  0.501831  8.029535
123  0.505371  7.972476  0.494873  8.141685
124  0.499390  8.068885  0.500610  8.049210
125  0.497314  8.102334  0.493774  8.159392
126  0.506226  7.958703  0.499023  8.074788
127  0.499756  8.062983  0.508423  7.923287

2018-06-08 16:43:47.723034 Finish.
Total elapsed time: 30:08:44.72.
