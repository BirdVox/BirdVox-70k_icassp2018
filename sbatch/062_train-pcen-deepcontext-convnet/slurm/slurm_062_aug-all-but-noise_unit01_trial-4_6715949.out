2018-06-07 10:35:13.423086: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:13.423295: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:13.423307: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:35:02.948201 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit02, unit03, unit05.
Validation set: unit07, unit10.
Test set: unit01.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.508301  7.838858  0.501587  7.945893
1    0.493530  8.074335  0.501465  7.947839
2    0.493774  8.070464  0.499756  7.975085
3    0.502441  7.932292  0.498413  7.996492
4    0.495605  8.041252  0.505615  7.881672
5    0.496094  8.033467  0.493408  8.076281
6    0.497925  8.004276  0.493286  8.078227
7    0.491699  8.103548  0.500366  7.965354
8    0.506714  7.864179  0.490723  8.119095
9    0.500244  7.967322  0.509277  7.823289
10   0.486816  8.181370  0.496704  8.023737
11   0.498047  8.002330  0.500122  7.969246
12   0.501465  7.947839  0.497803  8.006222
13   0.505981  7.875834  0.503540  7.914756
14   0.501099  7.953678  0.509888  7.813559
15   0.502930  7.924486  0.506592  7.866103
16   0.505737  7.879726  0.493896  8.068497
17   0.503784  7.910864  0.504395  7.901133
18   0.506470  7.868049  0.503784  7.910864
19   0.498047  8.002330  0.493896  8.068497
20   0.501221  7.951731  0.497070  8.017899
21   0.502319  7.934217  0.492798  8.086012
22   0.501099  7.953678  0.499146  7.984815
23   0.506592  7.866103  0.503418  7.916702
24   0.499268  7.982869  0.501953  7.940055
25   0.506226  7.871942  0.502075  7.938109
26   0.509888  7.813559  0.505859  7.877780
27   0.504028  7.906971  0.504761  7.895295
28   0.496460  8.027629  0.498413  7.996492
29   0.502075  7.938130  0.497314  8.014006
30   0.497437  8.012060  0.500000  7.971192
31   0.507935  7.844696  0.495972  8.035413
32   0.499512  7.978977  0.500244  7.967300
33   0.491699  8.103548  0.490601  8.121041
34   0.508667  7.833020  0.500000  7.971192
35   0.494995  8.050982  0.497681  8.008168
36   0.500854  7.957570  0.495361  8.045144
37   0.494385  8.060713  0.502808  7.926432
38   0.500977  7.955624  0.503784  7.910864
39   0.506592  7.866103  0.506714  7.864157
40   0.495239  8.047111  0.485107  8.208616
41   0.510498  7.803828  0.502197  7.936163
42   0.513550  7.755219  0.499756  7.975085
43   0.498779  7.990653  0.504395  7.901133
44   0.502197  7.936184  0.503418  7.916702
45   0.507324  7.854427  0.497803  8.006222
46   0.494141  8.064605  0.502197  7.936163
47   0.490356  8.124934  0.505127  7.889457
48   0.503540  7.914756  0.500122  7.969246
49   0.495850  8.037359  0.494629  8.056820
50   0.498901  7.988707  0.488770  8.150233
51   0.498047  8.002330  0.501587  7.945893
52   0.496216  8.031521  0.499023  7.986761
53   0.505005  7.891403  0.503540  7.914756
54   0.497803  8.006222  0.506836  7.862211
55   0.504883  7.893349  0.505615  7.881672
56   0.499390  7.980923  0.503906  7.908917
57   0.498047  8.002330  0.501587  7.945893
58   0.500977  7.955624  0.503174  7.920594
59   0.500610  7.961483  0.502563  7.930324
60   0.491211  8.111311  0.494385  8.060713
61   0.498047  8.002351  0.506714  7.864157
62   0.495972  8.035413  0.504761  7.895295
63   0.500000  7.971192  0.490723  8.119095
64   0.498901  7.988707  0.508545  7.834966
65   0.497925  8.004297  0.501221  7.951731
66   0.495728  8.039306  0.504883  7.893349
67   0.503662  7.912810  0.500854  7.957570
68   0.503418  7.916723  0.497192  8.015952
69   0.489868  8.132761  0.500732  7.959516
70   0.496826  8.021791  0.490356  8.124934
71   0.502563  7.930324  0.501343  7.949785
72   0.497314  8.014028  0.508057  7.842750
73   0.504395  7.901155  0.500488  7.963408
74   0.500122  7.969268  0.494751  8.054874
75   0.494629  8.056820  0.494751  8.054874
76   0.499146  7.984815  0.489014  8.146341
77   0.491821  8.101580  0.502686  7.928378
78   0.502441  7.932271  0.501465  7.947839
79   0.499878  7.973138  0.498535  7.994545
80   0.507080  7.858319  0.507202  7.856373
81   0.497559  8.010136  0.502808  7.926432
82   0.485596  8.200853  0.507812  7.846642
83   0.503662  7.912810  0.500732  7.959516
84   0.501831  7.942001  0.501099  7.953678
85   0.501343  7.949785  0.504517  7.899187
86   0.496216  8.031521  0.500366  7.965354
87   0.504272  7.903122  0.505981  7.875834
88   0.509399  7.821343  0.497192  8.015952
89   0.500122  7.969246  0.503906  7.908917
90   0.495728  8.039306  0.504639  7.897241
91   0.496216  8.031521  0.491577  8.105473
92   0.503784  7.910864  0.500854  7.957570
93   0.494385  8.060713  0.490967  8.115203
94   0.497559  8.010114  0.499146  7.984815
95   0.499512  7.979020  0.516357  7.710416
96   0.496826  8.021791  0.501221  7.951731
97   0.497192  8.015952  0.498535  7.994545
98   0.504028  7.906971  0.495728  8.039306
99   0.501831  7.942001  0.502075  7.938109
100  0.503174  7.920637  0.496460  8.027629
101  0.498657  7.992599  0.496704  8.023737
102  0.509277  7.823289  0.503174  7.920594
103  0.500610  7.961462  0.493774  8.070443
104  0.502930  7.924486  0.496582  8.025683
105  0.491211  8.111332  0.495605  8.041252
106  0.506348  7.869996  0.489624  8.136610
107  0.507324  7.854448  0.502563  7.930324
108  0.505615  7.881672  0.506348  7.869996
109  0.505371  7.885564  0.500854  7.957570
110  0.507202  7.856394  0.494507  8.058766
111  0.489502  8.138556  0.503662  7.912810
112  0.501099  7.953678  0.497803  8.006222
113  0.510986  7.796066  0.504150  7.905025
114  0.494629  8.056842  0.494507  8.058766
115  0.508545  7.834987  0.498779  7.990653
116  0.493652  8.072389  0.502319  7.934217
117  0.503906  7.908917  0.505371  7.885564
118  0.500488  7.963408  0.502075  7.938109
119  0.497803  8.006222  0.500000  7.971192
120  0.501221  7.951731  0.496094  8.033467
121  0.494751  8.054874  0.510986  7.796044
122  0.508057  7.842772  0.507690  7.848589
123  0.494141  8.064605  0.506836  7.862211
124  0.498413  7.996492  0.501831  7.942001
125  0.502197  7.936163  0.496704  8.023737
126  0.500122  7.969246  0.496826  8.021791
127  0.498779  7.990653  0.499146  7.984815

2018-06-08 16:46:02.826982 Finish.
Total elapsed time: 30:11:00.83.
