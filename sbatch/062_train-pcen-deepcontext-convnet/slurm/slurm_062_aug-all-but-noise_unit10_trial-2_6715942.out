2018-06-07 10:35:12.748301: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:12.748524: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:12.748536: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:35:02.573756 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit01, unit02, unit03.
Validation set: unit05, unit07.
Test set: unit10.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.510620  7.887336  0.494995  8.139717
1    0.510010  7.897109  0.496826  8.110204
2    0.505615  7.967919  0.493408  8.165295
3    0.513428  7.841975  0.500977  8.043307
4    0.505493  7.969693  0.493042  8.171198
5    0.507812  7.932503  0.493896  8.157425
6    0.509399  7.907075  0.492920  8.173165
7    0.504517  7.985648  0.501221  8.039372
8    0.498535  8.081929  0.505737  7.966573
9    0.505493  7.969844  0.503418  8.003957
10   0.506104  7.959877  0.506470  7.954768
11   0.495605  8.129021  0.508667  7.919352
12   0.499634  8.053603  0.521729  7.624788
13   0.521362  7.656794  0.504150  7.905025
14   0.521851  7.648838  0.503784  7.910864
15   0.523193  7.628332  0.513184  7.761014
16   0.527710  7.556670  0.504395  7.901133
17   0.511963  7.807930  0.512329  7.774637
18   0.523804  7.619116  0.501953  7.940055
19   0.518433  7.703650  0.503784  7.910864
20   0.513672  7.779612  0.518188  7.681225
21   0.521606  7.652408  0.537964  7.365958
22   0.528564  7.552163  0.707153  4.671592
23   0.526489  7.585075  0.702148  4.751317
24   0.524048  7.624490  0.695312  4.860062
25   0.524536  7.615869  0.698486  4.809764
26   0.532593  7.487191  0.697998  4.817677
27   0.524048  7.623696  0.708008  4.654715
28   0.533203  7.477503  0.707642  4.663850
29   0.530151  7.526735  0.703491  4.729867
30   0.524658  7.614652  0.699463  4.794796
31   0.520020  7.687874  0.711304  4.605038
32   0.495972  8.110849  0.492798  8.175133
33   0.508179  7.927008  0.492310  8.183003
34   0.501221  8.039287  0.499878  8.061015
35   0.494263  8.151286  0.505371  7.972476
36   0.504517  7.986099  0.505615  7.968541
37   0.512695  7.854230  0.505249  7.974444
38   0.504883  7.980067  0.507690  7.935093
39   0.494141  8.153340  0.510986  7.881969
40   0.501099  8.041104  0.496094  8.122009
41   0.493896  8.157318  0.507690  7.935093
42   0.495239  8.135653  0.505005  7.978379
43   0.506592  7.952736  0.493408  8.165295
44   0.499023  8.074531  0.501831  8.029535
45   0.514160  7.830663  0.491821  8.190873
46   0.501587  8.033384  0.497803  8.094463
47   0.503906  7.995851  0.494751  8.143652
48   0.504517  7.986142  0.496948  8.108236
49   0.500000  8.058812  0.502686  8.015762
50   0.495361  8.133600  0.497681  8.096431
51   0.495117  8.137621  0.491943  8.188905
52   0.516968  7.785388  0.499023  8.074788
53   0.494019  8.155329  0.503174  8.007892
54   0.506226  7.958617  0.501709  8.031502
55   0.502441  8.019547  0.497925  8.092496
56   0.491943  8.188798  0.503784  7.998054
57   0.496826  8.109989  0.515991  7.801300
58   0.500366  8.052931  0.497437  8.100366
59   0.501221  8.039201  0.502075  8.025600
60   0.491455  8.196690  0.499023  8.074788
61   0.491577  8.194637  0.501587  8.033470
62   0.495361  8.133729  0.496948  8.108236
63   0.492432  8.180821  0.497559  8.098399
64   0.504761  7.982206  0.496826  8.110204
65   0.498291  8.086465  0.504028  7.994119
66   0.505127  7.976175  0.494873  8.141685
67   0.505249  7.974293  0.504028  7.994119
68   0.489868  8.222268  0.494751  8.143652
69   0.490112  8.218290  0.512207  7.862294
70   0.499268  8.070681  0.486816  8.271542
71   0.499390  8.068671  0.492432  8.181035
72   0.496460  8.115999  0.506470  7.954768
73   0.500732  8.047049  0.497681  8.096431
74   0.503296  8.005731  0.501343  8.037405
75   0.502563  8.017558  0.501953  8.027567
76   0.497559  8.098184  0.495239  8.135782
77   0.502319  8.021557  0.497803  8.094464
78   0.489014  8.236062  0.495728  8.127912
79   0.506714  7.950726  0.490112  8.218419
80   0.494019  8.155350  0.494995  8.139717
81   0.501953  8.027417  0.509766  7.901645
82   0.505737  7.966488  0.490967  8.204646
83   0.491577  8.194679  0.507812  7.933125
84   0.489990  8.220236  0.489990  8.220386
85   0.499634  8.064843  0.502319  8.021665
86   0.512573  7.856198  0.505127  7.976411
87   0.496460  8.115849  0.499023  8.074788
88   0.498901  8.076627  0.500366  8.053145
89   0.510742  7.885711  0.506104  7.960671
90   0.492310  8.182703  0.508057  7.929190
91   0.504517  7.985927  0.500854  8.045275
92   0.503296  8.005645  0.501831  8.029535
93   0.506592  7.952650  0.498291  8.086593
94   0.506470  7.954639  0.504883  7.980346
95   0.497681  8.096302  0.500488  8.051178
96   0.497314  8.102226  0.502197  8.023632
97   0.499512  8.066746  0.506836  7.948865
98   0.500244  8.054984  0.505127  7.976411
99   0.515015  7.816976  0.499390  8.068885
100  0.497559  8.098291  0.500732  8.047243
101  0.495972  8.123891  0.500854  8.045275
102  0.498657  8.080541  0.493530  8.163327
103  0.495972  8.123805  0.498413  8.084626
104  0.502441  8.019504  0.498657  8.080691
105  0.508423  7.923159  0.494141  8.153490
106  0.503174  8.007870  0.497314  8.102334
107  0.501953  8.027374  0.512451  7.858359
108  0.496582  8.113989  0.499756  8.062983
109  0.503052  8.009666  0.500244  8.055113
110  0.504028  7.993990  0.496826  8.110204
111  0.500610  8.049081  0.487427  8.261705
112  0.495605  8.129729  0.500244  8.055113
113  0.501465  8.035266  0.501953  8.027567
114  0.493652  8.161188  0.494385  8.149555
115  0.497803  8.094335  0.501465  8.035437
116  0.500610  8.049146  0.508179  7.927223
117  0.502930  8.011720  0.493896  8.157425
118  0.488159  8.249663  0.498535  8.082658
119  0.492920  8.173036  0.501587  8.033470
120  0.504761  7.982249  0.494507  8.147587
121  0.494141  8.153318  0.500244  8.055113
122  0.493164  8.168951  0.493042  8.171198
123  0.491455  8.196647  0.501831  8.029535
124  0.491089  8.202507  0.501221  8.039372
125  0.494873  8.141513  0.494385  8.149555
126  0.497803  8.094313  0.497559  8.098399
127  0.502197  8.023439  0.497925  8.092496

2018-06-08 16:32:00.371122 Finish.
Total elapsed time: 29:56:58.37.
