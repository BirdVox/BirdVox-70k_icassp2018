2018-06-07 10:35:04.542883: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:04.543054: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:04.543065: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:35:02.143057 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit02, unit03, unit05.
Validation set: unit07, unit10.
Test set: unit01.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.498291  7.998438  0.503174  7.920594
1    0.504395  7.901133  0.501099  7.953678
2    0.500366  7.965354  0.497192  8.015952
3    0.499146  7.984815  0.499512  7.978977
4    0.513306  7.759068  0.504395  7.901133
5    0.506836  7.862211  0.500854  7.957570
6    0.493164  8.080173  0.494751  8.054874
7    0.498657  7.992599  0.498657  7.992599
8    0.501587  7.945893  0.499756  7.975085
9    0.507446  7.852481  0.498413  7.996492
10   0.503296  7.918648  0.505371  7.885564
11   0.506104  7.873888  0.513184  7.761014
12   0.492310  8.093796  0.499878  7.973138
13   0.501709  7.943969  0.502808  7.926432
14   0.504395  7.901155  0.494873  8.052928
15   0.492065  8.097688  0.499268  7.982869
16   0.504639  7.897241  0.494751  8.054874
17   0.493774  8.070443  0.499512  7.978977
18   0.492310  8.093839  0.507812  7.846642
19   0.502930  7.924486  0.504272  7.903079
20   0.496094  8.033467  0.497314  8.014006
21   0.493042  8.082141  0.502441  7.932271
22   0.504517  7.899187  0.490356  8.124934
23   0.504150  7.905025  0.500244  7.967300
24   0.501221  7.951731  0.501099  7.953678
25   0.500366  7.965376  0.499878  7.973138
26   0.502930  7.924508  0.503540  7.914756
27   0.511353  7.790206  0.491211  8.111311
28   0.499023  7.986761  0.495605  8.041252
29   0.504761  7.895316  0.500854  7.957570
30   0.496948  8.019866  0.503296  7.918648
31   0.500366  7.965376  0.499390  7.980923
32   0.499878  7.973138  0.498901  7.988707
33   0.508057  7.842750  0.498901  7.988707
34   0.507568  7.850535  0.507935  7.844696
35   0.503174  7.920594  0.501343  7.949785
36   0.508667  7.833020  0.496460  8.027629
37   0.499390  7.980923  0.493774  8.070443
38   0.495483  8.043198  0.490234  8.126880
39   0.506714  7.864157  0.500488  7.963408
40   0.505249  7.887510  0.493408  8.076281
41   0.490967  8.115225  0.499878  7.973138
42   0.510986  7.861119  0.494995  8.139717
43   0.505859  7.961217  0.499878  8.061015
44   0.516357  7.792158  0.497437  8.100366
45   0.510132  7.892031  0.498535  8.082658
46   0.505981  7.959785  0.512451  7.858359
47   0.504150  7.988977  0.500977  8.043307
48   0.516113  7.797037  0.509155  7.911482
49   0.498657  8.077409  0.505371  7.972476
50   0.509644  7.900759  0.495850  8.125944
51   0.501831  8.026081  0.492798  8.175133
52   0.519043  7.749387  0.490112  8.218419
53   0.512451  7.855720  0.499146  8.072821
54   0.510864  7.881213  0.495117  8.137749
55   0.501343  8.034209  0.493042  8.171198
56   0.505981  7.959292  0.507935  7.931158
57   0.516479  7.790470  0.497070  8.106269
58   0.500732  8.044089  0.500977  8.043307
59   0.498535  8.079055  0.495728  8.127912
60   0.512207  7.858819  0.496460  8.116106
61   0.508423  7.919984  0.487427  8.261705
62   0.502441  8.016565  0.499634  8.064950
63   0.508667  7.915920  0.499023  8.074788
64   0.490967  8.201192  0.503418  8.003957
65   0.511230  7.875160  0.496216  8.120042
66   0.507935  7.927940  0.489990  8.220386
67   0.512817  7.849625  0.508667  7.919352
68   0.512695  7.851249  0.506958  7.946898
69   0.513794  7.833412  0.501953  8.027567
70   0.497192  8.101084  0.500000  8.059048
71   0.503784  7.995008  0.501343  8.037405
72   0.507080  7.941799  0.501831  8.029535
73   0.503052  8.006620  0.497803  8.094464
74   0.501587  8.030424  0.496948  8.108236
75   0.503540  7.998643  0.503174  8.007892
76   0.508179  7.924262  0.508423  7.923287
77   0.505127  7.973043  0.499268  8.070853
78   0.503662  7.997040  0.489258  8.232191
79   0.510132  7.892911  0.498535  8.082658
80   0.506958  7.943659  0.499390  8.068885
81   0.508179  7.924048  0.502441  8.019697
82   0.504395  7.986412  0.498779  8.078723
83   0.502930  8.011762  0.497192  8.104301
84   0.503662  7.999914  0.494507  8.147587
85   0.500244  8.054963  0.509033  7.913450
86   0.502319  8.021514  0.506836  7.948866
87   0.505127  7.976239  0.487183  8.265640
88   0.502075  8.025492  0.500488  8.051178
89   0.497070  8.106097  0.503906  7.996086
90   0.505371  7.972304  0.501221  8.039372
91   0.507324  7.940888  0.506836  7.948865
92   0.489990  8.220236  0.493774  8.159392
93   0.511108  7.879894  0.494507  8.147587
94   0.487305  8.263458  0.498657  8.080691
95   0.499878  8.060930  0.499878  8.061015
96   0.507935  7.930943  0.499512  8.066918
97   0.491577  8.194679  0.509644  7.903612
98   0.489258  8.232063  0.503784  7.998054
99   0.501221  8.039201  0.497437  8.100366
100  0.497559  8.098227  0.496704  8.112171
101  0.503174  8.007720  0.495361  8.133814
102  0.492554  8.178939  0.502686  8.015762
103  0.502075  8.025385  0.504883  7.980346
104  0.498657  8.080562  0.501465  8.035437
105  0.493652  8.161145  0.505859  7.964606
106  0.492798  8.174983  0.500122  8.057080
107  0.506104  7.960542  0.505371  7.972476
108  0.507446  7.938792  0.497192  8.104301
109  0.494385  8.149362  0.504395  7.988216
110  0.495483  8.131740  0.504150  7.992151
111  0.507935  7.931029  0.492920  8.173165
112  0.493042  8.171005  0.499268  8.070853
113  0.501099  8.041168  0.503540  8.001989
114  0.502686  8.015655  0.499390  8.068885
115  0.500366  8.053081  0.504395  7.988216
116  0.499146  8.072649  0.502441  8.019697
117  0.499878  8.060930  0.503540  8.001989
118  0.501099  8.041104  0.497803  8.094464
119  0.499023  8.074638  0.501831  8.029535
120  0.489258  8.232063  0.500488  8.051178
121  0.503540  8.001882  0.487427  8.261705
122  0.503174  8.007720  0.502563  8.017729
123  0.495972  8.123741  0.501343  8.037405
124  0.500610  8.049146  0.491455  8.196776
125  0.498413  8.084540  0.493042  8.171198
126  0.505737  7.966530  0.480347  8.375822
127  0.506104  7.960563  0.500488  8.051178

2018-06-08 17:20:58.788709 Finish.
Total elapsed time: 30:45:56.79.
