2018-06-07 10:35:05.118572: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.118757: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.118768: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.118773: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.118778: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:35:02.520558 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit02, unit03, unit05.
Validation set: unit07, unit10.
Test set: unit01.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.966919  0.124621  0.906616  0.270847
1    0.968018  0.114524  0.929688  0.264074
2    0.968872  0.116249  0.946167  0.181821
3    0.963013  0.133394  0.956055  0.190817
4    0.963379  0.123927  0.962036  0.172452
5    0.771729  3.530816  0.704834  4.703232
6    0.672241  5.225321  0.718262  4.487343
7    0.757935  3.860172  0.697998  4.864737
8    0.861694  2.210310  0.634644  5.886203
9    0.572388  6.879326  0.500122  8.057080
10   0.500854  8.045275  0.497925  8.092496
11   0.500122  8.057080  0.495361  8.133814
12   0.506836  7.948865  0.500366  8.053145
13   0.500732  8.047200  0.493896  8.157425
14   0.499023  8.074767  0.500122  8.057080
15   0.511353  7.876066  0.503052  8.009859
16   0.501831  8.029535  0.509277  7.909515
17   0.501831  8.029535  0.499634  8.064950
18   0.491455  8.196733  0.504639  7.984281
19   0.495728  8.127912  0.500732  8.047243
20   0.497314  8.102334  0.504761  7.982314
21   0.504883  7.980346  0.501343  8.037405
22   0.509644  7.903612  0.501587  8.033470
23   0.509033  7.913450  0.496216  8.120042
24   0.501221  8.039351  0.500854  8.045275
25   0.506836  7.948823  0.503296  8.005924
26   0.498901  8.076756  0.506714  7.950833
27   0.491211  8.200711  0.505249  7.974444
28   0.501099  8.041318  0.502930  8.011827
29   0.504395  7.988195  0.507568  7.937060
30   0.501221  8.039351  0.497681  8.096431
31   0.494873  8.141685  0.500977  8.043307
32   0.502075  8.025578  0.500854  8.045275
33   0.501831  8.029513  0.491943  8.188905
34   0.503296  8.005924  0.500366  8.053145
35   0.503540  8.001946  0.508667  7.919352
36   0.510498  7.889796  0.498047  8.090528
37   0.504150  7.992130  0.504150  7.992151
38   0.496826  8.110182  0.494019  8.155457
39   0.500244  8.055091  0.497559  8.098399
40   0.503174  8.007892  0.492310  8.183003
41   0.499878  8.061015  0.496826  8.110204
42   0.491943  8.188884  0.497070  8.106269
43   0.496216  8.119977  0.498291  8.086593
44   0.497070  8.106247  0.493042  8.171198
45   0.495972  8.123977  0.500732  8.047243
46   0.505249  7.974422  0.500244  8.055113
47   0.500122  8.057080  0.502441  8.019697
48   0.496216  8.119977  0.502808  8.013794
49   0.500854  8.045254  0.494141  8.153490
50   0.500732  8.047243  0.501831  8.029535
51   0.497192  8.104301  0.491089  8.202678
52   0.502319  8.021643  0.503540  8.001989
53   0.500122  8.057080  0.493896  8.157425
54   0.505615  7.968541  0.490845  8.206613
55   0.501343  8.037383  0.500977  8.043307
56   0.489502  8.228256  0.500244  8.055113
57   0.498901  8.076756  0.499023  8.074788
58   0.510132  7.895742  0.504395  7.988216
59   0.502441  8.019697  0.498169  8.088561
60   0.496704  8.112150  0.498047  8.090528
61   0.499146  8.072821  0.501343  8.037405
62   0.494995  8.139696  0.498901  8.076756
63   0.502686  8.015762  0.498779  8.078723
64   0.504883  7.980346  0.510498  7.889839
65   0.507568  7.937039  0.492188  8.184970
66   0.492554  8.179025  0.501465  8.035437
67   0.510498  7.889818  0.508667  7.919352
68   0.506226  7.958660  0.501831  8.029535
69   0.501465  8.035394  0.508545  7.921320
70   0.504639  7.984281  0.508057  7.929190
71   0.493286  8.167263  0.497314  8.102334
72   0.491821  8.190852  0.495361  8.133814
73   0.502319  8.021643  0.499512  8.066918
74   0.500122  8.057080  0.492554  8.179068
75   0.496582  8.114117  0.499634  8.064950
76   0.507202  7.942963  0.488281  8.247932
77   0.498901  8.076756  0.502319  8.021665
78   0.506348  7.956736  0.503052  8.009859
79   0.505981  7.962638  0.501953  8.027567
80   0.503784  7.998054  0.501465  8.035437
81   0.504028  7.994076  0.500732  8.047243
82   0.499512  8.066875  0.502686  8.015762
83   0.508057  7.929190  0.496094  8.122009
84   0.502075  8.025578  0.496338  8.118074
85   0.502808  8.013794  0.505249  7.974444
86   0.494629  8.145620  0.499023  8.074788
87   0.496216  8.120042  0.508057  7.929190
88   0.500122  8.057080  0.496216  8.120042
89   0.494263  8.151501  0.495239  8.135782
90   0.508057  7.929169  0.499268  8.070853
91   0.504272  7.990184  0.498901  8.076756
92   0.504395  7.988216  0.502319  8.021664
93   0.491211  8.200711  0.500000  8.059048
94   0.509888  7.899656  0.500366  8.053145
95   0.493408  8.165274  0.495361  8.133814
96   0.504272  7.990141  0.500732  8.047243
97   0.503784  7.998054  0.492798  8.175133
98   0.502563  8.017729  0.496338  8.118074
99   0.502563  8.017687  0.502441  8.019697
100  0.500244  8.055113  0.480591  8.371887
101  0.492188  8.184906  0.500000  8.059048
102  0.505737  7.966573  0.506104  7.960671
103  0.501831  8.029535  0.497681  8.096431
104  0.497070  8.106204  0.497925  8.092496
105  0.503662  7.999979  0.501709  8.031502
106  0.493652  8.161317  0.508301  7.925255
107  0.493530  8.163327  0.497192  8.104301
108  0.499390  8.068843  0.502075  8.025600
109  0.502075  8.025600  0.511353  7.876066
110  0.495239  8.135782  0.488403  8.245964
111  0.497437  8.100345  0.496216  8.120042
112  0.500610  8.049210  0.501831  8.029535
113  0.489380  8.230202  0.492310  8.183003
114  0.500488  8.051178  0.503052  8.009859
115  0.503052  8.009838  0.503052  8.009859
116  0.498535  8.082637  0.496338  8.118074
117  0.500732  8.047243  0.493652  8.161360
118  0.503540  8.001989  0.503906  7.996086
119  0.494629  8.145598  0.501953  8.027567
120  0.489502  8.228256  0.493896  8.157425
121  0.497925  8.092474  0.498291  8.086593
122  0.498779  8.078723  0.495972  8.123977
123  0.505249  7.974444  0.499512  8.066918
124  0.509033  7.913407  0.497681  8.096431
125  0.496582  8.114139  0.496826  8.110204
126  0.510376  7.891764  0.501831  8.029535
127  0.495850  8.125944  0.502197  8.023632

2018-06-08 17:44:17.585340 Finish.
Total elapsed time: 31:09:15.59.
