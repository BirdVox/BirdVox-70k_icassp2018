2018-06-07 10:35:04.703690: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:04.703875: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:04.703887: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:35:02.437697 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit10, unit01, unit02.
Validation set: unit03, unit05.
Test set: unit07.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.506836  7.948844  0.507080  7.944930
1    0.505005  7.978357  0.497559  8.098399
2    0.501221  7.988328  0.496338  8.029575
3    0.506226  7.898796  0.501831  7.942001
4    0.498047  8.029548  0.500244  7.967300
5    0.498535  8.022107  0.503418  7.916702
6    0.502441  7.959232  0.490723  8.119095
7    0.499146  8.011733  0.497314  8.014006
8    0.504150  7.933123  0.495728  8.039306
9    0.501831  7.968683  0.498901  7.988707
10   0.497070  8.045482  0.498291  7.998438
11   0.491333  8.136734  0.503418  7.916702
12   0.491211  8.139623  0.492188  8.095742
13   0.506226  7.899739  0.507324  7.854427
14   0.493408  8.110986  0.497437  8.012060
15   0.504639  7.930315  0.497192  8.015952
16   0.498657  8.026703  0.500366  7.965354
17   0.486084  8.228502  0.495239  8.047090
18   0.503418  7.949647  0.493774  8.070443
19   0.504150  7.937713  0.508667  7.833020
20   0.498901  8.022360  0.490967  8.115203
21   0.508301  7.872554  0.500854  7.957570
22   0.485474  8.237246  0.492554  8.089904
23   0.497314  8.081381  0.496704  8.112171
24   0.501099  8.041297  0.495483  8.131847
25   0.496948  8.108086  0.494751  8.143652
26   0.494019  8.155329  0.499756  8.062983
27   0.503662  7.999871  0.505005  7.978379
28   0.504639  7.984217  0.500732  8.047243
29   0.492188  8.184906  0.495239  8.135782
30   0.502686  8.015697  0.494873  8.141684
31   0.507568  7.936910  0.509888  7.899677
32   0.506958  7.946748  0.491699  8.192841
33   0.491577  8.194679  0.495605  8.129879
34   0.498413  8.084497  0.499023  8.074788
35   0.493164  8.169101  0.505005  7.978379
36   0.506714  7.950747  0.496704  8.112171
37   0.491943  8.188755  0.496460  8.116106
38   0.494995  8.139545  0.497681  8.096431
39   0.502808  8.013644  0.502808  8.013794
40   0.507812  7.932996  0.500366  8.053145
41   0.500977  8.043265  0.500244  8.055113
42   0.493652  8.161103  0.497803  8.094464
43   0.503418  8.003785  0.513428  7.842618
44   0.500854  8.045146  0.490723  8.208581
45   0.506470  7.954639  0.503540  8.001989
46   0.507324  7.940867  0.489868  8.222354
47   0.498169  8.088368  0.494263  8.151522
48   0.497925  8.092410  0.500000  8.059048
49   0.510376  7.891700  0.494873  8.141684
50   0.505249  7.974336  0.498779  8.078723
51   0.501709  8.031395  0.507080  7.944930
52   0.492065  8.186852  0.500488  8.051178
53   0.498901  8.076605  0.499390  8.068885
54   0.498413  8.084433  0.505249  7.974444
55   0.498169  8.088454  0.501221  8.039372
56   0.501343  8.037298  0.501953  8.027567
57   0.491577  8.194744  0.501465  8.035437
58   0.505737  7.966380  0.519775  7.740306
59   0.494751  8.143523  0.504272  7.990184
60   0.495850  8.125773  0.499756  8.062983
61   0.490479  8.212366  0.494263  8.151522
62   0.498901  8.076670  0.492065  8.186938
63   0.490479  8.212366  0.513306  7.844586
64   0.499634  8.064800  0.499268  8.070853
65   0.507324  7.940845  0.496704  8.112171
66   0.494385  8.149383  0.498169  8.088561
67   0.504761  7.982121  0.496582  8.114139
68   0.499512  8.066768  0.493530  8.163327
69   0.489868  8.222118  0.495972  8.123977
70   0.498169  8.088325  0.489014  8.236126
71   0.501099  8.041168  0.501953  8.027567
72   0.494751  8.143438  0.495728  8.127912
73   0.505005  7.978164  0.495972  8.123977
74   0.494263  8.151458  0.505005  7.978379
75   0.497681  8.096238  0.500488  8.051178
76   0.502930  8.011762  0.493530  8.163327
77   0.499268  8.070703  0.489136  8.234159
78   0.491089  8.202550  0.495972  8.123977
79   0.500122  8.057037  0.503174  8.007892
80   0.499268  8.070724  0.489624  8.226289
81   0.496582  8.113989  0.496826  8.110204
82   0.499512  8.066725  0.496460  8.116106
83   0.507446  7.938878  0.499878  8.061015
84   0.502197  8.023503  0.503418  8.003957
85   0.498901  8.076670  0.489502  8.228256
86   0.502686  8.015698  0.504028  7.994119
87   0.501831  8.029427  0.497925  8.092496
88   0.508179  7.926944  0.488647  8.242029
89   0.503418  8.003849  0.498535  8.082658
90   0.506348  7.956628  0.499146  8.072821
91   0.503662  7.999914  0.500854  8.045275
92   0.489258  8.231998  0.502319  8.021665
93   0.490234  8.216365  0.499756  8.062983
94   0.503540  8.001796  0.503174  8.007892
95   0.498901  8.076627  0.507568  7.937060
96   0.493286  8.167091  0.505981  7.962638
97   0.497314  8.102141  0.490601  8.210548
98   0.507324  7.940888  0.513916  7.834748
99   0.497070  8.106162  0.498047  8.090528
100  0.495972  8.123955  0.498901  8.076756
101  0.501831  8.029342  0.497070  8.106269
102  0.502686  8.015698  0.494873  8.141684
103  0.507568  7.936867  0.503784  7.998054
104  0.500854  8.045168  0.490845  8.206613
105  0.499634  8.064736  0.514038  7.832781
106  0.502563  8.017515  0.496582  8.114139
107  0.491699  8.192669  0.497192  8.104301
108  0.501099  8.041211  0.506226  7.958703
109  0.501099  8.041190  0.500488  8.051178
110  0.497070  8.106162  0.500977  8.043307
111  0.513306  7.844521  0.497925  8.092496
112  0.505859  7.964370  0.500610  8.049210
113  0.494141  8.153361  0.503784  7.998054
114  0.503052  8.009795  0.503296  8.005924
115  0.497559  8.098270  0.498901  8.076756
116  0.501221  8.039308  0.495483  8.131847
117  0.496094  8.121880  0.503174  8.007892
118  0.502808  8.013709  0.508789  7.917385
119  0.494385  8.149426  0.499634  8.064950
120  0.499390  8.068757  0.488159  8.249899
121  0.503052  8.009688  0.514526  7.824910
122  0.500000  8.058898  0.502441  8.019697
123  0.502930  8.011698  0.493774  8.159392
124  0.495728  8.127783  0.504395  7.988216
125  0.502197  8.023525  0.497803  8.094464
126  0.503662  7.999936  0.499634  8.064950
127  0.498413  8.084454  0.498779  8.078723

2018-06-08 17:25:42.072366 Finish.
Total elapsed time: 30:50:40.07.
