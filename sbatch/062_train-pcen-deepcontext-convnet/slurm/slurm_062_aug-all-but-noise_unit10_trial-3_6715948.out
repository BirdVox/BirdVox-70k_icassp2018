2018-06-07 10:35:13.507503: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:13.507698: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:13.507711: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:35:02.803151 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit01, unit02, unit03.
Validation set: unit05, unit07.
Test set: unit10.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.502930  7.924529  0.502808  7.926432
1    0.498413  7.996513  0.494141  8.064605
2    0.501831  7.942022  0.504883  7.893349
3    0.506104  7.873888  0.499512  7.978977
4    0.502563  7.930346  0.500977  7.955624
5    0.499756  7.975085  0.491577  8.105473
6    0.496460  8.027629  0.504395  7.901133
7    0.504150  7.905068  0.491089  8.113257
8    0.507568  7.850556  0.495361  8.045144
9    0.504517  7.899187  0.494263  8.062659
10   0.499512  7.978977  0.508789  7.831074
11   0.497192  8.015952  0.504395  7.901133
12   0.500977  7.955624  0.502075  7.938109
13   0.498291  7.998481  0.488770  8.150233
14   0.491577  8.105494  0.509521  7.819397
15   0.491577  8.105494  0.496948  8.019845
16   0.500854  7.957570  0.503052  7.922540
17   0.501587  7.945915  0.498047  8.002330
18   0.500000  7.971214  0.500977  7.955624
19   0.494873  8.052928  0.498779  7.990653
20   0.510864  7.798012  0.500488  7.963408
21   0.503052  7.922562  0.502563  7.930324
22   0.496826  8.021812  0.500122  7.969246
23   0.499146  7.984815  0.500488  7.963408
24   0.506470  7.868049  0.493652  8.072389
25   0.513672  7.753273  0.505005  7.891403
26   0.499146  7.984895  0.500610  7.961462
27   0.501343  7.949828  0.505859  7.877780
28   0.504639  7.897284  0.505493  7.883618
29   0.508667  7.833127  0.501221  7.951731
30   0.502075  7.938173  0.493042  8.082120
31   0.488892  8.148372  0.509766  7.815505
32   0.495728  8.039391  0.507935  7.844696
33   0.502563  7.930324  0.503418  7.916702
34   0.505249  7.887703  0.499512  7.978977
35   0.496338  8.029618  0.481567  8.265052
36   0.498047  8.002373  0.495361  8.045144
37   0.500366  7.965376  0.512573  7.770745
38   0.501709  7.943947  0.518188  7.681225
39   0.508301  7.838901  0.500244  7.967300
40   0.503418  7.916788  0.505615  7.881672
41   0.496704  8.023780  0.500610  7.961462
42   0.499146  7.984879  0.494141  8.064605
43   0.501099  7.953763  0.495361  8.045144
44   0.492432  8.091914  0.491699  8.103527
45   0.498169  8.000448  0.504761  7.895295
46   0.490479  8.123009  0.492310  8.093796
47   0.509277  7.823311  0.499390  7.980923
48   0.491699  8.103591  0.490723  8.119095
49   0.499878  7.973160  0.500000  7.971192
50   0.500854  7.957634  0.487305  8.173586
51   0.495728  8.039348  0.505371  7.885564
52   0.492188  8.095807  0.501831  7.942001
53   0.496948  8.019909  0.499756  7.975085
54   0.510010  7.811763  0.501099  7.953678
55   0.501465  7.947882  0.501953  7.940055
56   0.504517  7.899208  0.506836  7.862211
57   0.491699  8.103612  0.499390  7.980923
58   0.503906  7.908939  0.502930  7.924486
59   0.490601  8.121063  0.502319  7.934217
60   0.506592  7.866168  0.505005  7.891403
61   0.514526  7.739672  0.502686  7.928378
62   0.504272  7.903079  0.504639  7.897241
63   0.504639  7.897348  0.489014  8.146341
64   0.487671  8.167833  0.500244  7.967300
65   0.503784  7.910928  0.494141  8.064605
66   0.496826  8.021877  0.493652  8.072389
67   0.494873  8.052950  0.509277  7.823289
68   0.492676  8.087979  0.501587  7.945893
69   0.496826  8.021898  0.496582  8.025683
70   0.504395  7.901133  0.499634  7.977031
71   0.500610  7.961505  0.502686  7.928378
72   0.495483  8.043219  0.493042  8.082120
73   0.488525  8.154168  0.505493  7.883618
74   0.505249  7.887575  0.500244  7.967300
75   0.496582  8.025790  0.505859  7.877780
76   0.496216  8.031543  0.493164  8.080173
77   0.501099  7.953699  0.495239  8.047090
78   0.500977  7.955688  0.497559  8.010114
79   0.498047  8.002394  0.499146  7.984815
80   0.494263  8.062702  0.499268  7.982869
81   0.501709  7.944119  0.495850  8.037359
82   0.497559  8.010200  0.510498  7.803828
83   0.510864  7.798033  0.504517  7.899187
84   0.498779  7.990653  0.505981  7.875834
85   0.495728  8.039370  0.501953  7.940055
86   0.509155  7.825278  0.507812  7.846642
87   0.508545  7.835095  0.499146  7.984815
88   0.509033  7.827310  0.507812  7.846642
89   0.504028  7.906993  0.498047  8.002330
90   0.507568  7.850556  0.507812  7.846642
91   0.502441  7.932421  0.498901  7.988707
92   0.490723  8.119160  0.505127  7.889457
93   0.508301  7.838901  0.503540  7.914756
94   0.500732  7.959516  0.500732  7.959516
95   0.499146  7.984858  0.494385  8.060713
96   0.501709  7.943990  0.503174  7.920594
97   0.495117  8.049057  0.503784  7.910864
98   0.492188  8.095785  0.500854  7.957570
99   0.498535  7.994653  0.498779  7.990653
100  0.501709  7.944033  0.498657  7.992599
101  0.500122  7.969311  0.494751  8.054874
102  0.499390  7.980987  0.501221  7.951731
103  0.500610  7.961483  0.505249  7.887510
104  0.498047  8.002351  0.498413  7.996492
105  0.500732  7.959623  0.502075  7.938109
106  0.499023  7.986890  0.496948  8.019845
107  0.492676  8.088022  0.498779  7.990653
108  0.505249  7.887510  0.495117  8.049036
109  0.497314  8.014092  0.499512  7.978977
110  0.504761  7.895316  0.503906  7.908917
111  0.502808  7.926454  0.500244  7.967300
112  0.502808  7.926475  0.501587  7.945893
113  0.510010  7.811656  0.490479  8.122988
114  0.497681  8.008168  0.494263  8.062659
115  0.500122  7.969311  0.500732  7.959516
116  0.504395  7.901262  0.498535  7.994545
117  0.502441  7.932313  0.498535  7.994545
118  0.497437  8.012210  0.494751  8.054874
119  0.506836  7.862318  0.503662  7.912810
120  0.504028  7.907036  0.502930  7.924486
121  0.508057  7.842793  0.500488  7.963408
122  0.504150  7.905047  0.503296  7.918648
123  0.500854  7.957656  0.499512  7.978977
124  0.502197  7.936227  0.501587  7.945893
125  0.494385  8.060734  0.496826  8.021791
126  0.502441  7.932271  0.501221  7.951731
127  0.506226  7.872049  0.502197  7.936163

2018-06-08 16:50:35.082526 Finish.
Total elapsed time: 30:15:33.08.
