2018-06-07 10:35:05.035690: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.035926: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.035939: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:35:02.045021 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.957642  0.165937  0.738281  1.555817
1    0.960571  0.144335  0.817139  0.933640
2    0.959839  0.146336  0.821167  0.891208
3    0.962646  0.135249  0.820068  0.914669
4    0.966309  0.129645  0.808594  0.829003
5    0.963135  0.133200  0.804199  1.522417
6    0.963989  0.133652  0.816772  0.757353
7    0.967529  0.125404  0.794800  0.860496
8    0.969727  0.109653  0.815063  1.200101
9    0.972046  0.096132  0.808228  1.091279
10   0.971436  0.105068  0.818115  0.693590
11   0.967407  0.118570  0.838989  0.832965
12   0.967529  0.130652  0.827515  0.696246
13   0.971069  0.088996  0.845703  0.787090
14   0.975830  0.096867  0.858154  0.509395
15   0.970947  0.092897  0.845581  0.599412
16   0.975586  0.095849  0.846069  0.903429
17   0.972168  0.107544  0.834473  0.807246
18   0.974731  0.098290  0.850952  0.720636
19   0.978271  0.091252  0.850952  0.562829
20   0.972412  0.098061  0.832886  0.881728
21   0.974121  0.097969  0.855347  0.607997
22   0.967651  0.209978  0.798706  3.084577
23   0.542969  7.358923  0.489868  8.222354
24   0.497681  8.093013  0.498657  8.080691
25   0.492676  8.177334  0.497559  8.098399
26   0.502563  8.017687  0.501099  8.041340
27   0.507690  7.935211  0.503174  8.007892
28   0.505127  7.976581  0.497803  8.094464
29   0.504883  7.980477  0.495728  8.127912
30   0.499634  8.064950  0.499146  8.072821
31   0.496704  8.112258  0.505127  7.976411
32   0.500122  8.057113  0.489746  8.224321
33   0.501221  8.039515  0.504150  7.992151
34   0.493896  8.157434  0.499512  8.066918
35   0.488037  8.251845  0.495972  8.123977
36   0.496582  8.112325  0.494629  8.145620
37   0.505615  7.966915  0.505127  7.976411
38   0.489746  8.223169  0.495605  8.129879
39   0.492188  8.181282  0.494507  8.147587
40   0.500854  8.045336  0.496338  8.118074
41   0.501709  8.028145  0.505859  7.964606
42   0.502808  8.013850  0.491455  8.196776
43   0.507202  7.943200  0.502808  8.013794
44   0.501587  8.031941  0.497925  8.092496
45   0.504272  7.988535  0.497925  8.092496
46   0.507080  7.945002  0.503174  8.007892
47   0.503540  8.000233  0.501953  8.027567
48   0.498657  8.079058  0.497437  8.100366
49   0.500000  8.055556  0.497314  8.102334
50   0.492310  8.183023  0.507202  7.942963
51   0.511108  7.878281  0.507446  7.939028
52   0.501953  8.025969  0.503540  8.001989
53   0.498779  8.076556  0.514404  7.826878
54   0.494995  8.139304  0.494873  8.141685
55   0.500244  8.054519  0.494507  8.147587
56   0.504639  7.982275  0.500977  8.043307
57   0.497803  8.092345  0.506348  7.956736
58   0.497437  8.099835  0.506836  7.948866
59   0.503174  8.003987  0.487915  8.253834
60   0.507202  7.940720  0.506348  7.956736
61   0.502563  8.013634  0.500244  8.055113
62   0.494019  8.153294  0.491699  8.192841
63   0.506226  7.956449  0.504761  7.982314
64   0.493042  8.170913  0.504395  7.988216
65   0.502441  8.015843  0.502930  8.011827
66   0.501587  8.032920  0.503174  8.007892
67   0.496338  8.116002  0.501221  8.039372
68   0.503784  7.994052  0.498779  8.078723
69   0.508301  7.924859  0.503540  8.001989
70   0.495972  8.123557  0.496460  8.116106
71   0.504028  7.990159  0.491577  8.194808
72   0.501099  8.040904  0.493286  8.167263
73   0.505859  7.958686  0.497925  8.092496
74   0.491943  8.185238  0.512573  7.856391
75   0.501343  8.037405  0.509033  7.913450
76   0.500488  8.051156  0.505249  7.974443
77   0.500244  8.055113  0.512573  7.856391
78   0.493042  8.171198  0.492676  8.177100
79   0.489746  8.224357  0.498779  8.078723
80   0.511597  7.872110  0.500610  8.049210
81   0.506592  7.952779  0.502686  8.015762
82   0.509521  7.905614  0.501831  8.029535
83   0.483398  8.326612  0.492432  8.181035
84   0.503540  8.001968  0.501099  8.041340
85   0.492432  8.181061  0.498901  8.076756
86   0.505859  7.964606  0.505981  7.962638
87   0.497314  8.102334  0.496948  8.108236
88   0.501709  8.031459  0.493164  8.169230
89   0.490967  8.204646  0.504272  7.990184
90   0.494263  8.149732  0.507080  7.944930
91   0.497681  8.096445  0.504028  7.994119
92   0.509888  7.899656  0.507324  7.940995
93   0.498413  8.084604  0.500610  8.049210
94   0.502686  8.015719  0.506592  7.952801
95   0.496216  8.120042  0.499023  8.074788
96   0.503418  8.003975  0.496216  8.120042
97   0.514038  7.832781  0.501953  8.027567
98   0.495972  8.123955  0.502563  8.017729
99   0.502319  8.021664  0.503418  8.003957
100  0.506470  7.954768  0.497437  8.100366
101  0.509399  7.907547  0.498291  8.086593
102  0.493042  8.171198  0.498047  8.090528
103  0.494995  8.139674  0.494873  8.141685
104  0.499268  8.070853  0.497314  8.102334
105  0.493164  8.169230  0.497803  8.094464
106  0.512573  7.856391  0.498047  8.090528
107  0.505493  7.970508  0.495117  8.137749
108  0.491943  8.188905  0.493286  8.167263
109  0.504517  7.986227  0.499512  8.066918
110  0.503784  7.998011  0.498535  8.082658
111  0.497437  8.100366  0.497681  8.096431
112  0.490723  8.208611  0.504272  7.990184
113  0.504150  7.992130  0.501465  8.035437
114  0.503662  7.999957  0.505005  7.978379
115  0.494507  8.147566  0.507935  7.931158
116  0.499023  8.074788  0.506226  7.958703
117  0.495728  8.126174  0.506226  7.958703
118  0.510254  7.892005  0.498657  8.080691
119  0.500610  8.049210  0.486206  8.281380
120  0.499146  8.072821  0.496582  8.114139
121  0.498047  8.090507  0.500366  8.053145
122  0.495117  8.137707  0.500122  8.057080
123  0.492065  8.186977  0.496948  8.108236
124  0.495728  8.127912  0.505981  7.962638
125  0.510010  7.897688  0.504272  7.990184
126  0.504150  7.992130  0.501709  8.031502
127  0.498901  8.076756  0.500977  8.043307

2018-06-08 16:49:10.358820 Finish.
Total elapsed time: 30:14:08.36.
