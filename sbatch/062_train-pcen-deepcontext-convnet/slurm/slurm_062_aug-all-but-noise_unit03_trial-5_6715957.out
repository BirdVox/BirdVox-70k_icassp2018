2018-06-07 10:35:05.597120: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.597273: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:05.597285: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:35:02.804989 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.964844  0.126079  0.817017  1.106271
1    0.962891  0.131057  0.852051  0.969786
2    0.973022  0.105992  0.871216  0.739260
3    0.969849  0.112820  0.837769  0.806754
4    0.957886  0.154274  0.830078  0.686271
5    0.967285  0.107281  0.857666  0.753600
6    0.972534  0.100510  0.856079  0.645592
7    0.969849  0.105799  0.848511  0.854414
8    0.974243  0.101940  0.847534  0.751702
9    0.972290  0.090320  0.867920  0.724965
10   0.969727  0.097525  0.857788  0.899989
11   0.971313  0.102108  0.859375  1.014755
12   0.968872  0.120022  0.849243  0.618799
13   0.971802  0.103425  0.854614  0.975705
14   0.973389  0.094451  0.854492  0.885473
15   0.975830  0.096550  0.866211  0.883252
16   0.972412  0.097152  0.854980  1.047224
17   0.977173  0.083331  0.869873  0.673086
18   0.971802  0.110341  0.836792  0.872317
19   0.972534  0.106012  0.852173  0.774769
20   0.975708  0.086211  0.850220  0.624174
21   0.968994  0.101322  0.841919  0.711035
22   0.964966  0.117940  0.854980  0.673312
23   0.973389  0.094118  0.842773  0.825294
24   0.972656  0.097532  0.829468  0.872400
25   0.973145  0.096515  0.792847  0.983206
26   0.972168  0.106405  0.839844  0.929920
27   0.973145  0.091926  0.794678  1.347966
28   0.976929  0.089652  0.804932  1.326957
29   0.978516  0.082592  0.794434  1.386604
30   0.978882  0.078899  0.828369  1.085660
31   0.975098  0.092422  0.821411  1.136261
32   0.976807  0.084668  0.832397  1.263694
33   0.974609  0.086511  0.806274  1.465627
34   0.975952  0.092530  0.783569  1.978722
35   0.975342  0.088075  0.813110  1.512548
36   0.973511  0.090993  0.810791  1.489008
37   0.971069  0.093613  0.830322  1.264928
38   0.974976  0.097271  0.807495  1.328172
39   0.973145  0.097319  0.808838  1.204456
40   0.976074  0.089678  0.798340  1.579890
41   0.971924  0.103711  0.812866  1.126570
42   0.978760  0.083102  0.823486  1.062062
43   0.978271  0.082460  0.823120  1.166021
44   0.974731  0.090869  0.803345  1.446286
45   0.974854  0.096898  0.798218  1.473443
46   0.973511  0.089970  0.808472  1.553545
47   0.977539  0.076050  0.822388  1.327550
48   0.976929  0.083179  0.806763  1.434853
49   0.978394  0.079590  0.787109  1.461144
50   0.978271  0.075126  0.823608  1.324334
51   0.979248  0.076130  0.813110  1.421963
52   0.980835  0.069765  0.768799  2.108720
53   0.977295  0.082595  0.754883  1.937349
54   0.975098  0.093534  0.801636  1.568372
55   0.976074  0.085699  0.747559  2.471426
56   0.975464  0.086921  0.714722  2.591559
57   0.974976  0.090217  0.712769  2.486049
58   0.978638  0.073889  0.698120  2.771205
59   0.979858  0.068844  0.760742  1.866759
60   0.979370  0.075191  0.725708  2.498883
61   0.977661  0.083575  0.804199  1.568383
62   0.981323  0.073001  0.797607  1.573336
63   0.976807  0.080641  0.794922  1.725708
64   0.979248  0.077233  0.804932  1.666794
65   0.978394  0.072386  0.768066  2.324844
66   0.980225  0.075105  0.796631  1.803431
67   0.978394  0.084980  0.723267  2.523782
68   0.979248  0.080843  0.725098  2.757847
69   0.977173  0.087852  0.711304  2.518316
70   0.980347  0.076936  0.737793  2.606058
71   0.979370  0.087044  0.685791  3.031845
72   0.980103  0.084250  0.667114  3.374338
73   0.973755  0.092071  0.675171  3.064038
74   0.975098  0.083612  0.722046  2.718226
75   0.978760  0.075911  0.727417  2.630537
76   0.981323  0.077937  0.740356  2.522421
77   0.977661  0.075470  0.684204  3.099790
78   0.982910  0.060009  0.670044  3.845791
79   0.980591  0.086089  0.702515  2.934599
80   0.979980  0.070038  0.652954  3.630489
81   0.980469  0.076223  0.718018  2.744016
82   0.976807  0.077867  0.677856  3.389484
83   0.978516  0.084834  0.666260  3.621905
84   0.979370  0.072586  0.675049  3.459821
85   0.820312  2.717030  0.500000  7.971192
86   0.513672  7.805390  0.501343  8.037405
87   0.492310  8.182810  0.487427  8.261705
88   0.502197  8.021479  0.491333  8.198743
89   0.502930  8.011762  0.508911  7.915417
90   0.493164  8.169230  0.498047  8.090528
91   0.498535  8.082637  0.494019  8.155457
92   0.494263  8.151501  0.500488  8.051178
93   0.500732  8.047221  0.503418  8.003957
94   0.497803  8.094464  0.495605  8.129879
95   0.495728  8.127890  0.497681  8.096431
96   0.489746  8.224257  0.493164  8.169230
97   0.502563  8.017708  0.497314  8.102334
98   0.501221  8.039351  0.500610  8.049210
99   0.508179  7.927180  0.499268  8.070853
100  0.499512  8.066875  0.497314  8.102334
101  0.502930  8.011762  0.506714  7.950833
102  0.506470  7.954725  0.501587  8.033470
103  0.500732  8.047221  0.498413  8.084626
104  0.494141  8.153447  0.499878  8.061015
105  0.497681  8.096431  0.502319  8.021664
106  0.498535  8.082658  0.494141  8.153490
107  0.502686  8.015740  0.495850  8.125944
108  0.519165  7.750101  0.503174  8.007892
109  0.496704  8.112150  0.499390  8.068885
110  0.497192  8.104280  0.504517  7.986249
111  0.502563  8.017708  0.502930  8.011827
112  0.506592  7.952801  0.494995  8.139717
113  0.494385  8.149512  0.500000  8.059048
114  0.492554  8.179046  0.500366  8.053145
115  0.492920  8.173122  0.509277  7.909515
116  0.491577  8.194787  0.499634  8.064950
117  0.496582  8.114117  0.500122  8.057080
118  0.508423  7.923287  0.494995  8.139717
119  0.494995  8.139653  0.501709  8.031502
120  0.504272  7.990141  0.505127  7.976411
121  0.505127  7.976368  0.502686  8.015762
122  0.498901  8.076756  0.496704  8.112171
123  0.496338  8.118053  0.509766  7.901645
124  0.501709  8.031459  0.497437  8.100366
125  0.506836  7.948844  0.489380  8.230224
126  0.506226  7.958682  0.499512  8.066918
127  0.504395  7.988195  0.490479  8.212516

2018-06-08 16:56:52.809131 Finish.
Total elapsed time: 30:21:50.81.
