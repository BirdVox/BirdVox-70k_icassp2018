2018-06-07 10:35:03.575881: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:03.576086: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:03.576097: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:03.576103: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-06-07 10:35:03.576108: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-06-07 10:35:01.535364 Start.
Training deep context adaptation on BirdVox-70k with PCEN input. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 52, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 52, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 26, 48)    0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 26, 48)    57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool3 (MaxPooling2D)        (None, 16, 13, 48)    0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv4 (Conv2D)              (None, 16, 13, 96)    115296      spec_pool3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_pool4 (MaxPooling2D)        (None, 8, 6, 96)      0           spec_conv4[0][0]                 
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 8)             584         bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 576, 8)        0           spec_pool4[0][0]                 
____________________________________________________________________________________________________
bg_transposed (Reshape)          (None, 1, 8)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 576, 8)        0           spec_reshape[0][0]               
                                                                   bg_transposed[0][0]              
____________________________________________________________________________________________________
multiply_transposed (Permute)    (None, 8, 576)        0           multiply[0][0]                   
____________________________________________________________________________________________________
dense_across_experts (Dense)     (None, 8, 64)         36928       multiply_transposed[0][0]        
____________________________________________________________________________________________________
dense_across_experts_transposed  (None, 64, 8)         0           dense_across_experts[0][0]       
____________________________________________________________________________________________________
mixture_of_experts (Lambda)      (None, 64)            0           dense_across_experts_transposed[0
____________________________________________________________________________________________________
dropout (Dropout)                (None, 64)            0           mixture_of_experts[0][0]         
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             72          bg_flatten[0][0]                 
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 65)            0           dropout[0][0]                    
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
edf (Dense)                      (None, 1)             66          concatenate_1[0][0]              
====================================================================================================
Total params: 240,334
Trainable params: 240,332
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.525269  7.623668  0.500000  8.058833
1    0.512817  7.823778  0.499268  8.070660
2    0.510864  7.855023  0.506714  7.950597
3    0.522095  7.678535  0.502075  8.025600
4    0.512695  7.830872  0.507202  7.942963
5    0.507446  7.926559  0.499146  8.072821
6    0.507080  7.944909  0.494995  8.139717
7    0.502197  8.023568  0.502563  8.017729
8    0.495972  8.123891  0.504150  7.992151
9    0.499512  8.066854  0.489502  8.228256
10   0.505249  7.974358  0.502075  8.025600
11   0.504272  7.990034  0.503906  7.996086
12   0.513062  7.848478  0.497192  8.104301
13   0.495605  8.129772  0.495972  8.123977
14   0.492920  8.173015  0.499878  8.061015
15   0.508545  7.921234  0.505859  7.964606
16   0.502563  8.017644  0.495728  8.127912
17   0.503540  8.001860  0.504761  7.982314
18   0.505005  7.978250  0.503418  8.003957
19   0.506592  7.952715  0.503784  7.998054
20   0.500122  8.056994  0.498413  8.084626
21   0.495972  8.123891  0.496826  8.110204
22   0.503906  7.996001  0.484009  8.316796
23   0.499634  8.064843  0.498169  8.088561
24   0.506348  7.956607  0.506470  7.954768
25   0.500244  8.055070  0.504761  7.982314
26   0.486206  8.281273  0.498535  8.082658
27   0.504639  7.984174  0.498535  8.082658
28   0.499512  8.066789  0.494019  8.155457
29   0.500854  8.045254  0.498657  8.080691
30   0.504761  7.982164  0.488647  8.242029
31   0.502563  8.017665  0.494751  8.143652
32   0.499634  8.064822  0.495728  8.127912
33   0.501221  8.039308  0.496338  8.118074
34   0.507080  7.944866  0.492310  8.183003
35   0.505981  7.962445  0.498047  8.090528
36   0.489136  8.234073  0.496704  8.112171
37   0.503662  7.999893  0.496094  8.122009
38   0.496460  8.116021  0.496338  8.118074
39   0.499756  8.062854  0.503540  8.001989
40   0.498901  8.076648  0.506714  7.950833
41   0.506958  7.946855  0.495605  8.129879
42   0.500610  8.049103  0.498047  8.090528
43   0.497559  8.098356  0.498901  8.076756
44   0.497070  8.106183  0.506470  7.954768
45   0.494629  8.145555  0.502563  8.017729
46   0.497314  8.102334  0.499268  8.070853
47   0.484253  8.312775  0.501953  8.027567
48   0.498291  8.086508  0.507446  7.939028
49   0.501343  8.037362  0.490845  8.206613
50   0.510010  7.897667  0.497437  8.100366
51   0.507324  7.940931  0.496704  8.112171
52   0.501221  8.039265  0.509277  7.909515
53   0.500488  8.051156  0.496704  8.112171
54   0.509277  7.909429  0.499512  8.066918
55   0.491089  8.202635  0.504761  7.982314
56   0.496948  8.108065  0.499634  8.064950
57   0.499634  8.064908  0.496094  8.122009
58   0.500488  8.051049  0.508179  7.927223
59   0.503296  8.005817  0.499634  8.064950
60   0.493408  8.165166  0.501465  8.035437
61   0.490601  8.210355  0.498413  8.084626
62   0.501099  8.041254  0.508545  7.921320
63   0.498413  8.084561  0.497681  8.096431
64   0.500122  8.056823  0.493042  8.171198
65   0.489990  8.220257  0.496338  8.118074
66   0.496338  8.118053  0.488770  8.240062
67   0.505249  7.974422  0.493652  8.161360
68   0.491089  8.202528  0.505859  7.964606
69   0.501343  8.037255  0.498169  8.088561
70   0.507446  7.938942  0.504517  7.986249
71   0.500488  8.051092  0.497559  8.098399
72   0.508911  7.915353  0.503052  8.009859
73   0.507202  7.942920  0.502563  8.017729
74   0.501099  8.041211  0.487915  8.253834
75   0.504028  7.993969  0.493164  8.169230
76   0.500244  8.055027  0.498169  8.088561
77   0.493286  8.167220  0.491333  8.198743
78   0.501343  8.037298  0.496460  8.116106
79   0.499023  8.074659  0.501099  8.041340
80   0.496094  8.121923  0.500000  8.059048
81   0.499634  8.064865  0.491699  8.192841
82   0.497681  8.096388  0.500122  8.057080
83   0.500000  8.059026  0.501343  8.037405
84   0.498047  8.090443  0.497192  8.104301
85   0.499268  8.070767  0.501465  8.035437
86   0.508667  7.919309  0.489136  8.234159
87   0.504517  7.986184  0.502930  8.011827
88   0.498901  8.076606  0.493652  8.161360
89   0.500854  8.045168  0.502808  8.013794
90   0.496094  8.121923  0.502197  8.023632
91   0.504761  7.982228  0.505127  7.976411
92   0.492798  8.175090  0.495972  8.123977
93   0.491577  8.194722  0.502930  8.011827
94   0.495605  8.129708  0.499512  8.066918
95   0.504028  7.994055  0.500854  8.045275
96   0.505493  7.970444  0.504639  7.984281
97   0.492554  8.178810  0.499390  8.068886
98   0.502563  8.017622  0.497192  8.104301
99   0.489746  8.224171  0.504517  7.986249
100  0.499878  8.060908  0.499512  8.066918
101  0.492676  8.177036  0.501221  8.039372
102  0.501831  8.029363  0.514771  7.820975
103  0.494507  8.147501  0.504761  7.982314
104  0.496216  8.119999  0.496704  8.112171
105  0.497803  8.094335  0.495483  8.131847
106  0.505371  7.972304  0.501831  8.029535
107  0.502563  8.017579  0.504395  7.988216
108  0.490601  8.210441  0.503174  8.007892
109  0.502808  8.013709  0.503174  8.007892
110  0.499023  8.074681  0.500000  8.059048
111  0.500488  8.051049  0.493286  8.167263
112  0.498779  8.078637  0.490112  8.218419
113  0.498413  8.084411  0.500122  8.057080
114  0.496094  8.121945  0.500000  8.059048
115  0.500366  8.053102  0.506958  7.946898
116  0.491333  8.198614  0.500610  8.049210
117  0.500366  8.052952  0.498047  8.090528
118  0.496704  8.112171  0.516113  7.799332
119  0.498535  8.082508  0.502930  8.011827
120  0.503784  7.997882  0.500977  8.043307
121  0.501953  8.027503  0.499512  8.066918
122  0.494995  8.139653  0.507202  7.942963
123  0.500610  8.049146  0.504028  7.994119
124  0.507446  7.938899  0.499146  8.072821
125  0.511841  7.868153  0.494141  8.153490
126  0.507812  7.933039  0.499878  8.061015
127  0.500854  8.045211  0.494873  8.141685

2018-06-08 17:27:31.806218 Finish.
Total elapsed time: 30:52:30.81.
