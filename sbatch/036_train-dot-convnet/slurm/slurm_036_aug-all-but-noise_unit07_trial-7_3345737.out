2017-12-10 19:13:23.796109: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 19:13:23.796306: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 19:13:23.796315: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 19:13:23.796319: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 19:13:23.796324: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-10 19:13:21.882766 Start.
Training dot-product context-aware convnet on BirdVox-70k. 
Training set: unit10, unit01, unit02.
Validation set: unit03, unit05.
Test set: unit07.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
multiply (Dot)                   (None, 1)             0           spec_dense1[0][0]                
                                                                   bg_dense1[0][0]                  
____________________________________________________________________________________________________
prelu (PReLU)                    (None, 1)             1           multiply[0][0]                   
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           prelu[0][0]                      
====================================================================================================
Total params: 681,951
Trainable params: 681,949
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.498535  8.039805  0.487183  8.214594
1   0.494995  8.085221  0.497803  8.036128
2   0.504517  7.925650  0.503784  7.934209
3   0.501709  7.964782  0.506104  7.892436
4   0.510254  7.824401  0.495361  8.060110
5   0.499390  7.994466  0.506714  7.867097
6   0.490845  8.144337  0.499512  8.094001
7   0.496216  8.138679  0.505005  7.991284
8   0.505005  7.984847  0.498047  8.089573
9   0.492188  8.177308  0.501587  8.021948
10  0.502686  7.999348  0.505615  7.947697
11  0.506348  7.931448  0.506836  7.919216
12  0.500854  8.010466  0.495239  8.095991
13  0.502563  7.975542  0.492676  8.129598
14  0.484131  8.262535  0.506470  7.903212
15  0.504761  7.927535  0.500244  7.996714
16  0.490479  8.149823  0.502563  7.954672
17  0.501831  7.964092  0.488525  8.174044
18  0.497314  8.031969  0.496826  8.037877
19  0.491577  8.119881  0.508301  7.851664
20  0.499268  7.994255  0.499634  7.987067
21  0.494263  8.071511  0.501953  7.947787
22  0.496582  8.032444  0.498535  8.000393
23  0.495483  8.048264  0.491455  8.111754
24  0.496948  8.023563  0.493042  8.085265
25  0.500610  7.964131  0.501953  7.942285
26  0.510498  7.805699  0.492920  8.085608
27  0.510986  7.797321  0.506226  7.872980
28  0.492188  8.096591  0.491821  8.102260
29  0.495850  8.037907  0.498901  7.989137
30  0.500122  7.969588  0.497681  8.008432
31  0.490723  8.128423  0.496094  8.046557

2017-12-10 23:18:23.026545 Finish.
Total elapsed time: 04:05:02.03.
