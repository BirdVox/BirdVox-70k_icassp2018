2017-12-10 15:57:17.208865: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 15:57:17.209122: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 15:57:17.209134: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-10 15:57:14.432563 Start.
Training dot-product context-aware convnet on BirdVox-70k. 
Training set: unit01, unit02, unit03.
Validation set: unit05, unit07.
Test set: unit10.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
multiply (Dot)                   (None, 1)             0           spec_dense1[0][0]                
                                                                   bg_dense1[0][0]                  
____________________________________________________________________________________________________
prelu (PReLU)                    (None, 1)             1           multiply[0][0]                   
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           prelu[0][0]                      
====================================================================================================
Total params: 681,951
Trainable params: 681,949
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.500854  8.084736  0.499756  8.098942
1   0.504639  8.017156  0.495728  8.157849
2   0.502808  8.041123  0.501709  8.056341
3   0.495361  8.156433  0.501465  8.055936
4   0.506226  7.977311  0.504150  8.008954
5   0.500732  8.062438  0.502930  8.025489
6   0.507690  7.947393  0.509644  7.914617
7   0.494629  8.155478  0.497192  8.113070
8   0.500854  8.053087  0.500244  8.062017
9   0.500122  8.063193  0.510986  7.887334
10  0.497192  8.109018  0.492798  8.179241
11  0.500610  8.052796  0.496216  8.123138
12  0.499268  8.073534  0.505493  7.972804
13  0.497925  8.094467  0.502808  8.015465
14  0.503540  8.003412  0.495239  8.136976
15  0.506592  7.953808  0.501953  8.028404
16  0.501953  8.028267  0.502075  8.026175
17  0.497437  8.100842  0.502319  8.022052
18  0.500000  8.059365  0.504639  7.984537
19  0.492432  8.181243  0.501709  8.031667
20  0.501831  8.029667  0.501953  8.027671
21  0.496704  8.112254  0.504272  7.990248
22  0.493408  8.165346  0.498413  8.084665
23  0.500366  8.035103  0.487183  8.443266
24  0.501343  8.194611  0.507080  8.074145
25  0.493164  8.283891  0.503052  8.112127
26  0.499390  8.161696  0.507324  8.025229
27  0.503662  8.077166  0.493164  8.239779
28  0.495972  8.188864  0.501099  8.100888
29  0.506714  8.005701  0.509277  7.959933
30  0.499756  8.109452  0.514526  7.867608
31  0.492065  8.226265  0.497192  8.140399

2017-12-10 20:06:10.456019 Finish.
Total elapsed time: 04:08:56.46.
