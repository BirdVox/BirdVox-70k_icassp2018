2017-12-10 19:13:25.760142: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 19:13:25.760447: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-10 19:13:25.760458: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-10 19:13:23.256956 Start.
Training dot-product context-aware convnet on BirdVox-70k. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
multiply (Dot)                   (None, 1)             0           spec_dense1[0][0]                
                                                                   bg_dense1[0][0]                  
____________________________________________________________________________________________________
prelu (PReLU)                    (None, 1)             1           multiply[0][0]                   
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           prelu[0][0]                      
====================================================================================================
Total params: 681,951
Trainable params: 681,949
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.494995  8.216585  0.510864  7.956511
1   0.497559  8.166915  0.499512  8.131445
2   0.499634  8.125711  0.497070  8.163327
3   0.503662  8.053587  0.499512  8.117051
4   0.500732  8.094141  0.497070  8.149991
5   0.494995  8.180454  0.500488  8.088986
6   0.498413  8.119691  0.502075  8.057979
7   0.501831  8.059410  0.504150  8.019582
8   0.492432  8.206199  0.509888  7.922635
9   0.506592  7.973727  0.503784  8.017013
10  0.514648  7.840105  0.496094  8.137438
11  0.495850  8.139805  0.509033  7.925806
12  0.500977  8.054317  0.505737  7.976298
13  0.500732  8.055832  0.503174  8.015404
14  0.506714  7.957405  0.497192  8.109987
15  0.502808  8.018719  0.495972  8.128188
16  0.508423  7.926895  0.508301  7.928302
17  0.501709  8.034082  0.492798  8.177282
18  0.500122  8.058877  0.504517  7.987725
19  0.504150  7.993369  0.501099  8.042323
20  0.501465  8.036237  0.499268  8.071488
21  0.504150  7.992659  0.502563  8.018125
22  0.500977  8.043619  0.494385  8.149792
23  0.496338  8.118257  0.504883  7.980483
24  0.499268  8.070956  0.505981  7.962713
25  0.501587  8.033525  0.494629  8.145659
26  0.499268  8.070881  0.504517  7.986268
27  0.494995  8.139731  0.492798  8.175142
28  0.497559  8.098405  0.502319  7.791623
29  0.503418  8.044610  0.500854  8.101618
30  0.498169  8.103057  0.503174  7.992863
31  0.504883  7.947720  0.501221  7.991636

2017-12-10 23:21:56.030919 Finish.
Total elapsed time: 04:08:33.03.
