2017-08-16 06:39:33.082956: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-16 06:39:33.083961: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-16 06:39:33.083986: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-08-16 06:39:33.083994: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-16 06:39:33.084002: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-08-16 06:39:06.199911 Start.
Training Salamon's ICASSP 2017 convnet on BirdVox-70k. 
Training set: unit02, unit03, unit05.
Validation set: unit07, unit10.
Test set: unit01.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
batch_normalization_1 (Batch (None, 128, 104, 1)       4         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 128, 104, 24)      624       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 64, 26, 24)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 64, 26, 48)        28848     
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 32, 6, 48)         0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 32, 6, 48)         57648     
_________________________________________________________________
dropout_1 (Dropout)          (None, 32, 6, 48)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 9216)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 64)                589888    
_________________________________________________________________
dropout_2 (Dropout)          (None, 64)                0         
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 65        
=================================================================
Total params: 677,077
Trainable params: 677,075
Non-trainable params: 2
_________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.500183  0.992035  0.503906  4.586994
1   0.503143  4.990057  0.503540  7.940317
2   0.497681  7.899926  0.511108  7.827992
3   0.504700  7.967884  0.493530  8.165682
4   0.503601  8.029861  0.506104  7.960892
5   0.500946  8.063370  0.505249  7.963061
6   0.495697  8.148946  0.491577  8.200944
7   0.496552  8.159594  0.507446  8.022685
8   0.500641  8.110886  0.492554  8.255188
9   0.499512  8.120559  0.502808  8.082701
10  0.499115  8.116267  0.503418  8.065910
11  0.505920  7.989190  0.508301  7.909995
12  0.497711  8.195291  0.507935  8.058895
13  0.502136  8.147906  0.493042  8.291418
14  0.499481  8.183193  0.494019  8.268578
15  0.496338  8.227089  0.497070  8.213185
16  0.496155  8.224142  0.501709  8.132730
17  0.500366  8.150385  0.496460  8.211078
18  0.499542  8.160827  0.507690  8.032417
19  0.503845  8.081641  0.497437  8.177709
20  0.495361  8.204847  0.496948  8.173015
21  0.500031  8.116374  0.509521  7.956712
22  0.502045  8.070693  0.506714  8.018398
23  0.493469  8.194048  0.504639  8.038851
24  0.503571  8.014484  0.507812  7.975807
25  0.502808  8.011378  0.502319  8.053205
26  0.497009  8.090667  0.506836  7.969809
27  0.499176  8.045222  0.506836  7.960298
28  0.500946  8.023871  0.502319  8.047145
29  0.501709  8.031333  0.492432  8.136849
30  0.499359  8.043999  0.503174  8.001865
31  0.500671  8.040685  0.497437  8.090506

2017-08-16 20:42:24.608562 Finish.
Total elapsed time: 14:03:18.61.
