2018-02-24 20:26:41.866486: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:41.866745: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:41.866756: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:17.545637 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.968872  0.124598  0.873169  0.393389
1    0.969116  0.117868  0.877441  0.364140
2    0.972778  0.102981  0.859009  0.353582
3    0.974731  0.095956  0.873901  0.471554
4    0.972046  0.111987  0.862671  0.907923
5    0.968628  0.113435  0.867554  0.382137
6    0.975342  0.094405  0.871338  0.544645
7    0.977417  0.087640  0.873657  0.462429
8    0.950684  0.179275  0.868408  0.568389
9    0.977539  0.094708  0.863770  0.436354
10   0.973022  0.135443  0.816040  1.008756
11   0.971558  0.121218  0.842651  0.538584
12   0.942505  0.195378  0.709839  0.844850
13   0.916260  0.707238  0.504761  8.024717
14   0.499023  8.112195  0.502319  8.053644
15   0.504028  8.022223  0.500732  8.071908
16   0.496582  8.136060  0.506958  7.966308
17   0.502686  8.033089  0.492188  8.200369
18   0.495483  8.145620  0.507202  7.955224
19   0.503418  8.014934  0.497681  8.106212
20   0.509277  7.918277  0.499878  8.068828
21   0.507324  7.947999  0.498291  8.092843
22   0.487183  8.271248  0.505371  7.977486
23   0.497192  8.108803  0.503662  8.004049
24   0.504395  7.991841  0.496826  8.113453
25   0.499146  8.075750  0.493286  8.169894
26   0.510254  7.896152  0.498657  8.082831
27   0.497314  8.104271  0.498901  8.078504
28   0.500488  8.052763  0.502563  8.019162
29   0.502930  8.013128  0.499390  8.070063
30   0.504639  7.985351  0.490601  8.211518
31   0.497314  8.102531  0.503418  7.919903
32   0.494141  8.095677  0.492554  8.121159
33   0.502808  7.956533  0.489502  8.167555
34   0.493164  8.108172  0.497314  8.041035
35   0.503662  7.938941  0.501465  7.973093
36   0.499268  8.007301  0.507080  7.881945
37   0.498047  8.025194  0.502441  7.954385
38   0.495483  8.064598  0.493896  8.089191
39   0.494263  8.082676  0.494507  8.078112
40   0.494995  8.069679  0.499146  8.002866
41   0.501221  7.969154  0.501465  7.964635
42   0.505127  7.905639  0.497925  8.019844
43   0.500610  7.976427  0.502563  7.944685
44   0.500488  7.977173  0.507080  7.871486
45   0.506714  7.876735  0.501831  7.953988
46   0.500854  7.968974  0.501343  7.960607
47   0.496460  8.037878  0.500488  7.973084
48   0.497681  8.017285  0.500000  7.979752
49   0.491821  8.109599  0.499146  7.992296
50   0.503052  7.929504  0.494019  8.073002
51   0.502930  7.930450  0.505493  7.889101
52   0.492432  8.096879  0.494629  8.061404
53   0.501221  7.955901  0.505493  7.887384
54   0.488281  8.161411  0.507324  7.857460
55   0.500732  7.962223  0.510498  7.806222
56   0.509033  7.829295  0.502686  7.930224
57   0.496094  8.035078  0.506226  7.873330
58   0.495728  8.040502  0.503296  7.919665
59   0.510254  7.808585  0.499023  7.987485
60   0.492920  8.084672  0.504272  7.903579
61   0.498657  7.993012  0.497803  8.006555
62   0.501953  7.940326  0.502563  7.930539
63   0.494507  8.058938  0.502441  7.932404
64   0.499878  7.973243  0.499512  7.979057
65   0.502808  7.926494  0.500732  7.959562
66   0.498901  7.988742  0.495972  8.035439
67   0.503174  7.920613  0.508423  7.836925
68   0.502563  7.930334  0.498657  7.992606
69   0.490112  8.128831  0.486572  8.185266
70   0.497925  8.004278  0.493408  8.076283
71   0.500732  7.959517  0.489746  8.134665
72   0.491211  8.111311  0.499268  7.982869
73   0.506592  7.866103  0.494019  8.066551
74   0.499390  7.980923  0.491333  8.109365
75   0.493286  8.078227  0.513062  7.762707
76   0.497437  8.150322  0.500488  8.103268
77   0.508545  7.972573  0.502441  8.070109
78   0.496582  8.163716  0.499756  8.111716
79   0.491333  8.246637  0.492920  8.220210
80   0.491455  8.242975  0.496704  8.157517
81   0.505005  8.022876  0.500366  8.096786
82   0.496826  8.152996  0.505493  8.012446
83   0.509766  7.942737  0.496338  8.158317
84   0.498657  8.120098  0.513428  7.881187
85   0.503906  8.033831  0.488892  8.275014
86   0.499756  8.099096  0.506470  7.990075
87   0.508789  7.951905  0.497925  8.126230
88   0.492310  8.215973  0.495117  8.169957
89   0.487915  8.285302  0.507202  7.973692
90   0.506836  7.978879  0.496826  8.139504
91   0.490723  8.237189  0.498047  8.118446
92   0.497192  8.131548  0.492920  8.199743
93   0.494019  8.181383  0.500000  8.084323
94   0.503052  8.034500  0.512573  7.880396
95   0.495361  8.157198  0.492310  8.205763
96   0.502686  8.037909  0.501831  8.051067
97   0.497681  8.117358  0.492432  8.201352
98   0.504028  8.013834  0.497437  8.119475
99   0.504517  8.004757  0.492554  8.196971
100  0.502930  8.029130  0.512817  7.869154
101  0.497803  8.110560  0.495605  8.145370
102  0.495239  8.150671  0.499878  8.075299
103  0.496216  8.133724  0.509521  7.918657
104  0.495850  8.138424  0.495972  8.135855
105  0.491211  8.211996  0.498779  8.089414
106  0.495239  8.145889  0.502319  8.031187
107  0.500854  8.054228  0.501587  8.041854
108  0.501221  8.047206  0.496582  8.121425
109  0.501221  8.034901  0.502930  7.976100
110  0.490723  8.170146  0.499146  8.035202
111  0.500244  8.017040  0.507690  7.897686
112  0.501953  7.988529  0.503540  7.962608
113  0.493164  8.127424  0.498779  8.037304
114  0.496460  8.073701  0.490112  8.174322
115  0.494995  8.095922  0.496216  8.075907
116  0.501465  7.991689  0.498657  8.035913
117  0.499023  8.029553  0.502197  7.978433
118  0.493896  8.110256  0.508057  7.883995
119  0.500000  8.011929  0.502563  7.970549
120  0.499146  8.024529  0.492798  8.125209
121  0.505493  7.922296  0.495483  8.081348
122  0.495728  8.076922  0.501831  7.979075
123  0.501709  7.980471  0.501709  7.979910
124  0.497070  8.053293  0.501465  7.982653
125  0.491333  8.143590  0.500488  7.997034
126  0.499268  8.015887  0.494263  8.095059
127  0.500122  8.001022  0.505737  7.910868

2018-02-25 12:06:24.433592 Finish.
Total elapsed time: 15:40:07.43.
