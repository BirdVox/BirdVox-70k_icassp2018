2018-02-24 20:27:40.854025: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:40.854301: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:40.854322: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:40.854332: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:40.854342: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:47.487314 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.979126  0.095968  0.828613  0.501368
1    0.978638  0.087039  0.849854  0.391107
2    0.896606  0.362819  0.805542  0.671153
3    0.890259  0.344835  0.785278  0.738960
4    0.658813  2.659700  0.688232  0.661959
5    0.581787  6.406849  0.500244  8.038623
6    0.496704  8.087266  0.503906  7.965582
7    0.499878  8.024250  0.510620  7.847846
8    0.505859  7.919352  0.501221  7.989175
9    0.497314  8.047864  0.489502  8.169024
10   0.497559  8.037617  0.509155  7.849929
11   0.497925  8.042836  0.501587  8.077875
12   0.503784  8.039589  0.496826  8.148566
13   0.503296  8.041585  0.491577  8.227939
14   0.511963  7.897185  0.502319  8.050578
15   0.501587  8.060622  0.503418  8.029454
16   0.504028  8.018188  0.502808  8.036519
17   0.497314  8.123897  0.491821  8.211341
18   0.505859  7.984122  0.506104  7.979287
19   0.496704  8.129999  0.506714  7.967911
20   0.491943  8.205319  0.504883  7.996125
21   0.512207  7.877500  0.491943  8.203560
22   0.488525  8.258146  0.499634  8.078609
23   0.498047  8.103729  0.500854  8.058028
24   0.509766  7.913972  0.501099  8.053248
25   0.488037  8.263373  0.499878  8.072121
26   0.496582  8.124857  0.492310  8.193334
27   0.489746  8.234274  0.496948  8.117810
28   0.500366  8.062348  0.511353  7.884898
29   0.500366  8.061611  0.500854  8.053374
30   0.497070  8.114008  0.491577  8.202187
31   0.500977  8.050333  0.494995  8.146390
32   0.504761  7.988642  0.502808  8.019780
33   0.492676  8.182753  0.492920  8.178486
34   0.499268  8.075855  0.504517  7.990934
35   0.505615  7.972923  0.496216  8.124124
36   0.500977  8.047106  0.502686  8.019281
37   0.500854  8.048530  0.499268  8.073850
38   0.492554  8.181824  0.501099  8.043861
39   0.495117  8.140053  0.497681  8.098523
40   0.504395  7.990115  0.494629  8.147331
41   0.490723  8.210123  0.503906  7.997466
42   0.488647  8.243262  0.499268  8.071947
43   0.504761  7.983284  0.498535  8.083511
44   0.508301  7.926004  0.490723  8.209233
45   0.498169  8.089129  0.502197  8.024122
46   0.498657  8.081113  0.500366  8.053506
47   0.495239  8.136090  0.502686  8.016022
48   0.499268  8.071073  0.505249  7.974627
49   0.499634  8.065105  0.491577  8.194935
50   0.502930  8.011932  0.503784  7.998141
51   0.500732  8.047314  0.505859  7.964663
52   0.504272  7.990231  0.501099  8.041377
53   0.491577  8.194838  0.510132  7.895766
54   0.504150  7.992170  0.501587  8.033484
55   0.494629  8.145631  0.490356  8.214492
56   0.501221  8.039379  0.498535  8.082663
57   0.510620  7.912410  0.506592  7.973694
58   0.519531  7.764993  0.501953  8.043274
59   0.499878  8.074584  0.494263  8.162303
60   0.496094  8.131275  0.505493  7.979547
61   0.490356  8.218945  0.496094  8.125520
62   0.500244  8.057358  0.497681  8.096194
63   0.496704  8.109706  0.497925  8.088153
64   0.491333  8.191134  0.500122  8.048877
65   0.494751  8.132363  0.499756  8.050407
66   0.509033  7.900344  0.504639  7.968226
67   0.508179  7.909632  0.503662  7.979468
68   0.503052  7.987062  0.504517  7.961567
69   0.501343  8.010068  0.502686  7.986565
70   0.497681  8.064314  0.501709  7.998060
71   0.506958  7.912409  0.494507  8.108954
72   0.503418  7.965004  0.503540  7.961189
73   0.502441  7.976912  0.502808  7.969300
74   0.492310  8.134969  0.506958  7.899764
75   0.508545  7.872869  0.492188  8.132070
76   0.502563  7.965153  0.497681  8.041518
77   0.495728  8.071248  0.499390  8.011477
78   0.490112  8.158058  0.504639  7.925167
79   0.488037  8.188591  0.499390  8.006375
80   0.500122  7.993523  0.503174  7.943708
81   0.493042  8.104121  0.512451  7.793591
82   0.504150  7.924868  0.505981  7.894630
83   0.504883  7.911141  0.502197  7.952960
84   0.500000  7.987036  0.494629  8.071719
85   0.503662  7.926804  0.491699  8.116627
86   0.496704  8.035985  0.501953  7.951461
87   0.500000  7.981800  0.501465  7.957660
88   0.497925  8.013355  0.505859  7.886132
89   0.498535  8.002217  0.511230  7.799159
90   0.498535  8.000935  0.495850  8.043150
91   0.499268  7.988109  0.500610  7.966170
92   0.496338  8.033800  0.502197  7.939924
93   0.497070  8.021243  0.498047  8.005278
94   0.512451  7.775287  0.492188  8.098005
95   0.496948  8.021817  0.502197  7.937862
96   0.499634  8.039723  0.489868  8.225839
97   0.500732  8.048806  0.500977  8.044233
98   0.504639  7.985039  0.503418  8.004577
99   0.499756  8.063497  0.504761  7.982731
100  0.494751  8.143993  0.500122  8.057353
101  0.506836  7.952822  0.492798  8.211722
102  0.494629  8.182165  0.503784  8.034561
103  0.493774  8.195867  0.493652  8.197804
104  0.497925  8.128915  0.502808  8.050190
105  0.507690  7.971468  0.497192  8.140658
106  0.495361  8.170154  0.506226  7.995026
107  0.496948  8.144542  0.499023  8.111076
108  0.509155  7.947753  0.499512  8.103170
109  0.503540  8.038222  0.501099  8.077554
110  0.498779  8.114917  0.497681  8.132603
111  0.501831  8.065684  0.500488  8.087302
112  0.497925  8.128594  0.485840  8.323353
113  0.509277  7.945556  0.499146  8.108831
114  0.506470  7.990745  0.501831  8.065477
115  0.500977  8.079211  0.494019  8.191321
116  0.504761  8.018135  0.494263  8.187298
117  0.499634  8.100678  0.504639  8.019958
118  0.503174  8.043514  0.502808  8.049359
119  0.499512  8.102420  0.503662  8.035459
120  0.491577  8.230175  0.501587  8.068763
121  0.499756  8.098198  0.506226  7.993835
122  0.501343  8.072448  0.502075  8.060549
123  0.499878  8.095865  0.503540  8.036734
124  0.509277  7.944147  0.495728  8.162426
125  0.491211  8.235099  0.498169  8.122818
126  0.501465  8.069553  0.502686  8.049730
127  0.499512  8.100728  0.508789  7.951030

2018-02-25 11:39:02.460642 Finish.
Total elapsed time: 15:12:15.46.
