2018-02-24 20:27:42.547199: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:42.547608: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:42.547635: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:42.547645: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:42.547655: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:52.367672 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit10, unit01, unit02.
Validation set: unit03, unit05.
Test set: unit07.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.933228  0.212443  0.961548  0.136635
1    0.936646  0.196320  0.971558  0.118067
2    0.942261  0.188200  0.965942  0.137343
3    0.944824  0.187357  0.980347  0.097175
4    0.950684  0.173522  0.974365  0.112204
5    0.658569  5.252814  0.491943  8.120184
6    0.499268  7.999114  0.493652  8.085264
7    0.497925  8.014919  0.506958  7.869007
8    0.495605  8.048604  0.489014  8.152475
9    0.499146  7.990020  0.500122  7.973625
10   0.505127  7.893189  0.492676  8.091111
11   0.504272  7.905773  0.508057  7.845030
12   0.493652  8.074339  0.505615  7.883324
13   0.495972  8.036826  0.499390  7.982119
14   0.507446  7.868677  0.506592  7.884963
15   0.502075  7.952038  0.489990  8.141138
16   0.501587  7.954174  0.503540  7.921337
17   0.497681  8.013594  0.506714  7.868598
18   0.497192  8.019678  0.505005  7.894505
19   0.495117  8.051668  0.495728  8.041522
20   0.494385  8.062607  0.494507  8.060372
21   0.503052  7.923919  0.501465  7.949013
22   0.506592  7.867113  0.503906  7.909779
23   0.496216  8.032262  0.502563  7.930955
24   0.509033  7.827723  0.496704  8.024197
25   0.507690  7.848982  0.497925  8.004609
26   0.507080  7.858603  0.499146  7.985053
27   0.504150  7.905227  0.488403  8.156240
28   0.506836  8.067049  0.497070  8.245516
29   0.505859  8.095317  0.497192  8.227481
30   0.501465  8.152691  0.496704  8.224037
31   0.500122  8.164515  0.494263  8.254861
32   0.487549  8.359614  0.499512  8.163556
33   0.500610  8.143050  0.505371  8.063672
34   0.485107  8.387954  0.500732  8.133885
35   0.500122  8.141725  0.498169  8.171281
36   0.492920  8.254122  0.500732  8.126483
37   0.508179  8.004862  0.498779  8.154788
38   0.499390  8.143454  0.511475  7.947184
39   0.508789  7.989037  0.502686  8.085983
40   0.492432  8.249855  0.501831  8.096947
41   0.506470  8.020787  0.500122  8.121694
42   0.500977  8.106519  0.505737  8.028366
43   0.507568  7.997431  0.495972  8.182906
44   0.509644  7.961093  0.489624  8.282300
45   0.506714  8.005365  0.499146  8.125852
46   0.493774  8.210914  0.513428  7.892610
47   0.495728  8.176366  0.508911  7.962314
48   0.495972  8.169314  0.499268  8.114613
49   0.501587  8.075654  0.491821  8.231467
50   0.494019  8.194469  0.498291  8.124011
51   0.500244  8.090952  0.503784  8.032306
52   0.499756  8.095673  0.506958  7.978020
53   0.501953  8.057155  0.491455  8.224829
54   0.499878  8.087575  0.504639  8.009351
55   0.497192  8.127931  0.492554  8.201267
56   0.497681  8.117257  0.506836  7.968331
57   0.493896  8.175595  0.504761  7.999206
58   0.508301  7.940939  0.502441  8.034195
59   0.499512  8.080305  0.500488  8.063478
60   0.488037  8.263158  0.494995  8.150027
61   0.489624  8.235696  0.510010  7.906242
62   0.491211  8.208448  0.494751  8.150622
63   0.502197  8.029912  0.496216  8.125660
64   0.496460  8.121135  0.493042  8.175664
65   0.500732  8.051214  0.500122  8.060582
66   0.500488  8.054270  0.496704  8.114879
67   0.501343  8.039780  0.508057  7.931254
68   0.503296  8.007722  0.505127  7.977962
69   0.499756  8.064325  0.492920  8.174313
70   0.502930  8.012813  0.500610  8.050047
71   0.504272  7.990898  0.509399  7.908148
72   0.491943  8.189414  0.504028  7.994542
73   0.499146  8.073175  0.488159  8.250192
74   0.500000  8.059290  0.506104  7.960868
75   0.497070  8.106431  0.492432  8.181166
76   0.494263  8.151628  0.493286  8.167346
77   0.491821  8.190939  0.508667  7.919404
78   0.505249  7.974484  0.498535  8.082689
79   0.503418  8.003980  0.492554  8.179085
80   0.493042  8.171211  0.499634  8.064960
81   0.489014  8.236133  0.492920  8.173170
82   0.510986  7.881973  0.509644  7.903614
83   0.494995  8.139719  0.502930  8.011828
84   0.505127  7.976412  0.497925  8.092496
85   0.494385  8.149555  0.502808  8.013794
86   0.494751  8.143652  0.491699  8.192841
87   0.496216  8.120042  0.488403  8.245964
88   0.511108  7.864466  0.502808  7.998996
89   0.498535  8.066034  0.496948  8.090239
90   0.504883  7.962640  0.496704  8.091908
91   0.488037  8.228955  0.506226  7.937845
92   0.497314  8.078761  0.503784  7.974452
93   0.510864  7.860407  0.506226  7.933169
94   0.496338  8.089607  0.488281  8.216836
95   0.505859  7.935379  0.504150  7.961386
96   0.496948  8.074961  0.499878  8.026991
97   0.505371  7.938145  0.500488  8.014698
98   0.506470  7.918041  0.494507  8.107439
99   0.498779  8.037999  0.506104  7.919887
100  0.497681  8.052814  0.494385  8.103986
101  0.500977  7.997518  0.498901  8.029205
102  0.503906  7.948015  0.495972  8.073095
103  0.500977  7.991892  0.500854  7.992410
104  0.496826  8.055212  0.492065  8.129679
105  0.506714  7.894733  0.495239  8.076244
106  0.504395  7.928889  0.502197  7.962518
107  0.496338  8.054562  0.504639  7.920861
108  0.498779  8.012948  0.508179  7.861781
109  0.501343  7.969494  0.498657  8.011051
110  0.500977  7.972876  0.503784  7.926933
111  0.501831  7.956951  0.502563  7.944176
112  0.505981  7.888655  0.498901  8.000521
113  0.497559  8.020994  0.494385  8.070683
114  0.503906  7.918052  0.495361  8.053470
115  0.493042  8.089710  0.503052  7.929422
116  0.502930  7.930730  0.495483  8.048830
117  0.495605  8.046338  0.503052  7.927106
118  0.490479  8.127093  0.499634  7.980699
119  0.494385  8.063998  0.494263  8.065581
120  0.498047  8.004936  0.508789  7.833382
121  0.490479  8.125037  0.505371  7.887371
122  0.492798  8.087609  0.491333  8.110765
123  0.498413  7.997723  0.495728  8.040378
124  0.494751  8.055812  0.495239  8.047901
125  0.511841  7.783124  0.496826  8.022393
126  0.506348  7.870513  0.501343  7.950223
127  0.498047  8.002702  0.496460  8.027940

2018-02-25 11:54:39.319294 Finish.
Total elapsed time: 15:27:47.32.
