2018-02-22 13:38:25.780060: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-22 13:38:25.780367: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-22 13:38:25.780380: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-22 13:37:53.172401 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit02, unit03, unit05.
Validation set: unit07, unit10.
Test set: unit01.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.972534  0.125451  0.932617  0.251715
1    0.977661  0.111331  0.955933  0.169852
2    0.975586  0.109993  0.947632  0.188064
3    0.975830  0.119248  0.904419  1.326322
4    0.513672  7.878129  0.496826  8.153856
5    0.507080  7.946525  0.495361  8.106786
6    0.483521  8.289186  0.504517  7.948940
7    0.490479  8.168348  0.504150  7.946327
8    0.501587  7.983734  0.494385  8.095296
9    0.499146  8.016552  0.502441  7.961308
10   0.490601  8.147694  0.498169  8.024766
11   0.497437  8.034426  0.508545  7.855410
12   0.487793  8.184536  0.505249  7.904614
13   0.499634  7.992684  0.507446  7.866751
14   0.497437  8.025103  0.497314  8.025879
15   0.504028  7.917807  0.497314  8.023855
16   0.495117  8.058013  0.503296  7.926796
17   0.507324  7.861845  0.513916  7.756063
18   0.504028  7.913087  0.507446  7.858019
19   0.500366  7.970386  0.508545  7.839519
20   0.508301  7.842992  0.497925  8.008013
21   0.501343  7.953176  0.505249  7.890574
22   0.506714  7.866936  0.494263  8.065168
23   0.488037  8.164183  0.510010  7.813664
24   0.508667  7.834878  0.504150  7.906700
25   0.501709  7.945462  0.501343  7.951149
26   0.498535  7.995778  0.504883  7.894456
27   0.510498  7.804827  0.497437  8.012955
28   0.481567  8.265856  0.507202  7.857091
29   0.498535  7.995189  0.490112  8.129398
30   0.505981  7.876344  0.499512  7.979428
31   0.507446  7.852881  0.492432  8.092202
32   0.502319  7.982997  0.507568  7.971977
33   0.504761  8.010087  0.502930  8.035097
34   0.509644  7.924447  0.502197  8.042446
35   0.499390  8.086204  0.496704  8.128140
36   0.497681  8.111277  0.495239  8.149576
37   0.499756  8.075853  0.499023  8.086776
38   0.499268  8.082045  0.505981  7.973062
39   0.507446  7.948750  0.505127  7.985453
40   0.492554  8.187485  0.501953  8.035377
41   0.502075  8.032849  0.498291  8.093299
42   0.498779  8.084928  0.506714  7.956551
43   0.495972  8.129247  0.500732  8.052079
44   0.507690  7.939532  0.492920  8.177219
45   0.492065  8.190642  0.496460  8.119471
46   0.500000  8.062105  0.500854  8.048037
47   0.498413  8.087121  0.498779  8.080962
48   0.501221  8.041382  0.497803  8.096255
49   0.498657  8.082287  0.513550  7.842062
50   0.496216  8.121290  0.493286  8.168356
51   0.503540  8.002948  0.508911  7.916250
52   0.499512  8.067641  0.503296  8.006546
53   0.499146  8.073355  0.509033  7.913904
54   0.496826  8.110590  0.501343  8.037729
55   0.499268  8.071126  0.492432  8.181261
56   0.508179  7.927410  0.485840  8.287435
57   0.495850  8.126070  0.489136  8.234260
58   0.494995  8.139798  0.493042  8.171262
59   0.499512  8.066969  0.491699  8.192880
60   0.501709  8.031533  0.499512  8.066942
61   0.500488  8.051196  0.500610  8.049223
62   0.495117  8.137759  0.501953  8.027575
63   0.502930  8.011832  0.502075  8.025603
64   0.507568  7.937063  0.506104  7.960640
65   0.496216  8.115031  0.496094  8.110293
66   0.503906  7.981810  0.499878  8.042811
67   0.494507  8.125882  0.502930  7.989191
68   0.503418  7.979246  0.498779  8.051105
69   0.496094  8.091980  0.503540  7.971370
70   0.498779  8.045479  0.505737  7.932791
71   0.494751  8.106266  0.503296  7.968387
72   0.505859  7.925942  0.495117  8.095638
73   0.509155  7.870348  0.491577  8.149112
74   0.502441  7.974503  0.496460  8.068470
75   0.497803  8.045737  0.500610  7.999667
76   0.500122  8.006205  0.500244  8.003029
77   0.511963  7.815035  0.496582  8.059090
78   0.500610  7.993776  0.509399  7.852579
79   0.498169  8.030597  0.487427  8.200846
80   0.513306  7.787318  0.496460  8.054936
81   0.514404  7.767964  0.496460  8.053154
82   0.482178  8.280003  0.493774  8.094290
83   0.495850  8.060408  0.501221  7.973989
84   0.502563  7.951821  0.508667  7.853761
85   0.504028  7.926983  0.505005  7.910687
86   0.498535  8.013124  0.509644  7.835326
87   0.499878  7.990327  0.509155  7.841737
88   0.502563  7.946155  0.502686  7.943537
89   0.492065  8.112190  0.499268  7.996712
90   0.497681  8.021366  0.497070  8.030452
91   0.490967  8.127125  0.495483  8.054489
92   0.514404  7.752231  0.504028  7.917036
93   0.503906  7.918388  0.501099  7.962557
94   0.493286  8.086538  0.500977  7.963369
95   0.502319  7.941423  0.505737  7.886399
96   0.500122  7.975414  0.497070  8.023569
97   0.508667  7.838224  0.508057  7.847498
98   0.494385  8.065036  0.495605  8.045163
99   0.499756  7.978616  0.497314  8.017172
100  0.508545  7.837799  0.496582  8.028197
101  0.490845  8.119377  0.501221  7.953686
102  0.505005  7.893117  0.497681  8.009655
103  0.508545  7.836255  0.499756  7.976187
104  0.501099  7.954622  0.507324  7.855223
105  0.502319  7.934890  0.499756  7.975643
106  0.501099  7.954143  0.505981  7.876214
107  0.500732  7.959827  0.497559  8.010364
108  0.501465  7.948040  0.505371  7.885722
109  0.507080  7.858444  0.493408  8.076377
110  0.503540  7.914830  0.499390  7.980978
111  0.509766  7.815547  0.506348  7.870026
112  0.504272  7.903102  0.496826  8.021807
113  0.500854  7.957581  0.503174  7.920602
114  0.499146  7.984820  0.493652  8.072393
115  0.508911  7.829130  0.483154  8.239755
116  0.504639  7.897242  0.501343  7.949786
117  0.499634  7.977445  0.496216  8.031522
118  0.501099  8.072540  0.503296  8.088480
119  0.503906  8.072339  0.501831  8.101041
120  0.505493  8.039053  0.497803  8.160525
121  0.493896  8.221683  0.498535  8.145320
122  0.495361  8.195208  0.494995  8.199946
123  0.498169  8.147803  0.505005  8.036688
124  0.496948  8.165715  0.497559  8.155074
125  0.501099  8.097270  0.500854  8.100473
126  0.505493  8.025005  0.498413  8.138424
127  0.499268  8.123969  0.503906  8.048516

2018-02-23 05:09:59.878617 Finish.
Total elapsed time: 15:32:06.88.
