2018-02-24 20:26:39.723545: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:39.723730: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:39.723742: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:17.246917 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit07, unit10, unit01.
Validation set: unit02, unit03.
Test set: unit05.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.953125  0.170720  0.953491  0.170056
1    0.948730  0.174835  0.951050  0.192045
2    0.957520  0.163499  0.954956  0.174481
3    0.951538  0.173133  0.952759  0.243160
4    0.952637  0.170297  0.957031  0.255771
5    0.911865  0.294168  0.803223  1.070895
6    0.912964  0.283580  0.961060  0.167242
7    0.940674  0.202909  0.960327  0.151551
8    0.948730  0.175899  0.943359  0.250169
9    0.932861  0.229284  0.928711  0.343258
10   0.592163  6.443489  0.500244  8.100920
11   0.508179  7.966971  0.497070  8.140837
12   0.490112  8.249062  0.509399  7.934665
13   0.495361  8.158087  0.501709  8.053165
14   0.497070  8.125757  0.505493  7.987978
15   0.503662  8.015776  0.501465  8.049586
16   0.502075  8.038366  0.502808  8.025260
17   0.500122  8.067415  0.505005  7.987646
18   0.497314  8.110670  0.503296  8.013379
19   0.498657  8.087375  0.506470  7.960723
20   0.499878  8.066333  0.509888  7.904393
21   0.511108  7.884193  0.490479  8.216212
22   0.499512  8.070186  0.502441  8.022562
23   0.498291  8.089111  0.492798  8.177326
24   0.499023  8.076704  0.494019  8.157115
25   0.503906  7.997525  0.497681  8.097667
26   0.505493  7.971575  0.499878  8.061925
27   0.506348  7.957515  0.506714  7.951494
28   0.491333  8.199306  0.507568  7.937534
29   0.503540  8.002391  0.504272  7.990520
30   0.504395  7.988500  0.505371  7.972712
31   0.498657  8.080889  0.491821  8.191038
32   0.495850  8.126082  0.497803  8.094578
33   0.501343  8.037500  0.487549  8.259816
34   0.501221  8.039438  0.501831  8.029589
35   0.503296  8.005970  0.491943  8.188943
36   0.505249  7.968830  0.505615  7.956581
37   0.497559  8.071564  0.506348  7.923112
38   0.499634  8.025555  0.500000  8.015819
39   0.502563  7.971940  0.500244  8.006152
40   0.500977  7.992122  0.497681  8.042443
41   0.493286  8.110533  0.497192  8.046372
42   0.495605  8.069969  0.499756  8.002161
43   0.493164  8.105755  0.494385  8.084849
44   0.493652  8.095203  0.503662  7.934342
45   0.500488  7.983766  0.499146  8.004035
46   0.498657  8.010775  0.501099  7.970842
47   0.503662  7.929047  0.503418  7.932042
48   0.494141  8.079122  0.504639  7.910963
49   0.496582  8.038677  0.490479  8.135278
50   0.504517  7.910833  0.507690  7.859612
51   0.498779  8.001107  0.495239  8.056993
52   0.503052  7.931939  0.495239  8.056001
53   0.512817  7.775316  0.499878  7.981168
54   0.501709  7.951578  0.507812  7.853886
55   0.503418  7.923587  0.498047  8.008866
56   0.503906  7.915130  0.502686  7.934276
57   0.495972  8.041016  0.499756  7.980399
58   0.493530  8.079379  0.504028  7.911750
59   0.493164  8.084703  0.502808  7.930717
60   0.501221  7.955785  0.491821  8.105406
61   0.506958  7.863876  0.498291  8.001836
62   0.493042  8.085318  0.494995  8.053983
63   0.498291  8.001252  0.490967  8.117834
64   0.512085  7.780987  0.499268  7.985156
65   0.487427  8.173768  0.501709  7.945918
66   0.505737  7.881551  0.496704  8.025419
67   0.502686  7.929927  0.498047  8.003749
68   0.508423  7.838211  0.496460  8.028812
69   0.505127  7.890532  0.502197  7.937135
70   0.496826  8.022669  0.500977  7.956412
71   0.500366  7.966061  0.496094  8.034096
72   0.498047  8.002889  0.500244  7.967793
73   0.493042  8.082554  0.492676  8.088337
74   0.499268  7.983200  0.494263  8.062944
75   0.493286  8.078474  0.495361  8.045354
76   0.496704  8.023916  0.505859  7.877931
77   0.501465  7.947966  0.507812  7.846748
78   0.501343  7.949873  0.505859  7.877851
79   0.500977  7.955682  0.499023  7.986808
80   0.502441  7.932308  0.507202  7.856403
81   0.501099  7.953701  0.501099  7.953696
82   0.490601  8.128996  0.501465  8.113844
83   0.500732  8.149737  0.499268  8.167090
84   0.494873  8.232651  0.501099  8.127683
85   0.491455  8.279476  0.492554  8.258427
86   0.504150  8.068680  0.494385  8.223411
87   0.505249  8.045921  0.504517  8.055433
88   0.507080  8.012000  0.494019  8.220460
89   0.496582  8.177187  0.505981  8.023757
90   0.502686  8.075027  0.498413  8.142048
91   0.496338  8.173707  0.500854  8.099120
92   0.500488  8.103276  0.499878  8.111365
93   0.506104  8.009305  0.500366  8.100059
94   0.500122  8.102303  0.497314  8.145861
95   0.494507  8.189450  0.506104  8.000865
96   0.506592  7.991358  0.504517  8.023166
97   0.501831  8.064849  0.496216  8.153753
98   0.494019  8.187606  0.498291  8.117183
99   0.482178  8.375385  0.493896  8.184994
100  0.503784  8.024168  0.499390  8.093553
101  0.501709  8.054781  0.500610  8.071114
102  0.504517  8.006839  0.506958  7.966191
103  0.500244  8.073174  0.499878  8.077865
104  0.499512  8.082625  0.499634  8.079536
105  0.509033  7.926985  0.505249  7.986953
106  0.505981  7.974193  0.501465  8.046061
107  0.506958  7.956663  0.494141  8.162421
108  0.502197  8.031799  0.505615  7.975969
109  0.494385  8.156311  0.502808  8.019903
110  0.497437  8.105889  0.499756  8.067945
111  0.494385  8.154014  0.502563  8.021707
112  0.492065  8.190489  0.493652  8.164504
113  0.500244  8.057899  0.490601  8.212996
114  0.498413  8.086777  0.500854  8.047147
115  0.507080  7.946562  0.496948  8.109642
116  0.497681  8.097644  0.497925  8.093530
117  0.495728  8.128794  0.495239  8.136525
118  0.504395  7.988843  0.510010  7.898230
119  0.498779  8.079157  0.491333  8.199098
120  0.500366  8.053436  0.494751  8.143886
121  0.498047  8.090718  0.496460  8.116256
122  0.501221  8.039491  0.491455  8.196867
123  0.502563  8.017801  0.508423  7.923342
124  0.495605  8.129920  0.506714  7.950863
125  0.509033  7.913472  0.507812  7.933141
126  0.490112  8.218430  0.503906  7.996094
127  0.491821  8.190879  0.492920  8.173169

2018-02-25 11:56:25.767648 Finish.
Total elapsed time: 15:30:08.77.
