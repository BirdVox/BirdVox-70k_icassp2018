2018-02-24 20:27:42.625255: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:42.625661: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:42.625681: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:42.625691: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:42.625701: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:50.018003 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit10, unit01, unit02.
Validation set: unit03, unit05.
Test set: unit07.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.867432  0.432854  0.857910  0.387180
1    0.896118  0.308453  0.914795  0.281500
2    0.915771  0.279217  0.943481  0.206590
3    0.923584  0.240421  0.947266  0.196435
4    0.850586  1.462180  0.491943  8.216670
5    0.496338  8.139866  0.497559  8.116093
6    0.501465  8.050431  0.502686  8.028381
7    0.500366  8.063911  0.500610  8.058302
8    0.490479  8.233581  0.504517  8.018054
9    0.509888  7.927122  0.498413  8.109007
10   0.496704  8.134514  0.502930  8.032344
11   0.510132  7.914757  0.507690  7.952706
12   0.494629  8.162022  0.497437  8.115622
13   0.500366  8.067389  0.498535  8.095936
14   0.495117  8.150162  0.496948  8.119820
15   0.505615  7.979375  0.488525  8.254109
16   0.504517  7.995704  0.497437  8.109188
17   0.500122  8.065323  0.490601  8.218231
18   0.499634  8.072120  0.511353  7.882740
19   0.497314  8.108553  0.498901  8.082535
20   0.511719  7.875540  0.501587  8.038456
21   0.497681  8.101060  0.495850  8.130228
22   0.497681  8.100399  0.497070  8.109931
23   0.505615  7.971924  0.503662  8.003135
24   0.498291  8.089461  0.506958  7.949528
25   0.500732  8.049657  0.509766  7.903850
26   0.503784  8.000070  0.496460  8.117939
27   0.500122  8.058748  0.495605  8.131388
28   0.501221  8.040738  0.492432  8.182263
29   0.505615  7.969646  0.508057  7.930177
30   0.504639  7.985164  0.501953  8.028350
31   0.494141  8.154185  0.500854  8.045887
32   0.494141  8.154029  0.506226  7.959174
33   0.502441  8.020109  0.491699  8.193197
34   0.500732  8.047553  0.507446  7.939294
35   0.496704  8.112401  0.504517  7.986444
36   0.500000  8.059215  0.506470  7.954909
37   0.502319  8.021785  0.497314  8.102435
38   0.499878  8.061100  0.510010  7.897780
39   0.494629  8.145679  0.501953  8.027616
40   0.500977  8.043348  0.502563  8.017763
41   0.495117  8.137777  0.503296  8.005947
42   0.499146  8.072839  0.501343  8.037420
43   0.496582  8.114151  0.499634  8.064960
44   0.500000  8.059056  0.505615  7.968547
45   0.496094  8.122014  0.513794  7.836719
46   0.501709  8.031505  0.504639  7.984284
47   0.495239  8.135784  0.497803  8.094465
48   0.502197  8.023633  0.509521  7.905581
49   0.498657  8.080691  0.503052  7.997533
50   0.499023  8.149726  0.494995  8.202057
51   0.503052  8.064749  0.496948  8.155413
52   0.496094  8.164760  0.502197  8.063770
53   0.498291  8.123180  0.510742  7.922046
54   0.496094  8.153306  0.489258  8.260109
55   0.504517  8.014830  0.501831  8.055651
56   0.504150  8.016741  0.497437  8.121833
57   0.508301  7.946691  0.498657  8.098465
58   0.496948  8.123721  0.495117  8.150885
59   0.505493  7.983404  0.498169  8.098063
60   0.506836  7.957745  0.503174  8.013936
61   0.507935  7.935806  0.495361  8.133975
62   0.497314  8.100520  0.496094  8.117617
63   0.497925  8.086028  0.501587  8.025203
64   0.502808  8.003271  0.504883  7.967676
65   0.501465  8.019634  0.509155  7.894461
66   0.508545  7.901613  0.497314  8.078043
67   0.493896  8.129926  0.499146  8.043611
68   0.491699  8.159709  0.500977  8.009173
69   0.503052  7.973493  0.508057  7.891098
70   0.504028  7.952766  0.507080  7.901561
71   0.503906  7.949676  0.505005  7.929688
72   0.499756  8.010984  0.501099  7.987211
73   0.504395  7.932402  0.494629  8.085853
74   0.497925  8.031187  0.505737  7.904552
75   0.495117  8.071902  0.490967  8.136152
76   0.503174  7.939759  0.497314  8.031434
77   0.495117  8.064864  0.507080  7.872596
78   0.494751  8.067740  0.500610  7.972967
79   0.498535  8.004825  0.482666  8.256644
80   0.502319  7.942279  0.498657  7.999669
81   0.500244  7.973497  0.506958  7.865638
82   0.503906  7.913578  0.505249  7.891502
83   0.497192  8.019374  0.497559  8.013006
84   0.496582  8.028131  0.500366  7.967393
85   0.490479  8.124690  0.489990  8.132168
86   0.502441  7.933419  0.495972  8.036338
87   0.496826  8.022540  0.493286  8.078820
88   0.496216  8.031993  0.492310  8.094161
89   0.504272  7.903365  0.510742  7.800152
90   0.496216  8.031687  0.494995  8.051104
91   0.501465  7.947931  0.496948  8.019910
92   0.498901  7.988756  0.505981  7.875868
93   0.508911  7.829152  0.505737  7.879742
94   0.502319  7.934228  0.503418  7.916709
95   0.502197  7.936168  0.499512  7.978980
96   0.495972  8.035415  0.497192  8.015954
97   0.498657  7.992600  0.501953  7.940055
98   0.506470  7.868050  0.491943  8.099634
99   0.502197  7.936163  0.511230  7.792152
100  0.497925  8.004276  0.494629  8.056820
101  0.494751  8.054874  0.498169  8.000384
102  0.497070  8.017899  0.500122  7.969246
103  0.501465  7.947839  0.500610  7.961462
104  0.484741  8.304556  0.498291  8.155161
105  0.500366  8.103164  0.497070  8.140551
106  0.502686  8.041124  0.489746  8.239155
107  0.490112  8.227626  0.493042  8.176100
108  0.495483  8.133775  0.494629  8.144489
109  0.497070  8.103469  0.504761  7.979051
110  0.497681  8.090582  0.497681  8.089397
111  0.494385  8.141019  0.501953  8.019526
112  0.495483  8.121964  0.493774  8.148544
113  0.508301  7.916357  0.503418  7.993608
114  0.502686  8.004714  0.505493  7.959381
115  0.499756  8.050269  0.499878  8.047736
116  0.492310  8.167794  0.491821  8.174965
117  0.505371  7.958318  0.503174  7.992701
118  0.507690  7.920031  0.498413  8.067253
119  0.496094  8.103531  0.494507  8.128116
120  0.500000  8.039812  0.489624  8.204483
121  0.498047  8.069442  0.491333  8.175700
122  0.505249  7.953056  0.494385  8.125453
123  0.498535  8.058469  0.497437  8.075152
124  0.496826  8.084039  0.501099  8.015068
125  0.499268  8.043391  0.495117  8.108674
126  0.494385  8.119456  0.499756  8.032917
127  0.499756  8.031995  0.514893  7.789741

2018-02-25 12:13:40.206072 Finish.
Total elapsed time: 15:46:50.21.
