2018-02-24 20:26:43.290291: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:43.290565: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:43.290577: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:16.680845 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.897827  0.305775  0.786255  0.785436
1    0.928101  0.233272  0.794312  0.670145
2    0.932251  0.222393  0.766846  0.755474
3    0.530029  7.565937  0.504395  8.009455
4    0.500610  8.066895  0.498657  8.095477
5    0.496460  8.128826  0.498047  8.101425
6    0.498901  8.086228  0.501587  8.041655
7    0.504028  8.001270  0.503784  8.004264
8    0.493530  8.168776  0.498779  8.083478
9    0.504150  7.996343  0.496094  8.125687
10   0.500977  8.023148  0.498291  8.030652
11   0.503296  7.946532  0.501587  7.970668
12   0.501587  7.968637  0.518188  7.702161
13   0.495605  8.060691  0.496338  8.047604
14   0.495728  8.056090  0.497192  8.031544
15   0.498047  8.016846  0.497192  8.029431
16   0.508423  7.849446  0.497559  8.021734
17   0.500732  7.970299  0.495117  8.059009
18   0.499268  7.992098  0.498901  7.997216
19   0.499634  7.984877  0.502075  7.945315
20   0.502930  7.931104  0.507080  7.864368
21   0.507202  7.861902  0.495483  8.048225
22   0.508057  7.847321  0.498169  8.004515
23   0.492310  8.097530  0.513428  7.760474
24   0.502319  7.937227  0.500488  7.966091
25   0.492188  8.098134  0.502075  7.940224
26   0.496216  8.033394  0.495850  8.039002
27   0.512329  7.776079  0.489624  8.137864
28   0.499756  7.976176  0.501343  7.950726
29   0.493042  8.082932  0.506714  7.864851
30   0.504517  7.899781  0.500854  7.958071
31   0.493042  8.082546  0.498901  7.989064
32   0.495117  8.049337  0.501587  7.946143
33   0.494995  8.051191  0.496094  8.033640
34   0.504028  7.907115  0.497559  8.010232
35   0.499146  7.984913  0.505859  7.877860
36   0.500977  7.955690  0.495117  8.049090
37   0.499756  7.975129  0.498413  7.996528
38   0.498779  7.990684  0.508789  7.831099
39   0.505127  7.889478  0.503906  7.908935
40   0.507080  7.858334  0.504639  7.897253
41   0.500244  7.967311  0.505005  7.891411
42   0.497070  8.017906  0.504639  7.897247
43   0.498779  7.990659  0.498047  8.002335
44   0.504639  7.897245  0.506958  7.860269
45   0.503052  7.922543  0.492676  8.087960
46   0.501099  7.953680  0.497803  8.006224
47   0.511108  7.867661  0.497681  8.202209
48   0.492554  8.266016  0.496704  8.187601
49   0.503784  8.068271  0.496094  8.188374
50   0.498413  8.148751  0.498047  8.152834
51   0.496948  8.169250  0.494141  8.213344
52   0.503174  8.066755  0.508301  7.983157
53   0.499146  8.129800  0.503052  8.065907
54   0.501465  8.090540  0.497681  8.150566
55   0.502563  8.070866  0.501709  8.083611
56   0.496460  8.167151  0.501587  8.083417
57   0.504639  8.033094  0.500610  8.096856
58   0.499878  8.107456  0.498413  8.129830
59   0.499512  8.110850  0.504761  8.024943
60   0.511230  7.919331  0.499268  8.110787
61   0.502197  8.062180  0.481079  8.401153
62   0.499756  8.098690  0.493408  8.199551
63   0.504761  8.015111  0.509399  7.938867
64   0.497681  8.126278  0.498535  8.111019
65   0.489136  8.261048  0.488037  8.277278
66   0.492065  8.210898  0.500244  8.077622
67   0.505981  7.983735  0.501831  8.049226
68   0.500366  8.071483  0.503906  8.013084
69   0.496338  8.133793  0.501099  8.055800
70   0.497925  8.105771  0.489990  8.232500
71   0.495239  8.146814  0.500000  8.069028
72   0.490234  8.225464  0.496338  8.126152
73   0.503174  8.015122  0.502686  8.022179
74   0.499390  8.074576  0.491821  8.195870
75   0.492920  8.177553  0.494629  8.149432
76   0.505005  7.981692  0.496948  8.111081
77   0.488403  8.248411  0.503784  8.000130
78   0.507080  7.946697  0.494385  8.151035
79   0.502930  8.013072  0.492310  8.184034
80   0.489258  8.233049  0.505615  7.969242
81   0.498291  8.087170  0.493530  8.163793
82   0.502930  8.012206  0.494873  8.141987
83   0.485474  8.293428  0.508057  7.929382
84   0.505249  7.974596  0.503296  8.006043
85   0.501953  8.027661  0.494141  8.153562
86   0.502319  8.021722  0.496948  8.108280
87   0.495483  8.131881  0.498291  8.086619
88   0.497559  8.098418  0.504883  7.980361
89   0.508179  7.927234  0.505737  7.966582
90   0.509155  7.911489  0.502075  8.025604
91   0.485718  8.289254  0.496582  8.114142
92   0.509277  7.909517  0.495728  8.127913
93   0.505859  7.964607  0.504761  7.982314
94   0.497437  8.100367  0.487915  8.253835
95   0.494141  8.153490  0.502197  8.023632
96   0.495728  8.127912  0.498535  8.082658
97   0.503540  8.001989  0.504150  7.992151
98   0.502686  8.015762  0.502686  8.015762
99   0.498169  8.088561  0.491211  8.200711
100  0.502319  8.065821  0.506348  8.055595
101  0.500122  8.145880  0.504395  8.072556
102  0.497559  8.178622  0.507935  8.010599
103  0.497314  8.177644  0.499390  8.142354
104  0.510864  7.957318  0.508423  7.994144
105  0.490967  8.270380  0.504395  8.054242
106  0.498047  8.153383  0.502197  8.085138
107  0.499023  8.133652  0.498657  8.137377
108  0.513062  7.905607  0.490601  8.261523
109  0.503052  8.060834  0.510254  7.943790
110  0.499390  8.114743  0.500366  8.096887
111  0.497192  8.145176  0.505737  8.006602
112  0.504517  8.023696  0.490845  8.239255
113  0.496460  8.147315  0.500732  8.076749
114  0.506470  7.982820  0.505615  7.993947
115  0.502319  8.043992  0.495483  8.150442
116  0.494995  8.155697  0.499146  8.086970
117  0.506226  7.971542  0.497681  8.105187
118  0.506958  7.954710  0.493652  8.164234
119  0.496582  8.114938  0.495483  8.129839
120  0.503296  8.002690  0.506592  7.947522
121  0.499634  8.055843  0.492920  8.160251
122  0.498047  8.075909  0.500610  8.032415
123  0.499146  8.053170  0.492554  8.155645
124  0.495850  8.100519  0.503418  7.977269
125  0.508545  7.892983  0.493652  8.127848
126  0.496826  8.074744  0.494385  8.111158
127  0.498047  8.050331  0.504761  7.940856

2018-02-25 11:40:03.178535 Finish.
Total elapsed time: 15:13:47.18.
