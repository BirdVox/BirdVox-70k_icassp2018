2018-02-24 20:27:53.643016: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:53.643297: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:53.643311: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:53.643318: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:53.643324: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:52.482168 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit07, unit10, unit01.
Validation set: unit02, unit03.
Test set: unit05.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.507202  7.952388  0.495850  8.134290
1    0.499146  8.080298  0.508545  7.928003
2    0.494385  8.155584  0.492432  8.186462
3    0.500488  8.056097  0.496704  8.116619
4    0.496948  8.112282  0.498901  8.080425
5    0.492188  8.188315  0.495117  8.140789
6    0.502930  8.014601  0.500000  8.061571
7    0.495361  8.136119  0.502197  8.025728
8    0.507080  7.946844  0.509521  7.907320
9    0.499268  8.118206  0.499390  8.143516
10   0.492432  8.241552  0.502563  8.067645
11   0.502563  8.060926  0.498901  8.114286
12   0.505737  7.999982  0.496094  8.151789
13   0.508667  7.946303  0.504395  8.012607
14   0.501831  8.051832  0.493896  8.177789
15   0.499023  8.093514  0.502686  8.032951
16   0.502319  8.037518  0.499390  8.083472
17   0.502808  8.027259  0.496094  8.134403
18   0.493652  8.172793  0.502808  8.024307
19   0.497192  8.113983  0.502686  8.024645
20   0.506348  7.964897  0.492676  8.184567
21   0.505127  7.983250  0.497070  8.112504
22   0.502441  8.025388  0.491455  8.201944
23   0.493042  8.175896  0.495728  8.132159
24   0.499634  8.068795  0.508667  7.922811
25   0.502686  8.018877  0.497314  8.105121
26   0.499390  8.071382  0.507324  7.943215
27   0.500977  8.045284  0.498291  8.088339
28   0.489380  8.231767  0.505249  7.975880
29   0.494629  8.146808  0.502319  8.022694
30   0.506348  8.001031  0.506348  8.068924
31   0.498901  8.166622  0.495361  8.212102
32   0.508423  7.997046  0.499023  8.145200
33   0.497192  8.172641  0.492676  8.243670
34   0.505737  8.031757  0.496460  8.179997
35   0.509521  7.968309  0.503784  8.059646
36   0.510620  7.948373  0.514282  7.888255
37   0.507690  7.993422  0.496216  8.177279
38   0.503540  8.058133  0.492188  8.240005
39   0.502441  8.073615  0.493286  8.220045
40   0.512207  7.913930  0.491455  8.247248
41   0.500488  8.100475  0.494019  8.203561
42   0.503662  8.046922  0.510254  7.939453
43   0.508179  7.971672  0.509033  7.956652
44   0.500732  8.089194  0.498047  8.131212
45   0.492554  8.218483  0.504150  8.030284
46   0.495605  8.166732  0.510010  7.933271
47   0.502930  8.046105  0.487915  8.286819
48   0.493530  8.195032  0.500000  8.089465
49   0.500977  8.072457  0.500488  8.079055
50   0.500610  8.075840  0.500000  8.084429
51   0.505737  7.990737  0.504150  8.015099
52   0.496582  8.135907  0.499390  8.089479
53   0.501099  8.060801  0.508667  7.937690
54   0.500977  8.060566  0.509277  7.925707
55   0.496094  8.137185  0.497925  8.106670
56   0.506104  7.973897  0.493774  8.171686
57   0.506958  7.958316  0.510254  7.904334
58   0.505249  7.984203  0.503174  8.016870
59   0.497803  8.102719  0.496582  8.121691
60   0.496948  8.115143  0.498047  8.096810
61   0.495117  8.143461  0.492188  8.190133
62   0.500610  8.053876  0.504150  7.996341
63   0.497681  8.100194  0.502441  8.023052
64   0.496826  8.113197  0.494141  8.156138
65   0.499390  8.071232  0.501709  8.033562
66   0.500122  8.058891  0.496582  8.115715
67   0.500977  8.044682  0.498169  8.089747
68   0.500488  8.052203  0.511841  7.869072
69   0.493774  8.160143  0.506592  7.953435
70   0.498169  8.089099  0.500000  8.059498
71   0.496094  8.122387  0.496460  8.116418
72   0.511841  7.868455  0.500610  8.049421
73   0.498291  8.086766  0.503662  8.000161
74   0.497681  8.096543  0.504150  7.992240
75   0.503662  8.000092  0.504395  7.988271
76   0.508667  7.919396  0.502197  8.023665
77   0.501343  8.037430  0.491333  8.198762
78   0.498291  8.086608  0.496948  8.108247
79   0.506348  7.956743  0.513916  7.834754
80   0.496704  8.112175  0.499146  8.072823
81   0.502563  8.017731  0.497192  8.104302
82   0.505615  7.956014  0.502686  7.963783
83   0.499634  8.012345  0.497925  8.039342
84   0.502563  7.965121  0.493286  8.112746
85   0.498169  8.034612  0.495605  8.075181
86   0.504028  7.940588  0.497314  8.047300
87   0.508545  7.867925  0.492554  8.122517
88   0.503784  7.943118  0.492920  8.115951
89   0.501953  7.971560  0.495483  8.074313
90   0.505127  7.920170  0.504517  7.929489
91   0.502197  7.966043  0.495361  8.074593
92   0.506958  7.889276  0.502930  7.953049
93   0.499512  8.007085  0.493408  8.103926
94   0.506226  7.899118  0.496216  8.058221
95   0.501099  7.979898  0.501953  7.965789
96   0.506836  7.887457  0.498413  8.021243
97   0.502197  7.960421  0.504517  7.922946
98   0.492554  8.113166  0.499512  8.001737
99   0.500000  7.993455  0.505737  7.901486
100  0.495117  8.070299  0.486328  8.209916
101  0.499878  7.993405  0.495239  8.066858
102  0.505005  7.910679  0.498901  8.007489
103  0.507690  7.866882  0.494263  8.080463
104  0.496460  8.044950  0.503906  7.925753
105  0.494629  8.073178  0.508179  7.856682
106  0.501221  7.967137  0.500488  7.978339
107  0.503418  7.931167  0.504883  7.907346
108  0.490479  8.136526  0.498413  8.009570
109  0.504028  7.919599  0.502808  7.938609
110  0.487915  8.175591  0.517334  7.706142
111  0.503174  7.931459  0.490112  8.139263
112  0.501221  7.961753  0.489136  8.154003
113  0.497070  8.027108  0.500732  7.968329
114  0.491333  8.117797  0.489868  8.140773
115  0.500977  7.963317  0.502319  7.941554
116  0.511597  7.793312  0.501953  7.946719
117  0.499390  7.987271  0.496948  8.025882
118  0.505249  7.893255  0.499878  7.978596
119  0.505737  7.884913  0.503540  7.919678
120  0.500488  7.968083  0.498047  8.006762
121  0.500610  7.965666  0.503540  7.918738
122  0.489258  8.146222  0.493408  8.079852
123  0.497559  8.013493  0.497559  8.013306
124  0.495972  8.038430  0.502441  7.935115
125  0.502197  7.938846  0.501465  7.950364
126  0.500244  7.969676  0.504395  7.903362
127  0.509766  7.817596  0.498047  8.004285

2018-02-25 12:01:56.406501 Finish.
Total elapsed time: 15:35:04.41.
