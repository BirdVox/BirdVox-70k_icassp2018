2018-02-24 20:26:34.872326: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:34.872583: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:34.872595: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:16.762985 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit01, unit02, unit03.
Validation set: unit05, unit07.
Test set: unit10.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.505859  7.977588  0.501831  8.041105
1    0.499268  8.081230  0.497803  8.103724
2    0.494019  8.163757  0.511963  7.873625
3    0.506104  7.967282  0.493530  8.169199
4    0.494995  8.144944  0.495361  8.138433
5    0.502319  8.025755  0.496094  8.125530
6    0.501099  8.036989  0.499268  8.004732
7    0.495972  8.046500  0.506226  7.877091
8    0.498657  7.996020  0.492188  8.098030
9    0.492920  8.085805  0.498657  7.993921
10   0.498169  8.001446  0.504150  7.905872
11   0.490967  8.187782  0.506592  8.009345
12   0.507080  7.978172  0.492554  8.198032
13   0.501221  8.051933  0.486328  8.287318
14   0.498657  8.086085  0.500366  8.056633
15   0.504639  7.986679  0.491943  8.190463
16   0.499023  8.075857  0.492188  8.185661
17   0.501221  8.039843  0.503540  8.002290
18   0.493286  8.167466  0.494507  8.147716
19   0.500488  8.051264  0.498779  8.078777
20   0.506714  7.950868  0.493896  8.157447
21   0.506348  7.956750  0.495850  8.125499
22   0.495361  8.224221  0.496704  8.188354
23   0.499878  8.112026  0.498047  8.119014
24   0.508179  7.940723  0.499512  8.063951
25   0.501587  8.019074  0.506470  7.930554
26   0.500488  8.017287  0.499512  8.024980
27   0.507690  7.888149  0.491211  8.144963
28   0.498779  8.019450  0.492798  8.110352
29   0.498901  8.009390  0.500977  7.972956
30   0.499268  7.997468  0.506226  7.884048
31   0.498901  7.998800  0.502686  7.936645
32   0.488892  8.155099  0.491455  8.112921
33   0.499268  7.987345  0.504761  7.898856
34   0.499512  7.981833  0.494873  8.055163
35   0.501953  7.941821  0.508789  7.832431
36   0.496704  8.024793  0.504639  7.898037
37   0.505127  7.890066  0.499634  7.977481
38   0.502441  7.932610  0.494141  8.064851
39   0.495239  8.047272  0.493286  8.078357
40   0.503418  7.916796  0.498291  7.998503
41   0.491455  8.107466  0.507202  7.856405
42   0.499756  7.975107  0.499756  7.975100
43   0.498413  7.996502  0.510986  7.796051
44   0.501343  7.952892  0.492798  8.245618
45   0.491455  8.365754  0.500244  8.206861
46   0.497925  8.231103  0.505371  8.099768
47   0.497192  8.222815  0.500244  8.165613
48   0.500854  8.149055  0.496582  8.211607
49   0.495972  8.215898  0.494629  8.232234
50   0.498047  8.172339  0.491821  8.268035
51   0.509033  7.986334  0.497803  8.163185
52   0.490723  8.273442  0.490234  8.277548
53   0.504150  8.049751  0.500977  8.097494
54   0.502319  8.072682  0.502075  8.073526
55   0.502930  8.056887  0.499268  8.113120
56   0.497681  8.136110  0.489868  8.259514
57   0.507080  7.979758  0.491455  8.229333
58   0.488159  8.280356  0.507324  7.969406
59   0.501709  8.058019  0.505493  7.995181
60   0.492676  8.200064  0.499878  8.082315
61   0.497803  8.114220  0.495605  8.148133
62   0.499268  8.087714  0.500000  8.074552
63   0.496338  8.132324  0.499512  8.079947
64   0.496338  8.129978  0.502441  8.030510
65   0.499146  8.082631  0.501465  8.044279
66   0.500610  8.057169  0.494873  8.148793
67   0.489380  8.236565  0.505493  7.976114
68   0.497559  8.103349  0.508179  7.931548
69   0.500732  8.051019  0.504639  7.987539
70   0.504761  7.985122  0.498779  8.081110
71   0.503906  7.998116  0.498169  8.090258
72   0.504639  7.985703  0.497192  8.105469
73   0.508545  7.922282  0.500854  8.046050
74   0.507935  7.931784  0.507812  7.933619
75   0.509399  7.907939  0.499023  8.075090
76   0.499146  8.073055  0.499023  8.074964
77   0.502441  8.019830  0.502197  8.023729
78   0.500122  8.057152  0.504883  7.980397
79   0.496704  8.112208  0.501953  8.027592
80   0.491699  8.192858  0.504395  7.988228
81   0.505859  7.964614  0.491333  8.198748
82   0.496948  8.108240  0.498047  8.090530
83   0.498413  8.084627  0.499878  8.061016
84   0.516235  7.797365  0.493896  8.157425
85   0.501343  8.037405  0.499512  8.066918
86   0.495239  8.135782  0.489258  8.232191
87   0.512939  7.850488  0.505493  7.970508
88   0.504761  7.982314  0.500977  8.043307
89   0.499268  8.088928  0.510498  7.995283
90   0.503906  8.065993  0.508667  7.968626
91   0.495361  8.169376  0.500366  8.080192
92   0.502686  8.036346  0.511597  7.888145
93   0.510010  7.908511  0.505371  7.977939
94   0.495361  8.133780  0.500488  8.048585
95   0.497437  8.094359  0.508301  7.918491
96   0.508423  7.914330  0.497925  8.079656
97   0.503906  7.982612  0.493408  8.148435
98   0.500000  8.042084  0.495361  8.114882
99   0.498413  8.065306  0.503174  7.988566
100  0.497803  8.073519  0.501831  8.008687
101  0.500732  8.025713  0.496460  8.093386
102  0.494751  8.120274  0.492310  8.158874
103  0.496338  8.094392  0.512207  7.841159
104  0.499878  8.037513  0.493286  8.142413
105  0.503052  7.986553  0.507446  7.916334
106  0.497803  8.069925  0.496826  8.085342
107  0.494141  8.128014  0.503296  7.981915
108  0.505859  7.940894  0.501587  8.008853
109  0.498291  8.061239  0.510132  7.872302
110  0.500854  8.020029  0.501343  8.012062
111  0.483032  8.303779  0.497681  8.070044
112  0.508423  7.898568  0.500366  8.026781
113  0.496582  8.086864  0.494385  8.121637
114  0.502441  7.992918  0.502197  7.996521
115  0.496460  8.087679  0.492798  8.145738
116  0.508545  7.894347  0.499512  8.037996
117  0.500000  8.029827  0.495361  8.103375
118  0.497070  8.075701  0.502197  7.993516
119  0.496460  8.084506  0.506714  7.920538
120  0.498047  8.058182  0.501465  8.003141
121  0.499023  8.041480  0.492432  8.145962
122  0.503296  7.972119  0.502441  7.985075
123  0.501953  7.992158  0.497925  8.055650
124  0.494019  8.117159  0.495972  8.085227
125  0.500366  8.014336  0.492798  8.134133
126  0.493530  8.121558  0.505493  7.929913
127  0.497681  8.053497  0.494751  8.099208

2018-02-25 11:38:16.486416 Finish.
Total elapsed time: 15:12:00.49.
