2018-02-24 20:27:50.652638: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:50.652899: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:50.652910: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:50.652915: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:50.652920: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:52.525073 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit10, unit01, unit02.
Validation set: unit03, unit05.
Test set: unit07.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.867554  0.364843  0.950806  0.206689
1    0.924561  0.247912  0.940430  0.199250
2    0.940186  0.189098  0.929077  0.217082
3    0.937012  0.225877  0.968628  0.136751
4    0.941162  0.188461  0.964722  0.150329
5    0.937012  0.200109  0.957886  0.178614
6    0.933472  0.205646  0.964355  0.147997
7    0.840698  2.068597  0.492188  8.214184
8    0.494507  8.172659  0.503784  8.019854
9    0.507080  7.964331  0.492432  8.198262
10   0.494995  8.155149  0.494507  8.161352
11   0.502075  8.037946  0.509155  7.922500
12   0.500488  8.061051  0.499023  8.083585
13   0.499878  8.068881  0.502563  8.024719
14   0.503540  8.008219  0.491455  8.202291
15   0.508301  7.930152  0.496094  8.126325
16   0.504272  7.993999  0.510376  7.895152
17   0.506836  7.951808  0.508057  7.931755
18   0.496582  8.116383  0.506714  7.952777
19   0.492920  8.174856  0.499634  8.066406
20   0.498535  8.083916  0.507812  7.934200
21   0.505493  7.971432  0.499634  8.065734
22   0.500244  8.055782  0.499878  8.061579
23   0.502930  8.012305  0.505615  7.968941
24   0.492310  8.183341  0.506348  7.957017
25   0.501465  8.035673  0.492432  8.181230
26   0.495605  8.130042  0.500122  8.057214
27   0.500244  8.055223  0.499756  8.063073
28   0.498901  8.076830  0.501953  8.027628
29   0.497437  8.100416  0.498047  8.090569
30   0.511719  7.870197  0.494507  8.147614
31   0.497437  8.102815  0.496704  8.183546
32   0.504761  8.041173  0.501099  8.093683
33   0.501831  8.078775  0.505615  8.015225
34   0.497681  8.141232  0.503906  8.039187
35   0.499756  8.104664  0.508423  7.963635
36   0.510010  7.936878  0.494995  8.177753
37   0.503418  8.040959  0.503296  8.041922
38   0.505249  8.009505  0.496338  8.152219
39   0.498169  8.121843  0.488403  8.278397
40   0.499756  8.094609  0.505859  7.995436
41   0.495972  8.154048  0.493286  8.196582
42   0.503662  8.028623  0.500732  8.075133
43   0.496094  8.149218  0.500122  8.083613
44   0.481812  8.378097  0.507202  7.968206
45   0.502686  8.040389  0.500488  8.075194
46   0.511230  7.901463  0.494751  8.166499
47   0.500366  8.075433  0.500977  8.065040
48   0.499512  8.088115  0.498901  8.097421
49   0.499878  8.081168  0.501221  8.059015
50   0.500122  8.076229  0.506470  7.973426
51   0.500000  8.077227  0.488281  8.265635
52   0.492432  8.198273  0.509155  7.928254
53   0.505005  7.994695  0.486694  8.289369
54   0.503906  8.011496  0.500610  8.064168
55   0.495483  8.146359  0.488770  8.254125
56   0.508179  7.940842  0.500488  8.064350
57   0.499512  8.079646  0.502197  8.035913
58   0.498901  8.088594  0.504395  7.999608
59   0.492920  8.184114  0.497681  8.106934
60   0.501221  8.049434  0.494141  8.163109
61   0.498535  8.091840  0.503906  8.004830
62   0.499023  8.083100  0.502441  8.027578
63   0.500610  8.056670  0.500488  8.058217
64   0.507324  7.947627  0.499023  8.081013
65   0.501465  8.041271  0.494995  8.145162
66   0.502686  8.020835  0.508911  7.920124
67   0.497559  8.102757  0.507080  7.948947
68   0.493530  8.167022  0.509277  7.912895
69   0.500488  8.054265  0.496948  8.111039
70   0.492310  8.185543  0.504639  7.986567
71   0.509033  7.915505  0.497803  8.096296
72   0.506592  7.954433  0.503662  8.001464
73   0.504028  7.995391  0.489868  8.223465
74   0.493042  8.172169  0.491333  8.199582
75   0.496826  8.110929  0.496582  8.114757
76   0.503662  8.000550  0.503906  7.996531
77   0.492554  8.179443  0.504028  7.994430
78   0.489258  8.232450  0.495361  8.134026
79   0.503540  8.002163  0.493164  8.169369
80   0.501953  8.027680  0.496948  8.108325
81   0.493896  8.157495  0.493896  8.157479
82   0.505737  7.966616  0.496704  8.112204
83   0.499512  8.066943  0.504761  7.982332
84   0.500366  8.053159  0.497681  8.096441
85   0.498413  8.084633  0.503296  8.005929
86   0.505981  7.962642  0.496948  8.108239
87   0.503662  8.000023  0.503662  8.000023
88   0.503296  8.005925  0.502075  8.025600
89   0.496216  8.120042  0.504883  7.980346
90   0.503662  8.000022  0.498047  8.090528
91   0.496460  8.121741  0.492920  8.207580
92   0.498413  8.117902  0.510864  7.916088
93   0.495850  8.157054  0.502808  8.043881
94   0.505859  7.924747  0.504272  7.969941
95   0.496216  8.095467  0.505127  7.951553
96   0.502319  7.995201  0.502563  7.990292
97   0.501831  8.001056  0.499512  8.037133
98   0.501221  8.009027  0.507935  7.901138
99   0.504639  7.952858  0.499878  8.027936
100  0.500000  8.025198  0.499390  8.034140
101  0.508789  7.883526  0.498291  8.050128
102  0.490601  8.171989  0.504639  7.947446
103  0.505005  7.940881  0.499512  8.027726
104  0.509033  7.875213  0.494507  8.106077
105  0.502808  7.973029  0.494995  8.096860
106  0.501709  7.989110  0.504028  7.951412
107  0.500366  8.009075  0.507324  7.897419
108  0.505249  7.929774  0.496338  8.071102
109  0.493774  8.111231  0.488037  8.201951
110  0.498413  8.035786  0.490967  8.153741
111  0.493896  8.106280  0.508179  7.877825
112  0.506104  7.910148  0.496582  8.061177
113  0.502197  7.970895  0.505249  7.921476
114  0.503784  7.944070  0.500366  7.997798
115  0.508911  7.860820  0.493530  8.105273
116  0.484863  8.242705  0.489502  8.168012
117  0.497925  8.033007  0.500000  7.999199
118  0.501709  7.971247  0.497437  8.038655
119  0.503296  7.944557  0.507690  7.873815
120  0.494507  8.083331  0.513672  7.777136
121  0.493164  8.103442  0.494263  8.085293
122  0.498291  8.020459  0.488525  8.175537
123  0.497070  8.038721  0.501587  7.966131
124  0.488770  8.169905  0.512451  7.791801
125  0.500000  7.989759  0.496460  8.045656
126  0.497803  8.023727  0.505371  7.902549
127  0.505249  7.903992  0.508911  7.845108

2018-02-25 11:50:03.089017 Finish.
Total elapsed time: 15:23:11.09.
