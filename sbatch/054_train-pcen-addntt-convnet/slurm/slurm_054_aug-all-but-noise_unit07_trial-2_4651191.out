2018-02-24 20:26:32.333243: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:32.333552: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:32.333565: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:16.680780 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit10, unit01, unit02.
Validation set: unit03, unit05.
Test set: unit07.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.755859  4.029424  0.833008  2.799181
1    0.765381  3.900521  0.838745  2.702805
2    0.766113  3.873777  0.898071  1.781308
3    0.782349  3.620277  0.823975  2.980322
4    0.808594  3.221639  0.818237  3.068089
5    0.803345  3.291968  0.844849  2.650169
6    0.800415  3.347610  0.844482  2.637438
7    0.783936  3.598276  0.513794  7.924085
8    0.506470  8.057598  0.498535  8.169749
9    0.528442  7.697933  0.556396  7.253220
10   0.540527  7.500473  0.565308  7.102957
11   0.708496  4.873878  0.770752  3.903315
12   0.757080  4.121475  0.792480  3.534225
13   0.797119  3.461729  0.837158  2.804161
14   0.811035  3.224114  0.836548  2.807673
15   0.817261  3.122092  0.832153  2.868390
16   0.689819  5.164452  0.718872  4.714459
17   0.668091  5.522976  0.499878  8.241897
18   0.507202  8.114302  0.504883  8.143276
19   0.509888  8.057042  0.512207  8.014259
20   0.510010  8.042843  0.504150  8.131191
21   0.505737  8.101297  0.500000  8.187923
22   0.500977  8.167427  0.504761  8.102511
23   0.514893  7.933617  0.505249  8.086113
24   0.498413  8.190379  0.505127  8.077607
25   0.504028  8.092138  0.512329  7.953435
26   0.505005  8.068353  0.496094  8.206834
27   0.509155  7.992763  0.500244  8.130711
28   0.503540  8.071767  0.505127  8.040901
29   0.495605  8.193836  0.504272  8.051956
30   0.502563  8.077277  0.504272  8.044979
31   0.511841  7.922305  0.502075  8.071625
32   0.501221  8.084840  0.507935  7.968986
33   0.508545  7.961512  0.499268  8.103518
34   0.509521  7.936570  0.500122  8.085141
35   0.496582  8.138353  0.507446  7.960647
36   0.500122  8.075951  0.510498  7.908260
37   0.503784  8.010273  0.498169  8.095553
38   0.499512  8.074143  0.511597  7.877239
39   0.506226  7.963383  0.498657  8.081603
40   0.501221  8.037867  0.506958  7.943959
41   0.499512  8.057957  0.492554  8.164953
42   0.496338  8.105274  0.504395  7.974709
43   0.501953  8.010962  0.508057  7.909465
44   0.500732  8.021503  0.506714  7.928080
45   0.504028  7.968160  0.505981  7.930135
46   0.505737  7.929422  0.505737  7.929719
47   0.501465  7.999784  0.508789  7.875715
48   0.500000  8.018880  0.504639  7.938147
49   0.510132  7.851337  0.506104  7.915529
50   0.504272  7.940891  0.508667  7.860418
51   0.501709  7.979536  0.516113  7.745458
52   0.495117  8.152032  0.503418  8.032014
53   0.497681  8.117072  0.501221  8.056251
54   0.497803  8.108113  0.497070  8.117504
55   0.500000  8.068927  0.502075  8.034068
56   0.499634  8.071363  0.496948  8.112583
57   0.501831  8.033219  0.495117  8.138743
58   0.496948  8.108090  0.497559  8.096901
59   0.506714  7.949523  0.505249  7.971455
60   0.506470  7.950607  0.495117  8.130205
61   0.497559  8.089926  0.497559  8.088567
62   0.504517  7.976312  0.500244  8.043095
63   0.503662  7.987304  0.499512  8.052168
64   0.497314  8.085922  0.501099  8.024316
65   0.505859  7.947167  0.488525  8.222256
66   0.507690  7.915389  0.494141  8.130364
67   0.493408  8.140759  0.495117  8.112265
68   0.500732  8.018068  0.503174  7.980314
69   0.494385  8.118549  0.500854  8.016007
70   0.492188  8.153089  0.500488  8.018161
71   0.494263  8.116786  0.495239  8.095702
72   0.503906  7.959967  0.499634  8.025501
73   0.487793  8.212509  0.498901  8.034087
74   0.508789  7.879005  0.506348  7.916833
75   0.506348  7.913917  0.504517  7.943697
76   0.498413  8.037562  0.499268  8.023339
77   0.503784  7.944174  0.506836  7.902131
78   0.503906  7.949256  0.496216  8.068757
79   0.502441  7.970058  0.499268  8.019518
80   0.506226  7.906279  0.507202  7.889379
81   0.494263  8.096097  0.507202  7.888733
82   0.499268  8.012997  0.504761  7.925685
83   0.493408  8.104021  0.498535  8.022967
84   0.502075  7.964736  0.496704  8.048954
85   0.496338  8.053577  0.505981  7.900679
86   0.498047  8.026337  0.499390  8.004103
87   0.495728  8.061694  0.499146  8.005339
88   0.504395  7.916979  0.500122  7.990217
89   0.512085  7.796856  0.500122  7.988084
90   0.500732  7.974933  0.503296  7.931029
91   0.501343  7.965155  0.498657  8.007407
92   0.499756  7.989679  0.505737  7.891873
93   0.493042  8.093838  0.490601  8.134141
94   0.505127  7.899636  0.502930  7.935473
95   0.498169  8.010606  0.496582  8.033542
96   0.500854  7.963662  0.503418  7.924758
97   0.493408  8.084580  0.499268  7.985770
98   0.499634  7.987834  0.496826  8.035026
99   0.504395  7.913776  0.446655  8.856573
100  0.488770  8.176126  0.474243  8.404723
101  0.486206  8.210114  0.478882  8.325822
102  0.494995  8.067319  0.481201  8.286960
103  0.492065  8.110425  0.497925  8.012717
104  0.485962  8.293669  0.489014  8.250275
105  0.492065  8.215386  0.498047  8.122638
106  0.494019  8.179282  0.493774  8.176495
107  0.497437  8.112996  0.496094  8.129673
108  0.498169  8.090961  0.497803  8.094404
109  0.504883  7.976327  0.504761  7.976498
110  0.492065  8.175725  0.501099  8.028629
111  0.500122  8.041321  0.500244  8.036559
112  0.501709  8.005822  0.509766  7.879602
113  0.496826  8.081618  0.510376  7.864925
114  0.504272  7.958118  0.496460  8.080515
115  0.501221  8.002448  0.500732  8.009885
116  0.501221  7.997600  0.498657  8.038933
117  0.497925  8.048750  0.498169  8.042901
118  0.498169  8.041097  0.500122  8.008175
119  0.503418  7.951722  0.507568  7.886160
120  0.484253  8.256222  0.493896  8.100935
121  0.499634  8.004090  0.487183  8.204788
122  0.494995  8.076969  0.503174  7.947154
123  0.504761  7.914946  0.497559  8.034090
124  0.504517  7.921767  0.502563  7.951553
125  0.500488  7.983497  0.496216  8.050462
126  0.489990  8.148631  0.509521  7.836194
127  0.504028  7.919787  0.498047  8.018117

2018-02-25 11:39:41.663152 Finish.
Total elapsed time: 15:13:25.66.
