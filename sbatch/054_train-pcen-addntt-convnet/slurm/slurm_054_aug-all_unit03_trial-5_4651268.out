2018-02-24 20:27:44.989639: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:44.989977: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:44.989998: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:44.990008: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:44.990017: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:51.519855 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.966919  0.138245  0.883179  0.332365
1    0.970703  0.124991  0.875854  0.403666
2    0.855225  1.830850  0.661255  5.514395
3    0.527222  7.680854  0.504150  8.056324
4    0.497559  8.157307  0.508179  7.980127
5    0.564209  7.121392  0.508545  8.072469
6    0.498657  8.224611  0.508179  8.065635
7    0.502686  8.150355  0.498047  8.221697
8    0.499878  8.189332  0.498169  8.214188
9    0.499268  8.194080  0.505371  8.093385
10   0.503174  8.126655  0.497192  8.220964
11   0.494873  8.256359  0.506836  8.061575
12   0.499023  8.185607  0.506836  8.057804
13   0.504639  8.091391  0.502441  8.124977
14   0.505859  8.068090  0.510254  7.995456
15   0.500488  8.151078  0.490723  8.306686
16   0.505737  8.062897  0.510498  7.984364
17   0.493164  8.261960  0.502930  8.102742
18   0.503662  8.089121  0.509277  7.996775
19   0.523315  7.784560  0.501099  8.140474
20   0.524658  7.755691  0.513062  7.938424
21   0.515259  7.882042  0.493164  8.217513
22   0.502319  8.065664  0.493164  8.206204
23   0.496216  8.154571  0.501099  8.128618
24   0.495850  8.212019  0.493896  8.235997
25   0.498169  8.162237  0.501709  8.100491
26   0.495972  8.187180  0.506104  8.021008
27   0.507935  7.987465  0.500244  8.105785
28   0.498535  8.128948  0.495972  8.165770
29   0.500000  8.097657  0.497437  8.134655
30   0.499268  8.101720  0.497070  8.133019
31   0.502563  8.041825  0.500610  8.069352
32   0.503662  8.017191  0.510010  7.912494
33   0.499023  8.084240  0.503296  8.012733
34   0.496460  8.118417  0.502197  8.023663
35   0.509155  7.909547  0.501343  8.030919
36   0.489990  8.208829  0.497925  8.079272
37   0.494873  8.124965  0.497314  8.083101
38   0.505981  7.942093  0.509033  7.890626
39   0.503174  7.981332  0.514160  7.803500
40   0.499390  8.036406  0.492920  8.137002
41   0.505249  7.938014  0.494629  8.104919
42   0.500366  8.011161  0.498535  8.038090
43   0.500610  8.002858  0.499390  8.020201
44   0.501953  7.977327  0.494995  8.086280
45   0.501099  7.987109  0.495117  8.080634
46   0.502930  7.954357  0.502319  7.962391
47   0.501221  7.978311  0.494995  8.075999
48   0.509033  7.850732  0.488892  8.170401
49   0.506592  7.886874  0.502319  7.953673
50   0.501953  7.958284  0.502686  7.945409
51   0.501953  7.955970  0.505615  7.896499
52   0.496216  8.045338  0.497925  8.017110
53   0.490601  8.132966  0.502075  7.949150
54   0.498657  8.002827  0.505493  7.893056
55   0.515625  7.730807  0.497803  8.014236
56   0.498413  8.003867  0.499878  7.979897
57   0.504517  7.905386  0.498535  8.000206
58   0.505371  7.890738  0.496704  8.028443
59   0.492310  8.098084  0.502441  7.936156
60   0.493408  8.079808  0.499146  7.987999
61   0.496826  8.024671  0.498047  8.004920
62   0.508057  7.845084  0.500854  7.959659
63   0.502686  7.930253  0.496460  8.029300
64   0.508545  7.836458  0.501343  7.951109
65   0.509888  7.814735  0.500488  7.964445
66   0.498413  7.997408  0.500732  7.960318
67   0.502441  7.932975  0.498169  8.000996
68   0.505493  7.884152  0.503296  7.919108
69   0.491943  8.100031  0.502441  7.932609
70   0.498169  8.000673  0.497559  8.010358
71   0.504272  7.903285  0.503540  7.914927
72   0.496948  8.019988  0.494751  8.054992
73   0.495239  8.047186  0.498657  7.992677
74   0.510620  7.801945  0.495850  8.037409
75   0.493896  8.068536  0.501831  7.942031
76   0.508423  7.836936  0.502930  7.924504
77   0.503418  8.006342  0.495483  8.146479
78   0.513550  7.854489  0.501099  8.054443
79   0.499146  8.085277  0.496704  8.124005
80   0.488281  8.259194  0.501221  8.050078
81   0.502075  8.035787  0.512939  7.860169
82   0.501221  8.048577  0.495850  8.134682
83   0.495239  8.144079  0.487061  8.275472
84   0.496826  8.117661  0.504639  7.991338
85   0.490967  8.211325  0.496582  8.120448
86   0.515503  7.815131  0.502441  8.025316
87   0.498901  8.082053  0.501221  8.044356
88   0.506226  7.963392  0.493286  8.171663
89   0.498413  8.088757  0.497803  8.098332
90   0.493530  8.166951  0.514404  7.830262
91   0.492554  8.182229  0.508911  7.918361
92   0.493652  8.164103  0.497681  8.098979
93   0.506836  7.951232  0.504272  7.992374
94   0.497314  8.104362  0.498169  8.090432
95   0.496094  8.123736  0.503052  8.011447
96   0.505371  7.973936  0.499146  8.074157
97   0.499756  8.064207  0.497437  8.101483
98   0.506348  7.957755  0.505005  7.979304
99   0.490723  8.209422  0.503174  8.008652
100  0.498047  8.091217  0.485474  8.293805
101  0.493042  8.171756  0.497192  8.104801
102  0.494263  8.151971  0.504517  7.986649
103  0.500610  8.049567  0.504761  7.982630
104  0.494751  8.143933  0.505493  7.970756
105  0.502930  8.012046  0.493652  8.161552
106  0.501465  8.035605  0.498291  8.086739
107  0.498657  8.066413  0.492920  8.137696
108  0.503784  7.958504  0.501831  7.985624
109  0.500610  8.002663  0.499878  8.012265
110  0.503906  7.946450  0.500854  7.993651
111  0.496216  8.066390  0.495605  8.074983
112  0.500000  8.003930  0.492065  8.129477
113  0.499146  8.015750  0.492676  8.118068
114  0.494263  8.092013  0.504272  7.931697
115  0.498779  8.018586  0.491821  8.128842
116  0.498413  8.023122  0.489868  8.158728
117  0.505615  7.907093  0.503540  7.939595
118  0.506714  7.888442  0.503784  7.934599
119  0.499146  8.008023  0.496094  8.056152
120  0.490967  8.137383  0.498657  8.014278
121  0.491089  8.134451  0.495239  8.067801
122  0.505127  7.909700  0.501221  7.971509
123  0.504639  7.916566  0.492065  8.116562
124  0.502319  7.952651  0.502930  7.942484
125  0.505615  7.899244  0.500366  7.982502
126  0.507202  7.873109  0.494873  8.069253
127  0.498047  8.018255  0.510742  7.815464

2018-02-25 11:30:32.663158 Finish.
Total elapsed time: 15:03:41.66.
