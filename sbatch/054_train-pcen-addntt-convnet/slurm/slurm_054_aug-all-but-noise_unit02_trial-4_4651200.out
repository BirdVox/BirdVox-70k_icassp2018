2018-02-24 20:26:40.384691: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:40.384905: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:40.384917: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:16.795095 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.973389  0.101776  0.803223  0.866739
1    0.977051  0.096812  0.834839  0.573424
2    0.979004  0.088351  0.750488  0.785780
3    0.975098  0.100205  0.792725  0.471560
4    0.976074  0.094815  0.768066  0.682371
5    0.980347  0.084021  0.789185  1.009292
6    0.979370  0.089070  0.804321  0.587185
7    0.794922  2.571503  0.601196  0.726364
8    0.716309  0.606533  0.602417  0.926303
9    0.707642  1.407713  0.503784  7.941563
10   0.506470  7.894439  0.509033  7.850517
11   0.505981  7.896952  0.509644  7.836547
12   0.496094  8.050864  0.504883  7.909155
13   0.498047  8.016756  0.505005  7.904522
14   0.506714  7.876127  0.501343  7.960661
15   0.501343  7.959692  0.499146  7.993798
16   0.500854  7.965733  0.495850  8.044741
17   0.500488  7.970097  0.501343  7.955145
18   0.497925  8.086736  0.506226  7.989692
19   0.494385  8.179180  0.500610  8.077538
20   0.500488  8.078350  0.498047  8.116595
21   0.501343  8.062485  0.493652  8.185496
22   0.501587  8.056762  0.497925  8.114982
23   0.494629  8.167386  0.494629  8.166699
24   0.501953  8.048033  0.504028  8.013998
25   0.505981  7.981993  0.494263  8.170375
26   0.504883  7.998749  0.497314  8.120305
27   0.492676  8.194680  0.497437  8.117569
28   0.507324  7.957853  0.497803  8.110986
29   0.501343  8.053616  0.505493  7.986415
30   0.494507  8.163206  0.508301  7.940589
31   0.498413  8.099688  0.505371  7.987267
32   0.503418  8.018482  0.496094  8.136270
33   0.497925  8.106495  0.506958  7.960633
34   0.503296  8.019397  0.496216  8.133248
35   0.503662  8.012962  0.505737  7.979244
36   0.504150  8.004550  0.502563  8.029853
37   0.499878  8.072861  0.503662  8.011586
38   0.500854  8.056555  0.502930  8.022819
39   0.504395  7.998919  0.508057  7.939599
40   0.501343  8.047518  0.504639  7.994096
41   0.499146  8.082336  0.498779  8.087937
42   0.500000  8.067960  0.506958  7.955506
43   0.494263  8.159827  0.499878  8.069016
44   0.499634  8.072649  0.498779  8.086118
45   0.490723  8.215677  0.502197  8.030429
46   0.498535  8.089161  0.498291  8.092802
47   0.497314  8.108255  0.505127  7.982045
48   0.499634  8.070306  0.498413  8.089704
49   0.501709  8.036313  0.501953  8.032112
50   0.497925  8.096786  0.495605  8.133916
51   0.490356  8.218279  0.497925  8.096052
52   0.504517  7.989579  0.498291  8.089701
53   0.507446  7.941925  0.495117  8.140440
54   0.500122  8.059578  0.503784  8.000363
55   0.506714  7.952966  0.494019  8.157419
56   0.495605  8.131682  0.496460  8.117755
57   0.485718  8.290758  0.502930  8.013198
58   0.500854  8.046521  0.493408  8.166421
59   0.503540  8.003007  0.491699  8.193754
60   0.504150  7.992972  0.505859  7.965337
61   0.500732  8.047894  0.497925  8.093072
62   0.502930  8.012336  0.490601  8.210995
63   0.501831  8.029926  0.495850  8.126284
64   0.500000  8.059343  0.494751  8.143905
65   0.497437  8.100583  0.494629  8.145804
66   0.497437  8.100522  0.499268  8.070984
67   0.502563  8.017839  0.508301  7.925345
68   0.497070  8.087378  0.495483  8.098333
69   0.505249  7.941743  0.500854  8.010868
70   0.492676  8.140278  0.499023  8.038070
71   0.496948  8.070098  0.502686  7.977541
72   0.518921  7.717578  0.502808  7.973295
73   0.508179  7.886457  0.505859  7.922189
74   0.496704  8.066864  0.498047  8.044142
75   0.500732  7.999981  0.509644  7.856537
76   0.495972  8.073095  0.496826  8.058041
77   0.511963  7.815275  0.492432  8.125177
78   0.496094  8.065315  0.498535  8.024895
79   0.505249  7.916367  0.497559  8.037466
80   0.501587  7.971758  0.503418  7.941075
81   0.495605  8.064164  0.493164  8.101626
82   0.497192  8.035992  0.503418  7.935337
83   0.500610  7.978750  0.509521  7.835354
84   0.505493  7.898314  0.491089  8.126713
85   0.497192  8.028248  0.504761  7.906458
86   0.502075  7.948223  0.500122  7.978344
87   0.491577  8.113643  0.500732  7.966793
88   0.502441  7.938744  0.504639  7.902945
89   0.492432  8.096873  0.498169  8.004760
90   0.497559  8.013926  0.486938  8.182704
91   0.500610  7.964286  0.495605  8.043649
92   0.496338  8.031614  0.496826  8.023498
93   0.506104  7.875319  0.497925  8.005456
94   0.502808  7.927408  0.490234  8.127670
95   0.516602  7.707168  0.502686  7.928890
96   0.494995  8.051392  0.505737  7.880046
97   0.511719  7.784619  0.498413  7.996683
98   0.501831  7.942149  0.498169  8.000494
99   0.499756  7.975168  0.506470  7.868110
100  0.492798  8.086057  0.506958  7.860297
101  0.495972  8.035436  0.498901  7.988723
102  0.502319  7.934228  0.510376  7.805782
103  0.500977  7.955629  0.492676  8.087961
104  0.502686  7.928380  0.506958  7.860266
105  0.502441  7.932271  0.503662  7.912810
106  0.501831  7.942001  0.497314  8.014006
107  0.505615  7.881672  0.504395  7.901133
108  0.494141  8.064605  0.492798  8.086012
109  0.502319  7.934217  0.499390  7.980923
110  0.495850  8.037359  0.498657  7.992599
111  0.507080  7.858319  0.503540  7.914756
112  0.498657  7.992599  0.498779  7.990653
113  0.503540  7.914756  0.498169  8.000384
114  0.511597  7.786314  0.504639  7.897241
115  0.494751  8.054874  0.503540  7.914756
116  0.515503  7.724039  0.489868  8.132718
117  0.506226  7.871942  0.503052  7.922540
118  0.504395  7.901133  0.505493  7.883618
119  0.506958  7.860265  0.495239  8.047090
120  0.514893  7.733769  0.503174  7.920594
121  0.494141  8.064605  0.501343  7.949785
122  0.493408  8.076281  0.509644  7.817451
123  0.500977  7.955624  0.499634  7.977031
124  0.502441  7.932271  0.493774  8.070443
125  0.486816  8.181370  0.498779  7.990653
126  0.502319  7.934217  0.500610  7.961462
127  0.500854  7.957570  0.505859  7.877780

2018-02-25 11:54:03.992086 Finish.
Total elapsed time: 15:27:47.99.
