2018-02-24 20:27:48.481648: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:48.481963: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:48.481983: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:48.481991: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:48.482001: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:50.092560 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit01, unit02, unit03.
Validation set: unit05, unit07.
Test set: unit10.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.496338  8.032863  0.507690  7.850960
1    0.496704  8.026340  0.506836  7.955727
2    0.505859  7.976014  0.504395  7.991632
3    0.506226  7.960613  0.492798  8.176160
4    0.499512  8.067592  0.502075  8.026027
5    0.499146  8.072722  0.493530  8.158349
6    0.498291  8.063871  0.504395  7.954216
7    0.493042  8.128167  0.505737  7.919858
8    0.504883  7.929063  0.502441  7.964013
9    0.504395  7.929642  0.507690  7.874108
10   0.496948  8.042826  0.491577  8.126074
11   0.502563  7.948860  0.500244  7.983882
12   0.508179  7.855674  0.505371  7.898811
13   0.495728  8.051127  0.497559  8.020586
14   0.487915  8.173146  0.501953  7.948230
15   0.498901  7.995915  0.499146  7.991112
16   0.497437  8.017576  0.508667  7.837804
17   0.498535  7.998708  0.502075  7.941694
18   0.498169  8.003483  0.499878  7.975789
19   0.500122  7.971523  0.499634  7.978965
20   0.492798  8.087663  0.504150  7.906418
21   0.498291  7.999620  0.499146  7.985806
22   0.495361  8.045980  0.501709  7.944642
23   0.503052  7.923123  0.494385  8.061193
24   0.503906  7.909317  0.502563  7.930652
25   0.498169  8.000653  0.497437  8.012279
26   0.512085  7.778707  0.498291  7.998580
27   0.487183  8.175647  0.506348  7.870086
28   0.493286  8.078299  0.510864  7.798046
29   0.494263  8.062702  0.498901  7.988740
30   0.505859  7.877805  0.499878  7.973158
31   0.513794  7.751298  0.501831  7.942011
32   0.494385  8.264052  0.498047  8.202463
33   0.495605  8.229534  0.506836  8.038064
34   0.497803  8.175626  0.498779  8.152569
35   0.496582  8.181892  0.491699  8.254906
36   0.505127  8.033557  0.500610  8.101699
37   0.497681  8.144805  0.496826  8.154651
38   0.504028  8.035053  0.502319  8.059230
39   0.498413  8.119159  0.503784  8.029674
40   0.506226  7.987694  0.501465  8.061900
41   0.509644  7.927791  0.496582  8.136124
42   0.496094  8.142015  0.490479  8.230622
43   0.494751  8.160050  0.505005  7.993139
44   0.508179  7.940516  0.501587  8.045361
45   0.505127  7.987052  0.496338  8.127524
46   0.500244  8.063508  0.503540  8.009383
47   0.510742  7.892419  0.503906  8.001771
48   0.493896  8.162388  0.498413  8.088910
49   0.500366  8.056848  0.503784  8.001213
50   0.497681  8.099130  0.502197  8.025905
51   0.504395  7.990135  0.512939  7.852081
52   0.496338  8.119400  0.494995  8.140800
53   0.499756  8.063871  0.499634  8.065664
54   0.505371  7.973052  0.496704  8.112625
55   0.493652  8.161720  0.491089  8.202956
56   0.504517  7.986465  0.503906  7.996250
57   0.502930  8.078100  0.500000  8.209933
58   0.503784  8.122631  0.503418  8.111667
59   0.501465  8.134960  0.496338  8.211276
60   0.502686  8.104806  0.502075  8.111019
61   0.489502  8.310738  0.501831  8.109244
62   0.498169  8.165746  0.500366  8.127852
63   0.492310  8.255333  0.508301  7.995214
64   0.504395  8.055851  0.503906  8.061387
65   0.488525  8.306990  0.490723  8.269251
66   0.495972  8.182344  0.508301  7.981302
67   0.490112  8.272166  0.505737  8.018005
68   0.510254  7.942917  0.508179  7.974063
69   0.491699  8.237413  0.494873  8.183982
70   0.503174  8.047959  0.509766  7.939481
71   0.497681  8.132094  0.495850  8.159440
72   0.501343  8.068804  0.499390  8.098202
73   0.502686  8.043080  0.509766  7.926986
74   0.509888  7.923137  0.502319  8.043274
75   0.498047  8.110394  0.498047  8.108687
76   0.503296  8.022492  0.496216  8.135060
77   0.499390  8.082478  0.505615  7.980753
78   0.500366  8.064102  0.506958  7.956648
79   0.500732  8.055910  0.504150  7.999786
80   0.498169  8.095281  0.502441  8.025552
81   0.503540  8.007090  0.492920  8.177558
82   0.495483  8.135633  0.507202  7.946183
83   0.506958  7.949641  0.497803  8.096766
84   0.500244  8.057051  0.505615  7.970145
85   0.498901  8.078089  0.496704  8.113258
86   0.503540  8.002880  0.498047  8.091243
87   0.505737  7.967151  0.505859  7.965061
88   0.500488  8.051539  0.505005  7.978659
89   0.500122  8.057299  0.500977  8.043473
90   0.490479  8.212643  0.503052  8.009953
91   0.508057  7.929261  0.503906  7.996138
92   0.494141  8.153527  0.503418  8.003983
93   0.500854  8.045294  0.492310  8.183015
94   0.504150  7.992160  0.498291  8.086599
95   0.500854  8.045279  0.500854  8.045278
96   0.493652  8.161362  0.488159  8.249900
97   0.500122  8.089656  0.507690  8.037103
98   0.496338  8.201114  0.503418  8.076939
99   0.496826  8.178672  0.498657  8.145586
100  0.500977  8.105689  0.500488  8.111304
101  0.493530  8.221569  0.505737  8.023033
102  0.503662  8.054881  0.486572  8.328787
103  0.506714  8.002703  0.501465  8.085896
104  0.500000  8.108173  0.492188  8.232779
105  0.500000  8.105598  0.494995  8.185019
106  0.496826  8.154305  0.498535  8.125564
107  0.507202  7.984713  0.502197  8.064231
108  0.492920  8.212646  0.501709  8.069869
109  0.499268  8.108139  0.487427  8.297911
110  0.502319  8.056823  0.501099  8.075454
111  0.498413  8.117727  0.505981  7.994729
112  0.493530  8.194440  0.506470  7.984907
113  0.495972  8.153173  0.496460  8.144366
114  0.502075  8.052953  0.494385  8.176008
115  0.496216  8.145626  0.498901  8.101476
116  0.511963  7.890116  0.500854  8.068333
117  0.503052  8.032118  0.500122  8.078545
118  0.496704  8.132868  0.502686  8.035695
119  0.510742  7.905100  0.500122  8.075541
120  0.496582  8.131890  0.502319  8.038708
121  0.498291  8.102951  0.500366  8.068820
122  0.492432  8.196047  0.498047  8.104880
123  0.510254  7.907484  0.502808  8.026865
124  0.506226  7.971153  0.502441  8.031527
125  0.493530  8.174557  0.491821  8.201504
126  0.495117  8.147799  0.500488  8.060650
127  0.511963  7.875142  0.504272  7.998543

2018-02-25 12:14:29.129894 Finish.
Total elapsed time: 15:47:39.13.
