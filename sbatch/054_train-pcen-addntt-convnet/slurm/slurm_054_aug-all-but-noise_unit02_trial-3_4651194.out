2018-02-24 20:26:35.978141: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:35.978456: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:35.978469: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:17.785126 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.491943  8.127565  0.508789  7.854136
1    0.494141  8.084983  0.498047  8.020509
2    0.503418  7.933262  0.501099  7.968774
3    0.502563  7.944201  0.494141  8.077338
4    0.511475  7.800001  0.502808  7.937235
5    0.500610  7.971439  0.507080  7.867513
6    0.496094  8.041973  0.495117  8.056887
7    0.501831  7.949276  0.504883  7.900077
8    0.501831  7.948246  0.496216  8.037307
9    0.495605  8.046633  0.506226  7.876936
10   0.504639  7.901893  0.501587  7.950217
11   0.503296  7.922680  0.503662  7.916561
12   0.504028  7.910469  0.499146  7.988069
13   0.495239  8.050122  0.504883  7.896165
14   0.500610  7.964081  0.511841  7.784848
15   0.503174  7.922841  0.502563  7.932397
16   0.484497  8.220256  0.499268  7.984620
17   0.499634  7.978634  0.494141  8.066063
18   0.502441  7.933594  0.505005  7.892595
19   0.499268  7.983941  0.497192  8.016908
20   0.494995  8.051832  0.506470  7.868797
21   0.499634  7.977687  0.499878  7.973707
22   0.501221  7.952224  0.514771  7.736135
23   0.502075  7.938467  0.494995  8.051282
24   0.498169  8.000635  0.503296  7.918854
25   0.499634  7.977200  0.493774  8.070579
26   0.510498  7.803938  0.501343  7.949871
27   0.503052  7.922607  0.498657  7.992651
28   0.501953  7.940094  0.494263  8.062688
29   0.506958  7.860287  0.508057  7.842766
30   0.500854  7.957581  0.502319  7.934224
31   0.497925  8.004281  0.501221  7.951735
32   0.507446  7.852483  0.507202  7.856375
33   0.497803  8.006223  0.495117  8.049037
34   0.499634  7.977031  0.507935  7.844696
35   0.493286  8.078227  0.488525  8.154125
36   0.506226  7.871942  0.497314  8.014006
37   0.502197  7.936163  0.499146  7.984815
38   0.505493  8.082387  0.507324  8.110033
39   0.500488  8.214910  0.487793  8.415026
40   0.502930  8.166834  0.501221  8.190108
41   0.493652  8.307813  0.497192  8.246422
42   0.507568  8.074853  0.502441  8.153121
43   0.492920  8.302253  0.496216  8.244765
44   0.502075  8.146011  0.495239  8.251865
45   0.503540  8.113822  0.507324  8.048571
46   0.502075  8.129015  0.498047  8.189788
47   0.490845  8.301832  0.502441  8.110889
48   0.502441  8.106991  0.495361  8.217232
49   0.499756  8.142665  0.502686  8.091738
50   0.500000  8.131468  0.505005  8.047278
51   0.494873  8.207218  0.491089  8.264887
52   0.501831  8.088580  0.497070  8.162194
53   0.500488  8.104146  0.500122  8.107140
54   0.498047  8.137842  0.503906  8.040703
55   0.504761  8.024396  0.504150  8.031750
56   0.501221  8.076644  0.497681  8.131426
57   0.499634  8.097820  0.503540  8.032781
58   0.502319  8.050522  0.504028  8.021088
59   0.500488  8.076391  0.488892  8.261594
60   0.490967  8.226554  0.500244  8.075469
61   0.496338  8.136989  0.499878  8.078526
62   0.499878  8.077221  0.501587  8.048404
63   0.497192  8.118057  0.498413  8.097234
64   0.497192  8.115848  0.490845  8.217127
65   0.494629  8.155182  0.495728  8.136551
66   0.500854  8.053070  0.513794  7.843694
67   0.496094  8.128246  0.490845  8.212138
68   0.500854  8.050160  0.496094  8.126283
69   0.504395  7.991950  0.504761  7.985534
70   0.505371  7.975251  0.495117  8.140106
71   0.501099  8.043339  0.498901  8.078423
72   0.500366  8.054536  0.491089  8.203815
73   0.499023  8.075718  0.498901  8.077499
74   0.498657  8.081286  0.492554  8.179531
75   0.500244  8.050476  0.493652  8.121451
76   0.496094  8.080238  0.498657  8.039084
77   0.501709  7.990314  0.498291  8.044687
78   0.508301  7.884993  0.495239  8.093110
79   0.508301  7.884762  0.497925  8.050062
80   0.506714  7.909821  0.504761  7.940832
81   0.489502  8.183960  0.493530  8.119600
82   0.493042  8.127237  0.494263  8.107622
83   0.495361  8.089943  0.498779  8.035280
84   0.501465  7.992283  0.497681  8.052420
85   0.495483  8.087245  0.489014  8.190174
86   0.498291  8.042042  0.488892  8.191653
87   0.494751  8.097986  0.506348  7.912842
88   0.493652  8.114953  0.504517  7.941456
89   0.495972  8.077369  0.500366  8.006982
90   0.500610  8.002743  0.496704  8.064655
91   0.499756  8.015619  0.487305  8.213721
92   0.502319  7.973928  0.496582  8.064954
93   0.506958  7.899071  0.507935  7.883018
94   0.496704  8.061549  0.501221  7.989014
95   0.503174  7.957320  0.515137  7.766025
96   0.496460  8.063172  0.496704  8.058654
97   0.496216  8.065782  0.507324  7.888011
98   0.503174  7.953473  0.503906  7.941069
99   0.496704  8.055133  0.502319  7.964835
100  0.492065  8.127504  0.497803  8.035213
101  0.499023  8.014903  0.500977  7.982896
102  0.500488  7.989791  0.498291  8.023912
103  0.503296  7.943198  0.493408  8.099890
104  0.497314  8.036664  0.506714  7.885849
105  0.508057  7.863472  0.499756  7.994826
106  0.495483  8.061962  0.501587  7.963673
107  0.492554  8.106711  0.499390  7.996755
108  0.500366  7.980231  0.505127  7.903379
109  0.504517  7.912185  0.489990  8.142851
110  0.502686  7.939576  0.495972  8.045741
111  0.489136  8.153896  0.499756  7.983774
112  0.497559  8.018043  0.492554  8.097091
113  0.495728  8.045806  0.498901  7.994541
114  0.506226  7.877169  0.491699  8.108169
115  0.500610  7.965579  0.499634  7.980645
116  0.502441  7.935441  0.488281  8.160767
117  0.501709  7.946331  0.496460  8.029669
118  0.492432  8.093596  0.497314  8.015480
119  0.500000  7.972436  0.497192  8.016985
120  0.506714  7.865017  0.500610  7.962164
121  0.500854  7.958145  0.495850  8.037820
122  0.498779  7.991024  0.492310  8.094087
123  0.502563  7.930555  0.500244  7.967477
124  0.488037  8.162046  0.501587  7.945996
125  0.507080  7.858397  0.506958  7.860322
126  0.493286  8.078270  0.494141  8.064635
127  0.496582  8.025704  0.498291  7.998452

2018-02-25 12:13:28.513434 Finish.
Total elapsed time: 15:47:11.51.
