2018-02-24 20:27:53.433917: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:53.434181: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:53.434200: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:53.434209: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:53.434218: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:50.594715 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit01, unit02, unit03.
Validation set: unit05, unit07.
Test set: unit10.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.951538  0.162593  0.875854  0.331999
1    0.943481  0.177817  0.910645  0.269231
2    0.918701  0.309198  0.738525  0.531940
3    0.782104  1.747047  0.496704  8.058305
4    0.506470  7.897922  0.506714  7.890551
5    0.519897  7.571769  0.495239  8.175488
6    0.496094  8.158898  0.509399  7.941537
7    0.505615  8.000040  0.496826  8.139346
8    0.494507  8.174645  0.496582  8.139203
9    0.501587  8.056743  0.493042  8.192751
10   0.497314  8.122329  0.510254  7.912267
11   0.505005  7.995505  0.505737  7.982380
12   0.500854  8.059877  0.502808  8.027234
13   0.497559  8.110777  0.501099  8.052693
14   0.497681  8.106851  0.502441  8.029216
15   0.506348  7.965436  0.505859  7.972517
16   0.499878  8.068213  0.502319  8.028176
17   0.496216  8.125936  0.507690  7.940394
18   0.491943  8.193677  0.508301  7.929520
19   0.498291  8.090408  0.492188  8.188356
20   0.493042  8.174205  0.496216  8.122690
21   0.503418  8.006292  0.520874  7.724637
22   0.507080  7.946714  0.499146  8.074364
23   0.501099  8.042677  0.499268  8.071999
24   0.491943  8.189890  0.498047  8.091363
25   0.504028  7.994828  0.509888  7.900271
26   0.500244  8.055613  0.494751  8.144067
27   0.505371  7.972821  0.505737  7.966856
28   0.494873  8.141917  0.502808  8.013982
29   0.495483  8.132000  0.503418  8.004079
30   0.491455  8.196874  0.503052  8.009937
31   0.500000  8.059110  0.512207  7.862342
32   0.500122  8.057119  0.503784  7.998084
33   0.493408  8.165318  0.494141  8.153508
34   0.507568  7.924529  0.500122  8.011049
35   0.499268  8.021468  0.500366  8.001231
36   0.502441  7.966198  0.492920  8.116262
37   0.500244  7.998097  0.501465  7.977333
38   0.496460  8.055981  0.507812  7.873897
39   0.487793  8.192043  0.502808  7.951681
40   0.498779  8.014958  0.491821  8.124949
41   0.491577  8.127938  0.500122  7.990812
42   0.510742  7.820629  0.508179  7.860625
43   0.497437  8.031034  0.499390  7.999051
44   0.493286  8.095534  0.503418  7.933189
45   0.489624  8.152304  0.494507  8.073669
46   0.493774  8.084581  0.502075  7.951486
47   0.499023  7.999405  0.509766  7.827422
48   0.497192  8.027170  0.507690  7.859114
49   0.501953  7.949919  0.502197  7.945373
50   0.502197  7.944752  0.503906  7.916895
51   0.495483  8.050596  0.493286  8.085058
52   0.501953  7.946353  0.499878  7.978916
53   0.493042  8.087412  0.498047  8.007151
54   0.501953  7.944442  0.506836  7.866178
55   0.499756  7.978668  0.503296  7.921862
56   0.501099  7.956559  0.496826  8.024353
57   0.503540  7.917035  0.495850  8.039369
58   0.494385  8.062485  0.511597  7.787862
59   0.497314  8.015361  0.500977  7.956796
60   0.490112  8.129842  0.497437  8.012932
61   0.497314  8.014755  0.500977  7.956259
62   0.498169  8.000925  0.511963  7.780930
63   0.498901  7.989091  0.499023  7.987081
64   0.501099  7.953945  0.498901  7.988927
65   0.502563  7.930507  0.495605  8.041400
66   0.502563  7.930446  0.506592  7.866202
67   0.500977  7.955703  0.509155  7.825299
68   0.506958  7.860316  0.501465  7.947879
69   0.496826  8.021823  0.489746  8.134689
70   0.502441  7.932290  0.506592  7.866118
71   0.493408  8.076293  0.493896  8.068506
72   0.506226  7.871948  0.496704  8.023742
73   0.499512  7.978980  0.498657  7.992602
74   0.500000  7.971194  0.489502  8.138557
75   0.489746  8.134665  0.492554  8.089905
76   0.498657  8.003211  0.498413  8.131707
77   0.496704  8.166384  0.504639  8.038684
78   0.506104  8.014951  0.503296  8.060074
79   0.501465  8.089447  0.499146  8.126683
80   0.495605  8.183584  0.509277  7.963055
81   0.498047  8.143890  0.502319  8.074840
82   0.494995  8.192692  0.494629  8.198385
83   0.489746  8.276862  0.483398  8.378938
84   0.495483  8.183899  0.496826  8.161991
85   0.494995  8.191221  0.503906  8.047293
86   0.497559  8.149289  0.498169  8.139119
87   0.503540  8.052193  0.499268  8.120687
88   0.502197  8.073072  0.497437  8.149393
89   0.497681  8.145020  0.493286  8.215394
90   0.506958  7.994543  0.496582  8.161277
91   0.502075  8.072201  0.495239  8.181824
92   0.495972  8.169428  0.507935  7.975995
93   0.495483  8.176035  0.498779  8.122238
94   0.498413  8.127433  0.499512  8.108992
95   0.499756  8.104288  0.486084  8.323857
96   0.494263  8.191201  0.498413  8.123447
97   0.496948  8.146166  0.500244  8.092123
98   0.504272  8.026242  0.501343  8.072485
99   0.499634  8.099022  0.509888  7.932714
100  0.505859  7.996584  0.500977  8.074202
101  0.511963  7.896020  0.502808  8.042461
102  0.502686  8.043290  0.499756  8.089354
103  0.500000  8.084256  0.498657  8.104722
104  0.495483  8.154705  0.490845  8.228288
105  0.488647  8.262533  0.497192  8.123630
106  0.503174  8.026069  0.491089  8.219705
107  0.499756  8.078892  0.504761  7.997113
108  0.491333  8.212475  0.494873  8.154361
109  0.499878  8.072689  0.486938  8.280263
110  0.500244  8.064876  0.496826  8.119063
111  0.505127  7.984433  0.492188  8.192180
112  0.495972  8.130444  0.498413  8.090377
113  0.491211  8.205817  0.503906  8.000577
114  0.489746  8.228265  0.502075  8.029025
115  0.492676  8.180074  0.501709  8.034051
116  0.497070  8.108454  0.505859  7.966452
117  0.495239  8.137344  0.507935  7.932457
118  0.495972  8.125060  0.502441  8.020583
119  0.515259  7.813833  0.502319  8.022249
120  0.504150  7.992623  0.496582  8.114511
121  0.496948  8.108531  0.506226  7.958930
122  0.513062  7.848697  0.496948  8.108369
123  0.505249  7.974545  0.486694  8.273584
124  0.505981  7.962693  0.508301  7.925294
125  0.496338  8.118103  0.498901  8.076775
126  0.497192  8.104315  0.501587  8.033479
127  0.500732  8.047249  0.507812  7.933129

2018-02-25 12:01:42.125503 Finish.
Total elapsed time: 15:34:52.13.
