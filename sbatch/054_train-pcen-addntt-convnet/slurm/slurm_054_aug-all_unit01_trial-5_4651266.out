2018-02-24 20:28:00.832629: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:28:00.832918: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:28:00.832935: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:28:00.832943: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:28:00.832950: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:51.519910 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit02, unit03, unit05.
Validation set: unit07, unit10.
Test set: unit01.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.974365  0.108414  0.963989  0.161222
1    0.978394  0.097406  0.957031  0.189632
2    0.981934  0.083949  0.959229  0.165582
3    0.981323  0.077650  0.958740  0.170906
4    0.979248  0.088408  0.962402  0.175851
5    0.981079  0.080226  0.966919  0.160102
6    0.984009  0.072597  0.961670  0.168668
7    0.981323  0.079310  0.953247  0.200144
8    0.985962  0.065489  0.963013  0.145062
9    0.984497  0.074796  0.967163  0.148328
10   0.982666  0.078553  0.962036  0.205431
11   0.982422  0.077236  0.930908  0.266932
12   0.904053  1.355464  0.489868  8.270975
13   0.497925  8.131163  0.495117  8.168649
14   0.499634  8.090837  0.498413  8.106240
15   0.502686  8.034229  0.505005  7.994074
16   0.490479  8.226075  0.493530  8.174976
17   0.499268  8.080994  0.493530  8.172106
18   0.499023  8.082477  0.500000  8.065745
19   0.499512  8.072814  0.506714  7.956013
20   0.509399  7.870455  0.505493  7.910649
21   0.500244  7.990853  0.500366  7.985976
22   0.495728  8.057767  0.505005  7.907949
23   0.499512  7.994020  0.492554  8.103588
24   0.491821  8.114166  0.497925  8.015857
25   0.489380  8.151253  0.494629  8.066805
26   0.497559  8.019455  0.501221  7.960472
27   0.496338  8.037802  0.502686  7.936123
28   0.500000  7.978516  0.504761  7.902220
29   0.488037  8.168480  0.499756  7.981317
30   0.499512  7.984903  0.510132  7.815297
31   0.500366  7.970714  0.498535  7.999642
32   0.496948  8.024697  0.498779  7.995266
33   0.499756  7.979474  0.508545  7.839135
34   0.499634  7.980992  0.505981  7.879591
35   0.494141  8.068168  0.499512  7.982349
36   0.510132  7.812858  0.509766  7.818518
37   0.509644  7.820294  0.496948  8.022521
38   0.504639  7.899758  0.493286  8.080588
39   0.504395  7.903346  0.501831  7.944069
40   0.504761  7.897226  0.498535  7.996342
41   0.499146  7.986486  0.498169  8.001931
42   0.500488  7.964840  0.501343  7.951105
43   0.504395  7.902348  0.499756  7.976198
44   0.496216  8.032541  0.498535  7.995474
45   0.506226  7.872787  0.499268  7.983634
46   0.494995  8.051674  0.496826  8.022413
47   0.492310  8.094355  0.494141  8.065103
48   0.495361  8.045588  0.510010  7.812005
49   0.494873  8.053275  0.510864  7.798294
50   0.512329  7.774904  0.484253  8.222470
51   0.508667  7.833221  0.496948  8.020017
52   0.501343  7.949934  0.506470  7.868175
53   0.500122  7.969353  0.500366  7.965444
54   0.493286  8.078303  0.502930  7.924548
55   0.506104  7.873939  0.499023  7.986803
56   0.499268  7.982903  0.495972  8.035441
57   0.493530  8.074357  0.497803  8.006239
58   0.504761  7.895308  0.497192  8.015963
59   0.503418  7.993010  0.503296  8.060717
60   0.491943  8.239892  0.496704  8.160618
61   0.498413  8.131416  0.501831  8.074813
62   0.506714  7.994783  0.510010  7.940367
63   0.510986  7.923402  0.496582  8.154355
64   0.491699  8.231878  0.504272  8.028042
65   0.494629  8.182325  0.506958  7.982449
66   0.492432  8.215455  0.493042  8.204485
67   0.494751  8.175829  0.497314  8.133400
68   0.502686  8.045741  0.496216  8.148933
69   0.506226  7.986534  0.507202  7.969733
70   0.502563  8.043468  0.503174  8.032600
71   0.504517  8.009957  0.510864  7.906649
72   0.495850  8.147693  0.500977  8.064098
73   0.514038  7.852646  0.499512  8.085865
74   0.505615  7.986605  0.496582  8.131328
75   0.500732  8.063593  0.510132  7.911263
76   0.495361  8.148544  0.496582  8.128086
77   0.499512  8.080121  0.496826  8.122673
78   0.494141  8.165264  0.503540  8.013078
79   0.496460  8.126549  0.502563  8.027537
80   0.504395  7.997425  0.498901  8.085378
81   0.502319  8.029735  0.504395  7.995747
82   0.494019  8.162483  0.495361  8.140347
83   0.484985  8.307127  0.508179  7.932845
84   0.500366  8.058351  0.496826  8.115003
85   0.502319  8.026088  0.503174  8.011950
86   0.511353  7.879788  0.497681  8.099827
87   0.509399  7.910645  0.500244  8.057922
88   0.505737  7.969121  0.511230  7.880330
89   0.498291  8.088662  0.506836  7.950716
90   0.513306  7.846241  0.498413  8.086095
91   0.505127  7.977715  0.500488  8.052325
92   0.503052  8.010870  0.505615  7.969422
93   0.500977  8.044076  0.501343  8.038068
94   0.498779  8.079296  0.507690  7.935582
95   0.507568  7.937479  0.487061  8.267960
96   0.500854  8.045574  0.496582  8.114388
97   0.504395  7.988424  0.499756  8.063154
98   0.500122  8.057221  0.499268  8.070967
99   0.502197  8.023725  0.493652  8.161434
100  0.498291  8.086653  0.501953  8.027613
101  0.501587  8.033506  0.497070  8.106296
102  0.497803  8.094485  0.500000  8.059064
103  0.504639  7.984294  0.499390  8.068894
104  0.502197  8.023639  0.504761  7.982318
105  0.489380  8.230227  0.504639  7.984284
106  0.495239  8.135784  0.496094  8.122010
107  0.499878  8.061016  0.507202  7.942964
108  0.506348  7.956736  0.496460  8.116106
109  0.503296  8.005924  0.499756  8.062983
110  0.500366  8.053145  0.500366  8.053145
111  0.501221  8.039372  0.493652  8.161360
112  0.496094  8.257623  0.504028  8.175552
113  0.494995  8.306667  0.501831  8.186565
114  0.508057  8.080610  0.509521  8.052290
115  0.498535  8.225738  0.495361  8.273523
116  0.504639  8.121009  0.505005  8.112222
117  0.507690  8.066251  0.495728  8.256431
118  0.501587  8.159481  0.493286  8.290796
119  0.500000  8.180208  0.499390  8.187695
120  0.496582  8.230692  0.498047  8.204843
121  0.498169  8.200723  0.500000  8.169074
122  0.500732  8.155214  0.500488  8.157109
123  0.500122  8.161050  0.495728  8.229933
124  0.496460  8.216254  0.501343  8.135691
125  0.501343  8.133898  0.504395  8.082931
126  0.492188  8.277969  0.496704  8.203467
127  0.503296  8.095578  0.503052  8.097881

2018-02-25 11:31:06.465587 Finish.
Total elapsed time: 15:04:15.47.
