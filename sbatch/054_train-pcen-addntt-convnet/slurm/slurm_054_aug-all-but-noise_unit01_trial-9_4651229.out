2018-02-24 20:26:40.924478: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:40.924709: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:40.924721: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:17.673647 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit02, unit03, unit05.
Validation set: unit07, unit10.
Test set: unit01.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.483032  8.351615  0.497437  8.111328
1    0.540527  7.425446  0.528076  7.651386
2    0.514282  7.870560  0.534180  7.546936
3    0.519165  7.768670  0.495972  8.078029
4    0.496826  8.062301  0.505615  7.920182
5    0.496948  8.056607  0.500854  7.992667
6    0.504028  7.940580  0.498535  8.026724
7    0.491943  8.130518  0.499634  8.006664
8    0.490845  8.145639  0.510986  7.823423
9    0.500366  7.991709  0.506836  7.887568
10   0.503906  7.933348  0.493896  8.092023
11   0.508301  7.861540  0.496094  8.055323
12   0.499268  8.003950  0.484375  8.240615
13   0.499878  7.992749  0.496826  8.040702
14   0.495728  8.057558  0.494751  8.072480
15   0.503296  7.935644  0.495483  8.059594
16   0.500977  7.971452  0.497437  8.027331
17   0.506592  7.880845  0.490967  8.127197
18   0.496826  8.035538  0.492310  8.107035
19   0.494385  8.073510  0.496704  8.036141
20   0.495850  8.049308  0.505615  7.893214
21   0.491577  8.116542  0.488892  8.159017
22   0.499390  7.991227  0.496338  8.039565
23   0.499390  7.990574  0.488647  8.161496
24   0.493774  8.079444  0.491211  8.120021
25   0.496582  8.033947  0.491699  8.111692
26   0.508911  7.836867  0.497925  8.011208
27   0.490479  8.130155  0.490723  8.125989
28   0.505493  7.890273  0.502808  7.933044
29   0.501221  7.957971  0.494263  8.068787
30   0.511475  7.794031  0.492310  8.098526
31   0.492432  8.097182  0.501465  7.952961
32   0.504150  7.909881  0.500366  7.968082
33   0.496582  8.030152  0.499878  7.976826
34   0.500977  7.985256  0.507324  7.899318
35   0.491455  8.148173  0.501221  7.989327
36   0.499878  8.008594  0.503174  7.954166
37   0.515625  7.754167  0.495972  8.066107
38   0.499756  8.004603  0.504761  7.923706
39   0.497314  8.041443  0.496704  8.050242
40   0.495850  8.063028  0.502686  7.953242
41   0.505005  7.915535  0.506470  7.891474
42   0.503906  7.931694  0.506470  7.890199
43   0.501099  7.975248  0.497192  8.036961
44   0.503906  7.929406  0.505737  7.899708
45   0.503784  7.930376  0.503296  7.937702
46   0.496338  8.048202  0.497437  8.030270
47   0.496704  8.041556  0.495605  8.058689
48   0.497437  8.029139  0.502441  7.948997
49   0.494019  8.082946  0.504517  7.915256
50   0.501221  7.967492  0.498291  8.013894
51   0.496460  8.042796  0.499756  7.989966
52   0.493042  8.096728  0.493530  8.088673
53   0.494873  8.067006  0.494507  8.072587
54   0.510620  7.815454  0.494873  8.066253
55   0.509399  7.834429  0.504395  7.913980
56   0.502686  7.940993  0.495239  8.059473
57   0.500488  7.975564  0.504150  7.916955
58   0.506592  7.877810  0.496094  8.044951
59   0.500854  7.968834  0.498169  8.011427
60   0.497559  8.020940  0.506348  7.880602
61   0.493652  8.082779  0.499878  7.983310
62   0.503296  7.928603  0.497192  8.025689
63   0.489990  8.140292  0.501831  7.951303
64   0.503906  7.918002  0.498291  8.007303
65   0.510620  7.810530  0.509766  7.823934
66   0.500244  7.975510  0.499390  7.988913
67   0.506836  7.869983  0.495728  8.046857
68   0.504272  7.910413  0.501221  7.958845
69   0.497559  8.017010  0.497559  8.016792
70   0.494385  8.067174  0.506226  7.878185
71   0.497803  8.012252  0.495239  8.052905
72   0.501221  7.957335  0.510620  7.807274
73   0.501709  7.949131  0.504272  7.908055
74   0.496582  8.030456  0.505127  7.894027
75   0.504272  7.907451  0.496948  8.024020
76   0.494751  8.058857  0.502563  7.934116
77   0.500366  7.968961  0.505371  7.888987
78   0.496948  8.023090  0.495972  8.038482
79   0.505737  7.882625  0.498291  8.001169
80   0.497559  8.012684  0.492554  8.092315
81   0.495728  8.041565  0.499023  7.988871
82   0.493042  8.084088  0.501099  7.955506
83   0.504150  7.906723  0.502808  7.928001
84   0.493774  8.071891  0.498901  7.990037
85   0.492310  8.095016  0.495361  8.046257
86   0.495361  8.046158  0.500488  7.964326
87   0.505493  7.884449  0.504517  7.899933
88   0.497559  8.010783  0.493408  8.076877
89   0.507812  7.847172  0.499634  7.977497
90   0.505615  7.882083  0.503296  7.919006
91   0.496460  8.027941  0.498047  8.002599
92   0.501587  7.946125  0.503296  7.918845
93   0.500122  7.969414  0.493774  8.070583
94   0.502075  7.938226  0.499878  7.973235
95   0.501221  7.951811  0.508301  7.838923
96   0.501953  7.940107  0.499146  7.984857
97   0.496460  8.027662  0.498779  7.990679
98   0.493774  8.070463  0.501831  7.942016
99   0.495972  8.035425  0.509644  7.817460
100  0.505737  7.879732  0.489014  8.146345
101  0.508911  7.829131  0.502441  7.932273
102  0.496216  8.031523  0.502441  7.932271
103  0.507080  7.864976  0.494385  8.099851
104  0.496826  8.045757  0.501709  7.962833
105  0.495972  8.053157  0.509888  7.830569
106  0.501587  7.962505  0.500977  7.971893
107  0.499512  7.994963  0.500488  7.979123
108  0.499146  8.000276  0.494019  8.081762
109  0.497681  8.023139  0.506104  7.888619
110  0.497559  8.024614  0.503418  7.930970
111  0.506226  7.885984  0.500122  7.983064
112  0.492920  8.097663  0.502563  7.943701
113  0.495239  8.060250  0.508057  7.855693
114  0.502075  7.950837  0.501221  7.964245
115  0.494873  8.065230  0.504395  7.913221
116  0.501587  7.957770  0.499023  7.998425
117  0.499512  7.990429  0.511963  7.791714
118  0.501465  7.958865  0.495728  8.050117
119  0.511353  7.800803  0.506836  7.872592
120  0.500732  7.969680  0.497559  8.020059
121  0.497192  8.025679  0.503906  7.918422
122  0.498413  8.005774  0.500610  7.970520
123  0.500732  7.968350  0.496704  8.032343
124  0.494629  8.065200  0.506104  7.882038
125  0.499023  7.994682  0.509766  7.823194
126  0.507812  7.854101  0.506958  7.867491
127  0.503052  7.929535  0.499146  7.991577

2018-02-25 12:12:30.359933 Finish.
Total elapsed time: 15:46:13.36.
