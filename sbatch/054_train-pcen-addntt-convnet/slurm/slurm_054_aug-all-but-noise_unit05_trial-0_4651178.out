2018-02-24 20:26:39.901279: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:39.901478: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:39.901491: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:16.675549 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit07, unit10, unit01.
Validation set: unit02, unit03.
Test set: unit05.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.913086  0.268092  0.927490  0.253259
1    0.857056  1.391214  0.503418  7.959320
2    0.600586  6.069336  0.867676  0.578987
3    0.918579  0.261232  0.901001  0.320811
4    0.928589  0.231840  0.936890  0.208939
5    0.898682  0.295270  0.862061  0.449168
6    0.896484  0.308915  0.861938  0.447791
7    0.912964  0.267773  0.861938  0.591009
8    0.921143  0.250961  0.840332  0.866879
9    0.786865  2.758698  0.496460  8.063210
10   0.832031  1.220924  0.838623  0.563726
11   0.895874  0.318506  0.900635  0.421856
12   0.897583  0.362264  0.880249  0.564449
13   0.915283  0.295025  0.808960  1.179729
14   0.902100  0.326766  0.840698  0.760847
15   0.917480  0.290999  0.906372  0.320898
16   0.912598  0.294671  0.908813  0.337095
17   0.629639  5.603874  0.496338  8.181447
18   0.501465  8.095329  0.493530  8.219921
19   0.497437  8.154040  0.497803  8.145351
20   0.503662  8.048418  0.500977  8.089319
21   0.494507  8.191462  0.505005  8.020206
22   0.494385  8.189546  0.500244  8.093345
23   0.506104  7.997326  0.502686  8.050907
24   0.502197  8.057422  0.496094  8.154500
25   0.500366  8.084467  0.504028  8.024317
26   0.493286  8.196443  0.508789  7.945583
27   0.492798  8.202435  0.497803  8.120896
28   0.494385  8.175183  0.502441  8.044541
29   0.501587  8.057579  0.500610  8.072598
30   0.496460  8.138809  0.497559  8.120423
31   0.499390  8.090258  0.503662  8.020745
32   0.496948  8.128328  0.494141  8.172950
33   0.491089  8.221519  0.499878  8.079234
34   0.496460  8.133711  0.491821  8.207860
35   0.496704  8.128545  0.500977  8.059064
36   0.494629  8.160764  0.512939  7.865016
37   0.495972  8.137893  0.498535  8.095959
38   0.496216  8.132735  0.502686  8.027845
39   0.506104  7.972153  0.491821  8.201754
40   0.493164  8.179522  0.504395  7.997921
41   0.497925  8.101630  0.503784  8.006621
42   0.492920  8.181185  0.489380  8.237702
43   0.502563  8.024690  0.498901  8.083207
44   0.502319  8.027633  0.500854  8.050770
45   0.512573  7.861443  0.496704  8.116791
46   0.491821  8.195092  0.508057  7.933020
47   0.504272  7.993657  0.495605  8.133008
48   0.500244  8.057930  0.503418  8.006475
49   0.501465  8.037688  0.494873  8.143680
50   0.493042  8.172967  0.497070  8.107824
51   0.494263  8.152890  0.496582  8.115330
52   0.490112  8.219458  0.495728  8.128808
53   0.503662  8.000796  0.495605  8.130540
54   0.518311  7.764483  0.508179  7.927700
55   0.506958  7.947303  0.500366  8.053483
56   0.498535  8.082941  0.506104  7.960904
57   0.500366  8.053338  0.496948  8.108393
58   0.494019  8.155585  0.499878  8.061117
59   0.497437  8.100448  0.493286  8.167327
60   0.502075  8.025651  0.509644  7.903651
61   0.490845  8.206644  0.494141  8.153513
62   0.500244  8.055130  0.497314  8.102346
63   0.494385  8.149564  0.503296  8.005931
64   0.496460  8.116111  0.499878  8.061018
65   0.497192  8.104304  0.497314  8.102335
66   0.492798  8.175134  0.513428  7.842619
67   0.493774  8.159393  0.502441  8.019697
68   0.516235  7.797365  0.500000  8.059048
69   0.506104  7.960671  0.500854  8.045275
70   0.503296  8.005924  0.492676  8.177100
71   0.503418  8.003957  0.502563  8.017729
72   0.506226  7.958703  0.501099  8.041340
73   0.507812  7.982105  0.508179  7.995127
74   0.498047  8.153936  0.502441  8.079930
75   0.498779  8.137100  0.503052  8.066710
76   0.497559  8.154191  0.496094  8.176891
77   0.503174  8.062097  0.491333  8.252348
78   0.501831  8.082662  0.490234  8.269140
79   0.507080  7.997246  0.502686  8.067722
80   0.492676  8.228733  0.495728  8.179224
81   0.501343  8.088405  0.498169  8.139248
82   0.500977  8.093678  0.509521  7.955626
83   0.505615  8.018255  0.501587  8.082842
84   0.510620  7.936888  0.504639  8.032929
85   0.502197  8.071898  0.511475  7.921969
86   0.504395  8.035674  0.488892  8.285125
87   0.495850  8.172532  0.502808  8.059925
88   0.507202  7.988617  0.501343  8.082569
89   0.496582  8.158796  0.502563  8.061864
90   0.501831  8.073130  0.511108  7.923043
91   0.500732  8.089712  0.503540  8.043873
92   0.507568  7.978344  0.508789  7.958053
93   0.505005  8.018418  0.497437  8.139762
94   0.499146  8.111563  0.504761  8.020388
95   0.499878  8.098412  0.497192  8.141009
96   0.499268  8.106864  0.490601  8.245851
97   0.494141  8.188080  0.497437  8.134234
98   0.500610  8.082354  0.498657  8.113102
99   0.501953  8.059246  0.487915  8.284773
100  0.499512  8.097121  0.486816  8.301003
101  0.496460  8.144830  0.500366  8.081127
102  0.499023  8.102035  0.500122  8.083589
103  0.508057  7.954970  0.495972  8.149024
104  0.496460  8.140431  0.501099  8.064940
105  0.502808  8.036681  0.500488  8.073348
106  0.494141  8.174957  0.505737  7.987335
107  0.507080  7.964998  0.500366  8.072517
108  0.505127  7.995100  0.496094  8.140014
109  0.506714  7.968166  0.500610  8.065869
110  0.500366  8.069143  0.496704  8.127508
111  0.495728  8.142599  0.493408  8.179333
112  0.503296  8.019327  0.497803  8.107231
113  0.497559  8.110547  0.501221  8.050902
114  0.495850  8.136872  0.492188  8.195298
115  0.498535  8.092405  0.496216  8.129211
116  0.509521  7.914192  0.503906  8.004148
117  0.503906  8.003619  0.502441  8.026707
118  0.497559  8.104913  0.508667  7.925378
119  0.503906  8.001651  0.499146  8.077932
120  0.489136  8.238848  0.499512  8.071194
121  0.502319  8.025557  0.494507  8.151108
122  0.492432  8.184216  0.503418  8.006808
123  0.491577  8.197361  0.502319  8.023930
124  0.499634  8.066960  0.497803  8.096228
125  0.495117  8.139298  0.501953  8.028911
126  0.496948  8.109402  0.497559  8.099397
127  0.501343  8.038260  0.493164  8.169952

2018-02-25 11:51:54.041084 Finish.
Total elapsed time: 15:25:38.04.
