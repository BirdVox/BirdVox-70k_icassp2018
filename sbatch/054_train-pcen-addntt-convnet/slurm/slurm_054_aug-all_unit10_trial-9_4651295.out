2018-02-24 20:27:54.712206: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:54.712502: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:54.712521: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:54.712531: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:54.712540: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:27:08.521282 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit01, unit02, unit03.
Validation set: unit05, unit07.
Test set: unit10.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.939819  0.189926  0.940063  0.205359
1    0.940918  0.190295  0.927612  0.259257
2    0.676392  4.583950  0.502319  8.072564
3    0.496826  8.155810  0.498169  8.129607
4    0.505005  8.015794  0.498169  8.122617
5    0.510742  7.917089  0.498901  8.105235
6    0.500610  8.075324  0.498291  8.110463
7    0.495850  8.147832  0.495850  8.145947
8    0.498413  8.102957  0.496582  8.130878
9    0.501587  8.048795  0.499756  8.076961
10   0.495239  8.148563  0.499634  8.076590
11   0.501099  8.051967  0.503540  8.011651
12   0.494629  8.154424  0.499146  8.080809
13   0.497437  8.107630  0.505249  7.981018
14   0.513306  7.850550  0.496948  8.113619
15   0.500000  8.063917  0.493652  8.165740
16   0.498779  8.082672  0.499268  8.074392
17   0.497803  8.097642  0.504028  7.996955
18   0.502075  8.028135  0.499756  8.065234
19   0.496704  8.114174  0.494751  8.145420
20   0.499756  8.064547  0.494019  8.156830
21   0.500854  8.046481  0.498535  8.083709
22   0.507935  7.932075  0.487671  8.258563
23   0.503296  8.006612  0.490723  8.209170
24   0.500244  8.055620  0.496460  8.116537
25   0.501099  8.041707  0.489868  8.222663
26   0.503296  8.006186  0.507202  7.943181
27   0.494873  8.141868  0.508179  7.927374
28   0.497803  8.094590  0.498535  8.082762
29   0.501221  8.039459  0.500854  8.045346
30   0.498291  8.086652  0.493286  8.166675
31   0.559448  7.139822  0.658936  5.551952
32   0.592285  6.627369  0.665161  5.446388
33   0.602661  6.455947  0.669067  5.377849
34   0.602905  6.448586  0.679443  5.210173
35   0.605713  6.400357  0.660889  5.505716
36   0.604980  6.409740  0.672485  5.316198
37   0.614136  6.259947  0.661499  5.491264
38   0.601807  6.456382  0.668945  5.368785
39   0.610840  6.308678  0.666748  5.402610
40   0.645874  5.741314  0.674683  5.268589
41   0.647827  5.709514  0.684082  5.115759
42   0.644409  5.762331  0.677002  5.226570
43   0.650269  5.666210  0.682983  5.128756
44   0.652222  5.632766  0.680542  5.165998
45   0.653076  5.616802  0.685303  5.087076
46   0.643921  5.762631  0.687744  5.047040
47   0.663574  5.444295  0.676636  5.222293
48   0.650757  5.649007  0.693237  4.956052
49   0.652588  5.618265  0.688354  5.031921
50   0.659546  5.504383  0.682129  5.131375
51   0.638916  5.837690  0.611206  6.292075
52   0.555298  7.143968  0.547363  7.247610
53   0.556885  7.096233  0.538452  7.387560
54   0.559204  7.057745  0.539795  7.363859
55   0.548096  7.233066  0.538574  7.382233
56   0.533691  7.461387  0.531494  7.493157
57   0.550415  7.193411  0.534912  7.437851
58   0.542969  7.311122  0.529907  7.516680
59   0.543823  7.295569  0.535156  7.431899
60   0.532593  7.473891  0.520630  7.662759
61   0.530396  7.508015  0.516968  7.720357
62   0.531982  7.481514  0.515015  7.750775
63   0.533081  7.463117  0.513428  7.775350
64   0.538818  7.372486  0.584961  6.639449
65   0.584595  6.645811  0.543579  7.294966
66   0.550537  7.185541  0.539551  7.358068
67   0.530151  7.508908  0.529907  7.511007
68   0.546997  7.239828  0.631104  5.969124
69   0.507080  7.984812  0.516602  7.831712
70   0.514648  7.862822  0.522583  7.734568
71   0.515747  7.844404  0.519897  7.777167
72   0.514038  7.871269  0.515747  7.843387
73   0.514282  7.866646  0.517456  7.815158
74   0.503540  8.039151  0.516724  7.826304
75   0.501709  8.068005  0.520142  7.770574
76   0.509888  7.935517  0.503906  8.031592
77   0.508545  7.956491  0.515625  7.842014
78   0.506470  7.987478  0.539062  7.463673
79   0.525024  7.689547  0.547607  7.325175
80   0.529907  7.610219  0.533691  7.548842
81   0.523315  7.715720  0.543701  7.386879
82   0.527832  7.642311  0.544556  7.372382
83   0.524536  7.694682  0.544312  7.375580
84   0.520752  7.753176  0.540771  7.432016
85   0.528809  7.624350  0.542114  7.409483
86   0.527344  7.647206  0.549927  7.282824
87   0.522217  7.729077  0.546509  7.335799
88   0.529541  7.610228  0.541992  7.409124
89   0.523804  7.701914  0.544189  7.372935
90   0.525757  7.668944  0.546753  7.328269
91   0.531372  7.579032  0.559814  7.119680
92   0.531250  7.579635  0.564941  7.036119
93   0.529785  7.602377  0.538940  7.454330
94   0.531616  7.571989  0.553467  7.219251
95   0.533813  7.535692  0.547974  7.306991
96   0.530518  7.587925  0.554932  7.193946
97   0.527832  7.630312  0.549561  7.279614
98   0.534668  7.519223  0.557739  7.146837
99   0.530396  7.587134  0.548218  7.299329
100  0.531860  7.562632  0.560059  7.107672
101  0.528076  7.622735  0.549316  7.279884
102  0.529907  7.592554  0.545044  7.347983
103  0.523560  7.693765  0.545532  7.339122
104  0.525513  7.661346  0.559204  7.117769
105  0.532959  7.540461  0.539185  7.439660
106  0.526123  7.649784  0.545532  7.336499
107  0.566040  7.002162  0.643311  5.725665
108  0.635742  5.863621  0.501953  7.973150
109  0.495483  8.129604  0.503540  8.002524
110  0.497437  8.098826  0.496826  8.107661
111  0.490479  8.208138  0.497437  8.096551
112  0.495239  8.131027  0.501343  8.033203
113  0.492065  8.180655  0.498657  8.075138
114  0.505615  7.963823  0.500610  8.043238
115  0.497437  8.093487  0.501099  8.034763
116  0.501343  8.030546  0.504150  7.985464
117  0.491699  8.183652  0.505371  7.965375
118  0.487671  8.247247  0.494629  8.136007
119  0.495239  8.125964  0.501343  8.028341
120  0.509277  7.901525  0.496460  8.105539
121  0.501587  8.023475  0.504639  7.974487
122  0.500977  8.032528  0.496338  8.106130
123  0.492676  8.164156  0.502075  8.013942
124  0.501343  8.025244  0.505615  7.956746
125  0.499023  8.061440  0.508789  7.905349
126  0.505615  7.955530  0.497559  8.083544
127  0.494995  8.123970  0.486084  8.265580

2018-02-25 11:53:10.570421 Finish.
Total elapsed time: 15:26:02.57.
