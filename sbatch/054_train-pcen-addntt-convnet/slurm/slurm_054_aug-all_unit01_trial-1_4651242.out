2018-02-24 20:27:38.042672: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:38.042993: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:38.043009: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:38.043015: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:38.043022: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:47.598188 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit02, unit03, unit05.
Validation set: unit07, unit10.
Test set: unit01.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.501587  7.949743  0.503418  7.919693
1    0.505493  7.886027  0.502930  7.926400
2    0.573242  5.823824  0.621582  0.697926
3    0.758301  0.589581  0.618286  0.693510
4    0.614990  4.814716  0.505371  8.002003
5    0.494141  8.180642  0.510498  7.914838
6    0.492554  8.202294  0.508667  7.940935
7    0.505859  7.984790  0.492554  8.197938
8    0.486938  8.287304  0.497803  8.111114
9    0.502686  8.031465  0.504761  7.997118
10   0.492554  8.193080  0.499512  8.080179
11   0.493896  8.170023  0.505249  7.986412
12   0.499390  8.080301  0.502686  8.026654
13   0.500977  8.053740  0.494995  8.149715
14   0.493042  8.180817  0.499878  8.070276
15   0.506470  7.963716  0.501831  8.038188
16   0.510498  7.898235  0.494141  8.161643
17   0.505127  7.984350  0.497681  8.104167
18   0.489746  8.231877  0.495728  8.135295
19   0.500854  8.052501  0.496704  8.119246
20   0.499390  8.075818  0.502563  8.024522
21   0.491577  8.201467  0.501587  8.039995
22   0.499634  8.071344  0.499023  8.081049
23   0.510132  7.901869  0.498779  8.084715
24   0.500488  8.057031  0.497681  8.102143
25   0.503662  8.005589  0.493774  8.164811
26   0.493652  8.166627  0.501099  8.046452
27   0.493286  8.172215  0.494751  8.148442
28   0.506470  7.959392  0.500854  8.049729
29   0.498047  8.094810  0.499512  8.071025
30   0.494629  8.149550  0.495850  8.129695
31   0.494995  8.143289  0.504639  7.987672
32   0.487915  8.257045  0.508667  7.922381
33   0.491699  8.195691  0.489258  8.234863
34   0.509766  7.904142  0.497314  8.104657
35   0.504639  7.986437  0.498901  8.078745
36   0.489868  8.224184  0.506470  7.956443
37   0.497314  8.103861  0.501465  8.036820
38   0.489746  8.225571  0.498535  8.083778
39   0.491211  8.201712  0.496582  8.115025
40   0.501709  8.032286  0.501953  8.028252
41   0.497437  8.100964  0.491333  8.199259
42   0.497314  8.102779  0.488892  8.238472
43   0.498413  8.084947  0.500000  8.059317
44   0.503052  8.010085  0.491455  8.196961
45   0.501099  8.041493  0.504639  7.984404
46   0.505371  7.972576  0.499512  8.066997
47   0.498047  8.090592  0.492188  8.185019
48   0.499878  8.061054  0.500854  8.045304
49   0.503296  8.005947  0.498047  8.090545
50   0.498657  8.080703  0.502808  8.013803
51   0.494141  8.153496  0.501099  8.041345
52   0.490112  8.218422  0.498657  8.080693
53   0.503906  7.996088  0.506714  7.950834
54   0.503784  7.998055  0.510742  7.885904
55   0.489990  8.220386  0.500854  8.045275
56   0.496094  8.122009  0.503662  8.000022
57   0.492920  8.173165  0.495361  8.133329
58   0.503784  7.998054  0.495483  8.126734
59   0.501221  8.014357  0.502319  7.989769
60   0.503296  7.974007  0.505493  7.938767
61   0.493042  8.137045  0.506958  7.914956
62   0.504150  7.959465  0.493530  8.128512
63   0.494507  8.112664  0.507080  7.911923
64   0.506836  7.915503  0.506104  7.926853
65   0.506714  7.916775  0.496094  8.085723
66   0.496460  8.079498  0.507080  7.909787
67   0.501953  7.991096  0.505249  7.938107
68   0.494019  8.116677  0.499390  8.030559
69   0.501343  7.998905  0.503906  7.957499
70   0.501465  7.995855  0.497681  8.055595
71   0.497559  8.056924  0.501953  7.986224
72   0.493286  8.123725  0.506226  7.916745
73   0.492676  8.132036  0.493896  8.111825
74   0.497681  8.050716  0.499634  8.018773
75   0.510376  7.846682  0.494629  8.096868
76   0.492432  8.131009  0.500000  8.009439
77   0.497314  8.051314  0.491821  8.137926
78   0.505859  7.913139  0.489258  8.176800
79   0.497070  8.051222  0.497192  8.048228
80   0.491455  8.138631  0.500610  7.991593
81   0.502686  7.957419  0.501099  7.981612
82   0.490845  8.143975  0.502075  7.963814
83   0.498169  8.024972  0.490845  8.140612
84   0.487915  8.186205  0.495728  8.060537
85   0.498535  8.014679  0.501465  7.966874
86   0.497559  8.028077  0.499268  7.999764
87   0.506104  7.889751  0.501099  7.968515
88   0.498169  8.014239  0.499390  7.993807
89   0.504272  7.915040  0.496216  8.042574
90   0.503052  7.932739  0.503540  7.924117
91   0.499268  7.991450  0.503296  7.926469
92   0.496094  8.040588  0.499878  7.979580
93   0.506714  7.869979  0.492310  8.099021
94   0.499512  7.983663  0.504028  7.911143
95   0.511475  7.791972  0.495483  8.046473
96   0.506470  7.870941  0.496948  8.022373
97   0.502930  7.926698  0.494385  8.062629
98   0.504883  7.895011  0.492798  8.087437
99   0.495117  8.050261  0.503784  7.911903
100  0.509521  7.820281  0.493652  8.073131
101  0.501953  7.940679  0.505981  7.876351
102  0.505127  7.889886  0.498657  7.992950
103  0.496094  8.033755  0.508301  7.839090
104  0.496460  8.027816  0.486816  8.181517
105  0.508667  7.833137  0.493530  8.074425
106  0.496094  8.033537  0.490234  8.126932
107  0.494995  8.051022  0.500122  7.969276
108  0.502930  7.924508  0.495117  8.049051
109  0.500366  7.965365  0.498779  7.990661
110  0.506714  7.864163  0.493774  8.070447
111  0.492554  8.089906  0.502930  7.924488
112  0.507690  7.848590  0.496826  8.021791
113  0.495239  8.047090  0.510010  7.811613
114  0.503540  7.914756  0.497070  8.017899
115  0.504639  7.897241  0.504395  7.901133
116  0.498047  8.002330  0.506348  7.869996
117  0.508179  7.840804  0.496460  8.027629
118  0.503662  7.912810  0.495605  8.041252
119  0.499146  7.984815  0.501831  7.942001
120  0.504395  7.974878  0.497192  8.160100
121  0.501221  8.095131  0.490479  8.268232
122  0.504150  8.047823  0.488525  8.299630
123  0.495117  8.193349  0.497070  8.161835
124  0.501953  8.083091  0.491699  8.248322
125  0.487793  8.311233  0.506714  8.006212
126  0.504883  8.035669  0.490356  8.269748
127  0.497437  8.155567  0.498413  8.139759

2018-02-25 12:01:29.237425 Finish.
Total elapsed time: 15:34:42.24.
