2018-02-24 20:26:40.647287: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:40.647507: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:40.647520: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:17.920481 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit02, unit03, unit05.
Validation set: unit07, unit10.
Test set: unit01.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.506592  7.965429  0.501831  8.040888
1    0.484375  8.321188  0.500000  8.068356
2    0.500732  8.055701  0.509521  7.913235
3    0.494873  8.148633  0.484497  8.315200
4    0.495728  8.133588  0.498779  8.083827
5    0.498657  8.085286  0.493164  8.173276
6    0.501953  8.031247  0.498413  8.087897
7    0.500732  8.050153  0.502197  8.025741
8    0.494507  8.149857  0.495850  8.127597
9    0.500732  8.048987  0.504150  7.993071
10   0.495117  8.139069  0.505005  7.979515
11   0.494629  8.146601  0.500244  8.054338
12   0.507324  7.941713  0.495850  8.125362
13   0.509155  7.963136  0.501465  8.093402
14   0.488892  8.278863  0.502686  8.044374
15   0.497925  8.114282  0.503906  8.012425
16   0.500610  8.062003  0.499512  8.076758
17   0.506836  7.956661  0.491577  8.200866
18   0.507080  7.949747  0.500977  8.047056
19   0.507690  7.938067  0.500854  8.047580
20   0.510010  7.899526  0.497192  8.105696
21   0.508423  7.924376  0.513062  7.849346
22   0.507812  7.933760  0.495117  8.138223
23   0.500122  8.057439  0.500854  8.045537
24   0.500732  8.047437  0.498047  8.090667
25   0.501709  8.031603  0.502319  8.021735
26   0.502563  8.017779  0.506348  7.956769
27   0.504639  7.984305  0.493652  8.161375
28   0.506958  7.946908  0.510254  7.893781
29   0.503662  8.000026  0.506836  7.948868
30   0.498901  8.076757  0.503662  8.000023
31   0.506348  7.956736  0.496948  8.108237
32   0.501709  8.031502  0.499268  8.069247
33   0.492065  8.268582  0.501221  8.145172
34   0.501953  8.116636  0.508179  8.003019
35   0.496216  8.182715  0.498779  8.131782
36   0.501465  8.080456  0.503052  8.047117
37   0.510010  7.929037  0.501831  8.052543
38   0.505859  7.982030  0.496582  8.123818
39   0.493652  8.164849  0.500122  8.056161
40   0.483032  8.323426  0.492065  8.174331
41   0.503296  7.990513  0.505127  7.956634
42   0.503296  7.981412  0.500610  8.019894
43   0.494995  8.105338  0.498901  8.039064
44   0.500122  8.015850  0.497803  8.049150
45   0.492920  8.123560  0.496338  8.065716
46   0.496704  8.056765  0.499268  8.012869
47   0.500244  7.994511  0.493774  8.094953
48   0.494385  8.082761  0.496704  8.043414
49   0.508423  7.854452  0.498291  8.013931
50   0.510864  7.811664  0.506104  7.885831
51   0.503418  7.927127  0.495972  8.044407
52   0.497803  8.013980  0.496460  8.034231
53   0.499634  7.982652  0.501465  7.952552
54   0.506470  7.872007  0.492554  8.093167
55   0.501099  7.956376  0.502075  7.940295
56   0.495483  8.044975  0.497803  8.007633
57   0.490479  8.124115  0.498291  7.999314
58   0.501831  7.942688  0.500122  7.969767
59   0.496826  8.022190  0.499146  7.985110
60   0.497803  8.006444  0.496338  8.029734
61   0.498291  7.998554  0.502197  7.936244
62   0.497559  8.010172  0.494507  8.058805
63   0.506470  7.868076  0.503906  7.908935
64   0.503052  7.922552  0.507568  7.850542
65   0.499390  7.980927  0.509399  7.821346
66   0.507202  7.856375  0.500122  7.969247
67   0.495483  8.043198  0.504395  7.901133
68   0.502563  7.930324  0.492188  8.095742
69   0.500000  8.177056  0.494995  8.305791
70   0.489380  8.364960  0.505493  8.082381
71   0.502441  8.118113  0.505737  8.054018
72   0.506348  8.036671  0.494019  8.228950
73   0.495972  8.192678  0.507568  8.001506
74   0.494507  8.208660  0.506958  8.004890
75   0.498657  8.136111  0.508179  7.980237
76   0.509766  7.952563  0.501343  8.086325
77   0.490234  8.263570  0.500244  8.100490
78   0.497437  8.144133  0.492188  8.227164
79   0.494995  8.180428  0.491821  8.230124
80   0.498535  8.120517  0.500122  8.093560
81   0.500000  8.094201  0.495605  8.163715
82   0.516479  7.825993  0.499390  8.100181
83   0.502686  8.045831  0.501099  8.070186
84   0.490601  8.238211  0.500366  8.079629
85   0.502075  8.050942  0.501831  8.053741
86   0.504395  8.011325  0.495239  8.157799
87   0.501831  8.050499  0.495361  8.153734
88   0.498413  8.103541  0.497803  8.112382
89   0.497314  8.119298  0.498047  8.106548
90   0.495972  8.139094  0.500488  8.065404
91   0.500488  8.064557  0.495728  8.140456
92   0.502930  8.023581  0.496948  8.119213
93   0.509888  7.909922  0.495239  8.145310
94   0.495850  8.134801  0.503296  8.014125
95   0.496704  8.119761  0.500000  8.066042
96   0.506226  7.965147  0.503662  8.005931
97   0.502075  8.031017  0.504517  7.991191
98   0.504028  7.998627  0.498047  8.094618
99   0.505981  7.966349  0.499146  8.076168
100  0.497681  8.099451  0.496582  8.116846
101  0.499146  8.075250  0.499023  8.076952
102  0.490967  8.206576  0.499634  8.066658
103  0.492432  8.182549  0.510620  7.889202
104  0.493652  8.162531  0.498413  8.085647
105  0.505249  7.975337  0.506836  7.949639
106  0.497314  8.103004  0.499512  8.067493
107  0.493164  8.169725  0.497314  8.102753
108  0.503418  8.004314  0.497437  8.100666
109  0.504883  7.980598  0.501831  8.029744
110  0.492798  8.175306  0.503296  8.006065
111  0.489868  8.222469  0.495361  8.133907
112  0.506836  7.948940  0.497681  8.095513
113  0.493042  8.228691  0.492432  8.247810
114  0.499023  8.141430  0.511719  7.937700
115  0.495728  8.191236  0.492798  8.236488
116  0.488892  8.297243  0.501831  8.089384
117  0.495483  8.188938  0.488281  8.302060
118  0.496338  8.171851  0.503418  8.057155
119  0.499878  8.111701  0.497314  8.150623
120  0.494751  8.189480  0.498047  8.134869
121  0.506348  8.000407  0.483643  8.360199
122  0.511108  7.920092  0.497925  8.127981
123  0.488403  8.277440  0.501831  8.060985
124  0.506226  7.988501  0.493408  8.190371
125  0.497070  8.129488  0.499878  8.082187
126  0.482178  8.361809  0.496338  8.133464
127  0.496094  8.134748  0.497314  8.112646

2018-02-25 12:12:46.086074 Finish.
Total elapsed time: 15:46:29.09.
