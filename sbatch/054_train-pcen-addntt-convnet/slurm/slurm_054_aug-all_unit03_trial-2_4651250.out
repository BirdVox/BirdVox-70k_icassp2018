2018-02-24 20:28:00.783126: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:28:00.783462: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:28:00.783475: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:28:00.783481: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:28:00.783486: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:49.841958 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.967163  0.143813  0.824951  0.542148
1    0.974365  0.124542  0.853394  0.410324
2    0.972046  0.115187  0.841064  0.438395
3    0.974731  0.106038  0.851196  0.710983
4    0.976562  0.101492  0.839111  0.415356
5    0.975952  0.099557  0.870361  0.413257
6    0.972290  0.107693  0.856689  0.402845
7    0.977417  0.091604  0.842163  0.473100
8    0.974854  0.101501  0.766724  0.533777
9    0.975708  0.102075  0.842651  0.471079
10   0.882446  1.744845  0.584717  6.744299
11   0.517700  7.748677  0.494629  8.113056
12   0.504395  7.954016  0.502075  7.987945
13   0.495850  8.084653  0.501587  7.990804
14   0.504150  7.947855  0.499390  8.021771
15   0.494141  8.103675  0.502319  7.971575
16   0.492065  8.133484  0.503052  7.956819
17   0.496704  8.056611  0.502197  7.967667
18   0.489014  8.176563  0.505859  7.906747
19   0.503906  7.936700  0.498535  8.021163
20   0.498291  8.023950  0.496094  8.057890
21   0.498169  8.023767  0.492188  8.118099
22   0.501099  7.975052  0.506958  7.880668
23   0.501953  7.959527  0.498779  8.009203
24   0.490356  8.142599  0.508667  7.849809
25   0.505249  7.903460  0.501099  7.968795
26   0.497681  8.022489  0.488525  8.167658
27   0.497437  8.024840  0.500122  7.981282
28   0.505371  7.896892  0.494751  8.065503
29   0.488281  8.167982  0.505737  7.889038
30   0.490479  8.131683  0.504395  7.909223
31   0.503662  7.920331  0.491821  8.108545
32   0.507812  7.853087  0.497681  8.014104
33   0.512207  7.782048  0.495361  8.050150
34   0.489868  8.137301  0.500000  7.975366
35   0.500854  7.961368  0.515015  7.735259
36   0.494751  8.057982  0.499268  7.985660
37   0.500610  7.963969  0.501953  7.942289
38   0.491821  8.103572  0.494385  8.062473
39   0.500610  7.963018  0.500732  7.960878
40   0.499634  7.978224  0.504395  7.902167
41   0.514404  7.746462  0.511597  7.927630
42   0.494629  8.218553  0.496826  8.169756
43   0.509521  7.957872  0.504028  8.040404
44   0.495605  8.171775  0.502075  8.063582
45   0.501465  8.070282  0.495728  8.159875
46   0.503784  8.027605  0.499390  8.096186
47   0.501953  8.052936  0.505859  7.988154
48   0.507080  7.966891  0.498901  8.097211
49   0.498901  8.095883  0.489258  8.250054
50   0.503784  8.014791  0.496704  8.127833
51   0.497314  8.117033  0.506592  7.966578
52   0.500854  8.058222  0.502441  8.031847
53   0.495605  8.141308  0.503784  8.008789
54   0.497070  8.116374  0.508301  7.934752
55   0.491455  8.205719  0.506836  7.957274
56   0.504517  7.994168  0.498779  8.086168
57   0.500610  8.056221  0.494263  8.158111
58   0.502808  8.019996  0.495117  8.143575
59   0.504150  7.997631  0.498413  8.089768
60   0.500610  8.054042  0.498779  8.083252
61   0.503174  8.012141  0.499512  8.070895
62   0.497681  8.100156  0.502441  8.023177
63   0.506104  7.963923  0.504883  7.983378
64   0.494141  8.156316  0.494995  8.142344
65   0.499390  8.071328  0.491089  8.204941
66   0.506714  7.952930  0.492554  8.181004
67   0.494995  8.141505  0.493286  8.168907
68   0.493652  8.162872  0.505005  7.979762
69   0.499512  8.068185  0.502930  8.012981
70   0.492310  8.184054  0.497192  8.105253
71   0.503296  8.006786  0.497314  8.103110
72   0.501709  8.032201  0.496460  8.116731
73   0.497314  8.102892  0.494751  8.144148
74   0.487793  8.256242  0.506104  7.961059
75   0.496582  8.114481  0.490234  8.216750
76   0.502075  8.025860  0.501831  8.029760
77   0.506470  7.954963  0.502930  8.011994
78   0.497070  8.106412  0.497437  8.100487
79   0.501709  8.031605  0.508789  7.917471
80   0.496948  8.108308  0.505127  7.976470
81   0.500732  8.047291  0.502563  8.017769
82   0.493530  8.163360  0.510742  7.885930
83   0.498169  8.088582  0.504639  7.984297
84   0.506592  7.952813  0.506470  7.954778
85   0.505127  7.976419  0.515625  7.807208
86   0.511597  7.872136  0.498169  8.088564
87   0.507324  7.940998  0.501221  8.039374
88   0.509888  7.901082  0.496704  8.134588
89   0.503418  8.004001  0.500732  8.033112
90   0.493286  8.144747  0.496338  8.090355
91   0.506958  7.917016  0.495728  8.092516
92   0.497314  8.064426  0.501587  7.993756
93   0.493896  8.114212  0.495483  8.086897
94   0.507935  7.886634  0.490601  8.161300
95   0.508789  7.869825  0.489136  8.181694
96   0.502808  7.962406  0.503174  7.955281
97   0.500244  8.000798  0.498779  8.022990
98   0.505981  7.907088  0.501099  7.983870
99   0.501099  7.982873  0.490112  8.157041
100  0.496460  8.054917  0.498413  8.022866
101  0.494507  8.084273  0.496460  8.052279
102  0.508911  7.852958  0.501343  7.972806
103  0.506470  7.890293  0.491333  8.130838
104  0.500122  7.989979  0.496460  8.047626
105  0.489624  8.155897  0.504761  7.913875
106  0.495605  8.059150  0.502563  7.947544
107  0.488770  8.166795  0.502075  7.954016
108  0.505615  7.896946  0.501831  7.956644
109  0.499756  7.989117  0.498169  8.013809
110  0.501343  7.962624  0.496582  8.037939
111  0.489136  8.156088  0.509399  7.832480
112  0.506714  7.874759  0.495483  8.053270
113  0.501587  7.955459  0.502930  7.933552
114  0.495361  8.053734  0.498169  8.008505
115  0.503296  7.926327  0.495483  8.050442
116  0.492065  8.104523  0.498779  7.997088
117  0.509155  7.831296  0.504272  7.908774
118  0.493042  8.087475  0.499512  7.984002
119  0.494385  8.065432  0.495605  8.045673
120  0.498413  8.000639  0.484131  8.228066
121  0.503784  7.914501  0.508057  7.846151
122  0.493530  8.077519  0.494507  8.061739
123  0.504272  7.905859  0.496704  8.026330
124  0.514282  7.745921  0.497437  8.014315
125  0.501587  7.947995  0.502197  7.938116
126  0.497437  8.013877  0.492798  8.087696
127  0.505859  7.879342  0.497803  8.007666

2018-02-25 12:09:40.267425 Finish.
Total elapsed time: 15:42:51.27.
