2018-02-24 20:26:34.726426: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:34.726709: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:34.726720: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:17.761743 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit10, unit01, unit02.
Validation set: unit03, unit05.
Test set: unit07.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.776367  1.371535  0.910156  0.454044
1    0.857178  0.428069  0.867432  0.539373
2    0.885376  0.366211  0.945312  0.221982
3    0.885864  0.361459  0.922241  0.319653
4    0.874878  0.375985  0.895386  0.424764
5    0.834839  1.485289  0.502686  8.074577
6    0.508057  7.980644  0.505005  8.023468
7    0.500610  8.089507  0.498291  8.122571
8    0.501343  8.069867  0.493530  8.192550
9    0.511841  7.894696  0.503906  8.020048
10   0.502930  8.033616  0.505859  7.984355
11   0.510986  7.899954  0.496094  8.138329
12   0.491577  8.209676  0.494995  8.153212
13   0.494141  8.165782  0.496216  8.131195
14   0.496582  8.124291  0.502319  8.030868
15   0.498047  8.098900  0.488770  8.247643
16   0.497314  8.109222  0.505859  7.970837
17   0.497559  8.104054  0.498413  8.089736
18   0.496582  8.118772  0.502197  8.027813
19   0.497559  8.102185  0.512085  7.867674
20   0.499512  8.070004  0.503540  8.004767
21   0.497803  8.096972  0.501709  8.033756
22   0.499878  8.063047  0.502075  8.027421
23   0.494141  8.155128  0.506470  7.956233
24   0.496094  8.123323  0.505981  7.963809
25   0.498047  8.091575  0.498047  8.091457
26   0.494507  8.148414  0.502808  8.014525
27   0.492676  8.177747  0.497803  8.095031
28   0.498535  8.083158  0.491455  8.197211
29   0.500488  8.051558  0.509399  7.907876
30   0.494507  8.147873  0.502075  8.025844
31   0.498535  8.082868  0.488647  8.242207
32   0.501831  8.029686  0.502930  8.011954
33   0.497681  8.096537  0.501953  8.027655
34   0.498535  8.102027  0.498047  8.139051
35   0.503174  8.032578  0.502930  8.024538
36   0.504639  7.993166  0.504639  7.990279
37   0.493164  8.171622  0.494873  8.143089
38   0.505981  7.965072  0.503418  8.005105
39   0.503174  8.008274  0.499512  8.065952
40   0.498047  8.088626  0.499878  8.058749
41   0.512085  7.863448  0.509644  7.901662
42   0.497925  8.087759  0.495605  8.123986
43   0.502197  8.018124  0.497925  8.085440
44   0.497681  8.088508  0.497437  8.091551
45   0.500000  8.049805  0.501221  8.029440
46   0.493164  8.156948  0.506714  7.939972
47   0.500122  8.044072  0.502197  8.009972
48   0.493408  8.149045  0.503174  7.992285
49   0.493164  8.150763  0.500366  8.034816
50   0.506836  7.930518  0.494629  8.123945
51   0.500610  8.027379  0.494385  8.125395
52   0.500122  8.032672  0.492554  8.152046
53   0.501831  8.002839  0.494751  8.114384
54   0.501709  8.002110  0.492798  8.142804
55   0.502930  7.979894  0.500366  8.019355
56   0.496582  8.078267  0.501221  8.002877
57   0.504028  7.956673  0.510132  7.857905
58   0.500000  8.017966  0.504395  7.946427
59   0.499023  8.030577  0.502808  7.968757
60   0.484741  8.255297  0.502563  7.969674
61   0.499878  8.011009  0.489258  8.178833
62   0.496948  8.054763  0.499146  8.018262
63   0.509644  7.849455  0.503052  7.953098
64   0.502075  7.967254  0.500000  7.998927
65   0.506592  7.892467  0.501099  7.978674
66   0.500488  7.987083  0.498413  8.018853
67   0.498291  8.019536  0.495728  8.059152
68   0.491211  8.129959  0.503052  7.940005
69   0.510620  7.818221  0.511841  7.797652
70   0.502808  7.940614  0.500854  7.970723
71   0.495605  8.053438  0.496704  8.034977
72   0.499878  7.983496  0.502808  7.935929
73   0.509033  7.835881  0.499878  7.981063
74   0.495728  8.046519  0.489746  8.141189
75   0.503052  7.928438  0.504272  7.908373
76   0.503296  7.923397  0.494995  8.055209
77   0.482666  8.251298  0.498047  8.005646
78   0.503662  7.915734  0.503784  7.913416
79   0.498657  7.994828  0.504517  7.901111
80   0.491089  8.114919  0.499512  7.980394
81   0.491455  8.108629  0.500000  7.972210
82   0.501709  7.944805  0.499023  7.987473
83   0.502075  7.938700  0.500122  7.969729
84   0.495728  8.039701  0.503540  7.915073
85   0.505493  7.883874  0.500244  7.967501
86   0.504272  7.903238  0.499146  7.984938
87   0.508545  7.835061  0.504272  7.903151
88   0.505737  7.879781  0.496460  8.027669
89   0.491333  8.109395  0.500000  7.971214
90   0.508179  7.840820  0.496094  8.033478
91   0.505249  7.887518  0.496704  8.023742
92   0.508179  7.840808  0.500000  7.971195
93   0.503540  7.914757  0.497681  8.008169
94   0.492798  8.086012  0.487671  8.167748
95   0.490112  8.128826  0.508789  7.831074
96   0.502563  7.930324  0.506592  7.866103
97   0.500122  7.969246  0.493530  8.074335
98   0.503174  7.920596  0.495605  8.041267
99   0.509888  7.828866  0.502075  8.164997
100  0.494019  8.271369  0.499634  8.175387
101  0.497314  8.211868  0.507080  8.053667
102  0.501465  8.143405  0.501587  8.140646
103  0.486816  8.377892  0.506958  8.052388
104  0.498047  8.195121  0.495483  8.235510
105  0.491211  8.303402  0.501709  8.133188
106  0.500488  8.151816  0.498779  8.178281
107  0.495850  8.224377  0.506104  8.057944
108  0.497681  8.192501  0.494629  8.240450
109  0.507324  8.034544  0.495361  8.226046
110  0.495850  8.216815  0.495483  8.221320
111  0.499023  8.162824  0.498291  8.173153
112  0.496704  8.197216  0.504517  8.069741
113  0.499268  8.152755  0.491821  8.271147
114  0.501343  8.116015  0.500000  8.135958
115  0.502930  8.087002  0.496460  8.189511
116  0.498047  8.162133  0.504517  8.056017
117  0.497070  8.174176  0.497314  8.168346
118  0.504639  8.048377  0.521118  7.780812
119  0.502441  8.079884  0.501831  8.087732
120  0.508911  7.971617  0.495361  8.187992
121  0.509399  7.959704  0.496460  8.166223
122  0.506470  8.002856  0.506836  7.994910
123  0.507324  7.985020  0.494385  8.191550
124  0.503784  8.038055  0.506714  7.988837
125  0.495117  8.173805  0.500244  8.089223
126  0.503418  8.036182  0.487061  8.297959
127  0.503174  8.036440  0.490479  8.239278

2018-02-25 12:09:24.881884 Finish.
Total elapsed time: 15:43:07.88.
