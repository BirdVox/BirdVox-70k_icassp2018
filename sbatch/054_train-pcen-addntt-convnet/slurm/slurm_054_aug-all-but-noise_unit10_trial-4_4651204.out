2018-02-24 20:26:31.252351: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:31.252731: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:31.252745: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:17.715295 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit01, unit02, unit03.
Validation set: unit05, unit07.
Test set: unit10.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.898193  0.299505  0.859741  0.376881
1    0.652710  5.101815  0.506836  7.930103
2    0.496338  8.093473  0.499268  8.042147
3    0.499878  8.028512  0.502930  7.976184
4    0.503906  7.957376  0.511597  7.831673
5    0.501831  7.984561  0.489014  8.186198
6    0.508057  7.880132  0.496460  8.062606
7    0.490601  8.153796  0.507812  7.877233
8    0.503784  7.939447  0.494751  8.081501
9    0.500000  7.996001  0.499512  8.002015
10   0.500244  7.988696  0.500610  7.981259
11   0.511353  7.808526  0.494019  8.083436
12   0.499268  7.998435  0.493896  8.082785
13   0.499512  7.992096  0.507202  7.868363
14   0.497925  8.015240  0.494385  8.070689
15   0.504639  7.906326  0.500854  7.965799
16   0.498779  7.998115  0.499146  7.991543
17   0.496216  8.037596  0.501831  7.947453
18   0.501953  7.944956  0.500610  7.965840
19   0.499634  7.980949  0.499878  7.976622
20   0.502563  7.933428  0.502686  7.931124
21   0.495728  8.041740  0.495361  8.047285
22   0.505859  7.879669  0.493408  8.077933
23   0.503174  7.922042  0.506958  7.861523
24   0.493530  8.075431  0.495850  8.038305
25   0.513184  7.761831  0.497681  8.008866
26   0.481567  8.265651  0.506348  7.870502
27   0.500610  7.961892  0.492065  8.098048
28   0.503174  7.920896  0.492310  8.094045
29   0.499878  7.973345  0.504761  7.895463
30   0.497437  8.012198  0.497192  8.016062
31   0.493042  8.082208  0.496948  8.019914
32   0.507446  7.852536  0.492676  8.088000
33   0.499268  7.982902  0.495361  8.045169
34   0.508301  7.857265  0.504883  7.951106
35   0.502563  7.977833  0.498535  8.031647
36   0.491943  8.131619  0.495239  8.074973
37   0.492798  8.111055  0.493652  8.094949
38   0.504639  7.917862  0.495728  8.058162
39   0.499512  7.996368  0.491211  8.127338
40   0.502319  7.949073  0.500366  7.979107
41   0.505005  7.904188  0.484253  8.234105
42   0.503296  7.929700  0.494629  8.067094
43   0.514771  7.745295  0.494385  8.069628
44   0.506714  7.872476  0.501587  7.953641
45   0.493408  8.083517  0.505127  7.896200
46   0.506470  7.874351  0.499268  7.988746
47   0.500366  7.970851  0.501465  7.952971
48   0.495850  8.042164  0.496826  8.026281
49   0.502319  7.938425  0.511841  7.786360
50   0.497925  8.007974  0.495728  8.042772
51   0.508057  7.846011  0.498291  8.001501
52   0.499878  7.976025  0.491577  8.108190
53   0.513428  7.759689  0.493774  8.072865
54   0.504761  7.897588  0.500366  7.967524
55   0.493286  8.080286  0.501587  7.947845
56   0.505493  7.885475  0.498901  7.990471
57   0.495972  8.039852  0.507935  7.977799
58   0.491211  8.293829  0.495117  8.214000
59   0.492920  8.240647  0.500366  8.113677
60   0.489868  8.278103  0.496094  8.173567
61   0.493896  8.205676  0.497925  8.137714
62   0.483643  8.365355  0.492798  8.215380
63   0.491577  8.232940  0.493530  8.199439
64   0.495117  8.172043  0.499634  8.097492
65   0.503296  8.036864  0.500366  8.082533
66   0.499023  8.102744  0.491943  8.215468
67   0.502930  8.037098  0.496704  8.136184
68   0.502930  8.034668  0.493774  8.181091
69   0.500610  8.069846  0.510498  7.909438
70   0.495239  8.154417  0.501465  8.053132
71   0.498169  8.105382  0.490601  8.226519
72   0.505127  7.991593  0.501343  8.051819
73   0.506226  7.972407  0.496460  8.129119
74   0.505859  7.976981  0.498291  8.098349
75   0.501465  8.046623  0.491455  8.207406
76   0.505859  7.974727  0.496460  8.125733
77   0.500854  8.054446  0.501831  8.038265
78   0.499023  8.083112  0.502930  8.019757
79   0.498779  8.086291  0.494751  8.150868
80   0.504028  8.001011  0.502441  8.026274
81   0.499878  8.067301  0.499878  8.067018
82   0.509521  7.911319  0.496704  8.117655
83   0.501099  8.046585  0.492065  8.191949
84   0.504150  7.996944  0.493530  8.167905
85   0.504395  7.992592  0.507690  7.939270
86   0.500610  8.053199  0.493774  8.163196
87   0.495605  8.133507  0.500000  8.062502
88   0.502563  8.021017  0.497437  8.103490
89   0.499146  8.075788  0.504761  7.985126
90   0.491211  8.203375  0.491577  8.197326
91   0.501953  8.029945  0.504272  7.992423
92   0.493164  8.171337  0.504517  7.988226
93   0.501831  8.031387  0.495239  8.137512
94   0.500977  8.044922  0.500244  8.056614
95   0.507690  7.936486  0.504639  7.985569
96   0.502441  8.020886  0.504395  7.989309
97   0.500610  8.050213  0.501099  8.042255
98   0.507690  7.935927  0.507202  7.943718
99   0.490723  8.209264  0.502197  8.024245
100  0.505127  7.976962  0.492188  8.185461
101  0.501953  8.028003  0.494629  8.146003
102  0.493164  8.169568  0.503906  7.996380
103  0.497070  8.106525  0.501099  8.041560
104  0.503662  8.000211  0.497803  8.094624
105  0.504272  7.990320  0.506592  7.952914
106  0.499634  8.065046  0.500732  8.047321
107  0.493286  8.167327  0.504883  7.980398
108  0.506226  7.958745  0.493164  8.169263
109  0.500854  8.045301  0.500610  8.049230
110  0.494629  8.145635  0.509033  7.913461
111  0.497803  8.094472  0.499756  8.062990
112  0.501709  8.031507  0.494263  8.151526
113  0.497925  8.092498  0.503052  8.009861
114  0.496948  8.108237  0.510132  7.895743
115  0.497314  8.102334  0.500122  8.057080
116  0.500977  8.043308  0.493530  8.163328
117  0.498291  8.086593  0.501709  8.031502
118  0.494995  8.112854  0.500488  8.012553
119  0.494873  8.100019  0.497314  8.059307
120  0.495850  8.081230  0.489502  8.181098
121  0.509277  7.864670  0.494263  8.102926
122  0.505981  7.915078  0.503052  7.960784
123  0.500977  7.992921  0.502197  7.972522
124  0.502808  7.961885  0.503906  7.943464
125  0.501099  7.987337  0.494507  8.091535
126  0.499268  8.014755  0.479370  8.331079
127  0.504150  7.935139  0.502808  7.955655

2018-02-25 12:19:22.229392 Finish.
Total elapsed time: 15:53:05.23.
