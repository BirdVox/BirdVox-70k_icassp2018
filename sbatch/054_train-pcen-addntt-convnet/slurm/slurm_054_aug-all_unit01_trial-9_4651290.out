2018-02-24 20:27:48.165938: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:48.166226: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:48.166244: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:48.166252: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:48.166259: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:27:08.405250 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit02, unit03, unit05.
Validation set: unit07, unit10.
Test set: unit01.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.495117  8.141839  0.498535  8.085721
1    0.491333  8.201093  0.499634  8.066699
2    0.501587  8.034803  0.496460  8.117090
3    0.496216  8.120785  0.501099  8.040940
4    0.481567  8.406545  0.494507  8.187194
5    0.501343  8.069033  0.496338  8.144147
6    0.504883  8.003357  0.497070  8.126796
7    0.499756  8.081751  0.502319  8.038873
8    0.507935  7.947109  0.503418  8.018744
9    0.504272  8.003976  0.500488  8.064028
10   0.490356  8.226504  0.499390  8.080113
11   0.510742  7.896417  0.492676  8.186925
12   0.511719  7.879359  0.500244  8.063697
13   0.503052  8.017877  0.508789  7.924849
14   0.505371  7.979422  0.506714  7.957271
15   0.493652  8.167320  0.508301  7.930745
16   0.506470  7.959816  0.495850  8.130558
17   0.505127  7.980618  0.502563  8.021538
18   0.501343  8.040845  0.496338  8.121155
19   0.502197  8.026384  0.497925  8.094931
20   0.500122  8.059228  0.494995  8.141591
21   0.510620  7.889503  0.496948  8.109638
22   0.510376  7.893009  0.502686  8.016777
23   0.495605  8.130736  0.503296  8.006634
24   0.494873  8.142273  0.504761  7.982791
25   0.498047  8.090917  0.496216  8.120350
26   0.486328  8.279658  0.496704  8.112361
27   0.499390  8.069033  0.497314  8.102444
28   0.497192  8.104385  0.504028  7.994180
29   0.504028  7.994164  0.501465  8.035470
30   0.500610  8.049233  0.508179  7.927238
31   0.505981  7.962649  0.500610  8.049217
32   0.500488  8.051182  0.496826  8.110207
33   0.493286  8.152718  0.505249  8.066031
34   0.493896  8.246248  0.503052  8.093614
35   0.494873  8.219001  0.511230  7.953629
36   0.494629  8.214288  0.498047  8.155936
37   0.494141  8.214649  0.499878  8.119711
38   0.507568  7.993860  0.495850  8.177510
39   0.507324  7.991613  0.488892  8.282582
40   0.501953  8.071652  0.495850  8.166324
41   0.502319  8.060737  0.504028  8.031105
42   0.497681  8.130083  0.494507  8.178518
43   0.493164  8.197905  0.499268  8.098627
44   0.490601  8.234953  0.502197  8.048262
45   0.499756  8.085468  0.504028  8.015665
46   0.500366  8.072437  0.497559  8.115593
47   0.490845  8.221080  0.500854  8.059953
48   0.503662  8.013675  0.508301  7.938198
49   0.501099  8.051502  0.496704  8.120031
50   0.507690  7.943349  0.502808  8.019637
51   0.507935  7.936332  0.485229  8.296710
52   0.491211  8.199737  0.502197  8.022944
53   0.509033  7.912295  0.501099  8.037091
54   0.501465  8.029528  0.494507  8.138698
55   0.503662  7.990959  0.499512  8.055311
56   0.493408  8.150777  0.513916  7.821961
57   0.497314  8.084738  0.497437  8.080867
58   0.500000  8.038058  0.500610  8.026357
59   0.500000  8.034105  0.501709  8.004849
60   0.500366  8.024243  0.496826  8.078643
61   0.498169  8.055206  0.503540  7.967528
62   0.499756  8.025825  0.497681  8.056863
63   0.503052  7.969219  0.505615  7.926328
64   0.508667  7.875694  0.506226  7.912635
65   0.495361  8.083909  0.490723  8.155940
66   0.495605  8.076238  0.495117  8.082177
67   0.489014  8.177709  0.495972  8.065027
68   0.515137  7.757813  0.497559  8.036395
69   0.496704  8.048444  0.505615  7.904829
70   0.505615  7.903363  0.501343  7.970035
71   0.498779  8.009544  0.504150  7.922584
72   0.496704  8.040044  0.492432  8.106931
73   0.494507  8.072701  0.496704  8.036550
74   0.501343  7.961555  0.495728  8.050057
75   0.499390  7.990730  0.498779  7.999543
76   0.503174  7.928640  0.496338  8.036804
77   0.500488  7.969891  0.504883  7.899114
78   0.495850  8.042477  0.494629  8.061318
79   0.494751  8.058820  0.501465  7.951261
80   0.498779  7.993617  0.508667  7.835552
81   0.506836  7.864372  0.498047  8.004145
82   0.496582  8.027207  0.504639  7.898497
83   0.501099  7.954714  0.497681  8.009004
84   0.502808  7.927108  0.506958  7.860798
85   0.501953  7.940477  0.504028  7.907295
86   0.501099  7.953928  0.493774  8.070630
87   0.501831  7.942142  0.505493  7.883720
88   0.498169  8.000458  0.502930  7.924538
89   0.500366  7.965391  0.503296  7.918672
90   0.510620  7.801899  0.503052  7.922551
91   0.501099  7.953685  0.501099  7.953682
92   0.498047  8.002333  0.501953  7.940057
93   0.493896  8.068498  0.491333  8.109366
94   0.496094  8.033467  0.512207  7.776583
95   0.497559  8.010377  0.505981  7.968231
96   0.495239  8.194411  0.503052  8.070266
97   0.502319  8.080979  0.494019  8.213652
98   0.490723  8.265635  0.508545  7.977208
99   0.506714  8.005535  0.503418  8.057449
100  0.496948  8.160505  0.500610  8.100234
101  0.505249  8.024211  0.500854  8.093768
102  0.508545  7.968533  0.509521  7.951495
103  0.500732  8.091861  0.509033  7.956756
104  0.506104  8.002669  0.496582  8.154818
105  0.500366  8.092513  0.497314  8.140382
106  0.490479  8.249259  0.493896  8.192855
107  0.493774  8.193529  0.506226  7.991542
108  0.496338  8.149638  0.500366  8.083433
109  0.492188  8.214009  0.498901  8.104547
110  0.495361  8.160390  0.498413  8.109988
111  0.493774  8.183578  0.504517  8.009264
112  0.501343  8.059288  0.497192  8.125061
113  0.502075  8.045279  0.499634  8.083558
114  0.506226  7.976285  0.493896  8.173992
115  0.490845  8.222214  0.483887  8.333410
116  0.495239  8.149524  0.505615  7.981393
117  0.502686  8.027774  0.499878  8.072203
118  0.497925  8.102910  0.493286  8.176919
119  0.495728  8.136861  0.498047  8.098788
120  0.502197  8.031252  0.496216  8.127038
121  0.503662  8.006444  0.496948  8.114101
122  0.504639  7.989636  0.500122  8.061942
123  0.506958  7.951312  0.500488  8.055160
124  0.507568  7.940653  0.496338  8.121294
125  0.498047  8.093414  0.503174  8.010459
126  0.492920  8.175451  0.504150  7.994169
127  0.498413  8.086408  0.503418  8.005517

2018-02-25 11:57:05.032929 Finish.
Total elapsed time: 15:29:57.03.
