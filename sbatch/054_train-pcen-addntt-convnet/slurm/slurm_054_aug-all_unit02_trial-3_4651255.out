2018-02-24 20:27:40.643285: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:40.643506: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:40.643526: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:40.643536: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:40.643546: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:49.270484 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.972534  0.115315  0.765137  1.579813
1    0.968994  0.143794  0.818481  0.465760
2    0.976929  0.102232  0.802612  0.461303
3    0.975098  0.107101  0.788208  0.499499
4    0.981445  0.085861  0.817505  0.513513
5    0.934326  0.245425  0.731567  0.673466
6    0.939087  0.240425  0.823364  0.542808
7    0.840210  2.195288  0.511841  7.827246
8    0.500244  8.005284  0.504883  7.925586
9    0.501465  7.975856  0.513062  7.787242
10   0.503784  7.932225  0.498779  8.009378
11   0.495728  8.055902  0.504150  7.919675
12   0.504395  7.914175  0.501709  7.955505
13   0.504395  7.911445  0.506470  7.877204
14   0.506470  7.876223  0.504272  7.910336
15   0.498047  8.008804  0.504395  7.906876
16   0.501587  7.951010  0.505615  7.886203
17   0.500977  7.959653  0.499512  7.982537
18   0.506226  7.875100  0.493530  8.077118
19   0.497437  8.014524  0.498047  8.004495
20   0.499878  7.975050  0.505371  7.887239
21   0.506470  7.869524  0.494263  8.063946
22   0.489990  8.131903  0.506958  7.861250
23   0.489014  8.147203  0.495483  8.043946
24   0.499512  7.979630  0.489502  8.139122
25   0.503540  7.915248  0.506714  7.864582
26   0.498047  8.002698  0.498535  7.994862
27   0.508423  7.837186  0.494263  8.062894
28   0.510254  7.807924  0.515381  7.726159
29   0.500854  7.957720  0.507324  7.854555
30   0.505249  7.887621  0.496338  8.029669
31   0.488770  8.150314  0.505127  7.889525
32   0.507080  7.858378  0.499023  7.986677
33   0.512817  7.853137  0.499512  7.452291
34   0.505493  8.016839  0.505127  7.271253
35   0.501343  8.078153  0.500732  7.275140
36   0.501099  8.076980  0.501099  7.178703
37   0.497925  8.123404  0.503906  7.038643
38   0.505371  7.999004  0.497803  7.070674
39   0.498535  8.105159  0.502930  6.879230
40   0.502319  8.040500  0.497803  6.833291
41   0.503540  8.015686  0.500732  8.061261
42   0.493896  8.142404  0.494507  8.131033
43   0.505371  7.954530  0.505737  7.945449
44   0.499146  8.047483  0.504150  7.964693
45   0.494385  8.117558  0.501953  7.994133
46   0.489136  8.195872  0.507935  7.893626
47   0.506104  7.920425  0.506714  7.908350
48   0.493042  8.124110  0.508789  7.870905
49   0.501831  7.979801  0.495483  8.079004
50   0.491333  8.143291  0.501343  7.981865
51   0.494995  8.081316  0.505127  7.918073
52   0.495972  8.062405  0.502686  7.953769
53   0.498413  8.020365  0.502686  7.950758
54   0.499268  8.003835  0.494995  8.070556
55   0.498047  8.020589  0.497437  8.029027
56   0.501953  7.955806  0.501831  7.956561
57   0.491821  8.115026  0.499512  7.991333
58   0.486206  8.202446  0.498413  8.006853
59   0.498535  8.004002  0.501343  7.958365
60   0.499634  7.984812  0.498291  8.005449
61   0.489014  8.152658  0.503296  7.924300
62   0.500977  7.960682  0.497559  8.014608
63   0.506348  7.873991  0.496948  8.023368
64   0.501709  7.947059  0.506470  7.870773
65   0.499756  7.977474  0.498901  7.990784
66   0.506592  7.867913  0.497559  8.011674
67   0.501343  7.951135  0.486450  8.188363
68   0.496704  8.024729  0.499268  7.983710
69   0.485352  8.205440  0.498779  7.991255
70   0.506958  7.860773  0.494629  8.057243
71   0.497314  8.014359  0.496216  8.031811
72   0.508911  7.829367  0.503052  7.922733
73   0.507202  7.856530  0.504639  7.897366
74   0.504395  7.901233  0.505859  7.877858
75   0.497437  8.012122  0.492065  8.097735
76   0.503296  7.918684  0.502930  7.924513
77   0.500854  7.957590  0.511353  7.790220
78   0.493896  8.068508  0.500000  7.971200
79   0.491577  8.105478  0.504883  7.893352
80   0.498901  7.988710  0.490112  8.128827
81   0.497681  8.008169  0.511475  7.788260
82   0.504639  7.897241  0.501709  7.943947
83   0.502441  7.932271  0.501587  7.945893
84   0.495972  8.035413  0.496826  8.021791
85   0.498169  8.000384  0.505737  7.879726
86   0.495850  8.088219  0.501831  8.094598
87   0.503784  8.063020  0.512085  7.929112
88   0.505737  8.031301  0.494751  8.208270
89   0.501099  8.105849  0.501709  8.095890
90   0.500000  8.123301  0.503906  8.060196
91   0.503540  8.065943  0.501831  8.093326
92   0.506348  8.020360  0.500488  8.114627
93   0.496338  8.181334  0.503174  8.070953
94   0.501953  8.090417  0.506592  8.015430
95   0.499390  8.131281  0.494385  8.211706
96   0.515259  7.874997  0.508057  7.990812
97   0.500366  8.114480  0.496826  8.171241
98   0.497192  8.165023  0.502197  8.084026
99   0.489380  8.290272  0.506714  8.010523
100  0.497803  8.153775  0.495361  8.192733
101  0.497314  8.160842  0.496094  8.180091
102  0.501953  8.085204  0.497192  8.161477
103  0.496582  8.170835  0.503906  8.052286
104  0.496216  8.175725  0.499146  8.127973
105  0.492554  8.233668  0.499756  8.117016
106  0.493286  8.220709  0.504395  8.041059
107  0.502808  8.066016  0.495239  8.187365
108  0.500366  8.104072  0.493774  8.209646
109  0.502441  8.069260  0.502563  8.066585
110  0.497437  8.148497  0.509644  7.951000
111  0.501221  8.086002  0.502686  8.061615
112  0.510742  7.930963  0.503052  8.054106
113  0.502075  8.069016  0.495850  8.168511
114  0.499268  8.112552  0.505493  8.011320
115  0.500488  8.091083  0.501709  8.070480
116  0.504395  8.026248  0.504761  8.019377
117  0.488281  8.284008  0.501831  8.064602
118  0.508789  7.951424  0.502197  8.056622
119  0.502319  8.053588  0.508423  7.954124
120  0.498047  8.120264  0.497070  8.134885
121  0.497803  8.121950  0.502319  8.048004
122  0.491089  8.227867  0.489868  8.246378
123  0.500244  8.077976  0.500488  8.072869
124  0.494263  8.172053  0.498291  8.105957
125  0.507080  7.963148  0.512573  7.873461
126  0.495117  8.153702  0.491455  8.211614
127  0.511475  7.887864  0.498291  8.099293

2018-02-25 11:27:10.350459 Finish.
Total elapsed time: 15:00:21.35.
