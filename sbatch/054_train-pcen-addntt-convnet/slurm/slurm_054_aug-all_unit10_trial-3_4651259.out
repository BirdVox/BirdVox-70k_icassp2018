2018-02-24 20:27:55.224253: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:55.224556: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:55.224573: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:55.224582: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:55.224590: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:49.571795 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit01, unit02, unit03.
Validation set: unit05, unit07.
Test set: unit10.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.488037  8.286157  0.513062  7.876328
1    0.501343  8.061802  0.489746  8.245856
2    0.504150  8.011530  0.501099  8.058765
3    0.495605  8.145693  0.502930  8.026143
4    0.497192  8.121132  0.492188  8.231242
5    0.495239  8.174590  0.492188  8.218028
6    0.495605  8.159088  0.515259  7.838963
7    0.500244  8.078359  0.507690  7.955958
8    0.506592  7.971686  0.494629  8.162661
9    0.501587  8.048927  0.506226  7.972668
10   0.495850  8.138606  0.500122  8.068508
11   0.502563  8.028072  0.498047  8.099841
12   0.490234  8.224856  0.504028  8.001662
13   0.518433  7.768733  0.501709  8.037567
14   0.499756  8.068417  0.503906  8.000923
15   0.497192  8.108617  0.495483  8.135670
16   0.499756  8.066379  0.500244  8.058107
17   0.494141  8.156137  0.507446  7.941350
18   0.498901  8.078799  0.501953  8.029350
19   0.493896  8.158986  0.502319  8.023019
20   0.502075  8.026780  0.498901  8.077773
21   0.495972  8.124858  0.501953  8.028323
22   0.497559  8.099049  0.497192  8.104855
23   0.489990  8.220860  0.495728  8.128312
24   0.511230  7.878374  0.498901  8.077041
25   0.503784  7.998294  0.503540  8.002188
26   0.507812  7.933292  0.503540  8.002126
27   0.501953  8.027680  0.501221  8.039464
28   0.493774  8.159467  0.490967  8.204706
29   0.504272  7.990233  0.504883  7.980385
30   0.498901  8.076786  0.501831  8.029245
31   0.494995  8.199780  0.495117  8.212214
32   0.495239  8.198623  0.495239  8.188055
33   0.496094  8.165592  0.504517  8.023014
34   0.497803  8.122780  0.499390  8.090546
35   0.495728  8.142696  0.507202  7.953759
36   0.500244  8.059204  0.501953  8.026643
37   0.486084  8.274733  0.502563  8.007239
38   0.511475  7.860749  0.504150  7.973197
39   0.498657  8.056753  0.503540  7.974988
40   0.491089  8.169837  0.498901  8.041721
41   0.495728  8.089001  0.506836  7.908670
42   0.503784  7.954317  0.502197  7.976688
43   0.494995  8.088797  0.504639  7.932420
44   0.500244  8.000049  0.486816  8.211760
45   0.505981  7.904057  0.500366  7.991480
46   0.505249  7.911720  0.501221  7.974089
47   0.503906  7.929591  0.497070  8.036948
48   0.505615  7.899253  0.500977  7.971791
49   0.495483  8.058091  0.493408  8.089952
50   0.493896  8.081070  0.497925  8.015798
51   0.497803  8.016802  0.501099  7.963356
52   0.506104  7.882762  0.498535  8.002650
53   0.504272  7.910497  0.501099  7.960439
54   0.499268  7.989045  0.499878  7.978756
55   0.497803  8.011341  0.496704  8.028380
56   0.494751  8.059094  0.510376  7.809589
57   0.506470  7.871503  0.486816  8.184479
58   0.504395  7.903936  0.499512  7.981488
59   0.504883  7.895601  0.500122  7.971252
60   0.500244  7.969088  0.498047  8.003911
61   0.492432  8.093250  0.498291  7.999666
62   0.493774  8.071522  0.506836  7.863149
63   0.499023  7.987578  0.503662  7.913513
64   0.505249  7.888117  0.492798  8.086528
65   0.498657  7.993040  0.495605  8.041622
66   0.505615  7.881985  0.506226  7.872201
67   0.496582  8.025899  0.494629  8.056997
68   0.500488  7.963553  0.506958  7.860382
69   0.501221  7.951826  0.495117  8.049111
70   0.502808  7.926492  0.492432  8.091896
71   0.499268  7.982905  0.499390  7.980950
72   0.497925  8.004297  0.499390  7.980938
73   0.496704  8.023749  0.492432  8.091859
74   0.505615  7.881678  0.496948  8.019849
75   0.496094  8.033470  0.502930  7.924488
76   0.515747  7.720148  0.495850  8.037360
77   0.499634  7.977031  0.493896  8.068497
78   0.488037  8.161909  0.504150  7.905025
79   0.494141  8.064605  0.492554  8.089904
80   0.505981  7.875834  0.493896  8.068497
81   0.500000  7.979733  0.498291  8.203263
82   0.502563  8.184414  0.494507  8.309199
83   0.507324  8.095552  0.494141  8.301147
84   0.497803  8.235678  0.500732  8.182158
85   0.503174  8.136927  0.499146  8.196111
86   0.495605  8.247812  0.499268  8.183554
87   0.502319  8.129492  0.496338  8.221145
88   0.497070  8.204915  0.504517  8.080578
89   0.506348  8.047052  0.491089  8.289081
90   0.492920  8.255932  0.501221  8.118593
91   0.502808  8.089720  0.498291  8.159303
92   0.492920  8.242884  0.497681  8.163230
93   0.506348  8.020813  0.506104  8.022089
94   0.504517  8.045183  0.505981  8.019142
95   0.493286  8.221489  0.503174  8.059887
96   0.499878  8.110915  0.498779  8.126566
97   0.492065  8.232843  0.501465  8.079438
98   0.503052  8.052061  0.503540  8.042422
99   0.490356  8.253242  0.510132  7.932852
100  0.493408  8.200842  0.507690  7.969100
101  0.502808  8.046339  0.485352  8.326256
102  0.506470  7.984502  0.501709  8.059884
103  0.496826  8.137300  0.498535  8.108485
104  0.496460  8.140724  0.496826  8.133628
105  0.505737  7.988860  0.493164  8.190393
106  0.498169  8.108653  0.488403  8.264998
107  0.492065  8.204963  0.498291  8.103622
108  0.495605  8.145959  0.498779  8.093867
109  0.504395  8.002469  0.509277  7.922889
110  0.493164  8.181771  0.508423  7.935007
111  0.503052  8.020803  0.502930  8.022008
112  0.499512  8.076382  0.501099  8.050101
113  0.504517  7.994352  0.495361  8.141274
114  0.498047  8.097391  0.506104  7.966952
115  0.494995  8.145461  0.506714  7.956057
116  0.505493  7.975257  0.502686  8.020052
117  0.500610  8.053084  0.497803  8.097939
118  0.503662  8.003139  0.494385  8.152331
119  0.498901  8.079229  0.500000  8.061233
120  0.498779  8.080656  0.490479  8.214211
121  0.507080  7.946418  0.499634  8.066244
122  0.490234  8.217578  0.504028  7.995090
123  0.489502  8.229095  0.488647  8.242745
124  0.490234  8.217064  0.493286  8.167780
125  0.498657  8.081130  0.507202  7.943330
126  0.503052  8.010167  0.504761  7.982568
127  0.492920  8.212382  0.502686  8.110353

2018-02-25 11:26:54.543879 Finish.
Total elapsed time: 15:00:05.54.
