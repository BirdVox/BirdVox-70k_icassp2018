2018-02-24 20:26:41.738278: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:41.738500: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:41.738513: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:24.097362 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit02, unit03, unit05.
Validation set: unit07, unit10.
Test set: unit01.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.500732  8.050340  0.499268  8.073115
1    0.500610  8.050909  0.502808  8.015026
2    0.499390  8.069805  0.505249  7.975105
3    0.509155  7.911972  0.493408  8.165644
4    0.499146  8.073078  0.487793  8.255393
5    0.498291  8.082132  0.492798  8.148297
6    0.500122  8.001556  0.501953  7.955983
7    0.506226  7.881757  0.502075  7.943756
8    0.506470  7.871696  0.506348  7.872195
9    0.493896  8.069942  0.498901  7.989593
10   0.502808  7.938119  0.507324  8.052749
11   0.501831  8.129902  0.498169  8.166515
12   0.501709  8.096992  0.503052  8.065232
13   0.501953  8.075894  0.500977  8.085487
14   0.505127  8.013831  0.500366  8.086253
15   0.505249  8.004014  0.502808  8.040089
16   0.507446  7.962544  0.498779  8.099635
17   0.504150  8.010814  0.488647  8.258571
18   0.492188  8.199667  0.500977  8.056260
19   0.501221  8.050807  0.505249  7.984447
20   0.496826  8.118967  0.501099  8.048937
21   0.504639  7.990877  0.494995  8.145376
22   0.514160  7.835677  0.499634  8.069076
23   0.501221  8.042880  0.506958  7.949835
24   0.492554  8.181535  0.503906  7.998124
25   0.509644  7.905302  0.505005  7.979754
26   0.513306  7.845710  0.495972  8.124877
27   0.502441  8.020422  0.494263  8.152092
28   0.501587  8.033922  0.494507  8.147936
29   0.495239  8.136054  0.505859  7.964812
30   0.500244  8.055270  0.498291  8.086710
31   0.506104  7.960758  0.506226  7.958766
32   0.496216  8.120088  0.497681  8.096464
33   0.501831  8.029558  0.503052  8.009875
34   0.496216  8.120053  0.504883  7.980354
35   0.500977  8.043312  0.508423  7.923291
36   0.502075  8.025602  0.498779  8.078724
37   0.500610  8.049211  0.499023  8.074789
38   0.494385  8.149555  0.502808  8.013794
39   0.504150  7.992151  0.506348  7.956736
40   0.498535  8.095300  0.489502  8.323363
41   0.505859  8.042123  0.498657  8.142222
42   0.496826  8.162293  0.486816  8.314081
43   0.484619  8.343081  0.500732  8.080640
44   0.490479  8.239276  0.507202  7.968010
45   0.494263  8.169978  0.512085  7.881597
46   0.492188  8.194725  0.503784  8.005775
47   0.505859  7.968717  0.504639  7.984196
48   0.482666  8.330584  0.503052  8.001667
49   0.495117  8.124319  0.514893  7.805203
50   0.505859  7.945453  0.490112  8.192746
51   0.505005  7.951679  0.500610  8.018115
52   0.491211  8.164475  0.506348  7.919702
53   0.498779  8.037060  0.500000  8.014343
54   0.505249  7.927581  0.494019  8.103596
55   0.509644  7.851660  0.504395  7.932570
56   0.494507  8.087629  0.501587  7.972251
57   0.497803  8.030277  0.492798  8.107837
58   0.490356  8.144727  0.490967  8.133041
59   0.501343  7.965859  0.495728  8.053689
60   0.500122  7.982122  0.507568  7.861972
61   0.498779  8.000821  0.506592  7.875067
62   0.499878  7.981049  0.500977  7.962542
63   0.496216  8.037582  0.508911  7.834383
64   0.501587  7.950460  0.495239  8.051014
65   0.501831  7.945382  0.507568  7.853411
66   0.499512  7.981431  0.501709  7.946012
67   0.499878  7.974882  0.499023  7.988209
68   0.506592  7.867311  0.508545  7.835955
69   0.507568  7.851348  0.502197  7.936818
70   0.501587  7.946423  0.512329  7.775055
71   0.501831  7.942333  0.498413  7.996748
72   0.502075  7.938308  0.497803  8.006372
73   0.503174  7.920708  0.501465  7.947922
74   0.502075  7.938170  0.505493  7.883661
75   0.494385  8.060744  0.502075  7.938130
76   0.505859  7.877795  0.499268  7.982878
77   0.501587  7.945900  0.495728  8.039309
78   0.491455  8.223545  0.502319  8.183765
79   0.503906  8.134768  0.507080  8.067015
80   0.494263  8.263434  0.501221  8.142543
81   0.496582  8.210560  0.499146  8.163102
82   0.490479  8.297742  0.505371  8.053034
83   0.501831  8.106174  0.489624  8.299286
84   0.504517  8.056159  0.491577  8.261834
85   0.489990  8.284940  0.496460  8.178338
86   0.511841  7.928411  0.498413  8.142932
87   0.494507  8.204213  0.505981  8.017661
88   0.509155  7.965072  0.503540  8.054203
89   0.494629  8.196582  0.500977  8.093058
90   0.499634  8.113581  0.494507  8.195123
91   0.512573  7.902899  0.494629  8.191115
92   0.500610  8.093736  0.496216  8.163605
93   0.518433  7.804578  0.502319  8.063357
94   0.499756  8.103753  0.494873  8.181526
95   0.494507  8.186504  0.511353  7.914048
96   0.504639  8.021325  0.491333  8.234837
97   0.501221  8.074510  0.491943  8.223073
98   0.509277  7.942706  0.502808  8.045996
99   0.495239  8.166989  0.501709  8.061701
100  0.495117  8.166938  0.500488  8.079345
101  0.498047  8.117675  0.506104  7.986787
102  0.496582  8.139230  0.499146  8.096878
103  0.501831  8.052568  0.503296  8.027927
104  0.495361  8.154800  0.502930  8.031791
105  0.495483  8.150807  0.500854  8.063230
106  0.501709  8.048474  0.496216  8.136030
107  0.492920  8.188198  0.503296  8.020005
108  0.500610  8.062371  0.499390  8.081134
109  0.497437  8.111738  0.508179  7.937728
110  0.505615  7.978223  0.498657  8.089562
111  0.497803  8.102571  0.500366  8.060506
112  0.493286  8.173928  0.505737  7.972561
113  0.499878  8.066382  0.497925  8.097262
114  0.501099  8.045562  0.512695  7.858125
115  0.494141  8.156727  0.501709  8.034299
116  0.498657  8.083103  0.498535  8.084708
117  0.494873  8.143425  0.497803  8.095916
118  0.507202  7.944175  0.501343  8.038395
119  0.504028  7.994930  0.507812  7.933773
120  0.500244  8.055632  0.494629  8.146024
121  0.489868  8.222670  0.500000  8.059287
122  0.498901  8.076938  0.510864  7.884070
123  0.495605  8.129978  0.485352  8.295222
124  0.493164  8.169280  0.503784  7.998088
125  0.504517  7.986272  0.493164  8.169245
126  0.498047  8.090539  0.497314  8.102340
127  0.501099  8.041344  0.494385  8.149557

2018-02-25 13:03:50.375571 Finish.
Total elapsed time: 16:37:26.38.
