2018-02-24 20:27:51.675198: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:51.675490: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:51.675509: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:51.675518: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:51.675527: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:52.525064 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit02, unit03, unit05.
Validation set: unit07, unit10.
Test set: unit01.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.972046  0.107452  0.956787  0.169537
1    0.969971  0.118927  0.900146  0.335686
2    0.975220  0.092786  0.956177  0.168015
3    0.973022  0.095188  0.932617  0.255096
4    0.978882  0.077911  0.962769  0.146597
5    0.982910  0.078512  0.940308  0.225006
6    0.980835  0.081478  0.962402  0.161620
7    0.980347  0.077741  0.964722  0.147571
8    0.977051  0.086560  0.945801  0.195033
9    0.982544  0.064104  0.959351  0.186129
10   0.979248  0.078856  0.955688  0.164152
11   0.977783  0.090691  0.967163  0.139391
12   0.981323  0.075351  0.931519  0.264010
13   0.982422  0.070034  0.956055  0.179556
14   0.971313  0.125164  0.924683  0.272078
15   0.971436  0.113481  0.938477  0.212873
16   0.976929  0.098368  0.961914  0.164073
17   0.980713  0.080582  0.967285  0.167276
18   0.983765  0.072510  0.964844  0.158949
19   0.966675  0.164460  0.934448  0.348723
20   0.967651  0.151811  0.944946  0.348592
21   0.980103  0.093716  0.964478  0.184359
22   0.984009  0.084254  0.966919  0.188184
23   0.980347  0.092041  0.944336  0.242526
24   0.982300  0.075902  0.963379  0.171395
25   0.986084  0.065738  0.964966  0.169581
26   0.982056  0.073673  0.961792  0.161016
27   0.982056  0.073502  0.957764  0.202429
28   0.984619  0.071085  0.964111  0.191119
29   0.980713  0.091594  0.861206  1.006345
30   0.950806  0.199239  0.918701  0.358616
31   0.938599  0.269971  0.911987  0.389011
32   0.953857  0.202881  0.944458  0.352970
33   0.612183  6.081477  0.501953  7.960940
34   0.504150  7.925151  0.500732  7.978905
35   0.493530  8.093034  0.500610  7.979480
36   0.507935  7.862062  0.492798  8.102730
37   0.490479  8.139080  0.500366  7.980823
38   0.501343  7.964649  0.495850  8.051620
39   0.487549  8.183368  0.495728  8.052394
40   0.502197  7.948683  0.499878  7.985092
41   0.491211  8.122714  0.505249  7.898365
42   0.502075  7.948433  0.506226  7.881738
43   0.490601  8.130328  0.494263  8.071439
44   0.496216  8.039815  0.498047  8.010141
45   0.505005  7.898752  0.495361  8.052037
46   0.505981  7.882292  0.495972  8.041442
47   0.503662  7.918432  0.493652  8.077612
48   0.498413  8.001339  0.496826  8.026270
49   0.496338  8.033710  0.489502  8.142356
50   0.505371  7.889053  0.504517  7.902373
51   0.494385  8.063621  0.503540  7.917395
52   0.492432  8.094245  0.501343  7.951944
53   0.490967  8.117150  0.486694  8.185060
54   0.500122  7.970808  0.499756  7.976474
55   0.500854  7.958807  0.499512  7.980070
56   0.508545  7.835933  0.494385  8.061561
57   0.497925  8.005022  0.495483  8.043848
58   0.500122  7.969816  0.494263  8.063152
59   0.505859  7.878209  0.504883  7.893719
60   0.492554  8.090225  0.502441  7.932546
61   0.489868  8.132956  0.503540  7.914959
62   0.527222  7.537106  0.601929  6.437370
63   0.556519  7.167594  0.619263  6.154614
64   0.546997  7.294414  0.482666  8.265954
65   0.497314  8.122052  0.500122  8.088734
66   0.497681  8.127070  0.508301  7.954904
67   0.498169  8.117275  0.497559  8.126187
68   0.510498  7.916738  0.496216  8.146056
69   0.495361  8.158970  0.505371  7.996776
70   0.495117  8.161217  0.511841  7.890832
71   0.503540  8.023813  0.490967  8.225660
72   0.493774  8.179616  0.502808  8.033230
73   0.491821  8.209541  0.496704  8.130074
74   0.495239  8.152941  0.500000  8.075466
75   0.511230  7.893735  0.495605  8.144867
76   0.484009  8.331095  0.503906  8.009703
77   0.492798  8.188092  0.488159  8.262208
78   0.508423  7.934974  0.488525  8.255067
79   0.502686  8.026246  0.508179  7.937127
80   0.511108  7.875216  0.505249  7.934030
81   0.499512  8.025192  0.498169  8.046121
82   0.509888  7.858818  0.494873  8.097705
83   0.502930  7.968786  0.489868  8.176535
84   0.494751  8.098212  0.503296  7.961500
85   0.499634  8.019399  0.497925  8.046155
86   0.504272  7.944469  0.499390  8.021817
87   0.504395  7.941532  0.501831  7.981898
88   0.498047  8.041722  0.495361  8.084024
89   0.501343  7.988152  0.495361  8.082988
90   0.490845  8.154468  0.506226  7.908726
91   0.492676  8.124202  0.492188  8.131438
92   0.507812  7.881783  0.496338  8.064151
93   0.496460  8.061633  0.499634  8.010452
94   0.507080  7.891151  0.501953  7.972285
95   0.504761  7.926917  0.503418  7.947705
96   0.500977  7.985999  0.504517  7.928925
97   0.503906  7.938010  0.502808  7.954870
98   0.501953  7.967831  0.503052  7.949644
99   0.501221  7.978160  0.515015  7.757566
100  0.494995  8.076038  0.506836  7.886572
101  0.508057  7.866415  0.500488  7.986371
102  0.500488  7.985672  0.495117  8.070596
103  0.495483  8.064060  0.506348  7.890156
104  0.497925  8.023745  0.500488  7.982183
105  0.497803  8.024316  0.490601  8.138454
106  0.502441  7.949019  0.499878  7.989224
107  0.496948  8.035286  0.509644  7.832252
108  0.507935  7.858879  0.488281  8.171585
109  0.505493  7.896596  0.512573  7.783139
110  0.505737  7.891561  0.509155  7.836519
111  0.502197  7.946921  0.506836  7.872452
112  0.495850  8.047110  0.501465  7.957107
113  0.489746  8.143477  0.494995  8.059348
114  0.501953  7.948000  0.507935  7.852229
115  0.500000  7.978337  0.496582  8.032448
116  0.505127  7.895866  0.500977  7.961684
117  0.501221  7.957467  0.507324  7.859843
118  0.505493  7.888737  0.504395  7.905960
119  0.493408  8.080836  0.494141  8.068893
120  0.496826  8.025830  0.500244  7.971096
121  0.504883  7.896917  0.493896  8.071842
122  0.492432  8.094987  0.501099  7.956611
123  0.498657  7.995343  0.503540  7.917313
124  0.501831  7.944385  0.498779  7.992868
125  0.492188  8.097799  0.498901  7.990610
126  0.492432  8.093610  0.493530  8.075956
127  0.507690  7.850081  0.503784  7.912231

2018-02-25 11:51:01.666910 Finish.
Total elapsed time: 15:24:09.67.
