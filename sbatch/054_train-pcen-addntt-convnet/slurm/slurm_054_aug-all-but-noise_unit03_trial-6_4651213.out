2018-02-24 20:26:46.389369: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:46.389698: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:46.389711: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:17.122776 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.969360  0.121556  0.806763  0.854260
1    0.973022  0.132537  0.816650  0.730251
2    0.974854  0.118943  0.813110  0.569404
3    0.973755  0.119015  0.826050  0.545545
4    0.974365  0.112942  0.794922  0.710575
5    0.974854  0.113331  0.840332  0.464740
6    0.974976  0.107190  0.836670  0.482917
7    0.979858  0.092458  0.764893  0.614421
8    0.979614  0.094883  0.835815  0.489634
9    0.979370  0.097115  0.828003  0.473926
10   0.976196  0.100609  0.801758  0.598150
11   0.975098  0.111050  0.820435  0.451914
12   0.978394  0.105038  0.789062  0.791721
13   0.977295  0.095402  0.811035  0.499147
14   0.982666  0.085382  0.820435  0.647254
15   0.973267  0.132662  0.803589  0.718128
16   0.974365  0.117094  0.786255  0.671060
17   0.977295  0.104764  0.742310  0.665840
18   0.981201  0.081590  0.824097  0.433916
19   0.983276  0.072857  0.813354  0.550818
20   0.979248  0.087533  0.808228  0.469041
21   0.781982  3.093437  0.513916  7.872323
22   0.498535  8.114330  0.499634  8.091290
23   0.505127  7.960925  0.500977  8.005564
24   0.500977  7.999415  0.498291  8.037102
25   0.500610  7.996380  0.496216  8.063109
26   0.499878  8.002065  0.507324  7.880912
27   0.500244  7.991737  0.501099  7.976200
28   0.497803  8.027088  0.499268  8.002169
29   0.495117  8.066958  0.497925  8.020884
30   0.495483  8.058637  0.501221  7.966053
31   0.505127  7.902777  0.495605  8.053614
32   0.492676  8.099460  0.498047  8.013007
33   0.509033  7.837119  0.498413  8.005720
34   0.500854  7.966162  0.504028  7.914955
35   0.495361  8.052582  0.511963  7.787392
36   0.506226  7.878392  0.499878  7.979144
37   0.498779  7.996260  0.507324  7.859654
38   0.499390  7.985810  0.489502  8.143120
39   0.498291  8.002711  0.492798  8.090008
40   0.501831  7.945749  0.500610  7.964972
41   0.507690  7.851885  0.503906  7.912008
42   0.493164  8.083078  0.500000  7.973918
43   0.500122  7.971808  0.493774  8.072847
44   0.497925  8.006534  0.506714  7.866275
45   0.502075  7.940096  0.495728  8.041166
46   0.507324  7.856170  0.504028  7.908599
47   0.503662  7.914330  0.497559  8.011530
48   0.495605  8.042570  0.492798  8.087234
49   0.493286  8.079361  0.494263  8.063705
50   0.492920  8.085031  0.505493  7.884504
51   0.509766  7.816318  0.497437  8.012803
52   0.500366  7.966031  0.492188  8.096356
53   0.502441  7.932827  0.507202  7.856874
54   0.500977  7.956074  0.500854  7.957972
55   0.498047  8.002689  0.499634  7.977349
56   0.490112  8.129108  0.489624  8.136857
57   0.501099  7.953895  0.497925  8.004465
58   0.500122  7.969410  0.506958  7.860406
59   0.504150  7.905147  0.513550  7.755279
60   0.502319  7.934305  0.498901  7.988781
61   0.502686  7.928441  0.514893  7.733821
62   0.491333  8.250171  0.496216  8.185331
63   0.499878  8.119182  0.497803  8.146861
64   0.509888  7.947939  0.510498  7.934416
65   0.505371  8.014068  0.500366  8.091973
66   0.498291  8.123054  0.502197  8.057857
67   0.509644  7.935864  0.492920  8.203533
68   0.495605  8.158556  0.509766  7.928696
69   0.500610  8.074789  0.499512  8.091075
70   0.494141  8.176351  0.500000  8.080655
71   0.513672  7.859142  0.496094  8.141356
72   0.494507  8.165914  0.508301  7.942592
73   0.494751  8.160079  0.497314  8.117878
74   0.505981  7.977369  0.505493  7.984450
75   0.495728  8.141125  0.492065  8.199444
76   0.499878  8.072869  0.499146  8.084039
77   0.496826  8.120834  0.494507  8.157646
78   0.494629  8.155148  0.488403  8.254976
79   0.487061  8.276139  0.506226  7.966767
80   0.497925  8.100124  0.505859  7.971808
81   0.498779  8.085529  0.500122  8.063498
82   0.494873  8.147740  0.505249  7.980144
83   0.493652  8.166729  0.496338  8.123118
84   0.509888  7.904417  0.500854  8.049717
85   0.494995  8.143881  0.503784  8.001945
86   0.502319  8.025301  0.502075  8.028987
87   0.507812  7.936280  0.505615  7.971469
88   0.497437  8.103083  0.502808  8.016305
89   0.491577  8.197128  0.509155  7.913616
90   0.502197  8.025595  0.500000  8.060845
91   0.506104  7.962315  0.501099  8.042836
92   0.502319  8.023027  0.498047  8.091761
93   0.495972  8.125092  0.500732  8.048246
94   0.501709  8.032404  0.500854  8.046081
95   0.495850  8.126664  0.502197  8.024270
96   0.502441  8.020263  0.496094  8.122506
97   0.494629  8.146057  0.491089  8.203060
98   0.500732  8.047576  0.506836  7.949153
99   0.499756  8.063232  0.502686  8.015975
100  0.493652  8.161542  0.495605  8.130034
101  0.504028  7.994250  0.506592  7.952910
102  0.509644  7.903704  0.494019  8.155533
103  0.490845  8.206676  0.499634  8.065002
104  0.500366  8.053187  0.498413  8.084659
105  0.486206  8.281407  0.505127  7.976433
106  0.499023  8.113298  0.503662  8.068565
107  0.500000  8.115684  0.501709  8.078759
108  0.500366  8.092606  0.507080  7.978625
109  0.509033  7.941585  0.500610  8.070293
110  0.490601  8.224940  0.504395  8.000301
111  0.494263  8.157532  0.502319  8.024928
112  0.496338  8.116438  0.499878  8.056244
113  0.491943  8.179217  0.501953  8.016177
114  0.500366  8.038199  0.496216  8.101132
115  0.491577  8.171997  0.507446  7.915949
116  0.512695  7.829337  0.506714  7.921789
117  0.499634  8.031866  0.494995  8.103041
118  0.493286  8.127614  0.505127  7.936190
119  0.488403  8.200251  0.499146  8.026463
120  0.501465  7.987057  0.496460  8.064440
121  0.495605  8.075762  0.499756  8.007321
122  0.486084  8.223121  0.486572  8.213205
123  0.494995  8.076911  0.497803  8.030173
124  0.500732  7.981611  0.492676  8.108238
125  0.492310  8.112388  0.512207  7.793531
126  0.499146  8.000247  0.493164  8.094137
127  0.497559  8.022739  0.494507  8.070100

2018-02-25 11:19:29.054792 Finish.
Total elapsed time: 14:53:12.05.
