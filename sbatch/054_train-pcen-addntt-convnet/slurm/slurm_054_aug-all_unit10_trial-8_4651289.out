2018-02-24 20:27:51.289503: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:51.289846: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:51.289860: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:51.289868: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:51.289875: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:52.367664 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit01, unit02, unit03.
Validation set: unit05, unit07.
Test set: unit10.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.953003  0.160879  0.948242  0.176715
1    0.948120  0.179270  0.900757  0.279807
2    0.956909  0.156810  0.945435  0.174743
3    0.962036  0.141617  0.966431  0.135793
4    0.958252  0.147866  0.966919  0.139509
5    0.953247  0.168889  0.738892  0.536159
6    0.960815  0.150518  0.927124  0.206640
7    0.964233  0.128903  0.946533  0.168627
8    0.962646  0.134720  0.969604  0.122601
9    0.960693  0.136949  0.963257  0.137851
10   0.958252  0.160803  0.960083  0.162263
11   0.958984  0.149170  0.967407  0.146230
12   0.962158  0.140712  0.971558  0.119360
13   0.963257  0.131438  0.973389  0.114373
14   0.963135  0.136569  0.976318  0.100774
15   0.962036  0.137572  0.971680  0.121344
16   0.944580  0.196102  0.966919  0.136906
17   0.965454  0.126035  0.969604  0.116972
18   0.968872  0.118685  0.965942  0.130975
19   0.961182  0.146162  0.972412  0.125767
20   0.964355  0.140561  0.969727  0.131767
21   0.967529  0.120649  0.978027  0.105522
22   0.894043  1.327697  0.500977  8.014390
23   0.520508  7.746521  0.500366  8.070701
24   0.491211  8.209064  0.496582  8.116927
25   0.502075  8.024292  0.489258  8.223986
26   0.502197  8.013753  0.506592  7.939964
27   0.499878  8.043690  0.490234  8.194255
28   0.496460  8.092112  0.501831  8.003680
29   0.491455  8.166504  0.497681  8.064721
30   0.509888  7.867744  0.498779  8.042517
31   0.504761  7.944972  0.494507  8.106295
32   0.507812  7.892138  0.492065  8.141182
33   0.499390  8.022517  0.499512  8.018699
34   0.493774  8.108388  0.503662  7.949001
35   0.502075  7.972635  0.499268  8.015752
36   0.494995  8.082306  0.503052  7.952326
37   0.497192  8.044280  0.499878  8.000029
38   0.501343  7.975315  0.504517  7.923377
39   0.499878  7.996062  0.499634  7.998709
40   0.493530  8.094838  0.492065  8.117037
41   0.498413  8.014754  0.492798  8.103208
42   0.497192  8.032148  0.503174  7.935809
43   0.495850  8.051656  0.492432  8.105247
44   0.501587  7.958450  0.498535  8.006282
45   0.507324  7.865398  0.491699  8.113751
46   0.498779  8.000184  0.500122  7.978100
47   0.499756  7.983311  0.497437  8.019677
48   0.496948  8.026898  0.496704  8.030242
49   0.491089  8.119258  0.494385  8.066225
50   0.498413  8.001555  0.504639  7.901870
51   0.492310  8.098029  0.499390  7.984773
52   0.495728  8.042808  0.502686  7.931546
53   0.500610  7.964328  0.496094  8.036043
54   0.495483  8.045514  0.510864  7.800057
55   0.498779  7.992499  0.507080  7.859954
56   0.499268  7.984318  0.493774  8.071715
57   0.496094  8.034586  0.502563  7.931297
58   0.496460  8.028477  0.500977  7.956353
59   0.500977  7.956253  0.495850  8.037895
60   0.505371  7.886022  0.507202  7.856758
61   0.499146  7.985139  0.495361  8.045244
62   0.496460  8.128586  0.490479  8.228715
63   0.501099  8.055711  0.505127  7.989312
64   0.502441  8.031552  0.501709  8.042429
65   0.500366  8.063331  0.502686  8.025267
66   0.506836  7.957798  0.502319  8.030062
67   0.505737  7.974508  0.503906  8.003582
68   0.505615  7.975650  0.503418  8.010696
69   0.497559  8.104807  0.506348  7.962826
70   0.502441  8.025500  0.504150  7.997678
71   0.500000  8.064322  0.503662  8.005053
72   0.496948  8.113045  0.496216  8.124635
73   0.498169  8.092957  0.495972  8.128181
74   0.503296  8.009953  0.504395  7.992075
75   0.502197  8.027334  0.502930  8.015376
76   0.491699  8.196249  0.498901  8.080027
77   0.503174  8.011036  0.502075  8.028620
78   0.499512  8.069823  0.507812  7.935918
79   0.510498  7.892527  0.494019  8.158042
80   0.506348  7.959224  0.494141  8.155883
81   0.497070  8.108572  0.497559  8.100613
82   0.502319  8.023795  0.495605  8.131927
83   0.510132  7.897710  0.501953  8.029457
84   0.489990  8.222200  0.498901  8.078495
85   0.500732  8.048910  0.503906  7.997683
86   0.498169  8.090089  0.501587  8.034931
87   0.498291  8.087989  0.506592  7.954132
88   0.508057  7.930459  0.511475  7.875306
89   0.503540  8.003137  0.507202  7.944052
90   0.500122  8.058114  0.500000  8.060026
91   0.492554  8.179993  0.500977  8.044180
92   0.501099  8.042163  0.504639  7.985056
93   0.494995  8.140445  0.509766  7.902326
94   0.497437  8.101005  0.496460  8.116703
95   0.493896  8.157981  0.498413  8.085143
96   0.504028  7.994599  0.495728  8.128356
97   0.494507  8.147998  0.502319  8.022042
98   0.495850  8.126292  0.500732  8.047561
99   0.503540  8.002280  0.499878  8.061280
100  0.500366  8.053386  0.488770  8.240279
101  0.498047  8.090725  0.493530  8.163504
102  0.498657  8.080849  0.507446  7.939169
103  0.495728  8.128037  0.500854  8.045386
104  0.513916  7.834846  0.492432  8.181121
105  0.501587  8.033544  0.506592  7.952865
106  0.498169  8.088617  0.496338  8.118122
107  0.502930  8.011868  0.502686  8.015796
108  0.503540  8.002018  0.503052  8.009883
109  0.498657  8.080711  0.503418  8.003973
110  0.500854  8.045289  0.496948  8.108248
111  0.491943  8.188914  0.503052  8.009866
112  0.494751  8.143658  0.502075  8.025604
113  0.497925  8.092499  0.496460  8.116109
114  0.489624  8.226291  0.497314  8.102335
115  0.509521  7.905581  0.497925  8.092497
116  0.498291  8.086594  0.495361  8.133779
117  0.501709  8.129163  0.484497  8.402574
118  0.501709  8.117461  0.488892  8.318152
119  0.501221  8.115485  0.499878  8.133683
120  0.491943  8.258865  0.500732  8.114716
121  0.504639  8.049648  0.498779  8.142104
122  0.506226  8.020326  0.509766  7.961582
123  0.497559  8.156803  0.499878  8.117933
124  0.485962  8.340855  0.503906  8.050281
125  0.499023  8.127721  0.499512  8.118612
126  0.496338  8.168597  0.490601  8.259917
127  0.497314  8.150606  0.505127  8.023601

2018-02-25 11:47:15.203811 Finish.
Total elapsed time: 15:20:23.20.
