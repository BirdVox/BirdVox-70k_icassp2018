2018-02-24 20:26:37.336645: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:37.336943: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:37.336956: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:17.498051 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit01, unit02, unit03.
Validation set: unit05, unit07.
Test set: unit10.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.494873  8.067641  0.508789  7.835440
1    0.503540  7.917013  0.502441  7.933301
2    0.502441  8.028094  0.494751  8.187071
3    0.498901  8.104707  0.499268  8.088300
4    0.500977  8.055519  0.499268  8.079144
5    0.497925  8.098540  0.505737  7.970854
6    0.497437  8.103559  0.504028  7.996434
7    0.497803  8.096214  0.498413  8.085913
8    0.498657  8.081672  0.498535  8.083386
9    0.492554  8.179626  0.500244  8.055529
10   0.493164  8.169550  0.496094  8.122248
11   0.495728  8.128096  0.495483  8.131984
12   0.497681  8.096537  0.509766  7.901724
13   0.488647  8.242089  0.501953  8.027612
14   0.502441  8.019731  0.503540  8.002014
15   0.498779  8.078742  0.501465  8.035451
16   0.515381  7.811148  0.502319  8.021672
17   0.499756  8.062988  0.502686  8.015766
18   0.499390  8.081754  0.498901  8.106358
19   0.504761  7.996376  0.499268  8.073057
20   0.498657  8.076080  0.496216  8.109113
21   0.494629  8.129701  0.503784  7.979398
22   0.499146  8.049677  0.504639  7.958663
23   0.498657  8.051051  0.500854  8.013217
24   0.500732  8.012705  0.488159  8.210818
25   0.500488  8.012192  0.499756  8.021888
26   0.502930  7.969508  0.504395  7.944439
27   0.503052  7.964281  0.507446  7.892703
28   0.498169  8.039198  0.508667  7.870458
29   0.501221  7.987874  0.496582  8.060549
30   0.509644  7.851099  0.496826  8.054231
31   0.492920  8.115338  0.499512  8.009085
32   0.495483  8.072170  0.497070  8.045735
33   0.513672  7.779951  0.494995  8.076585
34   0.504517  7.923690  0.497681  8.031566
35   0.501953  7.962367  0.505493  7.904841
36   0.506104  7.894042  0.505249  7.906594
37   0.486816  8.199409  0.501343  7.966780
38   0.504761  7.911276  0.499023  8.001735
39   0.501709  7.957950  0.496948  8.032886
40   0.499146  7.996938  0.509033  7.838399
41   0.494629  8.067181  0.495972  8.044934
42   0.491455  8.116152  0.508301  7.846824
43   0.488892  8.155542  0.499756  7.981650
44   0.501099  7.959612  0.497559  8.015440
45   0.501343  7.954562  0.497925  8.008525
46   0.489746  8.138443  0.500244  7.970630
47   0.503174  7.923530  0.504272  7.905642
48   0.507690  7.850827  0.503784  7.912797
49   0.495850  8.039032  0.509888  7.814988
50   0.496338  8.030799  0.501709  7.944981
51   0.504395  7.902009  0.493408  8.077013
52   0.498535  7.995158  0.494019  8.067056
53   0.509888  7.813977  0.505249  7.887850
54   0.494629  8.057098  0.503784  7.911086
55   0.497192  8.016132  0.496704  8.023878
56   0.498047  8.002442  0.492188  8.095829
57   0.500122  7.969314  0.499268  7.982920
58   0.496582  8.205731  0.506958  8.122324
59   0.495361  8.288317  0.508057  8.065806
60   0.494263  8.274620  0.494507  8.258565
61   0.500732  8.148548  0.492798  8.267639
62   0.503784  8.083415  0.496582  8.192975
63   0.510132  7.969263  0.499878  8.129683
64   0.499878  8.125738  0.494629  8.206748
65   0.497314  8.160555  0.499512  8.122497
66   0.509644  7.957065  0.505615  8.020064
67   0.503784  8.048029  0.500366  8.101714
68   0.501587  8.080907  0.499878  8.107421
69   0.501465  8.081001  0.484497  8.353715
70   0.507935  7.975296  0.492798  8.218662
71   0.497925  8.135491  0.503174  8.050375
72   0.502075  8.067610  0.493286  8.208808
73   0.498901  8.117850  0.509155  7.952122
74   0.496948  8.148418  0.498291  8.126307
75   0.504761  8.021545  0.504639  8.023014
76   0.503906  8.034299  0.498901  8.114429
77   0.503662  8.037129  0.501099  8.077861
78   0.498657  8.116596  0.494385  8.184821
79   0.498291  8.121190  0.494629  8.179524
80   0.508423  7.956467  0.493774  8.191824
81   0.507568  7.968713  0.511475  7.904949
82   0.501343  8.067423  0.502075  8.054762
83   0.496948  8.136516  0.499634  8.092324
84   0.500488  8.077623  0.497925  8.117991
85   0.508179  7.951750  0.500854  8.068816
86   0.503052  8.032401  0.499512  8.088444
87   0.500366  8.073652  0.501343  8.056880
88   0.503906  8.014533  0.495239  8.153192
89   0.497559  8.114785  0.496460  8.131467
90   0.496094  8.136366  0.498413  8.097981
91   0.496094  8.134396  0.493530  8.174753
92   0.500122  8.067588  0.503296  8.015526
93   0.503662  8.008769  0.499878  8.068926
94   0.496704  8.119304  0.493774  8.165769
95   0.498291  8.092278  0.499756  8.068000
96   0.497314  8.106750  0.497559  8.102242
97   0.503662  8.003358  0.487549  8.262595
98   0.495605  8.132323  0.500122  8.059137
99   0.498047  8.092258  0.496338  8.119501
100  0.496216  8.121219  0.500610  8.050161
101  0.491333  8.199512  0.494507  8.148193
102  0.513794  7.837195  0.498047  8.090896
103  0.498291  8.086877  0.503540  8.002200
104  0.497192  8.104459  0.495117  8.137863
105  0.501831  8.029617  0.496826  8.110261
106  0.493774  8.159433  0.493896  8.157452
107  0.504272  7.990202  0.497192  8.104313
108  0.505127  7.976418  0.506470  7.954773
109  0.489990  8.220389  0.496704  8.112173
110  0.502075  8.025601  0.493774  8.159393
111  0.495850  8.125944  0.509277  7.909515
112  0.507812  7.933125  0.500122  8.057080
113  0.504639  7.984281  0.488647  8.242029
114  0.493164  8.169230  0.492798  8.175133
115  0.492554  8.179068  0.499878  8.061015
116  0.503784  8.011535  0.503174  8.027209
117  0.498047  8.102406  0.490967  8.209659
118  0.496338  8.119597  0.499756  8.061005
119  0.505615  7.964069  0.497314  8.093071
120  0.507568  7.926661  0.494507  8.132094
121  0.494141  8.135442  0.491821  8.170038
122  0.501221  8.018059  0.499390  8.045212
123  0.496094  8.095934  0.496460  8.088351
124  0.501099  8.012840  0.493042  8.139791
125  0.492065  8.154035  0.489014  8.201427
126  0.501221  8.005706  0.496094  8.086391
127  0.492920  8.136076  0.503296  7.969796

2018-02-25 12:03:29.629176 Finish.
Total elapsed time: 15:37:12.63.
