2018-02-24 20:26:37.250510: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:37.250701: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:37.250713: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:17.166960 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.970215  0.124291  0.905762  0.476950
1    0.973511  0.107109  0.910522  0.609842
2    0.975586  0.100756  0.886230  0.708076
3    0.975342  0.092478  0.861816  0.882460
4    0.974365  0.108208  0.900513  0.559263
5    0.975830  0.097451  0.880859  0.824003
6    0.977417  0.094071  0.880005  0.816565
7    0.979858  0.078118  0.854126  1.108483
8    0.974609  0.092716  0.823242  0.926209
9    0.978394  0.092374  0.877808  0.617669
10   0.976074  0.100010  0.911255  0.694026
11   0.970337  0.141094  0.906494  0.439628
12   0.980713  0.082264  0.898438  0.519191
13   0.977661  0.099479  0.897827  0.556598
14   0.980103  0.085279  0.906006  0.526519
15   0.979980  0.079445  0.900391  0.566675
16   0.983032  0.073573  0.910889  0.476155
17   0.981812  0.081374  0.885376  0.542021
18   0.980713  0.080339  0.906128  0.377532
19   0.976318  0.098734  0.907227  0.338913
20   0.705688  4.639903  0.498657  8.113679
21   0.497437  8.129876  0.507080  7.971483
22   0.506348  7.980949  0.504517  8.008292
23   0.507446  7.959206  0.498901  8.095171
24   0.502441  8.036561  0.489136  8.249547
25   0.499146  8.086901  0.498047  8.103362
26   0.500977  8.045876  0.499268  8.059280
27   0.504639  7.955408  0.499756  8.023802
28   0.509033  7.871277  0.507202  7.896618
29   0.495728  8.076642  0.501587  7.980575
30   0.508667  7.865458  0.495972  8.065735
31   0.508423  7.865364  0.500610  7.988124
32   0.494507  8.083814  0.489990  8.154262
33   0.496582  8.047751  0.505371  7.906255
34   0.497437  8.031484  0.496948  8.038038
35   0.495850  8.054417  0.496582  8.041635
36   0.495605  8.056181  0.492676  8.101891
37   0.503052  7.935551  0.503296  7.930760
38   0.492798  8.097292  0.501953  7.950526
39   0.502563  7.940047  0.494751  8.063869
40   0.499023  7.995084  0.500610  7.969134
41   0.499146  7.991888  0.497314  8.020499
42   0.494263  8.068621  0.489868  8.138168
43   0.494385  8.065695  0.504150  7.909558
44   0.505981  7.879959  0.499878  7.976873
45   0.498657  7.995982  0.505493  7.886663
46   0.496460  8.030373  0.502441  7.934728
47   0.505737  7.881929  0.496338  8.031536
48   0.502441  7.934019  0.506104  7.875435
49   0.504150  7.906396  0.504639  7.898446
50   0.494873  8.053991  0.495483  8.044126
51   0.510742  7.800749  0.500854  7.958275
52   0.497559  8.010729  0.502808  7.926962
53   0.493896  8.068955  0.511963  7.780868
54   0.495605  8.041589  0.509033  7.827469
55   0.491577  8.105718  0.496216  8.031728
56   0.508057  7.842926  0.502563  7.930472
57   0.495605  8.041376  0.503418  7.916806
58   0.503296  7.918735  0.510376  7.805847
59   0.503174  7.920655  0.485107  8.208666
60   0.501099  7.953719  0.507812  7.846677
61   0.496338  8.029604  0.496094  8.033491
62   0.496216  8.031541  0.507812  7.846659
63   0.498047  8.002343  0.499878  7.973149
64   0.505859  7.877789  0.495605  8.041259
65   0.497070  8.017905  0.501709  7.943952
66   0.497314  8.033704  0.499268  8.196329
67   0.504150  8.088457  0.500366  8.133367
68   0.493774  8.232644  0.500610  8.116847
69   0.500854  8.108973  0.494141  8.213734
70   0.503418  8.061520  0.493286  8.222402
71   0.511963  7.919392  0.506592  8.004140
72   0.504883  8.030139  0.494873  8.190024
73   0.502197  8.070692  0.502441  8.065532
74   0.496826  8.154923  0.484863  8.346655
75   0.496704  8.154782  0.503174  8.049494
76   0.506348  7.997367  0.500122  8.096744
77   0.497681  8.135147  0.499634  8.102714
78   0.490723  8.245403  0.491455  8.232649
79   0.499023  8.109717  0.488647  8.276005
80   0.493042  8.204225  0.502686  8.047830
81   0.500488  8.082292  0.503418  8.034108
82   0.490479  8.241711  0.500610  8.077441
83   0.494995  8.166994  0.498169  8.114879
84   0.488159  8.275272  0.497070  8.130693
85   0.500000  8.082542  0.485962  8.307878
86   0.495605  8.151532  0.494263  8.172267
87   0.510498  7.909702  0.502930  8.030812
88   0.498413  8.102762  0.495972  8.141269
89   0.510376  7.908289  0.500854  8.060953
90   0.503784  8.012963  0.491089  8.216827
91   0.498169  8.101986  0.494019  8.168168
92   0.487671  8.269802  0.492310  8.194368
93   0.496826  8.120938  0.501465  8.045551
94   0.499146  8.082350  0.496094  8.130964
95   0.499268  8.079269  0.503662  8.007909
96   0.501831  8.036927  0.494141  8.160397
97   0.497925  8.098950  0.499512  8.072928
98   0.504883  7.985943  0.500366  8.058338
99   0.512695  7.859241  0.493408  8.169746
100  0.493408  8.169406  0.502075  8.029380
101  0.497559  8.101874  0.494263  8.154702
102  0.501831  8.032443  0.487061  8.270252
103  0.508423  7.925692  0.497559  8.100571
104  0.500000  8.061011  0.495239  8.137543
105  0.498657  8.082270  0.495239  8.137188
106  0.502930  8.013078  0.495728  8.129016
107  0.502319  8.022639  0.503540  8.002841
108  0.493896  8.158170  0.499878  8.061660
109  0.497559  8.098957  0.510254  7.894252
110  0.509521  7.905989  0.504150  7.992497
111  0.492554  8.179360  0.507080  7.945175
112  0.506958  7.947102  0.502563  8.017897
113  0.497559  8.098537  0.500366  8.053257
114  0.496460  8.116197  0.505249  7.974515
115  0.504761  7.982371  0.493042  8.171242
116  0.493652  8.161395  0.497070  8.106295
117  0.500366  8.053166  0.503906  7.996102
118  0.492065  8.186950  0.507690  7.935101
119  0.504028  7.994125  0.498413  8.084630
120  0.512695  7.854427  0.493164  8.169232
121  0.505371  7.972477  0.505493  7.970509
122  0.506470  7.954769  0.496948  8.103856
123  0.503540  8.047623  0.504150  8.042823
124  0.490479  8.248889  0.505859  7.994377
125  0.506104  7.984354  0.502808  8.031649
126  0.497314  8.115244  0.496948  8.117484
127  0.502930  8.019133  0.494995  8.142819

2018-02-25 12:01:31.687160 Finish.
Total elapsed time: 15:35:14.69.
