2018-02-24 20:27:51.120233: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:51.120526: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:51.120544: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:51.120552: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:51.120561: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:50.594766 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit02, unit03, unit05.
Validation set: unit07, unit10.
Test set: unit01.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.976685  0.108098  0.943970  0.202276
1    0.975464  0.108368  0.963379  0.154017
2    0.969971  0.125410  0.964722  0.154878
3    0.978516  0.086459  0.956909  0.221738
4    0.944458  0.216541  0.915649  0.380755
5    0.946899  0.217942  0.719116  0.565973
6    0.940796  0.221039  0.913086  0.349160
7    0.955811  0.192999  0.943115  0.251331
8    0.972046  0.132805  0.947021  0.218078
9    0.970337  0.132909  0.944824  0.213092
10   0.972290  0.118940  0.944824  0.233928
11   0.954468  0.345426  0.936768  0.243443
12   0.968262  0.135293  0.949463  0.248475
13   0.801025  2.986219  0.495850  8.067203
14   0.816772  2.215334  0.821655  0.588791
15   0.951904  0.205378  0.920898  0.262383
16   0.962280  0.159893  0.908691  0.294454
17   0.959717  0.159672  0.889160  0.408692
18   0.971191  0.128459  0.931885  0.245578
19   0.968872  0.129121  0.941040  0.227339
20   0.969360  0.127630  0.943237  0.187919
21   0.970581  0.126142  0.947388  0.198990
22   0.972046  0.114401  0.937622  0.226500
23   0.618774  6.083416  0.502441  8.049719
24   0.510986  7.908884  0.506470  7.978992
25   0.509521  7.927731  0.509399  7.927822
26   0.496582  8.132872  0.500000  8.076350
27   0.503784  8.014138  0.498413  8.099564
28   0.501953  8.041508  0.497803  8.107459
29   0.487549  8.271899  0.499023  8.086155
30   0.512207  7.872953  0.499878  8.070998
31   0.501343  8.046779  0.490479  8.221307
32   0.499512  8.075180  0.494629  8.153371
33   0.500244  8.062399  0.508667  7.926187
34   0.507324  7.947415  0.504395  7.994232
35   0.491943  8.194547  0.493896  8.162702
36   0.492554  8.184006  0.494507  8.152193
37   0.501831  8.033832  0.503052  8.013854
38   0.498047  8.094242  0.496338  8.121512
39   0.498901  8.079939  0.505981  7.965572
40   0.503174  8.010596  0.500000  8.061528
41   0.501831  8.031810  0.506836  7.950943
42   0.504150  7.994048  0.505737  7.968295
43   0.506714  7.952398  0.493652  8.162774
44   0.492554  8.180347  0.502563  8.018881
45   0.503418  8.004994  0.488159  8.250829
46   0.503296  8.006760  0.499756  8.063730
47   0.498169  8.089230  0.501221  8.039969
48   0.494751  8.144187  0.501709  8.031979
49   0.506104  7.961098  0.493530  8.163708
50   0.504150  7.992493  0.501221  8.039677
51   0.494629  8.145893  0.496704  8.112416
52   0.499023  8.075008  0.495850  8.126141
53   0.491943  8.189082  0.500610  8.049368
54   0.495117  8.137892  0.496338  8.118202
55   0.490112  8.218533  0.495605  8.129982
56   0.507812  7.933217  0.504761  7.982396
57   0.498291  8.086666  0.499634  8.065015
58   0.498535  8.082716  0.509277  7.909565
59   0.495117  8.137794  0.501953  8.027606
60   0.498535  8.082692  0.505981  7.962668
61   0.501953  8.027593  0.496216  8.120063
62   0.492920  8.173184  0.502930  8.011843
63   0.507202  7.942976  0.493774  8.159404
64   0.500977  8.043317  0.499023  8.074796
65   0.504761  7.982320  0.505127  7.976416
66   0.495483  8.131851  0.495361  8.133817
67   0.506104  7.960673  0.500610  8.049212
68   0.497437  8.100368  0.495972  8.123978
69   0.506348  7.956737  0.507812  7.933126
70   0.503662  8.000022  0.505859  7.964606
71   0.498291  8.031947  0.507446  7.880055
72   0.504150  7.931447  0.504028  7.932665
73   0.493408  8.101632  0.499268  8.007956
74   0.502563  7.955241  0.506104  7.898657
75   0.502319  7.958869  0.514282  7.768043
76   0.496460  8.052075  0.509766  7.839855
77   0.499268  8.007126  0.502930  7.948651
78   0.497070  8.041970  0.504272  7.927055
79   0.493652  8.096268  0.507324  7.878205
80   0.509399  7.845016  0.499023  8.010326
81   0.505371  7.909016  0.506104  7.897223
82   0.491455  8.130631  0.505859  7.900865
83   0.494141  8.087556  0.503662  7.935621
84   0.498535  8.017211  0.489380  8.163017
85   0.506104  7.896243  0.494507  8.080957
86   0.496582  8.047700  0.502930  7.946323
87   0.502686  7.950027  0.495239  8.068543
88   0.501953  7.961302  0.503662  7.933845
89   0.493164  8.100986  0.499268  8.003451
90   0.502686  7.948720  0.507324  7.874519
91   0.505249  7.907343  0.502075  7.957673
92   0.499878  7.992423  0.504639  7.916237
93   0.503906  7.927614  0.503784  7.929251
94   0.497925  8.022343  0.499268  8.000607
95   0.502808  7.943830  0.503784  7.927911
96   0.493164  8.096861  0.502441  7.948589
97   0.509888  7.829498  0.499268  7.998420
98   0.507446  7.867635  0.508057  7.857500
99   0.499023  8.001098  0.494141  8.078521
100  0.503540  7.928246  0.498779  8.003711
101  0.501099  7.966299  0.496460  8.039807
102  0.494995  8.062717  0.505981  7.887120
103  0.495728  8.050144  0.502686  7.938766
104  0.503052  7.932481  0.499268  7.992361
105  0.499756  7.984135  0.504272  7.911688
106  0.505737  7.887902  0.494019  8.074296
107  0.504761  7.902622  0.503052  7.929452
108  0.501221  7.958243  0.502930  7.930602
109  0.499146  7.990553  0.497437  8.017426
110  0.504517  7.904200  0.503662  7.917476
111  0.501831  7.946342  0.493042  8.086142
112  0.501831  7.945728  0.502930  7.927925
113  0.499634  7.980203  0.504028  7.909886
114  0.497681  8.010846  0.495728  8.041755
115  0.499878  7.975380  0.492554  8.091946
116  0.496948  8.021705  0.505249  7.889198
117  0.500732  7.961048  0.498169  8.001767
118  0.509277  7.824540  0.496094  8.034592
119  0.499390  7.981935  0.497314  8.014912
120  0.497925  8.005087  0.508911  7.829850
121  0.505371  7.886208  0.506592  7.866673
122  0.504028  7.907477  0.503052  7.922984
123  0.489136  8.144786  0.502075  7.938450
124  0.488770  8.150532  0.495361  8.045403
125  0.501221  7.951956  0.491821  8.101773
126  0.493530  8.074501  0.490234  8.127021
127  0.509277  7.823410  0.492188  8.095843

2018-02-25 12:01:07.580992 Finish.
Total elapsed time: 15:34:17.58.
