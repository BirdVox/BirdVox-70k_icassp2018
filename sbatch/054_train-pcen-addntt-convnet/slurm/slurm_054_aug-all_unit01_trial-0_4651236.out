2018-02-24 20:27:54.857945: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:54.858256: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:54.858269: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:48.370554 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit02, unit03, unit05.
Validation set: unit07, unit10.
Test set: unit01.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.965820  0.141870  0.825928  0.512251
1    0.839478  0.422566  0.846558  0.558367
2    0.775146  0.566519  0.821655  0.911894
3    0.600830  6.114354  0.499878  8.030850
4    0.502563  7.982391  0.501099  8.000462
5    0.508789  7.873679  0.497925  8.043056
6    0.498047  8.037913  0.504395  7.933735
7    0.497314  8.044044  0.505005  7.919021
8    0.501831  7.967501  0.495239  8.070579
9    0.499390  8.002632  0.503784  7.930876
10   0.496094  8.051969  0.487915  8.180913
11   0.500366  7.981122  0.504395  7.915668
12   0.501831  7.955434  0.500122  7.981626
13   0.497192  8.027391  0.494385  8.071251
14   0.504761  8.018409  0.502808  8.095736
15   0.502197  8.098551  0.507812  8.002534
16   0.498657  8.146176  0.498535  8.144578
17   0.495972  8.182828  0.508179  7.983128
18   0.510132  7.948929  0.491089  8.253199
19   0.496216  8.168030  0.493530  8.208810
20   0.499023  8.117862  0.501099  8.082021
21   0.502686  8.054134  0.505737  8.002650
22   0.497681  8.130296  0.513428  7.874288
23   0.509277  7.939079  0.499268  8.098334
24   0.501221  8.064868  0.496338  8.141614
25   0.501831  8.051227  0.499023  8.094667
26   0.502563  8.035913  0.508057  7.945719
27   0.502075  8.040598  0.499023  8.088302
28   0.500977  8.055464  0.505005  7.989227
29   0.493774  8.169060  0.497559  8.106936
30   0.490845  8.214145  0.493042  8.177774
31   0.502930  8.017566  0.504761  7.987263
32   0.504761  7.986583  0.505005  7.982012
33   0.498169  8.091656  0.504639  7.986877
34   0.500000  8.061229  0.494263  8.153324
35   0.496826  8.111696  0.509277  7.910726
36   0.499512  8.067905  0.505615  7.969327
37   0.497803  8.095094  0.503906  7.996579
38   0.508179  7.927610  0.500244  8.055408
39   0.504028  7.994347  0.504150  7.992321
40   0.500122  8.057208  0.493042  8.171290
41   0.498047  8.090597  0.507690  7.935141
42   0.506104  7.960705  0.487183  8.265663
43   0.493042  8.171214  0.504761  7.982324
44   0.496948  8.108244  0.492554  8.179073
45   0.488281  8.247935  0.511719  7.870166
46   0.497314  8.102335  0.499634  8.064951
47   0.500732  8.047243  0.512085  7.864261
48   0.493896  8.157425  0.501831  8.029535
49   0.497925  8.092221  0.499634  8.150424
50   0.495850  8.220558  0.495483  8.214975
51   0.492188  8.258272  0.502075  8.092384
52   0.491089  8.260953  0.506348  8.011694
53   0.490845  8.253911  0.506958  7.992463
54   0.509277  7.951637  0.496826  8.146539
55   0.499512  8.100625  0.494507  8.177490
56   0.506714  7.980312  0.511841  7.896129
57   0.501343  8.061303  0.499512  8.088389
58   0.505005  7.998894  0.509155  7.930862
59   0.497192  8.119845  0.492798  8.188206
60   0.495117  8.149623  0.503540  8.013754
61   0.498413  8.093962  0.504272  7.999029
62   0.495361  8.139612  0.498535  8.087529
63   0.508667  7.924542  0.494141  8.154656
64   0.496094  8.122059  0.495361  8.132263
65   0.491455  8.193070  0.498657  8.076766
66   0.506592  7.948785  0.492065  8.178868
67   0.500122  8.048921  0.496460  8.105780
68   0.510132  7.886291  0.496582  8.100761
69   0.502197  8.009691  0.508911  7.901086
70   0.500610  8.031849  0.500610  8.030258
71   0.493408  8.143484  0.504028  7.972562
72   0.506104  7.937866  0.489502  8.200902
73   0.503296  7.979362  0.498535  8.053608
74   0.495117  8.106450  0.500366  8.021099
75   0.496338  8.083655  0.506714  7.916554
76   0.490601  8.171758  0.498413  8.045512
77   0.511841  7.829751  0.500977  8.001246
78   0.502686  7.972303  0.498413  8.038704
79   0.495605  8.081764  0.485596  8.239631
80   0.504761  7.932399  0.503906  7.944317
81   0.503418  7.950419  0.505493  7.915647
82   0.498901  8.019078  0.511597  7.815025
83   0.495850  8.064449  0.494263  8.088130
84   0.494019  8.090452  0.492432  8.114189
85   0.489990  8.151606  0.499023  8.006105
86   0.493652  8.090311  0.495361  8.061663
87   0.496704  8.038931  0.502686  7.942272
88   0.501343  7.962465  0.512329  7.786132
89   0.506592  7.876505  0.506958  7.869608
90   0.494385  8.069092  0.509766  7.822956
91   0.497192  8.022572  0.499634  7.982855
92   0.486328  8.194277  0.496826  8.026248
93   0.494507  8.062645  0.505249  7.890846
94   0.507080  7.861190  0.488281  8.160456
95   0.510132  7.811742  0.503906  7.910658
96   0.501343  7.951249  0.502686  7.929589
97   0.505615  7.882678  0.500977  7.956444
98   0.505737  7.880399  0.508179  7.841345
99   0.508179  7.841242  0.496216  8.031867
100  0.493530  8.074611  0.509521  7.819612
101  0.507568  7.850703  0.500122  7.969375
102  0.502563  7.930424  0.502808  7.926507
103  0.494629  8.056877  0.503906  7.908958
104  0.510132  7.809697  0.500854  7.957592
105  0.497559  8.010130  0.508057  7.842761
106  0.498169  8.000391  0.498779  7.990658
107  0.495239  8.047094  0.500366  7.965356
108  0.505371  7.885566  0.497925  8.004277
109  0.507812  7.846643  0.497559  8.010114
110  0.504272  7.971667  0.498535  8.153743
111  0.503662  8.059641  0.503906  8.047201
112  0.504761  8.028234  0.507446  7.980685
113  0.494995  8.178459  0.501343  8.073667
114  0.496216  8.154516  0.502563  8.050655
115  0.493042  8.202981  0.498413  8.115411
116  0.486450  8.307487  0.506836  7.978257
117  0.498291  8.115496  0.492065  8.215412
118  0.492676  8.205249  0.502930  8.039688
119  0.499390  8.096524  0.508911  7.942855
120  0.492920  8.209598  0.494263  8.199397
121  0.502930  8.038189  0.506104  7.974710
122  0.506104  7.969279  0.499512  8.070299
123  0.496704  8.112511  0.498657  8.079207
124  0.492188  8.180709  0.495972  8.118890
125  0.510986  7.878263  0.501099  8.034707
126  0.493652  8.152352  0.504639  7.976167
127  0.499390  8.058874  0.496948  8.096833

2018-02-25 11:28:45.679956 Finish.
Total elapsed time: 15:01:57.68.
