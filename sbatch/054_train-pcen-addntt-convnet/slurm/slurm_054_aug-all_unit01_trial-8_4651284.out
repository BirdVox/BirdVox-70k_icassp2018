2018-02-24 20:27:55.336963: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:55.337256: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:55.337272: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:55.337279: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:55.337286: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:52.191039 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit02, unit03, unit05.
Validation set: unit07, unit10.
Test set: unit01.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.528931  7.374654  0.499023  8.080751
1    0.485596  8.295990  0.499268  8.074558
2    0.504639  7.952910  0.507935  7.947304
3    0.503906  8.010426  0.498291  8.099383
4    0.500854  8.056903  0.492310  8.193595
5    0.501465  8.045206  0.494751  8.152670
6    0.497925  8.100894  0.496460  8.123929
7    0.493042  8.178528  0.494873  8.148552
8    0.501587  8.039930  0.499634  8.071021
9    0.501465  8.041157  0.493530  8.168708
10   0.509155  7.916552  0.503784  8.002820
11   0.496094  8.126493  0.499146  8.077028
12   0.498291  8.090542  0.499512  8.070611
13   0.499390  8.072338  0.492920  8.176381
14   0.498047  8.093521  0.491943  8.191678
15   0.503052  8.012425  0.500610  8.051573
16   0.500488  8.053352  0.509033  7.915438
17   0.502563  8.019546  0.504028  7.995767
18   0.503540  8.003483  0.497681  8.097775
19   0.504517  7.987456  0.495728  8.128988
20   0.500977  8.044265  0.502075  8.026444
21   0.503174  8.008636  0.502441  8.020346
22   0.495850  8.126509  0.494507  8.148073
23   0.506348  7.957155  0.493164  8.169586
24   0.517578  7.776024  0.500610  8.049463
25   0.501709  8.031714  0.490723  8.208755
26   0.502563  8.017874  0.501221  8.039489
27   0.494629  8.145714  0.498169  8.088636
28   0.508301  7.925315  0.500244  8.055159
29   0.494385  8.149591  0.502197  8.023660
30   0.497070  8.106290  0.496460  8.116122
31   0.502319  8.021676  0.497437  8.100375
32   0.495605  8.129886  0.499390  8.068890
33   0.507324  7.940998  0.502686  8.015764
34   0.497559  8.098400  0.499390  8.068886
35   0.503052  8.009860  0.495117  8.137750
36   0.497559  8.098399  0.498169  8.088273
37   0.510742  7.949551  0.514160  7.896098
38   0.501221  8.092679  0.491943  8.231700
39   0.507812  7.970940  0.509033  7.944015
40   0.502930  8.034513  0.500732  8.062944
41   0.498779  8.087996  0.490845  8.208569
42   0.498047  8.088259  0.501465  8.028414
43   0.492554  8.165502  0.504150  7.975765
44   0.489136  8.210614  0.505249  7.949316
45   0.502686  7.986077  0.505371  7.939255
46   0.491089  8.163222  0.496826  8.068122
47   0.503296  7.961608  0.498779  8.030329
48   0.501221  7.988367  0.498901  8.022387
49   0.503784  7.941817  0.503784  7.939170
50   0.506958  7.886141  0.502441  7.955794
51   0.497437  8.033435  0.507080  7.877620
52   0.502808  7.943852  0.492065  8.113298
53   0.507080  7.872302  0.500122  7.981671
54   0.495361  8.056180  0.498657  8.002313
55   0.506836  7.870761  0.500488  7.970855
56   0.501099  7.960165  0.497803  8.011808
57   0.505493  7.888432  0.503784  7.914956
58   0.489868  8.136202  0.501221  7.954653
59   0.504883  7.928901  0.495361  8.212713
60   0.497803  8.150861  0.505615  8.017109
61   0.511841  7.915178  0.506592  7.998723
62   0.493774  8.204629  0.495239  8.180379
63   0.496948  8.152239  0.506348  8.000145
64   0.500732  8.090063  0.488525  8.286219
65   0.502930  8.053443  0.492676  8.218098
66   0.503784  8.038417  0.492676  8.216812
67   0.492676  8.216142  0.500732  8.085595
68   0.483398  8.364273  0.510742  7.922812
69   0.506714  7.986984  0.492432  8.216408
70   0.498535  8.117229  0.501099  8.075088
71   0.490845  8.239516  0.502441  8.051733
72   0.500732  8.078392  0.501465  8.065680
73   0.503540  8.031309  0.503052  8.038238
74   0.506226  7.986127  0.495972  8.150429
75   0.500732  8.072715  0.495239  8.160261
76   0.491943  8.212388  0.498047  8.113003
77   0.499146  8.094289  0.509033  7.933904
78   0.501221  8.058822  0.499268  8.089292
79   0.497070  8.123714  0.504639  8.000729
80   0.497681  8.111906  0.490356  8.228986
81   0.495483  8.145407  0.496094  8.134632
82   0.498535  8.094381  0.501343  8.048237
83   0.508057  7.939174  0.498047  8.099677
84   0.505859  7.972968  0.493530  8.170921
85   0.505371  7.979353  0.497070  8.112449
86   0.492065  8.192479  0.498901  8.081679
87   0.499268  8.075219  0.498169  8.092392
88   0.501709  8.034858  0.497314  8.105238
89   0.495728  8.130422  0.497314  8.104473
90   0.498901  8.078578  0.499023  8.076315
91   0.501099  8.042620  0.497681  8.097485
92   0.501953  8.028435  0.502563  8.018429
93   0.503052  8.010425  0.501343  8.037851
94   0.502808  8.014148  0.496338  8.118347
95   0.498169  8.088772  0.496704  8.112330
96   0.502930  8.011947  0.505737  7.966660
97   0.507690  7.935157  0.497192  8.104347
98   0.496826  8.110236  0.491699  8.192862
99   0.492554  8.179083  0.490601  8.210559
100  0.497925  8.092503  0.502930  8.011831
101  0.495361  8.133817  0.498901  8.076757
102  0.491943  8.188907  0.500122  8.057081
103  0.499390  8.068886  0.506592  7.952801
104  0.494751  8.143652  0.502930  8.011827
105  0.497070  8.106269  0.504883  7.980346
106  0.500977  8.043307  0.492554  8.178022
107  0.506592  7.904922  0.494507  8.095859
108  0.497803  8.043247  0.496338  8.066531
109  0.502197  7.973044  0.504028  7.943775
110  0.508423  7.873631  0.489746  8.171296
111  0.504395  7.937670  0.498047  8.038767
112  0.500977  7.991954  0.497437  8.048279
113  0.497681  8.044266  0.500488  7.999380
114  0.501587  7.981730  0.500244  8.002995
115  0.499512  8.014519  0.504028  7.942354
116  0.495605  8.076464  0.487793  8.200836
117  0.498779  8.025496  0.486328  8.223798
118  0.501831  7.976430  0.508789  7.865279
119  0.504761  7.929262  0.501709  7.977665
120  0.503784  7.944317  0.499512  8.012153
121  0.491943  8.132516  0.504395  7.933708
122  0.498169  8.032633  0.502319  7.966127
123  0.511475  7.819810  0.502808  7.957609
124  0.500366  7.996136  0.512451  7.803063
125  0.503540  7.944695  0.506592  7.895594
126  0.493042  8.111139  0.489502  8.167088
127  0.502319  7.962237  0.501099  7.981170

2018-02-25 12:05:55.439613 Finish.
Total elapsed time: 15:39:03.44.
