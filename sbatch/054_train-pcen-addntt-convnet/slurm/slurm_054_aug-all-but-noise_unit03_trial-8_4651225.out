2018-02-24 20:26:37.120633: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:37.120898: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:37.120911: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:17.169746 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.506348  7.875297  0.496216  8.035612
1    0.509155  7.828456  0.504272  7.905552
2    0.508179  7.842739  0.495361  8.046617
3    0.500488  7.964552  0.498657  7.993461
4    0.496338  8.030237  0.503052  7.923033
5    0.508179  7.841178  0.510254  7.807995
6    0.496948  8.113073  0.498901  8.091242
7    0.510742  7.893128  0.496826  8.113401
8    0.499268  8.072653  0.506714  7.816214
9    0.491455  8.193475  0.495605  8.106778
10   0.505859  7.927984  0.504517  7.937273
11   0.501465  7.978009  0.494141  8.088133
12   0.504639  7.916090  0.501221  7.966567
13   0.502075  7.950030  0.498535  8.003944
14   0.500610  7.969007  0.504639  7.903176
15   0.494507  8.063515  0.510498  7.807546
16   0.498657  7.995558  0.491211  8.113611
17   0.497803  8.008041  0.496582  8.027086
18   0.499756  7.976185  0.495483  8.044038
19   0.499634  7.977684  0.496338  8.030068
20   0.506226  7.872321  0.506592  7.866386
21   0.495605  8.041467  0.508057  7.842908
22   0.496460  8.027748  0.494873  8.053014
23   0.493042  8.082183  0.504395  7.901178
24   0.501465  7.947872  0.503296  7.918671
25   0.504272  7.903095  0.498901  7.988718
26   0.510254  7.807728  0.494507  8.058772
27   0.496948  8.019848  0.506104  7.873890
28   0.497437  8.012062  0.507080  7.857982
29   0.497314  8.276630  0.499512  8.299371
30   0.498291  8.305959  0.506348  8.164517
31   0.500977  8.241348  0.504639  8.173053
32   0.505981  8.143012  0.504639  8.156496
33   0.496948  8.272830  0.492065  8.344043
34   0.507080  8.094921  0.502686  8.158716
35   0.507446  8.075239  0.505127  8.105935
36   0.498779  8.201820  0.500977  8.160028
37   0.498413  8.195223  0.510986  7.986501
38   0.493652  8.260086  0.496338  8.211060
39   0.500244  8.142634  0.491211  8.282843
40   0.501831  8.106566  0.506714  8.022852
41   0.507446  8.006334  0.501587  8.096160
42   0.498047  8.148909  0.486450  8.331620
43   0.499878  8.111294  0.487427  8.308195
44   0.493286  8.210268  0.497803  8.134094
45   0.507812  7.969673  0.505737  8.000146
46   0.509521  7.936452  0.503296  8.034198
47   0.494385  8.175485  0.502808  8.037474
48   0.508789  7.939043  0.497803  8.114185
49   0.498413  8.102614  0.502319  8.037994
50   0.502075  8.040447  0.499634  8.078380
51   0.498901  8.088921  0.498901  8.087712
52   0.504395  7.998096  0.494873  8.150536
53   0.498047  8.098466  0.497437  8.107432
54   0.503540  8.008282  0.515747  7.810794
55   0.500854  8.050187  0.493896  8.161723
56   0.494629  8.149383  0.500488  8.054436
57   0.497070  8.109091  0.502686  8.018176
58   0.512695  7.856490  0.503296  8.007667
59   0.507690  7.936565  0.495728  8.129134
60   0.495605  8.130897  0.493164  8.170061
61   0.504028  7.994800  0.501221  8.039918
62   0.510010  7.898149  0.513184  7.846898
63   0.497681  8.096703  0.493164  8.169439
64   0.510132  7.894029  0.494263  8.116724
65   0.486572  8.343774  0.500732  8.118706
66   0.496338  8.184394  0.494873  8.203328
67   0.493530  8.220335  0.503052  8.064102
68   0.482056  8.394429  0.494141  8.197337
69   0.492310  8.222156  0.498657  8.116568
70   0.491821  8.221232  0.499268  8.098192
71   0.507446  7.963563  0.493530  8.181174
72   0.491699  8.206222  0.493530  8.172890
73   0.498047  8.096852  0.492798  8.176511
74   0.494507  8.145357  0.499512  8.061672
75   0.507812  7.925561  0.502197  8.011322
76   0.495605  8.112775  0.500244  8.035208
77   0.498413  8.060911  0.506592  7.927059
78   0.493774  8.128067  0.498169  8.054705
79   0.498291  8.049594  0.496704  8.071761
80   0.496338  8.074612  0.503784  7.952952
81   0.506226  7.911233  0.497314  8.050545
82   0.495728  8.073249  0.499878  8.004536
83   0.496582  8.054698  0.499756  8.001771
84   0.497314  8.038531  0.498901  8.011127
85   0.497070  8.038380  0.506226  7.890546
86   0.485962  8.211883  0.504517  7.914424
87   0.507568  7.864277  0.500732  7.971823
88   0.497437  8.023082  0.499146  7.994609
89   0.502441  7.940977  0.499268  7.990541
90   0.509033  7.833946  0.500854  7.963477
91   0.501831  7.947164  0.500122  7.973710
92   0.510742  7.803801  0.510498  7.807133
93   0.504883  7.896178  0.501465  7.950229
94   0.498779  7.992676  0.508667  7.834704
95   0.500366  7.966760  0.492554  8.091057
96   0.508179  7.841752  0.502930  7.925249
97   0.498413  7.997109  0.489258  8.142935
98   0.493530  8.074722  0.506104  7.874186
99   0.495483  8.043429  0.495850  8.037533
100  0.497437  8.012192  0.492554  8.090000
101  0.497925  8.004347  0.503052  7.922591
102  0.492798  8.086048  0.499512  7.979002
103  0.491211  8.111328  0.489258  8.142460
104  0.504395  7.901141  0.496460  8.015157
105  0.498535  8.124432  0.499512  8.185551
106  0.491577  8.311518  0.487549  8.374474
107  0.504761  8.095040  0.509521  8.016261
108  0.496338  8.226686  0.496826  8.216712
109  0.493896  8.261818  0.504028  8.096370
110  0.499878  8.161122  0.497314  8.200274
111  0.502441  8.115483  0.489380  8.323838
112  0.497070  8.197735  0.502319  8.110972
113  0.501465  8.122621  0.494141  8.238543
114  0.497803  8.177431  0.498169  8.169442
115  0.491333  8.277589  0.505127  8.053223
116  0.499512  8.141753  0.505371  8.045338
117  0.490723  8.279527  0.505005  8.047415
118  0.494019  8.222640  0.497559  8.163732
119  0.496460  8.179644  0.497925  8.154241
120  0.507446  7.999027  0.495117  8.196004
121  0.493408  8.221846  0.500854  8.100121
122  0.499268  8.124027  0.504150  8.043650
123  0.500244  8.104963  0.494873  8.189880
124  0.498535  8.129222  0.494385  8.194477
125  0.500000  8.102348  0.496094  8.163677
126  0.506470  7.994823  0.490723  8.247012
127  0.497314  8.139160  0.495239  8.170995

2018-02-25 12:01:20.977021 Finish.
Total elapsed time: 15:35:03.98.
