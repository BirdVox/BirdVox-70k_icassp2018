2018-02-24 20:26:37.559323: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:37.559578: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:37.559591: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:16.680780 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit01, unit02, unit03.
Validation set: unit05, unit07.
Test set: unit10.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.924194  0.231223  0.934814  0.195079
1    0.928955  0.215823  0.936523  0.200410
2    0.873535  0.401694  0.888916  0.349856
3    0.898804  0.313738  0.909668  0.271647
4    0.916992  0.263669  0.934937  0.220469
5    0.686157  4.597815  0.482056  8.287709
6    0.502930  7.949520  0.503418  7.937301
7    0.498169  8.017904  0.499390  7.995794
8    0.498169  8.013296  0.504883  7.904538
9    0.494629  8.066690  0.497925  8.012968
10   0.494385  8.068479  0.497314  8.020937
11   0.492554  8.096163  0.499634  7.982677
12   0.493896  8.073641  0.499512  7.983658
13   0.500244  7.956364  0.859253  0.366528
14   0.503906  8.060673  0.502319  8.083016
15   0.499268  8.126346  0.495605  8.180480
16   0.502197  8.070601  0.505859  8.008308
17   0.494629  8.186651  0.504395  8.026770
18   0.488159  8.286327  0.505249  8.008864
19   0.500122  8.089729  0.492432  8.211991
20   0.497559  8.127829  0.505249  8.002405
21   0.509399  7.934166  0.501709  8.056820
22   0.501953  8.051683  0.499756  8.085928
23   0.507690  7.956948  0.506836  7.969655
24   0.494873  8.161475  0.497803  8.113275
25   0.505005  7.996269  0.501953  8.044553
26   0.504883  7.996478  0.498779  8.094016
27   0.499390  8.083385  0.495972  8.137695
28   0.502075  8.038580  0.500000  8.071302
29   0.498413  8.096193  0.497437  8.111258
30   0.501953  8.037821  0.500977  8.052935
31   0.494751  8.152690  0.489624  8.234747
32   0.501221  8.047287  0.498535  8.090040
33   0.500854  8.052158  0.506714  7.957228
34   0.495117  8.143690  0.498413  8.090122
35   0.511719  7.875248  0.500732  8.051926
36   0.493042  8.175511  0.500610  8.053164
37   0.500732  8.050866  0.500610  8.052515
38   0.495850  8.128957  0.499512  8.069649
39   0.498291  8.089070  0.500244  8.057343
40   0.498535  8.084668  0.507935  7.932956
41   0.503906  7.997696  0.498535  8.084088
42   0.500488  8.052448  0.504028  7.995238
43   0.502197  8.024619  0.498291  8.087455
44   0.502441  8.020450  0.506348  7.957386
45   0.503174  8.008455  0.510010  7.898191
46   0.496704  8.112584  0.507202  7.943312
47   0.499512  8.067214  0.489624  8.226536
48   0.508667  7.919559  0.509766  7.901814
49   0.504639  7.984421  0.502075  8.025713
50   0.494873  8.141777  0.512329  7.860400
51   0.488525  8.244055  0.489990  8.220432
52   0.495483  8.131883  0.507812  7.933153
53   0.482910  8.334525  0.493286  8.167279
54   0.507935  7.931170  0.507690  7.935101
55   0.498413  8.084632  0.496704  8.112176
56   0.492188  8.178170  0.502930  7.992415
57   0.495850  8.099482  0.502563  7.989331
58   0.506470  7.924689  0.500366  8.019681
59   0.512329  7.826793  0.503052  7.972567
60   0.500854  8.005602  0.498657  8.038682
61   0.495117  8.093301  0.488647  8.194672
62   0.511353  7.831055  0.495361  8.084394
63   0.509521  7.857172  0.498413  8.032832
64   0.498901  8.023728  0.494629  8.090559
65   0.497925  8.036834  0.499146  8.016227
66   0.512451  7.803044  0.502441  7.961594
67   0.496582  8.054051  0.504395  7.928568
68   0.491577  8.132035  0.492188  8.121449
69   0.495850  8.062260  0.504883  7.917454
70   0.502319  7.957563  0.490601  8.143637
71   0.498535  8.016418  0.493164  8.101325
72   0.489624  8.157062  0.491089  8.133009
73   0.488647  8.171244  0.489624  8.154988
74   0.507080  7.876019  0.490967  8.132221
75   0.505859  7.894125  0.498413  8.012158
76   0.499390  7.995918  0.507568  7.864853
77   0.496704  8.037387  0.500977  7.968600
78   0.502075  7.950422  0.515015  7.743469
79   0.496826  8.032781  0.495361  8.055479
80   0.493896  8.078191  0.500854  7.966625
81   0.494873  8.061365  0.494995  8.058804
82   0.506592  7.873336  0.491821  8.108231
83   0.508545  7.841064  0.493286  8.083784
84   0.501465  7.952888  0.503174  7.925148
85   0.507324  7.858525  0.497070  8.021554
86   0.503540  7.918010  0.490601  8.123910
87   0.509766  7.818029  0.494751  8.057071
88   0.492676  8.089867  0.506348  7.871634
89   0.495239  8.048495  0.509521  7.820585
90   0.501587  7.946898  0.491577  8.106308
91   0.491333  8.110061  0.495605  8.041821
92   0.505493  7.884085  0.504517  7.899562
93   0.510010  7.811915  0.499146  7.985053
94   0.502197  7.936351  0.493652  8.072534
95   0.492065  8.097801  0.505005  7.891488
96   0.494385  8.060778  0.494385  8.060760
97   0.502319  7.934252  0.505371  7.885590
98   0.499756  7.975103  0.499268  7.982882
99   0.503296  7.918657  0.505615  7.881679
100  0.508423  7.836916  0.498901  7.988710
101  0.503906  7.908919  0.501099  7.953679
102  0.488037  8.161910  0.493408  8.076282
103  0.505615  7.881672  0.506470  7.868049
104  0.507324  7.854427  0.496338  8.029575
105  0.491333  8.245203  0.509155  7.959200
106  0.496826  8.156583  0.501221  8.085107
107  0.501099  8.086770  0.496338  8.163221
108  0.502075  8.070470  0.503052  8.054447
109  0.493896  8.201720  0.502686  8.059754
110  0.505493  8.014188  0.501709  8.074860
111  0.495728  8.170938  0.502808  8.056480
112  0.500244  8.097449  0.507080  7.986909
113  0.502563  8.059344  0.494995  8.180959
114  0.503296  8.046788  0.498779  8.119202
115  0.495972  8.164067  0.493896  8.197120
116  0.495972  8.163275  0.499878  8.099911
117  0.497070  8.144762  0.510498  7.927925
118  0.503906  8.033767  0.500244  8.092383
119  0.494385  8.186418  0.501465  8.071891
120  0.491943  8.224951  0.512085  7.899896
121  0.494141  8.188716  0.496094  8.156824
122  0.494385  8.183959  0.505615  8.002531
123  0.499756  8.096558  0.495361  8.166969
124  0.499634  8.097683  0.509888  7.931979
125  0.499023  8.106655  0.494751  8.175075
126  0.509033  7.944419  0.493530  8.193833
127  0.496582  8.144168  0.507202  7.972504

2018-02-25 11:40:39.880611 Finish.
Total elapsed time: 15:14:23.88.
