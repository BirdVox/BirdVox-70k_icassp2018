2018-02-24 20:27:40.869039: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:40.870083: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:40.870107: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:40.870117: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:40.870129: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:49.462716 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit07, unit10, unit01.
Validation set: unit02, unit03.
Test set: unit05.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.940430  0.202373  0.938599  0.286441
1    0.707153  0.591299  0.635986  0.719363
2    0.642090  0.684233  0.708496  0.579946
3    0.886108  0.338165  0.902588  0.290220
4    0.914795  0.269363  0.903198  0.426497
5    0.928467  0.230757  0.951172  0.183654
6    0.937622  0.196199  0.932129  0.271666
7    0.791016  2.880235  0.496826  8.132777
8    0.495605  8.149030  0.493286  8.183744
9    0.492676  8.191649  0.501099  8.054142
10   0.505005  7.989740  0.495239  8.145806
11   0.503662  8.008910  0.504517  7.993456
12   0.580811  5.785239  0.490845  8.144243
13   0.502808  7.948662  0.503052  7.940800
14   0.503784  7.926266  0.493164  8.093057
15   0.503418  7.927631  0.497314  8.023181
16   0.499023  7.994546  0.497803  8.012752
17   0.499634  7.982558  0.508301  7.843479
18   0.505981  7.879731  0.504272  7.906322
19   0.499268  7.985590  0.493286  8.080479
20   0.493530  8.076215  0.488892  8.149832
21   0.503540  7.916039  0.495850  8.038408
22   0.497559  8.010979  0.507446  7.853183
23   0.502808  7.927009  0.496826  8.022256
24   0.501709  7.944327  0.497070  8.018204
25   0.499390  7.981171  0.503784  7.911061
26   0.501343  7.949945  0.490234  8.127007
27   0.507935  7.844799  0.498413  7.996572
28   0.496460  8.027694  0.495605  8.041302
29   0.494141  8.116848  0.503418  8.018225
30   0.509644  7.917172  0.496460  8.129052
31   0.502197  8.036037  0.491089  8.214559
32   0.496826  8.121599  0.511597  7.883051
33   0.505859  7.975078  0.488159  8.259932
34   0.496216  8.129658  0.503418  8.013163
35   0.498413  8.093444  0.504761  7.990749
36   0.500244  8.063185  0.510498  7.897554
37   0.496338  8.125449  0.518188  7.772925
38   0.498535  8.089383  0.503296  8.012337
39   0.501221  8.045490  0.498169  8.094388
40   0.502930  8.017379  0.498169  8.093843
41   0.491577  8.199835  0.497681  8.101205
42   0.486694  8.278046  0.500122  8.061382
43   0.497437  8.104445  0.499390  8.072746
44   0.505249  7.978096  0.506714  7.954281
45   0.502686  8.019016  0.482056  8.351339
46   0.501953  8.030447  0.499634  8.067651
47   0.494385  8.152086  0.500854  8.047638
48   0.493164  8.171435  0.499512  8.068966
49   0.504028  7.996020  0.508057  7.930946
50   0.496338  8.119694  0.503662  8.001509
51   0.492310  8.184366  0.506836  7.950107
52   0.494019  8.156588  0.499390  8.069907
53   0.496948  8.109159  0.496216  8.120868
54   0.508057  7.929930  0.498047  8.091185
55   0.500366  8.053727  0.487915  8.254345
56   0.494385  8.150003  0.494995  8.140106
57   0.491333  8.199081  0.502930  8.012117
58   0.504883  7.980595  0.507080  7.945141
59   0.505127  7.976590  0.502808  8.013943
60   0.500000  8.059173  0.497559  8.098502
61   0.500244  8.055198  0.498901  8.076824
62   0.495850  8.126000  0.490356  8.214528
63   0.498413  8.084662  0.500977  8.043335
64   0.502930  8.011849  0.502197  8.023649
65   0.494385  8.149568  0.500977  8.043318
66   0.497803  8.094471  0.495483  8.131853
67   0.515381  7.811142  0.499390  8.068889
68   0.496460  8.116109  0.498779  8.078725
69   0.505859  7.964607  0.498657  8.080692
70   0.508301  7.925256  0.500244  8.055113
71   0.503784  7.998054  0.499756  8.062983
72   0.504639  7.984281  0.498169  8.088561
73   0.504883  7.980346  0.502319  8.021664
74   0.497192  8.124685  0.501221  8.138098
75   0.501831  8.104621  0.500610  8.115840
76   0.505737  8.031054  0.502319  8.084823
77   0.502930  8.074459  0.494995  8.201988
78   0.505859  8.026685  0.506104  8.022593
79   0.499023  8.136589  0.491699  8.254526
80   0.510498  7.951412  0.494507  8.209045
81   0.503174  8.069232  0.501587  8.094689
82   0.496094  8.183100  0.505127  8.037369
83   0.499634  8.125766  0.502930  8.072496
84   0.487183  8.326152  0.495361  8.194164
85   0.505371  8.032654  0.504761  8.042312
86   0.501465  8.095246  0.495728  8.187522
87   0.507812  7.992525  0.498901  8.135936
88   0.500366  8.112093  0.495239  8.194488
89   0.506836  8.007315  0.505493  8.028691
90   0.503174  8.065792  0.502441  8.077305
91   0.489746  8.281618  0.497559  8.155373
92   0.487427  8.318339  0.500366  8.109427
93   0.501709  8.087413  0.494385  8.205079
94   0.500000  8.114168  0.502197  8.078334
95   0.492676  8.231362  0.490234  8.270258
96   0.494995  8.193049  0.497192  8.157142
97   0.495361  8.186143  0.495850  8.177744
98   0.494141  8.204740  0.500977  8.093990
99   0.505249  8.024538  0.494873  8.191173
100  0.495483  8.180708  0.496460  8.164323
101  0.505249  8.021994  0.498657  8.127557
102  0.499146  8.118982  0.499756  8.108421
103  0.498413  8.129320  0.502319  8.065596
104  0.500366  8.096294  0.501465  8.077784
105  0.498901  8.118281  0.504395  8.028901
106  0.503784  8.037878  0.503052  8.048804
107  0.495117  8.175794  0.504272  8.027310
108  0.496582  8.150328  0.503906  8.031319
109  0.504761  8.016572  0.492188  8.218235
110  0.487061  8.299863  0.505493  8.001736
111  0.499634  8.095137  0.494385  8.178682
112  0.487549  8.287795  0.502075  8.052572
113  0.500610  8.075093  0.493164  8.194008
114  0.494873  8.165359  0.507690  7.957653
115  0.501343  8.058857  0.512085  7.884599
116  0.495239  8.155021  0.501709  8.049639
117  0.505371  7.989535  0.491333  8.214725
118  0.497070  8.121207  0.501587  8.047370
119  0.510864  7.896840  0.498169  8.100479
120  0.501587  8.044452  0.495361  8.143876
121  0.501099  8.050537  0.497925  8.100848
122  0.502441  8.027266  0.503540  8.008797
123  0.497070  8.112380  0.494141  8.158930
124  0.497437  8.105201  0.498413  8.088881
125  0.499023  8.078529  0.500977  8.046560
126  0.504395  7.991043  0.510986  7.884394
127  0.497925  8.094577  0.494141  8.155250

2018-02-25 10:46:52.729082 Finish.
Total elapsed time: 14:20:03.73.
