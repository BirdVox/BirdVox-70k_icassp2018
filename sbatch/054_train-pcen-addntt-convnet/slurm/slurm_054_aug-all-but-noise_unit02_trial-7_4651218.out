2018-02-24 20:26:42.202154: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:42.202439: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:42.202452: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:17.920550 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.500488  8.060557  0.507446  7.946617
1    0.493164  8.175908  0.510498  7.895266
2    0.497925  8.097276  0.505249  7.977528
3    0.502441  8.023132  0.500488  8.053689
4    0.489258  8.219417  0.502563  8.085497
5    0.498169  8.141887  0.499756  8.106634
6    0.494995  8.177497  0.495850  8.158717
7    0.497314  8.131335  0.500122  8.082696
8    0.506592  7.975681  0.493164  8.189596
9    0.493896  8.175683  0.506592  7.969092
10   0.497437  8.114974  0.491211  8.213736
11   0.496826  8.121857  0.510498  7.900198
12   0.498169  8.097792  0.500366  8.061311
13   0.499268  8.078090  0.496460  8.122468
14   0.496826  8.115805  0.494385  8.154441
15   0.505249  7.978713  0.502930  8.015518
16   0.492554  8.182265  0.504028  7.996856
17   0.489502  8.230604  0.498291  8.088581
18   0.508179  7.928910  0.502441  8.021107
19   0.497192  8.105484  0.490234  8.217427
20   0.501587  8.034278  0.507568  7.937716
21   0.505127  7.976947  0.496338  8.118502
22   0.494751  8.143996  0.501953  8.027837
23   0.508423  7.923501  0.500610  8.049374
24   0.503418  8.004084  0.498413  8.084722
25   0.496094  8.122082  0.504028  7.994173
26   0.508179  7.927263  0.500732  8.047271
27   0.494629  8.145641  0.496582  8.114153
28   0.497681  8.096441  0.502563  8.017736
29   0.499023  8.074793  0.504150  7.992154
30   0.497437  8.100368  0.497070  8.105986
31   0.504028  8.007399  0.497925  8.244121
32   0.496460  8.233849  0.497681  8.192130
33   0.497559  8.180862  0.504517  8.058164
34   0.505249  8.036598  0.499634  8.116719
35   0.498901  8.119928  0.505615  8.004703
36   0.501831  8.057501  0.502441  8.040447
37   0.497192  8.117350  0.499146  8.079611
38   0.494385  8.149391  0.503296  8.001369
39   0.508179  7.918013  0.505737  7.951571
40   0.503784  7.977758  0.496338  8.091659
41   0.499878  8.030796  0.492554  8.143269
42   0.496948  8.069274  0.502441  7.977890
43   0.494019  8.108690  0.508179  7.879583
44   0.493896  8.104218  0.505737  7.912499
45   0.505493  7.913722  0.504395  7.928668
46   0.503052  7.947759  0.509033  7.850178
47   0.496582  8.046682  0.499146  8.003901
48   0.503174  7.937969  0.508911  7.844867
49   0.493042  8.096403  0.489990  8.143666
50   0.494751  8.066535  0.491455  8.117907
51   0.501099  7.963129  0.509766  7.823972
52   0.504395  7.908733  0.502075  7.944888
53   0.494629  8.062880  0.499512  7.984356
54   0.496704  8.028523  0.504517  7.903414
55   0.498413  8.000233  0.493408  8.079566
56   0.506226  7.874832  0.502808  7.928953
57   0.492065  8.099893  0.492920  8.085974
58   0.500000  7.972849  0.504883  7.894771
59   0.499634  7.978255  0.500610  7.962504
60   0.496704  8.024626  0.510498  7.804577
61   0.493774  8.071076  0.499268  7.983395
62   0.491211  8.111751  0.502808  7.926794
63   0.506714  7.864456  0.501099  7.953919
64   0.500244  7.967497  0.495728  8.039462
65   0.493652  8.072515  0.505737  7.879825
66   0.498047  8.002408  0.498535  7.994605
67   0.508301  7.838904  0.490723  8.119130
68   0.504028  7.906998  0.497803  8.006241
69   0.489990  8.130786  0.498779  7.990663
70   0.497803  8.006229  0.500244  7.967305
71   0.506958  7.860269  0.499146  7.984817
72   0.503418  7.916703  0.497803  8.006223
73   0.487793  8.165802  0.497803  8.006222
74   0.497559  8.010114  0.500732  7.959516
75   0.501221  7.951731  0.496582  8.025683
76   0.497925  8.041246  0.495605  8.141861
77   0.492676  8.178702  0.515625  7.807268
78   0.503418  8.003976  0.495117  8.137752
79   0.553589  2.600993  0.499023  8.025443
80   0.500000  8.008599  0.503418  7.953321
81   0.506592  7.902549  0.499878  8.009490
82   0.503174  7.956908  0.500488  7.999690
83   0.504150  7.941280  0.494141  8.100833
84   0.487671  8.203948  0.498413  8.032663
85   0.506470  7.904188  0.507935  7.880801
86   0.503418  7.952771  0.503418  7.952734
87   0.500000  8.007184  0.494751  8.090824
88   0.504639  7.933144  0.499268  8.018723
89   0.499023  8.022564  0.512817  7.802600
90   0.496582  8.061371  0.501465  7.983465
91   0.494873  8.088487  0.510010  7.847103
92   0.491455  8.142833  0.500610  7.996797
93   0.493408  8.111532  0.500122  8.004408
94   0.502930  7.959552  0.488037  8.196875
95   0.499512  8.013835  0.491821  8.136326
96   0.497314  8.048631  0.493774  8.104941
97   0.503418  7.951064  0.498169  8.034604
98   0.507202  7.890441  0.496338  8.063485
99   0.494629  8.090560  0.504272  7.936642
100  0.500366  7.998727  0.500610  7.994637
101  0.497192  8.048917  0.504639  7.929985
102  0.505249  7.920021  0.500610  7.993729
103  0.495605  8.073259  0.503906  7.940656
104  0.494629  8.088273  0.492554  8.121060
105  0.515503  7.754881  0.495728  8.069822
106  0.502075  7.968282  0.505371  7.915382
107  0.501465  7.977283  0.496216  8.060579
108  0.493774  8.099095  0.497925  8.032509
109  0.502930  7.952283  0.501465  7.975186
110  0.492188  8.122621  0.505371  7.911961
111  0.497192  8.041851  0.502197  7.961550
112  0.498413  8.021352  0.505859  7.902100
113  0.498047  8.026097  0.508423  7.860114
114  0.503418  7.939327  0.505127  7.911494
115  0.494873  8.074371  0.491577  8.126310
116  0.498901  8.008933  0.504639  7.916848
117  0.508301  7.857845  0.500122  7.987606
118  0.507690  7.866323  0.499512  7.996081
119  0.497437  8.028541  0.498291  8.014291
120  0.493286  8.093462  0.504517  7.913803
121  0.510864  7.812000  0.496216  8.044924
122  0.499878  7.985952  0.502197  7.948389
123  0.502441  7.943928  0.494751  8.065967
124  0.500000  7.981742  0.510986  7.806056
125  0.511230  7.801649  0.500488  7.972397
126  0.500244  7.975807  0.503052  7.930571
127  0.501343  7.957367  0.498901  7.995847

2018-02-25 12:12:11.615462 Finish.
Total elapsed time: 15:45:54.62.
