2018-02-24 20:27:57.652353: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:57.652711: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:57.652723: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:57.652730: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:57.652736: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:52.367718 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit07, unit10, unit01.
Validation set: unit02, unit03.
Test set: unit05.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.501831  8.042774  0.494019  8.167396
1    0.494019  8.166371  0.498779  8.088693
2    0.506470  7.963921  0.507202  7.951338
3    0.503418  8.011632  0.497192  8.111303
4    0.502808  8.020184  0.496338  8.123872
5    0.510132  7.901002  0.505371  7.977218
6    0.492798  8.179405  0.493408  8.169116
7    0.487183  8.269056  0.507812  7.936155
8    0.502075  8.028285  0.490601  8.212907
9    0.492188  8.187042  0.500732  8.049043
10   0.499390  8.070452  0.505615  7.969887
11   0.499390  8.070045  0.499634  8.065935
12   0.496704  8.113010  0.494629  8.146323
13   0.499390  8.069479  0.502197  8.024124
14   0.509766  7.902054  0.492432  8.181371
15   0.505127  7.976688  0.498901  8.076980
16   0.495361  8.133997  0.500366  8.053291
17   0.501465  8.035555  0.491211  8.200804
18   0.495361  8.133889  0.505005  7.978438
19   0.498535  8.082721  0.490356  8.214523
20   0.486450  8.277475  0.496338  8.118097
21   0.501221  8.039390  0.505615  7.968554
22   0.512085  7.864272  0.496948  8.108244
23   0.501465  8.035443  0.503296  8.004943
24   0.489746  8.186427  0.499634  7.997765
25   0.504517  7.915716  0.507446  7.865544
26   0.501831  7.952661  0.491943  8.108237
27   0.497559  8.017218  0.493286  8.084027
28   0.507446  7.857307  0.504883  7.897319
29   0.501221  7.955052  0.502319  7.936963
30   0.504517  7.901492  0.506958  7.862177
31   0.498779  7.992261  0.502808  7.927767
32   0.493896  8.069619  0.500366  7.966285
33   0.503662  7.913590  0.490356  8.125579
34   0.505127  7.889995  0.503784  7.911305
35   0.500854  7.957935  0.496460  8.027926
36   0.505981  7.876077  0.492432  8.092045
37   0.496094  8.033625  0.506592  7.866228
38   0.499512  7.979076  0.496094  8.033545
39   0.500122  7.969307  0.509399  7.821390
40   0.505005  7.891438  0.503174  7.920621
41   0.494995  8.051003  0.502075  7.938124
42   0.506958  7.860276  0.502441  7.932279
43   0.502319  7.934223  0.502686  7.928383
44   0.499878  7.973142  0.500610  7.961464
45   0.490845  8.117151  0.497803  8.006223
46   0.499756  7.975085  0.483643  8.231969
47   0.497681  8.008168  0.501465  7.947840
48   0.492065  8.097688  0.501099  7.953678
49   0.502686  7.928383  0.503662  7.912812
50   0.502075  7.983790  0.500977  8.124715
51   0.494995  8.203508  0.486938  8.324970
52   0.499512  8.119613  0.496704  8.162910
53   0.507568  7.986479  0.500122  8.105284
54   0.496338  8.165186  0.508667  7.965395
55   0.501343  8.082427  0.494873  8.185694
56   0.493042  8.214227  0.495972  8.166030
57   0.497681  8.137535  0.498535  8.122815
58   0.501587  8.072699  0.501343  8.075706
59   0.496216  8.157431  0.509399  7.944021
60   0.499146  8.108391  0.497314  8.136994
61   0.497192  8.138060  0.487549  8.292585
62   0.496216  8.151983  0.496704  8.143197
63   0.494995  8.169827  0.496826  8.139388
64   0.496826  8.138463  0.507935  7.958480
65   0.490723  8.234966  0.504395  8.013655
66   0.504517  8.010741  0.506714  7.974369
67   0.499023  8.097373  0.506958  7.968522
68   0.509644  7.924283  0.501221  8.059084
69   0.505249  7.993208  0.489868  8.240166
70   0.494019  8.172335  0.498535  8.098599
71   0.502563  8.032756  0.499878  8.075129
72   0.498169  8.101789  0.495117  8.150097
73   0.510498  7.901339  0.505737  7.977234
74   0.501221  8.049233  0.503540  8.011060
75   0.492310  8.191328  0.499390  8.076480
76   0.502075  8.032510  0.507690  7.941337
77   0.498535  8.088289  0.506958  7.951933
78   0.503296  8.010418  0.500366  8.057118
79   0.496338  8.121581  0.493408  8.168356
80   0.495605  8.132549  0.488770  8.242361
81   0.506958  7.948877  0.497925  8.094175
82   0.502075  8.027025  0.498657  8.081881
83   0.504883  7.981341  0.500488  8.051994
84   0.492676  8.177772  0.505737  7.967114
85   0.507446  7.939466  0.494019  8.155803
86   0.506470  7.956207  0.498535  8.135169
87   0.496582  8.173480  0.500000  8.118498
88   0.503906  8.055348  0.504272  8.048611
89   0.515015  7.876407  0.494141  8.208206
90   0.503784  8.053423  0.491943  8.241111
91   0.495972  8.175747  0.500488  8.102552
92   0.504517  8.037074  0.510254  7.944302
93   0.501587  8.081097  0.505249  8.021283
94   0.495728  8.171573  0.496338  8.160280
95   0.499756  8.104150  0.501831  8.069368
96   0.500366  8.090942  0.502686  8.052126
97   0.499512  8.100804  0.501343  8.069630
98   0.505249  8.005292  0.507080  7.973973
99   0.504883  8.006798  0.505737  7.990905
100  0.496948  8.128680  0.499756  8.081512
101  0.493286  8.182178  0.490356  8.226344
102  0.495972  8.134221  0.499146  8.080959
103  0.497925  8.097704  0.500732  8.050171
104  0.501221  8.039569  0.503662  7.997778
105  0.508179  7.922873  0.500366  8.044476
106  0.496948  8.096002  0.503784  7.984017
107  0.500244  8.037448  0.502563  7.997435
108  0.500977  8.019713  0.501099  8.014722
109  0.508423  7.894946  0.494629  8.111830
110  0.499878  8.025178  0.495239  8.096156
111  0.508423  7.883078  0.500732  8.002790
112  0.499390  8.021400  0.508423  7.874610
113  0.508423  7.871945  0.503662  7.945208
114  0.496948  8.049738  0.499390  8.008353
115  0.502197  7.961277  0.510986  7.818892
116  0.504883  7.914089  0.494019  8.085242
117  0.512817  7.783663  0.499878  7.988131
118  0.496582  8.039031  0.499756  7.986855
119  0.503662  7.923172  0.504517  7.908211
120  0.500244  7.975150  0.496216  8.038264
121  0.502686  7.934168  0.502686  7.933278
122  0.494507  8.062914  0.509277  7.826742
123  0.501221  7.954610  0.495117  8.051391
124  0.503296  7.920579  0.496704  8.025286
125  0.506226  7.873190  0.506592  7.867084
126  0.495972  8.036189  0.496582  8.026279
127  0.498169  8.000846  0.504883  7.893695

2018-02-25 11:45:25.023444 Finish.
Total elapsed time: 15:18:33.02.
