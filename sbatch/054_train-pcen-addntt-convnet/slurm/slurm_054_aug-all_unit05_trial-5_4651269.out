2018-02-24 20:27:50.726910: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:50.727216: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:50.727236: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:50.727246: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:50.727255: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:52.525069 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit07, unit10, unit01.
Validation set: unit02, unit03.
Test set: unit05.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.913208  0.287155  0.947266  0.183555
1    0.924438  0.242098  0.918213  0.299036
2    0.928589  0.249441  0.954712  0.171025
3    0.933716  0.210973  0.947754  0.175929
4    0.939575  0.200587  0.943115  0.205849
5    0.897583  0.330901  0.900391  0.361939
6    0.925415  0.236370  0.950562  0.174082
7    0.933105  0.221828  0.922729  0.265624
8    0.875732  1.275370  0.485352  8.323088
9    0.493042  8.195644  0.507935  7.952797
10   0.510376  7.911402  0.509766  7.919401
11   0.489990  8.236633  0.495972  8.138821
12   0.500000  8.072688  0.491943  8.201408
13   0.500977  8.051117  0.508667  7.935792
14   0.502197  7.993593  0.493896  8.099300
15   0.500122  7.997999  0.499878  8.000171
16   0.502808  7.952094  0.500488  7.987784
17   0.504517  7.922427  0.499390  8.003072
18   0.502686  7.949533  0.496460  8.047818
19   0.504272  7.922374  0.511719  7.802791
20   0.493042  8.099730  0.495605  8.058066
21   0.509888  7.829627  0.506470  7.883387
22   0.496582  8.040334  0.495239  8.061067
23   0.496216  8.044865  0.500488  7.976131
24   0.496826  8.033930  0.507812  7.858210
25   0.501221  7.962762  0.483643  8.242474
26   0.498535  8.004559  0.503174  7.930127
27   0.495361  8.054229  0.503174  7.929242
28   0.509155  7.833478  0.507446  7.860328
29   0.500244  7.974782  0.492554  8.097030
30   0.497803  8.013021  0.496460  8.034110
31   0.497803  8.012412  0.500732  7.965423
32   0.505859  7.883428  0.500854  7.962967
33   0.503784  7.916031  0.488770  8.155177
34   0.503662  7.917549  0.504761  7.899836
35   0.496094  8.037825  0.507080  7.862498
36   0.501343  7.953799  0.491211  8.115163
37   0.489746  8.138364  0.506470  7.871601
38   0.507324  7.857838  0.502197  7.939436
39   0.502563  7.933910  0.501465  7.979556
40   0.491943  8.122526  0.504028  7.925320
41   0.500732  7.975558  0.496826  8.035904
42   0.503296  7.931356  0.497803  8.017690
43   0.495483  8.053696  0.499512  7.988597
44   0.497070  8.026808  0.498535  8.002801
45   0.499146  7.992526  0.500000  7.978397
46   0.499390  7.987697  0.505859  7.884150
47   0.500854  7.963589  0.505737  7.885413
48   0.490723  8.124488  0.493164  8.085285
49   0.500854  7.962429  0.498779  7.995270
50   0.494019  8.070947  0.497925  8.008458
51   0.509399  7.825328  0.509888  7.817352
52   0.499390  7.984538  0.504761  7.898735
53   0.497803  8.009500  0.497314  8.017125
54   0.498657  7.995568  0.491577  8.108294
55   0.496582  8.028366  0.496704  8.026284
56   0.491943  8.102053  0.492920  8.086358
57   0.492065  8.099862  0.498047  8.004386
58   0.508057  7.844695  0.498169  8.002220
59   0.506226  7.873674  0.500854  7.959200
60   0.493530  8.075869  0.510498  7.805268
61   0.504272  7.904429  0.504028  7.908233
62   0.488647  8.153358  0.499512  7.980074
63   0.497925  8.005297  0.501343  7.950730
64   0.498413  7.997366  0.493896  8.069303
65   0.495483  8.043939  0.493042  8.082798
66   0.495605  8.041872  0.502075  7.938672
67   0.503540  7.915267  0.494751  8.055335
68   0.509644  7.817866  0.503296  7.919019
69   0.484741  8.214785  0.500366  7.965647
70   0.499756  7.975343  0.504395  7.901359
71   0.499756  7.975283  0.501709  7.944118
72   0.500732  7.959664  0.495972  8.035540
73   0.493286  8.078335  0.504761  7.895385
74   0.501099  7.953754  0.501343  7.949849
75   0.498291  7.998490  0.496582  8.025726
76   0.495972  8.035448  0.501099  7.953705
77   0.500000  7.971215  0.494995  8.051000
78   0.503296  7.918662  0.502197  7.936173
79   0.489380  8.140510  0.487427  8.171646
80   0.499756  7.975089  0.503052  7.922543
81   0.502197  7.952468  0.498901  8.017151
82   0.488892  8.176608  0.500244  7.995545
83   0.506348  7.898183  0.504517  7.927317
84   0.496948  8.047917  0.500977  7.983636
85   0.495483  8.071148  0.496216  8.059407
86   0.506348  7.897815  0.499023  8.014511
87   0.499878  8.000817  0.498779  8.018257
88   0.498901  8.016233  0.498169  8.027830
89   0.497070  8.045260  0.503662  7.940084
90   0.510742  7.827120  0.501831  7.969090
91   0.500122  7.996237  0.493530  8.101223
92   0.506226  7.898723  0.498291  8.025108
93   0.484009  8.252685  0.497437  8.038494
94   0.505249  7.913819  0.503296  7.944827
95   0.497559  8.036158  0.501343  7.975688
96   0.494385  8.086469  0.500000  7.996797
97   0.498413  8.021938  0.501099  7.978961
98   0.499268  8.007982  0.505737  7.904664
99   0.499878  7.997893  0.484009  8.250696
100  0.504028  7.931341  0.494385  8.084879
101  0.493652  8.096345  0.500488  7.987147
102  0.510498  7.827343  0.503296  7.941930
103  0.497192  8.038995  0.498413  8.019287
104  0.503784  7.933403  0.501221  7.974008
105  0.488403  8.178076  0.502563  7.952049
106  0.501099  7.975115  0.494751  8.076015
107  0.492554  8.110740  0.496216  8.052045
108  0.505981  7.896036  0.500977  7.975496
109  0.501831  7.961535  0.505737  7.898914
110  0.500122  7.988079  0.502197  7.954632
111  0.499634  7.995128  0.497070  8.035616
112  0.495483  8.060528  0.493408  8.093216
113  0.490234  8.143411  0.498047  8.018451
114  0.494263  8.078363  0.496094  8.048747
115  0.499023  8.001612  0.500000  7.985606
116  0.499146  7.998789  0.505371  7.899091
117  0.493652  8.085467  0.505371  7.898187
118  0.498413  8.008660  0.504395  7.912842
119  0.505005  7.902654  0.502686  7.939168
120  0.510376  7.816107  0.501709  7.953820
121  0.489990  8.140192  0.509399  7.830309
122  0.500854  7.966091  0.498901  7.996784
123  0.503418  7.924346  0.505493  7.890831
124  0.498657  7.999395  0.489868  8.139100
125  0.506226  7.877927  0.496216  8.037113
126  0.494751  8.060092  0.486694  8.188165
127  0.510010  7.816112  0.496338  8.033732

2018-02-25 11:45:06.618968 Finish.
Total elapsed time: 15:18:14.62.
