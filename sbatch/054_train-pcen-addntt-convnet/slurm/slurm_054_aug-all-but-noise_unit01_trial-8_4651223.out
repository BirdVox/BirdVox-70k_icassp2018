2018-02-24 20:26:35.174315: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:35.174624: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:35.174637: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:17.312760 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit02, unit03, unit05.
Validation set: unit07, unit10.
Test set: unit01.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.645020  5.734801  0.488159  8.228391
1    0.490967  8.180412  0.505493  7.945850
2    0.504883  7.953005  0.501953  7.997286
3    0.502197  7.991196  0.505615  7.934643
4    0.507446  7.903551  0.500977  8.004884
5    0.491333  8.156976  0.500488  8.009422
6    0.493774  8.114986  0.496216  8.074633
7    0.500854  7.999351  0.496216  8.072002
8    0.496948  8.059104  0.507935  7.882757
9    0.504883  7.930275  0.498657  8.028407
10   0.498047  8.037071  0.502441  7.965957
11   0.498535  8.027221  0.502319  7.965889
12   0.504272  7.933785  0.506226  7.901687
13   0.499390  8.009740  0.501221  7.979624
14   0.496094  8.060466  0.493774  8.096551
15   0.505737  7.904970  0.508179  7.865187
16   0.490234  8.150429  0.503418  7.939420
17   0.503174  7.942506  0.496338  8.050685
18   0.497803  8.026554  0.504517  7.918745
19   0.497803  8.023485  0.493408  8.096011
20   0.522949  7.625103  0.507080  7.874979
21   0.525391  7.597339  0.575439  6.885160
22   0.564697  7.051735  0.569580  6.968273
23   0.566772  7.013417  0.565552  7.031358
24   0.546021  7.342265  0.567017  7.008113
25   0.546753  7.332270  0.573486  6.902444
26   0.553345  7.225271  0.561890  7.086641
27   0.557251  7.160851  0.560913  7.103412
28   0.556274  7.176103  0.565430  7.027737
29   0.550049  7.273215  0.554565  7.203694
30   0.541504  7.411771  0.542847  7.389885
31   0.541138  7.416945  0.545898  7.339697
32   0.543213  7.380733  0.547974  7.305444
33   0.531006  7.576509  0.546631  7.326402
34   0.537231  7.477368  0.534424  7.524033
35   0.531982  7.561183  0.541504  7.407663
36   0.523438  7.696216  0.527954  7.624936
37   0.526123  7.654072  0.526978  7.641957
38   0.517090  7.796927  0.521973  7.721821
39   0.514404  7.841454  0.524780  7.673765
40   0.512207  7.874063  0.508789  7.930773
41   0.513672  7.851846  0.508667  7.930799
42   0.503296  8.014541  0.507935  7.939876
43   0.506348  7.964862  0.524048  7.684131
44   0.514526  7.837147  0.525024  7.672017
45   0.513794  7.851654  0.524902  7.667955
46   0.520752  7.737545  0.507446  7.949218
47   0.513672  7.851961  0.510864  7.889259
48   0.507812  7.941405  0.513550  7.848178
49   0.498657  8.091425  0.516724  7.792578
50   0.505737  7.976210  0.504517  7.986052
51   0.529663  7.604813  0.550659  7.291046
52   0.539673  7.457744  0.534790  7.532486
53   0.535889  7.509456  0.536987  7.490550
54   0.527100  7.647750  0.526855  7.647982
55   0.523193  7.705044  0.534668  7.518568
56   0.521729  7.727676  0.531616  7.567077
57   0.504883  7.996828  0.513550  7.856365
58   0.494751  8.158359  0.521118  7.730494
59   0.506836  7.960025  0.521362  7.726796
60   0.507812  7.944485  0.510376  7.902441
61   0.508301  7.935296  0.511108  7.887589
62   0.503784  8.006836  0.505737  7.974759
63   0.500000  8.066754  0.504028  8.001293
64   0.509399  7.914252  0.507568  7.943222
65   0.498779  8.080985  0.507812  7.936762
66   0.490967  8.209627  0.492310  8.186302
67   0.541992  7.388440  0.545532  7.320672
68   0.574951  6.860551  0.526123  7.623363
69   0.606445  6.350554  0.517944  7.751710
70   0.602173  6.413730  0.504517  7.960312
71   0.597900  6.475663  0.499390  8.037274
72   0.605835  6.346646  0.490356  8.173217
73   0.577637  6.810625  0.501099  8.101687
74   0.500610  8.091974  0.486206  8.320833
75   0.501953  8.066793  0.501099  8.080479
76   0.493774  8.196588  0.504395  8.027248
77   0.491699  8.231653  0.502930  8.045079
78   0.505127  8.015125  0.508911  7.952293
79   0.507568  7.975558  0.501221  8.076010
80   0.495117  8.176137  0.500244  8.093467
81   0.489502  8.266466  0.500732  8.085400
82   0.507568  7.975128  0.507568  7.973328
83   0.494019  8.191600  0.496704  8.148547
84   0.502808  8.049791  0.501953  8.063469
85   0.505249  8.010281  0.495605  8.167507
86   0.496216  8.157514  0.501465  8.069460
87   0.507935  7.968541  0.505615  8.003928
88   0.503662  8.035388  0.504517  8.021671
89   0.502319  8.058621  0.500610  8.086424
90   0.494995  8.176481  0.493164  8.205952
91   0.497803  8.131022  0.503296  8.042376
92   0.503052  8.044534  0.505737  8.002872
93   0.507690  7.971200  0.508423  7.959272
94   0.494385  8.185411  0.508301  7.961052
95   0.498291  8.120395  0.495728  8.163358
96   0.500122  8.090680  0.496338  8.151400
97   0.504639  8.017514  0.496338  8.151065
98   0.505127  8.011114  0.500977  8.068558
99   0.503906  8.030367  0.504272  8.020666
100  0.499634  8.098834  0.501343  8.067387
101  0.497314  8.135787  0.486816  8.299284
102  0.509521  7.936705  0.492432  8.210174
103  0.497559  8.130876  0.498657  8.112899
104  0.496338  8.150000  0.498413  8.112768
105  0.493530  8.194712  0.509399  7.938635
106  0.503418  8.034634  0.497192  8.134659
107  0.493774  8.189392  0.506836  7.978488
108  0.498291  8.115818  0.497681  8.123670
109  0.500732  8.075643  0.496582  8.140548
110  0.505981  7.988589  0.497925  8.117729
111  0.508057  7.955838  0.497437  8.126485
112  0.485107  8.324701  0.501343  8.059022
113  0.499146  8.097360  0.496338  8.142065
114  0.507080  7.968373  0.499023  8.095948
115  0.508545  7.943595  0.502319  8.043343
116  0.502563  8.038787  0.504517  8.005061
117  0.505737  7.984673  0.498779  8.096080
118  0.495361  8.152366  0.496216  8.138002
119  0.496948  8.123585  0.509888  7.916690
120  0.504272  8.006107  0.498779  8.092137
121  0.503296  8.018656  0.506714  7.961183
122  0.489624  8.239488  0.506592  7.965396
123  0.498291  8.098479  0.506226  7.968130
124  0.494873  8.150597  0.493530  8.173387
125  0.499634  8.074312  0.513428  7.851399
126  0.503662  8.008196  0.501099  8.047180
127  0.501343  8.044489  0.501099  8.046191

2018-02-25 11:19:14.456058 Finish.
Total elapsed time: 14:52:57.46.
