2018-02-24 20:26:35.641892: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:35.642187: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:35.642199: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:17.920481 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit01, unit02, unit03.
Validation set: unit05, unit07.
Test set: unit10.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.955322  0.159517  0.934204  0.210909
1    0.954712  0.162947  0.958252  0.169735
2    0.957520  0.157813  0.971802  0.117994
3    0.960693  0.142274  0.959839  0.151457
4    0.958496  0.160539  0.964600  0.136324
5    0.962036  0.137875  0.961792  0.157513
6    0.963623  0.132706  0.971191  0.120195
7    0.968750  0.121441  0.967529  0.131250
8    0.969238  0.114462  0.970215  0.123107
9    0.966187  0.124923  0.967651  0.133403
10   0.971069  0.109808  0.970093  0.117005
11   0.966431  0.141616  0.962402  0.143432
12   0.973267  0.109753  0.957275  0.156412
13   0.968262  0.120216  0.973267  0.117423
14   0.971069  0.106285  0.972656  0.112343
15   0.961670  0.158301  0.960205  0.159498
16   0.948242  0.204843  0.955444  0.180966
17   0.953491  0.177248  0.960205  0.156305
18   0.922607  0.292807  0.955200  0.177601
19   0.946167  0.197306  0.956421  0.158803
20   0.589600  6.535160  0.871094  0.524997
21   0.921753  0.265086  0.929565  0.220220
22   0.949951  0.190472  0.938354  0.198025
23   0.953979  0.172695  0.952515  0.177025
24   0.809326  2.256703  0.926025  0.244359
25   0.945068  0.207487  0.953369  0.181498
26   0.957886  0.165132  0.962769  0.175498
27   0.952515  0.205293  0.919312  0.297885
28   0.941284  0.214305  0.946411  0.208242
29   0.945801  0.201381  0.920166  0.321878
30   0.611694  6.115345  0.490234  8.161519
31   0.487183  8.206114  0.491455  8.134460
32   0.584351  6.446473  0.930420  0.244949
33   0.935791  0.234422  0.833740  0.408831
34   0.938965  0.221786  0.950317  0.199014
35   0.926392  0.344996  0.906372  0.300036
36   0.933716  0.243903  0.915161  0.324927
37   0.943237  0.212737  0.921143  0.285677
38   0.945923  0.214145  0.894409  0.372051
39   0.940918  0.232131  0.930054  0.261111
40   0.942871  0.229403  0.922241  0.278671
41   0.944580  0.224816  0.947632  0.244133
42   0.946411  0.212566  0.927246  0.284347
43   0.761719  3.502812  0.712158  4.680685
44   0.683838  5.153641  0.701660  4.843342
45   0.687744  5.084036  0.694092  4.958575
46   0.691040  5.025972  0.702637  4.817117
47   0.689209  5.050591  0.697998  4.888192
48   0.566895  6.996346  0.490234  8.216023
49   0.499878  8.056411  0.497803  8.085014
50   0.496704  8.099513  0.494873  8.126056
51   0.495483  8.114218  0.501099  8.022752
52   0.498413  8.063901  0.506470  7.933882
53   0.497192  8.080377  0.503662  7.975878
54   0.494995  8.112807  0.501709  8.004559
55   0.494019  8.126025  0.495972  8.093769
56   0.496460  8.084918  0.510742  7.856167
57   0.500488  8.018617  0.500122  8.023437
58   0.497437  8.065256  0.496338  8.081775
59   0.508545  7.886186  0.506470  7.918285
60   0.497925  8.053538  0.495239  8.095371
61   0.500488  8.010716  0.513550  7.801503
62   0.511841  7.827771  0.494141  8.108970
63   0.500244  8.010685  0.501343  7.992181
64   0.502075  7.979521  0.500122  8.009666
65   0.504028  7.946405  0.492310  8.132236
66   0.504639  7.934695  0.503906  7.945379
67   0.501587  7.981372  0.503296  7.953138
68   0.500732  7.993030  0.504883  7.925881
69   0.490479  8.154553  0.504639  7.927836
70   0.505737  7.909366  0.499878  8.001821
71   0.497070  8.045641  0.499390  8.007725
72   0.493896  8.094378  0.502808  7.951392
73   0.498169  8.024444  0.493774  8.093604
74   0.499878  7.995424  0.505615  7.903084
75   0.496948  8.040408  0.496338  8.049292
76   0.504150  7.923923  0.498901  8.006790
77   0.500244  7.984595  0.487793  8.182314
78   0.493774  8.086202  0.501831  7.957013
79   0.495972  8.049707  0.503784  7.924446
80   0.497437  8.024962  0.503662  7.925039
81   0.512573  7.782332  0.496704  8.034689
82   0.495117  8.059385  0.492920  8.093819
83   0.505371  7.894753  0.494019  8.075183
84   0.498047  8.010437  0.502075  7.945699
85   0.496704  8.030839  0.500488  7.970032
86   0.502441  7.938445  0.501465  7.953572
87   0.494629  8.062140  0.487793  8.170718
88   0.499878  7.977678  0.503784  7.915035
89   0.507446  7.856311  0.494995  8.054479
90   0.496826  8.024981  0.496094  8.036360
91   0.497925  8.006895  0.501831  7.944356
92   0.503174  7.922709  0.499634  7.978915
93   0.495239  8.048766  0.507202  7.857850
94   0.504150  7.906326  0.500488  7.964541
95   0.495239  8.048075  0.511108  7.794945
96   0.501953  7.940782  0.500000  7.971807
97   0.489868  8.133238  0.498413  7.996925
98   0.502563  7.930685  0.498657  7.992893
99   0.497925  8.004517  0.508667  7.833212
100  0.501465  7.947993  0.506836  7.862331
101  0.489502  8.138651  0.492798  8.086083
102  0.503540  7.914811  0.498657  7.992640
103  0.501099  7.953708  0.501221  7.951753
104  0.500488  7.963424  0.500977  7.955634
105  0.493408  8.076289  0.508057  7.842755
106  0.494507  8.058770  0.489990  8.130774
107  0.496216  8.031523  0.497437  8.012061
108  0.503052  7.922541  0.501343  7.949786
109  0.510010  7.868356  0.504883  8.077984
110  0.506836  8.042884  0.505249  8.066959
111  0.505737  8.058555  0.507446  8.030586
112  0.500244  8.146342  0.492554  8.269979
113  0.500244  8.145712  0.498169  8.178841
114  0.493164  8.259179  0.495972  8.213583
115  0.496826  8.199450  0.502441  8.108568
116  0.493164  8.257706  0.503662  8.088086
117  0.495361  8.221446  0.499878  8.148197
118  0.498657  8.167398  0.486816  8.357757
119  0.502441  8.105395  0.510010  7.982871
120  0.497925  8.177095  0.495117  8.221765
121  0.502930  8.095231  0.498657  8.163463
122  0.502197  8.105743  0.493896  8.238851
123  0.497803  8.175177  0.504395  8.068193
124  0.501953  8.106777  0.498901  8.155174
125  0.511353  7.953664  0.503906  8.072838
126  0.489380  8.306099  0.503662  8.074995
127  0.508301  7.999297  0.502808  8.086879

2018-02-25 12:09:12.127922 Finish.
Total elapsed time: 15:42:55.13.
