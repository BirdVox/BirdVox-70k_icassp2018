2018-02-24 20:26:39.250196: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:39.250395: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:39.250408: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:17.661121 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit01, unit02, unit03.
Validation set: unit05, unit07.
Test set: unit10.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.499512  7.983138  0.490112  8.131999
1    0.498413  8.059931  0.505615  7.978514
2    0.500488  8.056082  0.496704  8.114561
3    0.501587  8.035028  0.503296  8.006919
4    0.505249  7.970177  0.497070  8.090691
5    0.503174  7.974093  0.500244  8.006832
6    0.503540  7.946601  0.492188  8.121350
7    0.503418  7.938043  0.500488  7.981085
8    0.504883  7.908302  0.497314  8.026551
9    0.496704  8.034411  0.497192  8.024946
10   0.503296  7.926304  0.509644  7.823896
11   0.503296  7.924119  0.500488  7.967995
12   0.497192  8.019828  0.498901  7.991936
13   0.497314  8.016717  0.500610  7.963703
14   0.490112  8.130694  0.513672  7.754763
15   0.508545  7.836235  0.494873  8.053961
16   0.498779  7.991503  0.487793  8.166488
17   0.495117  8.049598  0.507202  7.856824
18   0.501465  7.948206  0.502319  7.934510
19   0.496094  8.033705  0.499268  7.983057
20   0.494873  8.053080  0.492188  8.095862
21   0.503540  7.914851  0.497192  8.016027
22   0.508423  7.836971  0.492554  8.089949
23   0.504272  7.903115  0.498901  7.988734
24   0.500854  7.957590  0.493042  8.082135
25   0.495850  8.037371  0.502808  7.926441
26   0.505981  7.875840  0.501831  7.942005
27   0.486450  8.187212  0.501709  7.943949
28   0.497070  8.017900  0.498169  8.000385
29   0.501465  8.206622  0.498169  8.395758
30   0.490234  8.509445  0.495117  8.417014
31   0.491455  8.463269  0.508545  8.175305
32   0.501953  8.269779  0.499756  8.293617
33   0.498413  8.304282  0.498535  8.291486
34   0.499023  8.273298  0.498169  8.276871
35   0.501709  8.210064  0.500366  8.222059
36   0.493286  8.326943  0.499023  8.225328
37   0.497559  8.240195  0.503906  8.129234
38   0.503784  8.122945  0.499390  8.185622
39   0.492310  8.291983  0.507202  8.044299
40   0.502197  8.117737  0.511719  7.957163
41   0.508789  7.997706  0.494507  8.221371
42   0.505005  8.046067  0.501831  8.091280
43   0.509399  7.963803  0.495117  8.188679
44   0.502563  8.063789  0.504883  8.021706
45   0.503906  8.033196  0.510742  7.918937
46   0.497070  8.135659  0.501709  8.057421
47   0.510376  7.914664  0.496704  8.132130
48   0.506226  7.976140  0.497192  8.119367
49   0.503296  8.018958  0.507812  7.944262
50   0.504883  7.989882  0.500488  8.059231
51   0.499268  8.077675  0.496582  8.119829
52   0.507080  7.949695  0.500977  8.047229
53   0.502319  8.024909  0.489624  8.228923
54   0.497681  8.098583  0.500366  8.054866
55   0.498779  8.080110  0.495605  8.130970
56   0.496582  8.115006  0.500000  8.059718
57   0.506470  7.955292  0.502441  8.020095
58   0.509155  7.911788  0.496826  8.110432
59   0.496094  8.122181  0.493042  8.171323
60   0.513184  7.846646  0.498047  8.090594
61   0.498169  8.088609  0.505981  7.962672
62   0.514526  7.824934  0.495361  8.133831
63   0.503662  8.000033  0.500977  8.043315
64   0.491943  8.188910  0.491211  8.200714
65   0.507446  7.939030  0.509033  7.913451
66   0.497925  8.092497  0.507080  7.944931
67   0.494141  8.153490  0.504517  7.986249
68   0.504517  7.992973  0.499756  8.241932
69   0.499390  8.255220  0.498535  8.254662
70   0.494873  8.305433  0.501587  8.190479
71   0.494629  8.297963  0.496094  8.270331
72   0.495850  8.271243  0.501709  8.174090
73   0.493408  8.305654  0.504395  8.126481
74   0.495117  8.274128  0.497925  8.227026
75   0.505249  8.107187  0.491577  8.325754
76   0.490967  8.333786  0.504272  8.117483
77   0.495239  8.261201  0.499390  8.192378
78   0.497437  8.221881  0.503540  8.121476
79   0.495605  8.247286  0.505615  8.083815
80   0.502930  8.124918  0.500000  8.169906
81   0.508911  8.023996  0.492920  8.279415
82   0.500854  8.149154  0.513062  7.949982
83   0.501465  8.134446  0.502075  8.122110
84   0.506226  8.052684  0.507324  8.032405
85   0.507202  8.031776  0.501587  8.119646
86   0.495850  8.209466  0.496704  8.193001
87   0.503174  8.086017  0.494995  8.215104
88   0.493042  8.243842  0.505981  8.032509
89   0.501099  8.108443  0.503174  8.072200
90   0.496216  8.181571  0.499390  8.127614
91   0.503906  8.052044  0.497803  8.147634
92   0.496826  8.160630  0.486450  8.325119
93   0.495972  8.168956  0.502930  8.054114
94   0.495605  8.169549  0.495605  8.166942
95   0.500122  8.091628  0.501221  8.071426
96   0.503052  8.039529  0.503784  8.025371
97   0.497925  8.117586  0.495728  8.150815
98   0.506836  7.969723  0.508667  7.938214
99   0.504639  8.001299  0.500122  8.072310
100  0.504150  8.005753  0.500244  8.067148
101  0.497925  8.103126  0.508301  7.934545
102  0.496094  8.130117  0.505615  7.975531
103  0.501221  8.045395  0.497925  8.097613
104  0.502319  8.026013  0.479858  8.387328
105  0.498901  8.079800  0.508301  7.927757
106  0.488770  8.242124  0.500610  8.050874
107  0.503784  7.999402  0.504028  7.995185
108  0.487671  8.258618  0.491455  8.197432
109  0.502319  8.022176  0.493774  8.159779
110  0.495972  8.124272  0.493286  8.167479
111  0.500000  8.059209  0.513916  7.834864
112  0.510986  7.882053  0.496948  8.108294
113  0.503784  7.998095  0.494629  8.145646
114  0.504761  7.982332  0.495117  8.137761
115  0.495117  8.137757  0.496338  8.118079
116  0.498291  8.086596  0.503540  8.001991
117  0.488159  8.249900  0.501831  8.029535
118  0.507202  7.942963  0.503784  7.998054
119  0.492676  8.177100  0.504272  7.990184
120  0.506714  7.950833  0.497681  8.096431
121  0.504028  7.994119  0.491333  8.198743
122  0.502441  8.010720  0.493774  8.182333
123  0.502563  8.075401  0.494141  8.202898
124  0.495239  8.179690  0.500854  8.084718
125  0.505981  7.999127  0.502808  8.047838
126  0.501465  8.067851  0.509399  7.938615
127  0.509644  7.933787  0.508545  7.950760

2018-02-25 12:10:17.579841 Finish.
Total elapsed time: 15:44:00.58.
