2018-02-24 20:27:37.072555: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:37.072859: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:37.072880: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:37.072890: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:37.072899: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:49.693742 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit10, unit01, unit02.
Validation set: unit03, unit05.
Test set: unit07.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.917358  0.258232  0.960449  0.151668
1    0.929077  0.224572  0.958008  0.171396
2    0.907349  0.333559  0.936401  0.295048
3    0.880981  0.363131  0.948364  0.218422
4    0.906006  0.295811  0.953857  0.208468
5    0.901733  0.320281  0.970093  0.151718
6    0.914307  0.268708  0.970337  0.153162
7    0.922119  0.248187  0.966797  0.159997
8    0.923340  0.247051  0.958252  0.165518
9    0.883545  0.352499  0.947998  0.197077
10   0.893921  0.312906  0.963989  0.174842
11   0.917847  0.254728  0.957031  0.185157
12   0.637451  5.466798  0.503540  7.942240
13   0.493774  8.095644  0.495728  8.062422
14   0.488525  8.175461  0.512451  7.792346
15   0.494873  7.346320  0.496460  8.167606
16   0.503174  8.055985  0.494019  8.200785
17   0.500854  8.088494  0.501831  8.070812
18   0.493896  8.197053  0.506348  7.994819
19   0.505615  8.005291  0.489136  8.269656
20   0.488770  8.274476  0.502930  8.045222
21   0.501343  8.069914  0.493896  8.189099
22   0.504395  8.019160  0.499146  8.103071
23   0.502441  8.049334  0.505981  7.991688
24   0.494019  8.183979  0.500977  8.071319
25   0.487305  8.291214  0.492188  8.212055
26   0.505981  7.989294  0.510620  7.914105
27   0.508423  7.949116  0.499390  8.094313
28   0.490723  8.233619  0.500977  8.067957
29   0.503784  8.022322  0.491699  8.216726
30   0.498657  8.104198  0.498169  8.111688
31   0.498169  8.111311  0.500610  8.071581
32   0.488647  8.264023  0.496582  8.135752
33   0.489624  8.247525  0.495239  8.156638
34   0.500488  8.071656  0.499390  8.088984
35   0.491455  8.216498  0.507568  7.956405
36   0.505615  7.987511  0.507812  7.951720
37   0.492310  8.201227  0.509033  7.931302
38   0.498047  8.108015  0.505615  7.985660
39   0.500488  8.067937  0.500122  8.073478
40   0.506958  7.962942  0.506836  7.964555
41   0.501831  8.044877  0.511597  7.887126
42   0.503052  8.024513  0.496338  8.132387
43   0.489624  8.240267  0.500854  8.058917
44   0.505615  7.981854  0.491089  8.215660
45   0.505493  7.983164  0.493286  8.179591
46   0.503174  8.019897  0.494263  8.163201
47   0.499634  8.076307  0.498535  8.093689
48   0.502563  8.028437  0.497803  8.104845
49   0.491455  8.206833  0.501221  8.049103
50   0.495850  8.135348  0.510986  7.891044
51   0.497925  8.101243  0.497192  8.112718
52   0.497314  8.110423  0.494995  8.147476
53   0.500488  8.058610  0.508423  7.930392
54   0.500000  8.065830  0.504395  7.994674
55   0.500732  8.053383  0.505493  7.976332
56   0.497681  8.101946  0.505859  7.969814
57   0.504272  7.995094  0.500366  8.057760
58   0.501465  8.039769  0.498901  8.080806
59   0.505493  7.974291  0.505005  7.981897
60   0.503174  8.011160  0.490234  8.219473
61   0.500854  8.048066  0.507202  7.945527
62   0.503296  8.008276  0.502808  8.015940
63   0.497925  8.094451  0.487183  8.267409
64   0.494751  8.145251  0.505249  7.975878
65   0.501709  8.032787  0.501831  8.030675
66   0.509644  7.904624  0.500366  8.054034
67   0.494263  8.152302  0.500366  8.053821
68   0.501709  8.032088  0.504639  7.984783
69   0.515625  7.807631  0.501587  8.033831
70   0.499634  8.065255  0.500732  8.047495
71   0.494751  8.143862  0.506714  7.951004
72   0.503418  8.004096  0.498047  8.090640
73   0.505493  7.970598  0.494385  8.149624
74   0.496216  8.120096  0.506104  7.960713
75   0.493652  8.161392  0.501221  8.039396
76   0.503296  8.005942  0.501587  8.033483
77   0.498291  8.086603  0.506104  7.960677
78   0.500732  8.047247  0.511597  7.872135
79   0.501709  8.031505  0.489014  8.236128
80   0.499268  8.070854  0.501709  8.031503
81   0.499023  8.074789  0.493286  8.167263
82   0.499390  8.068885  0.494019  8.155457
83   0.490967  8.204646  0.501587  8.033470
84   0.494385  8.149555  0.499512  8.066918
85   0.505615  7.968541  0.502441  8.019697
86   0.494995  8.139717  0.496582  8.114139
87   0.502075  8.018670  0.504395  7.951637
88   0.490845  8.170480  0.503052  7.975957
89   0.509644  7.870833  0.501709  7.997291
90   0.499512  8.032281  0.507690  7.901855
91   0.502930  7.977719  0.499878  8.026335
92   0.505249  7.940666  0.491577  8.158585
93   0.502319  7.987281  0.503540  7.967770
94   0.490479  8.175946  0.510010  7.864513
95   0.495605  8.094089  0.499146  8.037587
96   0.502319  7.986919  0.505615  7.934301
97   0.504150  7.957575  0.505615  7.934139
98   0.506470  7.920427  0.494629  8.109104
99   0.495850  8.089542  0.507202  7.908449
100  0.504272  7.955042  0.498169  8.052227
101  0.504028  7.958686  0.510010  7.863193
102  0.500366  8.016790  0.499878  8.024423
103  0.500000  8.022314  0.499268  8.033821
104  0.496704  8.074506  0.510376  7.856353
105  0.489502  8.188930  0.496094  8.083627
106  0.495361  8.095074  0.503296  7.968338
107  0.505615  7.931106  0.496460  8.076795
108  0.506226  7.920822  0.503906  7.957498
109  0.493896  8.116760  0.501709  7.991877
110  0.494507  8.106343  0.502319  7.981424
111  0.508301  7.885673  0.494629  8.103227
112  0.497070  8.063872  0.503906  7.954440
113  0.502930  7.969532  0.503906  7.953467
114  0.494019  8.110578  0.510254  7.851204
115  0.499634  8.019942  0.506714  7.906476
116  0.499634  8.018727  0.501099  7.994729
117  0.499634  8.017409  0.500977  7.995305
118  0.503784  7.949818  0.502319  7.972422
119  0.502197  7.973590  0.502075  7.974734
120  0.504639  7.933037  0.494751  8.089818
121  0.497681  8.042233  0.502686  7.961543
122  0.499023  8.019003  0.499878  8.004436
123  0.497192  8.046287  0.497925  8.033627
124  0.509521  7.847751  0.491211  8.138650
125  0.500000  7.997507  0.496948  8.045120
126  0.502686  7.952610  0.494141  8.087781
127  0.509521  7.841522  0.503784  7.931927

2018-02-25 11:24:01.801870 Finish.
Total elapsed time: 14:57:12.80.
