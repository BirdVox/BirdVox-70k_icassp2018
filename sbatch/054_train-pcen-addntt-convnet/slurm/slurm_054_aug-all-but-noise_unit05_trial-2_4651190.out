2018-02-24 20:26:36.554647: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:36.554902: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:36.554914: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:16.680780 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit07, unit10, unit01.
Validation set: unit02, unit03.
Test set: unit05.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.941284  0.195645  0.907593  0.441168
1    0.926025  0.263902  0.897827  0.836977
2    0.934692  0.211834  0.910767  0.719596
3    0.946899  0.207147  0.933228  0.782628
4    0.788940  1.353815  0.671021  0.714182
5    0.551636  5.405260  0.506226  7.915243
6    0.500610  8.001552  0.500366  8.002491
7    0.498291  8.033051  0.501221  7.983955
8    0.503662  7.942914  0.511841  7.810496
9    0.503662  7.939057  0.508179  7.865294
10   0.510498  7.826726  0.490356  8.146297
11   0.493774  8.090411  0.495117  8.067658
12   0.494873  8.070326  0.496826  8.038007
13   0.490479  8.138128  0.499023  8.000862
14   0.509521  7.832552  0.498657  8.004840
15   0.502197  7.947570  0.503052  7.933142
16   0.501343  7.959654  0.503906  7.918078
17   0.502075  7.946624  0.500366  7.973246
18   0.503540  7.922081  0.495483  8.049976
19   0.510010  7.817894  0.500977  7.961427
20   0.506226  7.877311  0.497681  8.013120
21   0.502075  7.942683  0.499023  7.990973
22   0.506470  7.871935  0.511719  7.787941
23   0.496582  8.028975  0.503906  7.911942
24   0.496094  8.036251  0.494507  8.061321
25   0.506836  7.864562  0.493896  8.070653
26   0.496460  8.029613  0.494751  8.056694
27   0.494873  8.054602  0.494019  8.068087
28   0.499878  7.974552  0.489746  8.135962
29   0.492432  8.093045  0.502075  7.939206
30   0.501831  7.943012  0.488892  8.149216
31   0.504395  7.901989  0.502930  7.925272
32   0.506470  7.868774  0.490723  8.119760
33   0.501099  7.954289  0.494263  8.063219
34   0.500244  7.967815  0.510010  7.812083
35   0.500977  7.956054  0.495117  8.049428
36   0.494019  8.066908  0.507935  7.845020
37   0.498657  7.992892  0.497925  8.004539
38   0.496582  8.025919  0.497437  8.012271
39   0.496826  8.021979  0.494629  8.056986
40   0.497314  8.014153  0.493164  8.080302
41   0.503052  7.922652  0.498413  7.996588
42   0.506470  7.868133  0.514648  7.737732
43   0.503784  7.910924  0.491821  8.101631
44   0.497314  8.014049  0.504517  7.899222
45   0.494019  8.066580  0.497314  8.014030
46   0.505859  7.877799  0.495972  8.035428
47   0.502319  7.934228  0.504028  7.906980
48   0.499512  7.978984  0.499390  7.980928
49   0.497192  8.015956  0.503540  7.914759
50   0.496704  8.023739  0.504150  7.905027
51   0.498779  7.990654  0.499634  7.977031
52   0.498047  8.002330  0.511108  7.794098
53   0.501831  7.942001  0.503540  7.914756
54   0.501465  7.947989  0.499146  7.984816
55   0.504028  7.906971  0.496216  8.031521
56   0.510254  7.807722  0.503784  7.910864
57   0.501343  7.971387  0.498291  8.130818
58   0.500610  8.091839  0.506592  7.993469
59   0.503784  8.037107  0.507446  7.976583
60   0.499512  8.103199  0.495728  8.162995
61   0.500122  8.091123  0.496582  8.147193
62   0.498413  8.116804  0.504028  8.025458
63   0.509644  7.934193  0.503418  8.033805
64   0.492554  8.208243  0.507446  7.967547
65   0.499390  8.096794  0.491821  8.218183
66   0.499390  8.095630  0.502319  8.047851
67   0.499512  8.092573  0.503662  8.025149
68   0.509155  7.936101  0.497559  8.122511
69   0.506714  7.974453  0.503540  8.025117
70   0.509888  7.922321  0.495972  8.146134
71   0.492920  8.194841  0.493652  8.182550
72   0.503174  8.028597  0.495728  8.148126
73   0.499146  8.092542  0.498535  8.101880
74   0.491333  8.217461  0.506470  7.972974
75   0.502075  8.043288  0.500610  8.066372
76   0.498901  8.093387  0.502930  8.027919
77   0.485474  8.308734  0.501709  8.046500
78   0.491699  8.207284  0.499390  8.082768
79   0.499756  8.076305  0.494629  8.158375
80   0.496704  8.124364  0.489624  8.237915
81   0.499146  8.083887  0.498047  8.101033
82   0.506470  7.964723  0.489136  8.243563
83   0.482910  8.343373  0.506470  7.963104
84   0.501709  8.039325  0.505493  7.977822
85   0.496338  8.124901  0.490356  8.220831
86   0.508179  7.933116  0.499023  8.080236
87   0.501343  8.042436  0.497559  8.103022
88   0.495483  8.136093  0.494751  8.147532
89   0.495972  8.127520  0.500244  8.058331
90   0.501465  8.038361  0.498047  8.093168
91   0.507568  7.939446  0.500977  8.045450
92   0.511597  7.874057  0.503296  8.007645
93   0.491333  8.200282  0.503784  7.999421
94   0.497925  8.093713  0.491089  8.203754
95   0.494751  8.144605  0.512085  7.865099
96   0.501099  8.042078  0.491821  8.191518
97   0.508057  7.929755  0.492920  8.173655
98   0.500000  8.059474  0.501099  8.041707
99   0.494873  8.142001  0.504883  7.980616
100  0.502197  8.023863  0.498779  8.078918
101  0.500366  8.053310  0.486206  8.281516
102  0.504517  7.986363  0.504395  7.988310
103  0.499268  8.070930  0.494385  8.149616
104  0.497192  8.104351  0.513306  7.844625
105  0.498047  8.090559  0.502563  8.017753
106  0.497192  8.104320  0.493774  8.159406
107  0.505127  7.976421  0.500488  8.051185
108  0.500366  8.053151  0.505981  7.962642
109  0.494873  8.141687  0.497925  8.092498
110  0.494385  8.144403  0.502441  8.049578
111  0.498657  8.108869  0.487549  8.286975
112  0.495972  8.150484  0.497559  8.124183
113  0.505127  8.001498  0.500000  8.083441
114  0.498535  8.106384  0.511597  7.895196
115  0.496826  8.132634  0.491089  8.224481
116  0.493408  8.186499  0.505615  7.989154
117  0.506958  7.966948  0.491211  8.220206
118  0.503418  8.022925  0.510498  7.908287
119  0.499756  8.080938  0.510254  7.911244
120  0.497192  8.121309  0.503052  8.026411
121  0.492798  8.191251  0.507202  7.958653
122  0.505005  7.993659  0.496216  8.134916
123  0.503906  8.010572  0.496826  8.124302
124  0.496948  8.121961  0.493164  8.182582
125  0.504395  8.001207  0.501709  8.044133
126  0.495117  8.150029  0.500977  8.055236
127  0.504395  7.999802  0.500244  8.066356

2018-02-25 11:38:41.132569 Finish.
Total elapsed time: 15:12:25.13.
