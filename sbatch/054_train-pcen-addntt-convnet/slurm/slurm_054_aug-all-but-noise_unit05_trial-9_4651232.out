2018-02-24 20:26:43.369727: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:43.369978: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:43.369991: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:17.661112 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit07, unit10, unit01.
Validation set: unit02, unit03.
Test set: unit05.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.908569  0.258573  0.913696  0.310259
1    0.905151  0.326819  0.909058  0.386539
2    0.906250  0.267325  0.907837  0.387160
3    0.860107  0.383709  0.921387  0.324426
4    0.645386  5.274839  0.501465  8.066566
5    0.499634  8.092231  0.504028  8.017954
6    0.497681  8.117473  0.493652  8.179831
7    0.500366  8.069469  0.506348  7.971067
8    0.492065  8.199589  0.492554  8.190156
9    0.501343  8.047173  0.492310  8.191544
10   0.500122  8.064585  0.500122  8.063624
11   0.493774  8.165128  0.495361  8.138800
12   0.507324  7.945354  0.496582  8.117918
13   0.498169  8.091857  0.498657  8.083541
14   0.504272  7.992664  0.498779  8.080802
15   0.494507  8.108478  0.495483  8.082797
16   0.497314  8.047939  0.501953  7.969379
17   0.506958  7.886286  0.490723  8.142185
18   0.504639  7.917991  0.503540  7.933364
19   0.507568  7.867359  0.495850  8.052526
20   0.502686  7.942135  0.504272  7.915517
21   0.503418  7.928007  0.503296  7.928888
22   0.507568  7.859854  0.506836  7.870664
23   0.507812  7.854342  0.494507  8.065754
24   0.507690  7.854957  0.490601  8.126823
25   0.505737  8.026050  0.501587  8.095867
26   0.500122  8.110647  0.504517  8.032539
27   0.503784  8.039218  0.497314  8.139047
28   0.499390  8.102239  0.499756  8.093351
29   0.504272  8.018215  0.495117  8.163675
30   0.494751  8.167886  0.498047  8.113220
31   0.500977  8.064732  0.496704  8.132428
32   0.493164  8.188505  0.500854  8.063635
33   0.501343  8.054979  0.505249  7.991275
34   0.501831  8.045712  0.503052  8.025412
35   0.502930  8.026814  0.496338  8.132515
36   0.498901  8.090691  0.505005  7.991819
37   0.506470  7.967742  0.490723  8.221093
38   0.496582  8.126208  0.496094  8.133637
39   0.495117  8.148950  0.502075  8.036372
40   0.492065  8.197291  0.508667  7.929286
41   0.501953  8.037088  0.493164  8.178337
42   0.504761  7.991014  0.498901  8.085049
43   0.502563  8.025623  0.494629  8.153113
44   0.504150  7.999255  0.503662  8.006735
45   0.496338  8.124409  0.494629  8.151579
46   0.489136  8.239756  0.505371  7.977713
47   0.496338  8.122967  0.493774  8.163946
48   0.493530  8.167559  0.498901  8.080670
49   0.501709  8.035119  0.501343  8.040730
50   0.493164  8.172284  0.498657  8.083480
51   0.493774  8.161938  0.500366  8.055454
52   0.500000  8.061140  0.506592  7.954684
53   0.505127  7.978106  0.493164  8.170744
54   0.492798  8.176485  0.496338  8.119272
55   0.492188  8.186032  0.506592  7.953732
56   0.493042  8.172016  0.498413  8.085337
57   0.483765  8.321350  0.503540  8.002522
58   0.490234  8.216910  0.499146  8.073211
59   0.497803  8.094797  0.497192  8.104581
60   0.497070  8.106505  0.499756  8.063179
61   0.505493  7.970672  0.506714  7.950967
62   0.490723  8.208691  0.501465  8.035526
63   0.494995  8.139789  0.504639  7.984338
64   0.504639  7.984326  0.493286  8.167298
65   0.501343  8.037432  0.514404  7.826899
66   0.502319  8.021681  0.504272  7.990196
67   0.507202  7.942972  0.499023  8.074795
68   0.504761  7.982318  0.501221  8.039376
69   0.504639  7.984284  0.500488  8.051179
70   0.507690  7.935094  0.503784  7.998055
71   0.499023  8.074789  0.497314  8.102334
72   0.504028  7.994119  0.498657  8.080691
73   0.503662  8.000022  0.514282  7.828845
74   0.508057  7.929190  0.500122  8.057080
75   0.498657  8.076345  0.509277  7.909515
76   0.501099  8.041340  0.501587  8.033470
77   0.498901  8.026399  0.506226  7.898702
78   0.504150  7.931047  0.505249  7.912806
79   0.504761  7.919906  0.496948  8.043784
80   0.496338  8.052886  0.495361  8.067842
81   0.503662  7.934940  0.505249  7.909088
82   0.492676  8.109028  0.491821  8.122160
83   0.506348  7.890127  0.497437  8.031760
84   0.499390  8.000231  0.489990  8.149702
85   0.497681  8.026757  0.498901  8.006967
86   0.498535  8.012506  0.502686  7.946049
87   0.501831  7.959404  0.494629  8.073964
88   0.513062  7.779860  0.493652  8.089049
89   0.489014  8.162770  0.498779  8.006852
90   0.502197  7.952135  0.499634  7.992775
91   0.498779  8.006167  0.498535  8.009825
92   0.495239  8.062130  0.508789  7.845868
93   0.496338  8.044115  0.502563  7.944604
94   0.502686  7.942386  0.499146  7.998543
95   0.501587  7.959331  0.494385  8.073851
96   0.499878  7.985967  0.499023  7.999270
97   0.504761  7.907475  0.505615  7.893514
98   0.494995  8.062476  0.494141  8.075743
99   0.505493  7.894392  0.489380  8.150905
100  0.494507  8.069780  0.492310  8.202250
101  0.498901  8.157214  0.498779  8.161916
102  0.497803  8.177248  0.499390  8.151258
103  0.499512  8.148878  0.499146  8.154365
104  0.503418  8.085088  0.495605  8.210593
105  0.503418  8.084258  0.502686  8.095650
106  0.498413  8.164112  0.499512  8.146001
107  0.498413  8.163318  0.497681  8.174737
108  0.496704  8.190111  0.499146  8.150397
109  0.507202  8.020194  0.507080  8.021822
110  0.499512  8.143490  0.493164  8.245488
111  0.501587  8.109433  0.504761  8.057987
112  0.489380  8.305623  0.504395  8.063347
113  0.492676  8.251984  0.506592  8.027439
114  0.499146  8.147228  0.507935  8.005340
115  0.493042  8.245171  0.500488  8.124943
116  0.493286  8.240831  0.491333  8.272120
117  0.515869  7.876462  0.496948  8.181246
118  0.498779  8.151551  0.496704  8.184821
119  0.507568  8.009533  0.501343  8.109696
120  0.497559  8.170504  0.504761  8.054231
121  0.494263  8.223248  0.503662  8.071546
122  0.501831  8.100849  0.497803  8.165560
123  0.514038  7.903648  0.504150  8.062783
124  0.499512  8.137294  0.507812  8.003235
125  0.494751  8.213482  0.494995  8.209254
126  0.502686  8.084986  0.505249  8.043340
127  0.507690  8.003641  0.502441  8.087882

2018-02-25 12:09:19.014866 Finish.
Total elapsed time: 15:43:02.01.
