2018-02-24 20:26:36.639036: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:36.639302: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:36.639315: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:16.715852 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.505005  7.916238  0.499756  7.997119
1    0.500977  7.975396  0.506714  7.881857
2    0.501465  7.963818  0.504150  7.919409
3    0.492310  8.106828  0.493042  8.093887
4    0.503174  7.931274  0.506592  7.875758
5    0.498169  8.009145  0.489990  8.138688
6    0.498047  8.009503  0.500854  7.964038
7    0.509766  7.821350  0.500488  7.968661
8    0.495117  8.053766  0.506714  7.868389
9    0.503174  7.924386  0.505737  7.883100
10   0.500244  7.970307  0.503174  7.923253
11   0.493286  8.105678  0.502075  8.031719
12   0.494629  8.092734  0.507568  7.872383
13   0.501221  7.969579  0.495361  8.059827
14   0.495728  8.051820  0.487427  8.182268
15   0.496094  8.042653  0.502319  7.942110
16   0.505005  7.898269  0.499146  7.990751
17   0.500610  7.966646  0.503906  7.913416
18   0.501587  7.949832  0.501221  7.955159
19   0.503052  7.925547  0.500854  7.960190
20   0.500732  7.961817  0.502197  7.938170
21   0.509644  7.819214  0.495972  8.036950
22   0.501953  7.941402  0.511475  7.789432
23   0.513794  7.752309  0.497559  8.011002
24   0.510620  7.802656  0.499268  7.983536
25   0.510620  7.802459  0.497314  8.014500
26   0.492432  8.092274  0.497070  8.018259
27   0.503296  7.918955  0.508179  7.841062
28   0.503418  7.916919  0.506592  7.866284
29   0.497192  8.016103  0.497192  8.016076
30   0.507568  7.850637  0.497192  8.016035
31   0.505249  7.887578  0.502319  7.934271
32   0.493042  8.082163  0.517700  7.689043
33   0.497314  8.014034  0.498657  7.992621
34   0.498291  7.998455  0.500488  7.963421
35   0.489502  8.138567  0.487305  8.173594
36   0.493286  8.078234  0.501587  7.945898
37   0.502808  8.035805  0.500122  8.123307
38   0.496094  8.184724  0.496582  8.174646
39   0.503052  8.068811  0.499634  8.122395
40   0.498169  8.144545  0.507812  7.987639
41   0.495117  8.190801  0.505371  8.024050
42   0.496338  8.168175  0.503662  8.048635
43   0.505249  8.021574  0.497925  8.138130
44   0.505127  8.020558  0.500610  8.091857
45   0.508789  7.958542  0.482788  8.376130
46   0.498657  8.118865  0.504761  8.018996
47   0.489258  8.267401  0.501953  8.061298
48   0.511841  7.900474  0.496216  8.150862
49   0.512451  7.887753  0.504028  8.022087
50   0.499268  8.097431  0.489380  8.255414
51   0.499023  8.098634  0.499268  8.093361
52   0.510986  7.903187  0.501831  8.049472
53   0.502075  8.044312  0.504395  8.005716
54   0.487183  8.281987  0.500000  8.074260
55   0.518433  7.776088  0.490967  8.217733
56   0.497803  8.106565  0.495728  8.139050
57   0.501343  8.047649  0.492065  8.196312
58   0.493286  8.175835  0.492432  8.188832
59   0.503296  8.013012  0.503662  8.006427
60   0.509521  7.911368  0.500366  8.058341
61   0.493530  8.167993  0.503540  8.006147
62   0.514038  7.836489  0.507324  7.944277
63   0.497192  8.107207  0.504639  7.986831
64   0.504395  7.990457  0.507812  7.935075
65   0.499146  8.074520  0.499390  8.070350
66   0.501831  8.030800  0.495361  8.134893
67   0.502563  8.018652  0.497192  8.105079
68   0.499878  8.061673  0.495483  8.132394
69   0.501221  8.039829  0.489746  8.224696
70   0.492554  8.179376  0.503784  7.998303
71   0.504150  7.992353  0.497681  8.096591
72   0.495483  8.131974  0.504883  7.980445
73   0.501831  8.029612  0.494263  8.151581
74   0.500366  8.053190  0.492188  8.185004
75   0.490356  8.214508  0.500000  8.059066
76   0.495850  8.125957  0.497192  8.104310
77   0.510620  7.887878  0.499390  8.068889
78   0.511108  7.880004  0.492920  8.173167
79   0.498413  8.084627  0.493408  8.165296
80   0.494385  8.149555  0.504517  7.986249
81   0.508545  7.921320  0.496948  8.108236
82   0.490845  8.206613  0.499878  8.061015
83   0.503662  8.000080  0.492920  8.157859
84   0.502930  7.996945  0.497803  8.077694
85   0.504395  7.971724  0.494629  8.126505
86   0.497925  8.073011  0.495605  8.109003
87   0.500366  8.032073  0.497681  8.073818
88   0.499878  8.037672  0.491455  8.170798
89   0.512573  7.832922  0.499512  8.039914
90   0.498291  8.058089  0.499878  8.031466
91   0.501221  8.008693  0.499756  8.030643
92   0.499634  8.031146  0.506714  7.916795
93   0.494873  8.104055  0.498047  8.051913
94   0.508057  7.890763  0.499634  8.023442
95   0.497314  8.058800  0.489258  8.185597
96   0.497803  8.047717  0.507202  7.896193
97   0.504761  7.933441  0.501587  7.982349
98   0.502563  7.965103  0.503052  7.955630
99   0.503174  7.952019  0.504028  7.936726
100  0.504395  7.929253  0.508789  7.857561
101  0.498047  8.027229  0.508179  7.864124
102  0.506226  7.893738  0.493896  8.088784
103  0.495117  8.067879  0.499023  8.004180
104  0.502686  7.944448  0.510864  7.812734
105  0.492310  8.107296  0.490356  8.137218
106  0.497314  8.025163  0.499390  7.990983
107  0.501709  7.953002  0.497192  8.024037
108  0.502075  7.945314  0.500488  7.969770
109  0.497314  8.019616  0.505249  7.892404
110  0.507568  7.854799  0.499146  7.988486
111  0.502197  7.939321  0.493164  8.082852
112  0.509155  7.827508  0.498657  7.994496
113  0.490234  8.128465  0.504517  7.900487
114  0.509521  7.820466  0.495728  8.040165
115  0.503540  7.915450  0.510742  7.800483
116  0.497070  8.021275  0.499756  7.975433
117  0.495117  8.195192  0.501343  8.063379
118  0.496948  8.122240  0.500854  8.051904
119  0.493530  8.167069  0.508911  7.917263
120  0.502441  8.020744  0.509155  7.911997
121  0.502441  8.019986  0.495972  8.124116
122  0.500854  8.045352  0.502075  8.025636
123  0.503174  8.007911  0.495483  8.131856
124  0.500732  8.059163  0.502808  8.035743
125  0.494751  8.157069  0.493774  8.166350
126  0.498779  8.081664  0.503174  8.007187
127  0.500244  8.050315  0.490967  8.194928

2018-02-25 11:51:40.954524 Finish.
Total elapsed time: 15:25:24.95.
