2018-02-24 20:26:35.199026: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:35.199398: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:35.199412: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:17.939271 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.501587  7.953996  0.507446  7.858916
1    0.498169  8.005591  0.497803  8.010361
2    0.500366  7.968703  0.503296  7.921309
3    0.501709  7.946099  0.504517  7.900895
4    0.508789  7.832453  0.506226  7.873034
5    0.486938  8.180304  0.508057  7.843445
6    0.493652  8.072946  0.500732  7.959953
7    0.489380  8.140851  0.497681  8.008440
8    0.495605  8.041467  0.496460  8.027795
9    0.502319  7.934347  0.498169  8.000484
10   0.492188  8.241630  0.499146  8.098726
11   0.497925  8.108042  0.496704  8.121144
12   0.496460  8.122089  0.501465  8.039245
13   0.495239  8.138412  0.497070  8.107999
14   0.490112  8.219627  0.498535  8.083460
15   0.494629  8.146179  0.500854  8.045643
16   0.496948  8.108490  0.498535  8.082823
17   0.501465  8.035549  0.514771  7.821046
18   0.500488  8.051224  0.490479  8.212353
19   0.484863  8.338270  0.501099  8.160866
20   0.485107  8.411450  0.501709  8.142669
21   0.503418  8.111645  0.495850  8.228593
22   0.504395  8.088825  0.496704  8.207909
23   0.490723  8.299846  0.505615  8.058995
24   0.495728  8.213252  0.496460  8.198175
25   0.508789  7.998235  0.495483  8.206936
26   0.502441  8.092579  0.492798  8.242843
27   0.506348  8.023329  0.490723  8.268877
28   0.490234  8.273081  0.505371  8.028129
29   0.491211  8.250212  0.502686  8.063558
30   0.492432  8.223284  0.504761  8.022929
31   0.511597  7.910131  0.494263  8.182607
32   0.507324  7.970501  0.497559  8.122267
33   0.499390  8.089160  0.493042  8.186401
34   0.510498  7.904174  0.497314  8.110383
35   0.516724  7.797021  0.503296  8.007134
36   0.500610  8.046044  0.497803  8.086886
37   0.495972  8.112231  0.509155  7.898202
38   0.505127  7.958663  0.494873  8.118382
39   0.489502  8.200367  0.500244  8.025486
40   0.502686  7.983066  0.496948  8.071063
41   0.502197  7.984057  0.493896  8.113104
42   0.497070  8.059379  0.504395  7.939533
43   0.492188  8.131234  0.503540  7.947393
44   0.501831  7.971966  0.486206  8.218455
45   0.504395  7.926064  0.493286  8.100802
46   0.494873  8.073338  0.505737  7.898041
47   0.502563  7.946737  0.502319  7.948800
48   0.502197  7.949104  0.501709  7.955321
49   0.491699  8.113513  0.504272  7.911750
50   0.508545  7.842491  0.505005  7.897850
51   0.503540  7.920280  0.502563  7.934989
52   0.499756  7.979027  0.511475  7.791536
53   0.499390  7.983650  0.491821  8.103808
54   0.505615  7.883497  0.487061  8.178940
55   0.498657  7.993777  0.498413  7.997415
56   0.505737  7.880456  0.498901  7.989267
57   0.506226  7.872376  0.500000  7.971517
58   0.501831  7.942247  0.503296  7.918827
59   0.501465  7.947972  0.487793  8.165896
60   0.503052  7.922608  0.494141  8.064651
61   0.494385  8.060746  0.489136  8.143498
62   0.501465  8.073554  0.502319  8.136525
63   0.499878  8.171277  0.496826  8.215992
64   0.509644  8.005273  0.498779  8.176376
65   0.494629  8.239585  0.498779  8.169111
66   0.500122  8.144176  0.495605  8.213779
67   0.506958  8.027846  0.504639  8.062355
68   0.492310  8.258405  0.509644  7.976401
69   0.495850  8.196280  0.502197  8.091557
70   0.498901  8.142391  0.499878  8.124387
71   0.497559  8.159597  0.503540  8.061027
72   0.492676  8.234044  0.498047  8.145382
73   0.505371  8.025290  0.506104  8.011443
74   0.497925  8.141269  0.499146  8.119589
75   0.503540  8.046792  0.499512  8.109751
76   0.490967  8.245550  0.494385  8.188528
77   0.491943  8.225993  0.504883  8.015551
78   0.488281  8.281306  0.501709  8.063052
79   0.497681  8.126216  0.495850  8.153976
80   0.506104  7.987018  0.499878  8.085693
81   0.500122  8.080162  0.503418  8.025464
82   0.491333  8.218755  0.499023  8.093328
83   0.502686  8.032915  0.495605  8.145672
84   0.504395  8.002736  0.495361  8.147090
85   0.501221  8.051493  0.504272  8.001181
86   0.492798  8.185097  0.508545  7.930283
87   0.510742  7.893956  0.522339  7.706162
88   0.510498  7.896223  0.502075  8.031227
89   0.498779  8.083679  0.504761  7.986631
90   0.498413  8.088385  0.504395  7.991448
91   0.506226  7.961482  0.497681  8.098786
92   0.495239  8.137779  0.498535  8.084324
93   0.509033  7.914842  0.494629  8.146761
94   0.506348  7.957674  0.491455  8.197530
95   0.485352  8.295763  0.496582  8.114620
96   0.499756  8.063364  0.495117  8.138043
97   0.499634  8.065179  0.503662  8.000193
98   0.509033  7.913580  0.501099  8.041435
99   0.499023  8.074859  0.499146  8.072871
100  0.503784  7.998091  0.494019  8.155482
101  0.513550  7.840668  0.502930  8.011838
102  0.497437  8.100374  0.499146  8.072826
103  0.498047  8.090532  0.497559  8.098401
104  0.502319  8.021666  0.505127  7.976412
105  0.499390  8.068886  0.498901  8.076756
106  0.500977  8.043307  0.507568  7.937060
107  0.506104  7.960671  0.500488  8.051178
108  0.494141  8.153490  0.498047  8.090528
109  0.503540  8.001989  0.502197  8.023632
110  0.495850  8.139614  0.497681  8.139734
111  0.510498  7.929237  0.503296  8.040459
112  0.494385  8.179750  0.501587  8.062266
113  0.499146  8.098771  0.493530  8.185958
114  0.505859  7.987273  0.498047  8.109762
115  0.504395  8.006673  0.499023  8.090461
116  0.497314  8.115998  0.490234  8.227202
117  0.497803  8.104978  0.496704  8.120952
118  0.496826  8.117543  0.493896  8.162803
119  0.504883  7.986267  0.501709  8.035486
120  0.500244  8.057504  0.489502  8.227429
121  0.504883  7.980925  0.503296  8.004927
122  0.497192  8.100963  0.508423  7.920652
123  0.494385  8.143204  0.501221  8.032971
124  0.503784  7.990869  0.493774  8.149208
125  0.496948  8.097383  0.504395  7.977437
126  0.497681  8.083246  0.502197  8.010004
127  0.495117  8.121643  0.496094  8.104825

2018-02-25 12:11:11.241432 Finish.
Total elapsed time: 15:44:54.24.
