2018-02-24 20:27:43.307602: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:43.307937: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:43.307958: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:43.307967: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:43.307975: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:48.840370 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit02, unit03, unit05.
Validation set: unit07, unit10.
Test set: unit01.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.968018  0.119562  0.936279  0.265517
1    0.972290  0.104817  0.949707  0.201302
2    0.969604  0.112517  0.949829  0.200353
3    0.976440  0.093611  0.936279  0.190360
4    0.975830  0.097095  0.950195  0.208422
5    0.980957  0.084471  0.962891  0.144812
6    0.980103  0.083191  0.934937  0.292610
7    0.976685  0.100524  0.913696  0.286970
8    0.980713  0.082523  0.927734  0.230392
9    0.982544  0.071973  0.928467  0.236859
10   0.977051  0.091179  0.964844  0.140779
11   0.981689  0.078846  0.924683  0.237867
12   0.983398  0.069313  0.899414  0.321729
13   0.927002  0.247617  0.831055  0.612078
14   0.934570  0.258757  0.923828  0.270551
15   0.943237  0.216333  0.906860  0.301753
16   0.518311  7.770952  0.902100  0.386212
17   0.957397  0.186584  0.930054  0.275844
18   0.965332  0.164677  0.939453  0.229533
19   0.971436  0.134490  0.950195  0.198045
20   0.967651  0.173122  0.959106  0.184067
21   0.776489  3.437626  0.497681  8.128337
22   0.804321  2.276786  0.824219  0.635247
23   0.930176  0.294381  0.861328  0.523738
24   0.921509  0.304528  0.812866  0.959740
25   0.899414  0.384558  0.822510  1.089998
26   0.849365  1.494258  0.502319  8.124491
27   0.496582  8.209859  0.496338  8.207872
28   0.499634  8.149759  0.491455  8.276845
29   0.500977  8.119164  0.506104  8.032497
30   0.508789  7.985599  0.503296  8.070675
31   0.494385  8.211193  0.492432  8.239689
32   0.510742  7.941874  0.497437  8.153762
33   0.504883  8.031429  0.505493  8.019372
34   0.506836  7.995733  0.499023  8.119740
35   0.498657  8.123918  0.495239  8.177351
36   0.496704  8.152242  0.500732  8.085870
37   0.496338  8.155391  0.504395  8.024267
38   0.503052  8.044751  0.500732  8.081012
39   0.503052  8.042594  0.501709  8.063230
40   0.494019  8.186249  0.499878  8.090892
41   0.498779  8.107743  0.490967  8.232825
42   0.504639  8.011667  0.493652  8.187965
43   0.500366  8.079009  0.502075  8.050731
44   0.495972  8.148410  0.493408  8.189039
45   0.500610  8.072293  0.502808  8.036222
46   0.499756  8.084781  0.494507  8.168761
47   0.495605  8.150452  0.496216  8.140018
48   0.502563  8.037130  0.502808  8.032623
49   0.498291  8.104870  0.500244  8.072840
50   0.497925  8.109692  0.514771  7.837643
51   0.507812  7.949282  0.493164  8.184878
52   0.489502  8.243413  0.499268  8.085520
53   0.495728  8.142105  0.497925  8.106218
54   0.499268  8.084119  0.500488  8.063990
55   0.501709  8.043876  0.499878  8.072953
56   0.492920  8.184680  0.505249  7.985539
57   0.497192  8.114990  0.503906  8.006372
58   0.492554  8.188962  0.502563  8.027234
59   0.490967  8.213774  0.501221  8.048125
60   0.499756  8.071373  0.501953  8.035595
61   0.502686  8.023440  0.499756  8.070312
62   0.505981  7.969629  0.497314  8.108988
63   0.506470  7.961097  0.503418  8.009961
64   0.498291  8.092285  0.501099  8.046720
65   0.493896  8.162504  0.499512  8.071699
66   0.494507  8.152081  0.498657  8.084901
67   0.497070  8.110207  0.507812  7.936795
68   0.499512  8.070332  0.503418  8.007119
69   0.496826  8.113128  0.504761  7.985004
70   0.507080  7.947401  0.500977  8.045564
71   0.493408  8.167352  0.496582  8.116002
72   0.495850  8.127630  0.495850  8.127457
73   0.507690  7.936449  0.496216  8.121247
74   0.492188  8.186040  0.497314  8.103275
75   0.505737  7.967400  0.500122  8.057799
76   0.509033  7.914074  0.496094  8.122545
77   0.509277  7.909975  0.494751  8.144042
78   0.499878  8.061346  0.508057  7.929466
79   0.506470  7.954999  0.502686  8.015952
80   0.499756  8.063140  0.503052  8.009986
81   0.498047  8.090632  0.494507  8.147670
82   0.512451  7.858425  0.498657  8.080743
83   0.497925  8.092537  0.489990  8.220418
84   0.492188  8.184995  0.510498  7.889858
85   0.500610  8.049225  0.500366  8.053156
86   0.505005  7.978387  0.491699  8.192846
87   0.495728  8.127916  0.493896  8.157428
88   0.503662  8.000024  0.488525  8.243998
89   0.491699  8.192842  0.501221  8.039373
90   0.498169  8.088561  0.503174  8.007892
91   0.497314  8.102334  0.506714  7.950833
92   0.487671  8.257769  0.493652  8.161360
93   0.502930  8.011827  0.506226  7.958703
94   0.501343  8.037405  0.494507  8.147587
95   0.493774  8.152704  0.499878  8.046557
96   0.487793  8.232772  0.508057  7.904977
97   0.500244  8.026690  0.493042  8.139167
98   0.498291  8.053817  0.504761  7.949204
99   0.499023  8.039506  0.507446  7.904157
100  0.495605  8.092022  0.503784  7.960779
101  0.496338  8.078731  0.492920  8.132492
102  0.509277  7.871045  0.490356  8.172036
103  0.501709  7.990436  0.502319  7.980102
104  0.512329  7.819947  0.502808  7.971171
105  0.505859  7.921969  0.494141  8.108247
106  0.503052  7.965651  0.499023  8.029342
107  0.500610  8.003526  0.514648  7.779209
108  0.508423  7.877954  0.503662  7.953346
109  0.502075  7.978149  0.509888  7.853101
110  0.505127  7.928508  0.502441  7.970830
111  0.493286  8.116301  0.497192  8.053536
112  0.501221  7.988829  0.491577  8.142081
113  0.497559  8.046234  0.502441  7.967897
114  0.491821  8.136712  0.492065  8.132319
115  0.500977  7.989751  0.507080  7.891934
116  0.499390  8.014022  0.503906  7.941491
117  0.499390  8.012964  0.499146  8.016314
118  0.495850  8.068308  0.501465  7.978227
119  0.500000  8.001011  0.501831  7.971239
120  0.489136  8.173044  0.493652  8.100439
121  0.501343  7.977227  0.503784  7.937687
122  0.494995  8.077181  0.505005  7.916968
123  0.506836  7.887138  0.497803  8.030502
124  0.499512  8.002609  0.499146  8.007792
125  0.499878  7.995463  0.508911  7.850794
126  0.499268  8.003884  0.494995  8.071341
127  0.501953  7.959769  0.494751  8.073941

2018-02-25 10:21:51.149084 Finish.
Total elapsed time: 13:55:03.15.
