2018-02-24 20:26:43.645382: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:43.645658: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:43.645671: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:17.661119 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.978271  0.102449  0.829224  0.451942
1    0.977417  0.099196  0.854248  0.403558
2    0.974609  0.103851  0.758423  0.846132
3    0.980103  0.099200  0.847900  0.519863
4    0.983154  0.081700  0.865112  0.438964
5    0.967651  0.146361  0.865845  0.459368
6    0.978638  0.104392  0.872192  0.373111
7    0.979004  0.087044  0.870605  0.381220
8    0.980103  0.081835  0.859497  0.414392
9    0.980713  0.089315  0.859985  0.404936
10   0.980713  0.082963  0.878540  0.387738
11   0.982666  0.074587  0.865479  0.403318
12   0.979004  0.089925  0.856567  0.440513
13   0.695312  4.725126  0.502686  7.982446
14   0.499512  8.026082  0.503662  7.954493
15   0.500977  7.993553  0.494385  8.095318
16   0.498535  8.026477  0.501709  7.973409
17   0.504517  7.926543  0.498779  8.016025
18   0.497925  8.027907  0.497192  8.037926
19   0.499390  8.001420  0.496216  8.050605
20   0.498047  8.020143  0.508789  7.847666
21   0.501953  7.955544  0.494385  8.075139
22   0.495239  8.060552  0.495117  8.061569
23   0.490601  8.132730  0.495972  8.046286
24   0.508789  7.841204  0.499146  7.994228
25   0.489746  8.143423  0.514160  7.753573
26   0.483887  8.235627  0.505859  7.884773
27   0.485107  8.215100  0.496460  8.033622
28   0.505371  7.891109  0.493286  8.083338
29   0.494873  8.057644  0.496094  8.037802
30   0.500122  7.973233  0.508301  7.842510
31   0.489624  8.139958  0.491089  8.116312
32   0.502441  7.935060  0.495850  8.039894
33   0.502319  7.936522  0.501587  7.947979
34   0.506958  7.862154  0.501709  7.945647
35   0.499878  7.974671  0.504639  7.898613
36   0.506226  7.873172  0.501831  7.943097
37   0.502930  7.925465  0.501587  7.946760
38   0.504883  7.894119  0.500244  7.967979
39   0.505127  7.890056  0.499146  7.985341
40   0.495117  8.049499  0.494385  8.061116
41   0.500732  7.972386  0.489136  8.269171
42   0.502197  8.050627  0.511353  7.897943
43   0.499878  8.077334  0.506836  7.963307
44   0.498779  8.089188  0.504761  7.991403
45   0.503906  8.002837  0.495483  8.134994
46   0.500244  8.057118  0.503784  7.998740
47   0.501343  8.035820  0.502808  8.010645
48   0.493286  8.160690  0.495850  8.118082
49   0.492920  8.163101  0.491943  8.176985
50   0.509155  7.900940  0.486694  8.257372
51   0.496460  8.100064  0.491577  8.176281
52   0.506958  7.929469  0.495483  8.110788
53   0.502563  7.996320  0.499756  8.039474
54   0.494995  8.113781  0.505127  7.950655
55   0.499756  8.034697  0.497192  8.073968
56   0.495361  8.101580  0.501099  8.008524
57   0.501587  7.999168  0.493652  8.124083
58   0.497925  8.054410  0.500854  8.006137
59   0.502319  7.981242  0.510864  7.843467
60   0.499390  8.024879  0.504761  7.937727
61   0.490723  8.160033  0.508911  7.868570
62   0.504150  7.943006  0.491821  8.138099
63   0.494751  8.089967  0.503296  7.952317
64   0.503662  7.945093  0.504639  7.928143
65   0.495239  8.076650  0.505249  7.915734
66   0.494751  8.081804  0.502075  7.963751
67   0.501343  7.974184  0.498901  8.011871
68   0.494385  8.082686  0.501099  7.974470
69   0.510132  7.829325  0.502686  7.946914
70   0.508301  7.856319  0.513306  7.775467
71   0.499512  7.994363  0.502808  7.940819
72   0.499146  7.998254  0.495605  8.053758
73   0.500122  7.980873  0.504150  7.915789
74   0.495483  8.053152  0.499878  7.982300
75   0.501221  7.960156  0.505981  7.883540
76   0.496216  8.038564  0.499268  7.989267
77   0.508301  7.844666  0.496338  8.034813
78   0.507568  7.855255  0.502930  7.928709
79   0.499023  7.990537  0.489258  8.145797
80   0.505859  7.880750  0.493042  8.084729
81   0.501465  7.950133  0.493286  8.080222
82   0.495605  8.042988  0.498901  7.990202
83   0.493652  8.073677  0.498657  7.993694
84   0.500000  7.972126  0.493896  8.069281
85   0.505493  7.884279  0.500000  7.971739
86   0.505615  7.950613  0.491333  8.256945
87   0.495117  8.193421  0.498901  8.130477
88   0.504883  8.032541  0.504028  8.044880
89   0.504517  8.035743  0.496582  8.162418
90   0.502563  8.064899  0.500122  8.103172
91   0.499634  8.110039  0.501709  8.075607
92   0.491333  8.241918  0.504883  8.022602
93   0.506470  7.996142  0.500732  8.087741
94   0.498413  8.124276  0.502563  8.056533
95   0.505127  8.014387  0.488159  8.287045
96   0.499878  8.097346  0.498901  8.112266
97   0.501099  8.076041  0.503906  8.029973
98   0.501953  8.060646  0.503296  8.038191
99   0.498291  8.118054  0.511108  7.910650
100  0.498657  8.110534  0.494141  8.182521
101  0.491699  8.221068  0.497559  8.125816
102  0.497314  8.128948  0.502930  8.037634
103  0.503174  8.032900  0.505493  7.994714
104  0.490234  8.239863  0.496948  8.130852
105  0.500732  8.069073  0.498535  8.103700
106  0.499023  8.095055  0.496582  8.133629
107  0.490479  8.231244  0.491577  8.212773
108  0.486328  8.296631  0.504761  7.998785
109  0.497314  8.118077  0.494873  8.156700
110  0.502686  8.030071  0.510010  7.911314
111  0.498291  8.099515  0.494507  8.159829
112  0.489868  8.233940  0.502075  8.036535
113  0.503174  8.018201  0.499512  8.076607
114  0.500366  8.062241  0.495850  8.134454
115  0.514893  7.826960  0.504761  7.989715
116  0.493164  8.176111  0.492920  8.179535
117  0.512939  7.856376  0.501465  8.040853
118  0.504150  7.997127  0.491333  8.203289
119  0.504150  7.996299  0.499634  8.068711
120  0.495483  8.135252  0.505493  7.973570
121  0.515991  7.804049  0.483398  8.329082
122  0.495483  8.134025  0.496460  8.118027
123  0.496826  8.111896  0.506592  7.954276
124  0.498535  8.083943  0.506226  7.959809
125  0.504639  7.985234  0.506226  7.959512
126  0.503418  8.004644  0.506470  7.955344
127  0.498901  8.077238  0.502930  8.012224

2018-02-25 12:12:16.176188 Finish.
Total elapsed time: 15:45:59.18.
