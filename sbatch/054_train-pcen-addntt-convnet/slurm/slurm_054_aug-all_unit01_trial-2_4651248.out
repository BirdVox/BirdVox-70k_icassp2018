2018-02-24 20:27:57.185627: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:57.186018: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:57.186042: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:57.186052: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:57.186062: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:49.922241 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit02, unit03, unit05.
Validation set: unit07, unit10.
Test set: unit01.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.978638  0.088532  0.961548  0.183313
1    0.978271  0.089439  0.886230  0.312420
2    0.979980  0.085057  0.963867  0.149044
3    0.979858  0.083630  0.919678  0.252666
4    0.978882  0.082063  0.945679  0.188197
5    0.977783  0.087095  0.919922  0.222034
6    0.976440  0.094189  0.957397  0.184287
7    0.921753  0.291081  0.899048  0.327900
8    0.959229  0.180170  0.936035  0.262749
9    0.966553  0.149112  0.918579  0.268296
10   0.951050  0.453013  0.495117  8.168664
11   0.501465  8.061736  0.502075  8.047668
12   0.503906  7.985450  0.496338  8.063287
13   0.503784  7.940145  0.494507  8.084323
14   0.496338  8.052341  0.503662  7.933066
15   0.498413  8.014715  0.494995  8.067335
16   0.504028  7.921758  0.501465  7.961169
17   0.493286  8.090317  0.494751  8.065804
18   0.498657  8.002532  0.490601  8.130038
19   0.495239  8.055277  0.504517  7.906613
20   0.503052  7.929306  0.500854  7.963714
21   0.501709  7.949550  0.496094  8.038561
22   0.499512  7.983626  0.512329  7.778867
23   0.499390  7.984787  0.497070  8.021418
24   0.511353  7.793425  0.509033  7.830116
25   0.506592  7.868790  0.507568  7.852988
26   0.497559  8.012363  0.498779  7.992709
27   0.507568  7.852422  0.499023  7.988490
28   0.505737  7.881316  0.502197  7.937621
29   0.492676  8.089301  0.499756  7.976319
30   0.508057  7.843888  0.494629  8.057868
31   0.503174  7.921561  0.500732  7.960406
32   0.504272  7.903901  0.495605  8.042009
33   0.497192  8.016652  0.496948  8.020488
34   0.493042  8.082713  0.499878  7.973684
35   0.508057  7.843252  0.492065  8.098149
36   0.499023  8.054039  0.506714  8.012987
37   0.496094  8.178320  0.503906  8.048450
38   0.500366  8.102647  0.499878  8.107834
39   0.500854  8.089712  0.496460  8.158252
40   0.504150  8.032213  0.499146  8.110866
41   0.499268  8.107052  0.506348  7.991145
42   0.500366  8.085911  0.500000  8.090218
43   0.494263  8.181223  0.491699  8.221113
44   0.500122  8.084034  0.503052  8.035530
45   0.499512  8.091399  0.502075  8.048921
46   0.495361  8.156057  0.505493  7.991695
47   0.502686  8.035962  0.498901  8.095988
48   0.487915  8.272157  0.503052  8.027287
49   0.501831  8.046117  0.500122  8.072828
50   0.509033  7.928405  0.495239  8.149955
51   0.497681  8.109858  0.492432  8.193725
52   0.507690  7.947078  0.510132  7.907031
53   0.502563  8.028353  0.507812  7.943093
54   0.498291  8.095936  0.496948  8.116963
55   0.501953  8.035711  0.503418  8.011527
56   0.502075  8.032631  0.502319  8.028168
57   0.494385  8.155565  0.499268  8.076383
58   0.498291  8.091680  0.504639  7.988937
59   0.507446  7.943289  0.498291  8.090474
60   0.495239  8.139318  0.497192  8.107507
61   0.496704  8.115080  0.510864  7.886561
62   0.495361  8.136188  0.494507  8.149721
63   0.502197  8.025556  0.491821  8.192597
64   0.496582  8.115689  0.495239  8.137169
65   0.501587  8.034714  0.509521  7.906690
66   0.505615  7.969535  0.493774  8.160278
67   0.493774  8.160184  0.498413  8.085329
68   0.504150  7.992779  0.488037  8.252423
69   0.496094  8.122503  0.508545  7.921756
70   0.500122  8.057466  0.497681  8.096770
71   0.504272  7.990482  0.500366  8.053405
72   0.501465  8.035664  0.505981  7.962834
73   0.494995  8.139887  0.510986  7.882114
74   0.495972  8.124101  0.498535  8.082763
75   0.494507  8.147676  0.490967  8.204720
76   0.510254  7.893837  0.503418  8.004008
77   0.504150  7.992194  0.504883  7.980380
78   0.503052  8.009887  0.497681  8.096453
79   0.502563  8.017747  0.515503  7.809184
80   0.504639  7.984292  0.510010  7.897718
81   0.493286  8.167269  0.507690  7.935097
82   0.495361  8.133818  0.498291  8.086596
83   0.499390  8.068887  0.493652  8.161361
84   0.504517  7.986250  0.493530  8.163328
85   0.494385  8.149555  0.507324  7.940996
86   0.505859  7.964606  0.491333  8.198743
87   0.499023  8.074788  0.500000  8.059048
88   0.499023  8.035596  0.503540  7.923916
89   0.504883  7.896776  0.494507  8.059707
90   0.499023  7.987174  0.505615  7.881808
91   0.508789  7.831135  0.508057  7.842771
92   0.501587  7.945902  0.500366  7.965357
93   0.503906  7.908919  0.499756  7.975085
94   0.499268  8.036683  0.495239  8.154279
95   0.504028  8.012597  0.493286  8.185726
96   0.498413  8.103073  0.507080  7.963360
97   0.495483  8.150259  0.492798  8.193526
98   0.501099  8.059712  0.510864  7.902287
99   0.505859  7.982934  0.504883  7.998649
100  0.509644  7.921889  0.490601  8.228798
101  0.499634  8.083170  0.504395  8.006405
102  0.498657  8.098845  0.495972  8.142095
103  0.506104  7.978751  0.501709  8.049543
104  0.503662  8.018019  0.506836  7.966818
105  0.503906  8.013990  0.490112  8.236271
106  0.494751  8.161450  0.493530  8.181068
107  0.505981  7.980316  0.507446  7.956640
108  0.489990  8.237929  0.494751  8.161122
109  0.497681  8.113823  0.500977  8.060617
110  0.503662  8.017243  0.506958  7.964027
111  0.502808  8.030824  0.504761  7.999240
112  0.497681  8.113247  0.492920  8.189866
113  0.494873  8.158261  0.502808  8.030242
114  0.498535  8.098969  0.495117  8.153916
115  0.501587  8.049483  0.505737  7.982427
116  0.496582  8.129823  0.497681  8.111939
117  0.502808  8.029115  0.497925  8.107621
118  0.496216  8.134961  0.497314  8.117039
119  0.506226  7.973182  0.508545  7.935565
120  0.501221  8.053370  0.498291  8.100336
121  0.501953  8.041042  0.498657  8.093888
122  0.502808  8.026703  0.494019  8.168068
123  0.514038  7.845080  0.495850  8.137924
124  0.500610  8.060860  0.504517  7.997559
125  0.492920  8.184125  0.507080  7.955532
126  0.502808  8.024030  0.495239  8.145643
127  0.501099  8.050820  0.494751  8.152745

2018-02-25 12:13:15.249506 Finish.
Total elapsed time: 15:46:26.25.
