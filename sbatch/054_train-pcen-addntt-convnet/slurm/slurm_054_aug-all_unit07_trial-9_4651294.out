2018-02-24 20:27:32.464815: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:32.465718: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:32.465733: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:32.465740: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:32.465746: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:27:08.052568 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit10, unit01, unit02.
Validation set: unit03, unit05.
Test set: unit07.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.577148  6.008419  0.502075  8.046332
1    0.503784  8.015733  0.491577  8.209957
2    0.496460  8.129417  0.498169  8.100238
3    0.505371  7.982651  0.499023  8.084234
4    0.504395  7.994415  0.500488  8.054577
5    0.500366  8.054268  0.503052  8.008851
6    0.500244  8.058228  0.496826  8.107350
7    0.493164  8.173165  0.504517  7.989641
8    0.505127  7.979581  0.494629  8.145713
9    0.498779  8.079172  0.502197  8.019151
10   0.494263  8.153238  0.500366  8.046787
11   0.506714  7.933855  0.489990  8.157349
12   0.502075  7.954724  0.490112  8.139890
13   0.500610  7.970078  0.503174  7.927349
14   0.497437  8.017657  0.493652  8.076436
15   0.505371  7.889520  0.496338  8.032940
16   0.507324  7.856767  0.498779  8.003759
17   0.497559  8.018213  0.496582  8.031354
18   0.496338  8.034169  0.501099  7.957423
19   0.492554  8.093078  0.502563  7.933007
20   0.494873  8.055239  0.513672  7.755209
21   0.502319  7.935932  0.507690  7.850065
22   0.506714  7.865439  0.502686  7.929482
23   0.502075  7.939066  0.486572  8.186085
24   0.506104  7.874598  0.504883  7.893956
25   0.497070  8.018421  0.498657  7.993044
26   0.505615  7.882052  0.489502  8.138877
27   0.496948  8.020116  0.499756  7.975311
28   0.500977  7.955814  0.510864  7.798063
29   0.496704  8.119788  0.505127  7.971204
30   0.499878  8.051284  0.495972  8.111252
31   0.505493  7.957998  0.502686  8.001404
32   0.495605  8.113039  0.495483  8.113760
33   0.503662  7.982187  0.504517  7.967382
34   0.500488  8.030446  0.500854  8.023449
35   0.494629  8.121561  0.503174  7.984191
36   0.496582  8.088152  0.497925  8.065610
37   0.502563  7.990535  0.495483  8.102278
38   0.503784  7.968820  0.510376  7.862598
39   0.495972  8.091109  0.505371  7.940120
40   0.500366  8.018772  0.505249  7.939777
41   0.496338  8.080691  0.505737  7.929677
42   0.500366  8.014138  0.507812  7.894244
43   0.498535  8.040963  0.495605  8.086470
44   0.506226  7.915960  0.492188  8.138546
45   0.496704  8.065326  0.501831  7.982363
46   0.496216  8.070660  0.496948  8.057748
47   0.495117  8.085711  0.496460  8.063066
48   0.494873  8.087138  0.508423  7.869888
49   0.494263  8.094416  0.496582  8.056217
50   0.501099  7.983008  0.499634  8.005155
51   0.497559  8.037057  0.497192  8.041712
52   0.494629  8.081426  0.497314  8.037459
53   0.505615  7.904003  0.500610  7.982675
54   0.495483  8.063329  0.498535  8.013599
55   0.501343  7.967801  0.496338  8.046559
56   0.505737  7.895721  0.496094  8.048482
57   0.493164  8.094251  0.502319  7.947369
58   0.510742  7.812209  0.500488  7.974815
59   0.498535  8.005135  0.496582  8.035469
60   0.501465  7.956872  0.491455  8.115714
61   0.504272  7.910687  0.501831  7.948939
62   0.492310  8.100115  0.494385  8.066431
63   0.489380  8.145670  0.508667  7.837656
64   0.492554  8.094058  0.506592  7.869794
65   0.502563  7.933599  0.501953  7.942933
66   0.510254  7.810248  0.500977  7.957818
67   0.504517  7.901091  0.500732  7.961148
68   0.494629  8.058219  0.511719  7.785548
69   0.493530  8.075333  0.510986  7.796873
70   0.498413  7.997182  0.494629  8.057384
71   0.500122  7.969708  0.497681  8.008539
72   0.491211  8.111610  0.489380  8.140738
73   0.504517  7.899374  0.507812  7.846787
74   0.504272  7.903192  0.506836  7.862297
75   0.499146  7.984881  0.499512  7.979026
76   0.501465  7.947877  0.496460  8.027657
77   0.498901  7.988728  0.503052  7.922555
78   0.504761  7.895306  0.492310  8.093804
79   0.499390  7.980929  0.486450  8.187213
80   0.493896  8.068500  0.501465  7.947841
81   0.515381  7.725986  0.505859  7.877781
82   0.496582  8.025684  0.498901  7.988708
83   0.490356  8.124934  0.497803  8.006222
84   0.492188  8.095742  0.503052  7.922540
85   0.492310  8.093796  0.508789  7.831074
86   0.505127  7.889457  0.501709  7.943947
87   0.510376  7.864666  0.508301  7.986300
88   0.499512  8.122335  0.503296  8.060263
89   0.503784  8.052246  0.491089  8.256728
90   0.496826  8.164102  0.496094  8.175749
91   0.497070  8.159836  0.505005  8.031766
92   0.502319  8.074861  0.494019  8.208453
93   0.491455  8.249556  0.493530  8.215882
94   0.494629  8.197932  0.496582  8.166199
95   0.501587  8.085259  0.500000  8.110555
96   0.500732  8.098448  0.506104  8.011561
97   0.492554  8.229621  0.494385  8.199757
98   0.492310  8.232831  0.489502  8.277695
99   0.504150  8.041176  0.499390  8.117478
100  0.496948  8.156372  0.502319  8.069324
101  0.497070  8.153424  0.494751  8.190284
102  0.500854  8.091354  0.501953  8.073073
103  0.506958  7.991801  0.497925  8.136773
104  0.499878  8.104637  0.487671  8.300713
105  0.493530  8.205562  0.505005  8.019881
106  0.498657  8.121431  0.503784  8.038009
107  0.496948  8.147377  0.495483  8.170150
108  0.504761  8.019753  0.506226  7.995255
109  0.505249  8.010085  0.501099  8.076049
110  0.501221  8.073129  0.503296  8.038708
111  0.501465  8.067234  0.508057  7.959981
112  0.497437  8.130141  0.497925  8.121240
113  0.497925  8.120205  0.498657  8.107353
114  0.504517  8.011866  0.503662  8.024585
115  0.497314  8.125854  0.502930  8.034298
116  0.500854  8.066713  0.500854  8.065679
117  0.501953  8.046959  0.494141  8.171872
118  0.500366  8.070546  0.502075  8.042025
119  0.507935  7.946642  0.502075  8.040150
120  0.499023  8.088444  0.500854  8.058046
121  0.493286  8.179191  0.494873  8.152783
122  0.498657  8.091005  0.502808  8.023338
123  0.494019  8.164276  0.497925  8.100606
124  0.496826  8.117653  0.494873  8.148489
125  0.502197  8.029839  0.505981  7.968265
126  0.496948  8.113331  0.494751  8.148232
127  0.492554  8.183179  0.491943  8.192566

2018-02-25 11:53:10.678702 Finish.
Total elapsed time: 15:26:02.68.
