2018-02-24 20:27:40.792282: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:40.793308: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:40.793326: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:40.793334: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:40.793342: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:49.841849 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.505859  7.898848  0.502930  7.943484
1    0.502075  7.955270  0.496094  8.048878
2    0.498169  8.014242  0.498169  8.012763
3    0.497437  8.023131  0.500000  7.981020
4    0.503296  7.927381  0.495361  8.052839
5    0.495117  8.055825  0.506104  7.879821
6    0.495850  8.042551  0.494751  8.059370
7    0.504150  7.908925  0.500488  7.966752
8    0.499390  7.983796  0.502075  7.940545
9    0.502563  7.932397  0.501587  7.947631
10   0.502319  7.935679  0.505859  7.878991
11   0.494629  8.057829  0.496948  8.020670
12   0.494995  8.051661  0.499146  7.984960
13   0.511353  7.790652  0.498169  8.000739
14   0.499268  7.983154  0.496948  8.018034
15   0.502197  7.936341  0.498535  7.990374
16   0.505127  7.889565  0.492310  8.086626
17   0.495605  8.156436  0.510620  7.972604
18   0.500366  8.105648  0.500244  8.086117
19   0.493774  8.179956  0.507080  7.957807
20   0.497070  8.114964  0.503418  8.009484
21   0.504395  7.991957  0.492188  8.187345
22   0.498291  8.088190  0.501953  8.028570
23   0.503662  8.000688  0.498291  8.087005
24   0.509888  7.899947  0.493774  8.159555
25   0.500000  8.059152  0.503784  7.998115
26   0.495361  8.133853  0.493896  8.157447
27   0.506714  7.950847  0.497437  8.100374
28   0.502563  8.017734  0.503052  8.009861
29   0.506592  8.013037  0.498779  8.192057
30   0.501099  8.138265  0.503052  8.091723
31   0.495483  8.199110  0.509277  7.966641
32   0.498901  8.121023  0.498901  8.110542
33   0.498413  8.109071  0.497314  8.117775
34   0.500000  8.067142  0.502808  8.014922
35   0.497192  8.097791  0.503906  7.984393
36   0.492188  8.165517  0.502319  7.998525
37   0.494385  8.120099  0.499634  8.031686
38   0.500488  8.013785  0.505127  7.935714
39   0.501099  7.996198  0.498535  8.033463
40   0.499512  8.014619  0.499023  8.019244
41   0.499878  8.002750  0.495728  8.066149
42   0.485474  8.227110  0.491089  8.135172
43   0.492188  8.115475  0.504150  7.922663
44   0.506104  7.889646  0.497192  8.029914
45   0.494507  8.071130  0.503174  7.931437
46   0.486938  8.188931  0.502075  7.946351
47   0.497437  8.019206  0.500732  7.965632
48   0.504761  7.900531  0.497192  8.020370
49   0.510986  7.799774  0.505005  7.894499
50   0.497803  8.008797  0.498169  8.002483
51   0.504639  7.898957  0.488647  8.153550
52   0.500610  7.954957  0.495117  8.140387
53   0.494995  8.142604  0.497803  8.097647
54   0.507202  7.947619  0.500122  8.060329
55   0.504517  7.990133  0.491577  8.196291
56   0.497925  8.094981  0.501953  8.030650
57   0.505005  7.981893  0.492798  8.176397
58   0.505493  7.973899  0.502441  8.022445
59   0.502563  8.020389  0.492310  8.183746
60   0.495239  8.136920  0.500000  8.060896
61   0.498169  8.089953  0.495117  8.138464
62   0.513184  7.850290  0.500366  8.054469
63   0.498169  8.089328  0.502686  8.017143
64   0.497925  8.092846  0.501099  8.042044
65   0.505249  7.975658  0.500610  8.049379
66   0.506470  7.955720  0.496826  8.109202
67   0.502686  8.015511  0.503540  8.001595
68   0.505249  7.974037  0.497437  8.098258
69   0.490723  8.204940  0.502563  8.015800
70   0.501831  8.027081  0.496826  8.106456
71   0.497314  8.098227  0.502441  8.016027
72   0.497437  8.095320  0.501099  8.036417
73   0.501099  8.035863  0.497070  8.099504
74   0.505981  7.956821  0.505737  7.960067
75   0.492065  8.177342  0.505737  7.958663
76   0.505737  7.957901  0.501221  8.029112
77   0.509644  7.893990  0.503784  7.986526
78   0.507080  7.933054  0.496704  8.097508
79   0.505981  7.948588  0.498535  8.066243
80   0.494385  8.131299  0.503174  7.990027
81   0.503296  7.986873  0.503662  7.979784
82   0.504761  7.960961  0.499634  8.041346
83   0.492432  8.154759  0.490234  8.188338
84   0.502808  7.986386  0.505249  7.945917
85   0.499878  8.029948  0.504883  7.948518
86   0.498047  8.055816  0.504639  7.949003
87   0.499756  8.025086  0.510620  7.850086
88   0.502197  7.982541  0.502197  7.980683
89   0.489136  8.187041  0.503662  7.953555
90   0.497314  8.052848  0.497314  8.050924
91   0.508545  7.869971  0.505493  7.916698
92   0.499634  8.008213  0.507202  7.885654
93   0.486328  8.216578  0.497437  8.037631
94   0.503784  7.934643  0.496948  8.041847
95   0.507935  7.864999  0.506714  7.882782
96   0.505371  7.902604  0.503296  7.934134
97   0.505371  7.899601  0.498047  8.014955
98   0.499023  7.998089  0.495972  8.045488
99   0.497192  8.024892  0.497925  8.012128
100  0.506348  7.876880  0.501587  7.951858
101  0.490845  8.122311  0.496826  8.026197
102  0.508667  7.836780  0.498169  8.003542
103  0.495972  8.038068  0.493530  8.076526
104  0.495972  8.037224  0.489014  8.147806
105  0.496338  8.030766  0.506470  7.868993
106  0.489258  8.143200  0.502930  7.925068
107  0.504395  7.901587  0.497437  8.012403
108  0.504395  7.901395  0.505737  7.879918
109  0.503540  7.914898  0.515137  7.729978
110  0.500610  7.961535  0.505127  7.889507
111  0.492798  8.086047  0.496582  8.025706
112  0.505737  7.879742  0.493164  8.080183
113  0.489136  8.144401  0.492920  8.084069
114  0.499023  7.986763  0.496582  8.025684
115  0.496338  8.029576  0.499023  7.986761
116  0.508667  7.833020  0.502319  7.934217
117  0.505371  7.885564  0.505127  7.885338
118  0.495972  8.207468  0.499512  8.165997
119  0.499634  8.163359  0.498291  8.184303
120  0.497681  8.193398  0.511108  7.976193
121  0.492676  8.272469  0.495117  8.232261
122  0.494873  8.235288  0.514771  7.913635
123  0.500854  8.136937  0.512939  7.941113
124  0.496338  8.207606  0.497192  8.192700
125  0.502441  8.106905  0.508423  8.009262
126  0.508301  8.009938  0.500000  8.142396
127  0.507324  8.022951  0.500977  8.123826

2018-02-25 12:13:51.928837 Finish.
Total elapsed time: 15:47:02.93.
