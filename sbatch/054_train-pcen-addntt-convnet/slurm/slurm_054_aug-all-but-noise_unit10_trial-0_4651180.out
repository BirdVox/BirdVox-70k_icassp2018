2018-02-24 20:26:35.610415: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:35.610700: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:35.610713: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:16.589507 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit01, unit02, unit03.
Validation set: unit05, unit07.
Test set: unit10.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.500977  8.059398  0.503418  8.018406
1    0.505249  7.987512  0.485474  8.304969
2    0.505981  7.973327  0.493408  8.174958
3    0.497559  8.107178  0.502441  8.027646
4    0.503174  8.015119  0.504883  7.986893
5    0.490112  8.224371  0.493286  8.172652
6    0.501831  8.034430  0.504272  7.994610
7    0.498413  8.088639  0.500488  8.054798
8    0.506104  7.963944  0.491333  8.201686
9    0.499268  8.073504  0.503784  8.000428
10   0.505005  7.980507  0.503784  7.999950
11   0.492798  8.176823  0.504395  7.989708
12   0.499878  8.062341  0.490356  8.215647
13   0.503174  8.008915  0.492432  8.181319
14   0.500854  8.044555  0.492676  8.171315
15   0.498779  8.054774  0.514160  7.796238
16   0.493774  8.112994  0.499390  8.016473
17   0.492432  8.122230  0.501099  7.979482
18   0.498901  8.010942  0.504395  7.920152
19   0.498413  8.012933  0.507935  7.858792
20   0.497437  8.024247  0.489868  8.143159
21   0.490479  8.131996  0.497559  8.017808
22   0.504150  7.911638  0.502686  7.933997
23   0.494751  8.059677  0.492188  8.099795
24   0.500610  7.964901  0.498291  8.001316
25   0.498413  7.998913  0.503296  7.920654
26   0.503906  7.910589  0.492798  8.087380
27   0.502197  7.937290  0.509644  7.818362
28   0.498779  7.991395  0.499756  7.975675
29   0.490601  8.121515  0.501587  7.946264
30   0.499146  7.985109  0.496582  8.025775
31   0.491577  8.219526  0.494507  8.186857
32   0.498413  8.110236  0.508789  7.933710
33   0.495972  8.130884  0.496704  8.113125
34   0.497314  8.098853  0.498901  8.069479
35   0.499634  8.054483  0.503052  7.996917
36   0.499390  8.052681  0.498779  8.059951
37   0.505371  7.952730  0.494385  8.125867
38   0.497314  8.077411  0.509888  7.875314
39   0.502441  7.992598  0.502441  7.991255
40   0.496948  8.077674  0.504517  7.955933
41   0.501221  8.007551  0.501587  8.000846
42   0.498047  8.056541  0.501221  8.005248
43   0.498291  8.051356  0.502808  7.978788
44   0.500610  8.013325  0.505493  7.935013
45   0.498413  8.047465  0.495605  8.091818
46   0.505127  7.939645  0.496582  8.075499
47   0.493164  8.129630  0.486084  8.242145
48   0.501343  7.998526  0.509644  7.865829
49   0.492798  8.134021  0.496338  8.077205
50   0.498413  8.043732  0.491699  8.150364
51   0.493408  8.122701  0.499390  8.026910
52   0.490845  8.162685  0.508423  7.881981
53   0.499512  8.023558  0.502686  7.972455
54   0.500000  8.014742  0.500244  8.010304
55   0.502319  7.976653  0.493774  8.112293
56   0.491577  8.146713  0.494751  8.095485
57   0.493408  8.116239  0.497803  8.045508
58   0.499878  8.011728  0.501709  7.981821
59   0.501343  7.986921  0.505127  7.925834
60   0.493652  8.107987  0.504028  7.941771
61   0.501953  7.974036  0.494995  8.084126
62   0.499023  8.019050  0.504517  7.930604
63   0.506104  7.904419  0.493896  8.098125
64   0.495728  8.068020  0.498779  8.018439
65   0.502319  7.961067  0.504883  7.919250
66   0.505737  7.904678  0.494507  8.082757
67   0.500854  7.980602  0.494385  8.082777
68   0.489624  8.157716  0.505005  7.911543
69   0.492676  8.107146  0.499023  8.004994
70   0.504150  7.922322  0.502197  7.952522
71   0.504517  7.914634  0.492310  8.108332
72   0.500122  7.982901  0.506592  7.878882
73   0.500366  7.977291  0.501587  7.956997
74   0.500977  7.965932  0.494995  8.060507
75   0.509888  7.822342  0.497681  8.016224
76   0.500610  7.968836  0.497803  8.012931
77   0.512695  7.774890  0.503296  7.924140
78   0.500732  7.964459  0.489502  8.142969
79   0.501343  7.953719  0.504639  7.900714
80   0.501831  7.945064  0.498657  7.995272
81   0.503906  7.911248  0.513306  7.761075
82   0.490112  8.130553  0.499146  7.986281
83   0.492432  8.093094  0.504517  7.900226
84   0.504883  7.894218  0.512939  7.765620
85   0.501221  7.952318  0.501221  7.952204
86   0.497681  8.008549  0.499146  7.985115
87   0.503052  7.922778  0.485962  8.195176
88   0.495239  8.047231  0.502808  7.926538
89   0.501221  7.951812  0.505127  7.889515
90   0.511475  7.788303  0.496582  8.025713
91   0.497070  8.017920  0.497437  8.012075
92   0.500244  7.967310  0.505127  7.889463
93   0.498901  7.988712  0.501099  7.953680
94   0.495483  8.043240  0.506348  7.869997
95   0.500122  7.969247  0.495605  8.041252
96   0.498413  7.999368  0.505981  7.877517
97   0.494385  8.060921  0.508423  7.836923
98   0.502563  7.930328  0.487915  8.163856
99   0.492310  8.093796  0.504883  7.893349
100  0.483398  8.334799  0.497192  8.184611
101  0.495850  8.186236  0.498047  8.141563
102  0.493652  8.209769  0.503784  8.044786
103  0.508423  7.969237  0.499268  8.116208
104  0.501465  8.080401  0.502075  8.070217
105  0.493530  8.207649  0.494263  8.195553
106  0.494751  8.187398  0.503906  8.039544
107  0.495483  8.175007  0.498169  8.131414
108  0.498779  8.121258  0.502563  8.059934
109  0.503052  8.051719  0.508423  7.964791
110  0.518311  7.805046  0.498291  8.127336
111  0.503418  8.044296  0.494873  8.181605
112  0.502319  8.061148  0.502930  8.050858
113  0.502808  8.052354  0.502075  8.063672
114  0.493408  8.202861  0.504150  8.029194
115  0.490234  8.252951  0.503784  8.033994
116  0.506226  7.994064  0.504150  8.026917
117  0.492188  8.219121  0.501221  8.072893
118  0.497437  8.133238  0.502075  8.057807
119  0.496948  8.139764  0.498535  8.113491
120  0.484741  8.335114  0.498291  8.115994
121  0.490601  8.239217  0.498169  8.116483
122  0.494263  8.178692  0.506104  7.987076
123  0.505127  8.002049  0.498291  8.111454
124  0.499756  8.087068  0.493774  8.182694
125  0.504028  8.016642  0.499146  8.094561
126  0.504639  8.005248  0.502686  8.035952
127  0.499268  8.090281  0.502686  8.034427

2018-02-25 11:25:21.988202 Finish.
Total elapsed time: 14:59:05.99.
