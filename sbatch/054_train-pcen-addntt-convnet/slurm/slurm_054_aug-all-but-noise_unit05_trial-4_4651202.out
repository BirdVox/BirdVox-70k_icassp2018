2018-02-24 20:26:43.036826: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:43.037083: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:43.037095: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:17.988030 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit07, unit10, unit01.
Validation set: unit02, unit03.
Test set: unit05.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.901733  0.290055  0.912109  0.388206
1    0.812378  1.726489  0.875488  0.453794
2    0.887695  0.345057  0.879883  0.353422
3    0.918091  0.280413  0.925781  0.448715
4    0.915039  0.287555  0.925049  0.364200
5    0.922485  0.263725  0.882935  0.639862
6    0.811768  2.381463  0.498779  8.029804
7    0.503296  7.952275  0.498535  8.023906
8    0.499146  7.994360  0.499023  8.028448
9    0.506592  7.905175  0.503540  7.951112
10   0.497192  8.049941  0.508057  7.874495
11   0.501953  7.969820  0.493530  8.102214
12   0.500977  7.981825  0.506836  7.886808
13   0.493042  8.105280  0.494507  8.080549
14   0.497192  8.036492  0.501343  7.969129
15   0.499634  7.995286  0.501953  7.957259
16   0.501953  7.956296  0.487183  8.190838
17   0.502197  7.950605  0.504150  7.918626
18   0.502686  7.941196  0.496216  8.043574
19   0.493164  8.091511  0.496704  8.034374
20   0.504639  7.907220  0.499878  7.982473
21   0.507080  7.867048  0.502563  7.938459
22   0.509033  7.834759  0.498047  8.009361
23   0.496948  8.026365  0.500610  7.967484
24   0.504883  7.898906  0.499023  7.991866
25   0.495361  8.049831  0.499390  7.985204
26   0.505005  7.895311  0.506226  7.875490
27   0.489990  8.133993  0.497803  8.009127
28   0.494141  8.067225  0.507202  7.858721
29   0.494263  8.064763  0.492065  8.099560
30   0.492798  8.087677  0.493774  8.071913
31   0.507935  7.845995  0.509277  7.824426
32   0.494629  8.057817  0.500366  7.966219
33   0.506592  7.866855  0.500122  7.969892
34   0.497314  8.014562  0.501343  7.950257
35   0.503540  7.915158  0.496460  8.027967
36   0.504028  7.907256  0.500366  7.965590
37   0.509521  7.819593  0.505981  7.875994
38   0.501709  7.944078  0.494873  8.053033
39   0.492310  8.093881  0.502197  7.936229
40   0.510254  7.807774  0.501221  7.951772
41   0.506714  7.864189  0.499512  7.979001
42   0.501831  8.061109  0.507935  8.028141
43   0.504883  8.066660  0.494019  8.235166
44   0.504395  8.064827  0.503418  8.078235
45   0.497437  8.173211  0.503052  8.081499
46   0.493286  8.237980  0.506104  8.030531
47   0.497803  8.163547  0.503784  8.066367
48   0.495972  8.191523  0.497192  8.171066
49   0.502319  8.087624  0.505127  8.041541
50   0.490723  8.272847  0.498657  8.144066
51   0.490479  8.274964  0.505371  8.033966
52   0.501465  8.095933  0.496582  8.173610
53   0.493286  8.225673  0.495728  8.185231
54   0.500977  8.099501  0.505249  8.029482
55   0.501343  8.091256  0.499268  8.123487
56   0.507568  7.988450  0.511963  7.916345
57   0.504883  8.029165  0.505249  8.021938
58   0.502197  8.069783  0.510498  7.934622
59   0.504639  8.027681  0.493774  8.201387
60   0.504028  8.034700  0.501953  8.066716
61   0.504761  8.020028  0.495972  8.160239
62   0.508423  7.958104  0.488525  8.277354
63   0.507690  7.967003  0.504517  8.016702
64   0.500122  8.086096  0.505249  8.002016
65   0.495361  8.159971  0.507568  7.961799
66   0.504517  8.009604  0.492920  8.195141
67   0.501709  8.052140  0.505981  7.981945
68   0.488770  8.258087  0.498535  8.099415
69   0.499634  8.080495  0.493286  8.181611
70   0.501709  8.044718  0.496216  8.132145
71   0.494019  8.166518  0.495361  8.143856
72   0.498169  8.097658  0.495239  8.143961
73   0.494507  8.154925  0.498657  8.087217
74   0.507568  7.942853  0.502686  8.020852
75   0.500488  8.055642  0.485107  8.302958
76   0.499512  8.070269  0.502930  8.014688
77   0.500732  8.049684  0.495728  8.129962
78   0.493896  8.159147  0.506836  7.950285
79   0.513062  7.849692  0.501831  8.030481
80   0.501709  8.032268  0.493408  8.165900
81   0.503296  8.006404  0.495117  8.138118
82   0.512939  7.850774  0.500244  8.055326
83   0.489624  8.226450  0.497925  8.092613
84   0.507202  7.943049  0.505371  7.972536
85   0.501587  8.033513  0.496582  8.114168
86   0.511597  7.872151  0.495239  8.135795
87   0.503784  7.998063  0.493530  8.163333
88   0.499268  8.070856  0.500244  8.055115
89   0.499390  8.068887  0.502075  8.025600
90   0.502563  8.017730  0.488647  8.242029
91   0.505005  7.978379  0.506958  7.946898
92   0.495728  8.127912  0.491211  8.200711
93   0.493164  8.169230  0.503174  8.007892
94   0.495728  8.127912  0.497559  8.098399
95   0.500977  8.032669  0.498535  8.062671
96   0.492432  8.148895  0.493530  8.123006
97   0.495361  8.088385  0.499634  8.015686
98   0.498291  8.033728  0.504761  7.927616
99   0.497925  8.034293  0.501343  7.977731
100  0.500488  7.989702  0.503418  7.941496
101  0.501465  7.971412  0.505615  7.904146
102  0.501343  7.971304  0.504395  7.921784
103  0.495483  8.063105  0.501953  7.959283
104  0.506958  7.878853  0.494629  8.074859
105  0.503540  7.932213  0.505859  7.894735
106  0.509521  7.835843  0.510376  7.821763
107  0.497681  8.023680  0.500366  7.980415
108  0.503540  7.929404  0.497925  8.018493
109  0.498413  8.010289  0.500000  7.984550
110  0.503418  7.929673  0.502319  7.946780
111  0.499512  7.991120  0.498413  8.008278
112  0.511108  7.805494  0.497314  8.024990
113  0.506714  7.874758  0.512451  7.782887
114  0.495361  8.054964  0.505737  7.889171
115  0.506104  7.882965  0.499756  7.983794
116  0.504150  7.913397  0.494141  8.072639
117  0.503662  7.920451  0.502441  7.939605
118  0.505371  7.892536  0.500000  7.977846
119  0.503784  7.917145  0.495972  8.041366
120  0.509399  7.826978  0.501099  7.959018
121  0.498047  8.007388  0.503418  7.921437
122  0.498169  8.004830  0.493164  8.084312
123  0.494629  8.060687  0.501831  7.945620
124  0.500122  7.972590  0.491821  8.104717
125  0.492065  8.100568  0.497803  8.008851
126  0.502930  7.926901  0.488770  8.152438
127  0.503418  7.918714  0.493408  8.078128

2018-02-25 12:12:14.202397 Finish.
Total elapsed time: 15:45:57.20.
