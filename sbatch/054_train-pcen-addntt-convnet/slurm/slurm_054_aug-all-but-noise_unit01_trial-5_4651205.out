2018-02-24 20:26:36.552657: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:36.553409: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:36.553422: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:17.783551 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit02, unit03, unit05.
Validation set: unit07, unit10.
Test set: unit01.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.499023  7.999013  0.490723  8.129746
1    0.499023  7.996091  0.501709  7.952061
2    0.498291  8.005538  0.489990  8.136935
3    0.494751  8.060253  0.502563  7.934977
4    0.516724  7.708621  0.497925  8.007755
5    0.516235  7.715370  0.507324  7.856022
6    0.500244  8.080157  0.499268  8.139666
7    0.495850  8.183601  0.491699  8.241552
8    0.494507  8.190067  0.498901  8.113787
9    0.504761  8.015129  0.506226  7.987710
10   0.510864  7.909853  0.508423  7.946363
11   0.494995  8.160423  0.500488  8.069683
12   0.503784  8.014691  0.492188  8.199860
13   0.505859  7.977994  0.495483  8.143826
14   0.502930  8.022586  0.505005  7.987990
15   0.507812  7.941739  0.498169  8.096234
16   0.503784  8.004909  0.497070  8.112352
17   0.494141  8.158904  0.510986  7.886752
18   0.501709  8.035737  0.506104  7.964391
19   0.497314  8.105610  0.504150  7.995011
20   0.499390  8.071387  0.502563  8.019896
21   0.496582  8.116021  0.505127  7.978028
22   0.501709  8.032896  0.500366  8.054332
23   0.511353  7.877080  0.493408  8.166149
24   0.492065  8.187661  0.504761  7.982916
25   0.492676  8.177605  0.501953  8.027983
26   0.498291  8.086938  0.506958  7.947178
27   0.504883  7.980575  0.504639  7.984244
28   0.499634  8.071550  0.505859  8.123200
29   0.506958  8.089544  0.499023  8.193506
30   0.495605  8.233376  0.498901  8.167635
31   0.503174  8.088376  0.503662  8.070034
32   0.495728  8.187088  0.487915  8.302536
33   0.507812  7.976990  0.502686  8.050631
34   0.503906  8.023679  0.500610  8.068923
35   0.507446  7.953157  0.503540  8.008813
36   0.489990  8.218672  0.497925  8.086171
37   0.495605  8.117568  0.506714  7.935037
38   0.505249  7.953353  0.503784  7.971804
39   0.500122  8.025663  0.504517  7.951210
40   0.513672  7.801215  0.503906  7.952991
41   0.501099  7.994173  0.496704  8.060775
42   0.493408  8.110174  0.495728  8.070167
43   0.494507  8.086887  0.498657  8.018086
44   0.502686  7.951498  0.506470  7.888901
45   0.491943  8.118461  0.502075  7.955002
46   0.498535  8.009723  0.497559  8.023660
47   0.498657  8.004708  0.499023  7.997507
48   0.513184  7.770570  0.502075  7.946539
49   0.498413  8.003946  0.480591  8.287157
50   0.493042  8.087866  0.506348  7.875002
51   0.500366  7.969729  0.497681  8.011954
52   0.494995  8.054270  0.497925  8.007101
53   0.500610  7.963899  0.494629  8.058899
54   0.491943  8.101415  0.502686  7.929885
55   0.497070  8.019179  0.503418  7.917776
56   0.494873  8.053833  0.505005  7.892154
57   0.506348  7.870623  0.510498  7.804343
58   0.496094  8.033892  0.498901  7.989051
59   0.500854  7.957850  0.495728  8.039529
60   0.503784  7.911043  0.505127  7.889598
61   0.505493  7.883730  0.502563  7.930410
62   0.492676  8.088024  0.503296  7.918697
63   0.499146  7.984853  0.499146  7.984843
64   0.500244  7.967321  0.503784  7.910878
65   0.498169  8.000394  0.498901  7.988714
66   0.506958  7.860270  0.503784  7.910867
67   0.498779  7.990656  0.502686  7.928380
68   0.506226  7.874091  0.499023  7.986768
69   0.501831  8.053132  0.493896  8.176853
70   0.499146  8.087641  0.506592  7.964841
71   0.503296  8.014804  0.502686  8.022328
72   0.510986  7.888310  0.494263  8.153386
73   0.503052  8.011956  0.499512  8.067140
74   0.504395  7.988152  0.501587  8.031792
75   0.504150  7.989858  0.499023  8.070536
76   0.497803  8.088964  0.497314  8.095715
77   0.500366  8.046041  0.505127  7.969111
78   0.500488  8.042034  0.511230  7.869735
79   0.499268  8.059401  0.504639  7.972706
80   0.501831  8.016387  0.501831  8.015287
81   0.500854  8.029740  0.494995  8.122016
82   0.494507  8.128647  0.510620  7.870587
83   0.502075  8.005621  0.496338  8.095873
84   0.504150  7.970095  0.494873  8.116748
85   0.499146  8.047374  0.494751  8.116153
86   0.498047  8.062320  0.497925  8.062959
87   0.498657  8.049972  0.494629  8.112866
88   0.493164  8.134893  0.497925  8.057656
89   0.506104  7.925934  0.502197  7.986864
90   0.502319  7.983584  0.489990  8.178798
91   0.507812  7.893343  0.507568  7.895905
92   0.489258  8.186512  0.499390  8.023677
93   0.505737  7.921200  0.488281  8.198212
94   0.507324  7.893377  0.485596  8.238542
95   0.504639  7.933751  0.494873  8.088246
96   0.505981  7.910002  0.499023  8.019790
97   0.500732  7.991451  0.502930  7.955339
98   0.501587  7.975712  0.507812  7.875440
99   0.493896  8.096320  0.505737  7.906587
100  0.497437  8.038007  0.498901  8.013751
101  0.494873  8.077114  0.491821  8.124919
102  0.497925  8.026808  0.490234  8.148616
103  0.499268  8.003846  0.500732  7.979742
104  0.497803  8.025730  0.495605  8.060049
105  0.502686  7.946493  0.499146  8.002251
106  0.503174  7.937377  0.503784  7.926996
107  0.493652  8.087891  0.511963  7.795350
108  0.491699  8.117790  0.497192  8.029606
109  0.493042  8.095178  0.496338  8.042039
110  0.498291  8.010321  0.497803  8.017524
111  0.501465  7.958575  0.502319  7.944385
112  0.503784  7.920480  0.497070  8.026964
113  0.500977  7.964155  0.504639  7.905241
114  0.500244  7.974787  0.492798  8.092989
115  0.489258  8.148938  0.497681  8.014175
116  0.491577  8.111021  0.494263  8.067756
117  0.496582  8.030355  0.513428  7.761379
118  0.489014  8.150210  0.499634  7.980522
119  0.494507  8.061910  0.497314  8.016814
120  0.504150  7.907527  0.500977  7.957831
121  0.501221  7.953676  0.494019  8.068245
122  0.504395  7.902606  0.491333  8.110336
123  0.499634  8.056929  0.499146  8.098250
124  0.510132  7.916319  0.499756  8.076684
125  0.496826  8.120048  0.507446  7.947830
126  0.505127  7.982563  0.508667  7.924106
127  0.508179  7.930266  0.498413  8.084465

2018-02-25 11:48:45.268900 Finish.
Total elapsed time: 15:22:28.27.
