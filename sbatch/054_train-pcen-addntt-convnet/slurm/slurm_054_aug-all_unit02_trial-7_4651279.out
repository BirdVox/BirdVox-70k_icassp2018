2018-02-24 20:27:53.419674: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:53.419990: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:53.420007: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:53.420015: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:53.420022: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:52.094239 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.531128  7.567781  0.496948  8.095294
1    0.504517  7.967508  0.500122  8.029048
2    0.498047  8.062109  0.501953  7.997838
3    0.500610  8.016192  0.502930  7.969632
4    0.494385  8.110117  0.488037  8.207202
5    0.498413  8.043398  0.503662  7.955880
6    0.500854  8.000756  0.513306  7.798723
7    0.501587  7.984089  0.507446  7.891270
8    0.498047  8.038883  0.496094  8.071097
9    0.495850  8.073032  0.499634  8.008968
10   0.490845  8.149798  0.489868  8.156106
11   0.502563  7.960890  0.501465  7.968804
12   0.502563  7.957626  0.503418  7.938484
13   0.504272  7.937190  0.497437  8.063742
14   0.498413  8.032200  0.487793  8.184516
15   0.490356  8.149213  0.491333  8.124822
16   0.502808  8.080234  0.501465  8.126026
17   0.496216  8.202884  0.505127  8.049428
18   0.508667  7.984461  0.494141  8.211458
19   0.500366  8.110511  0.505615  8.021844
20   0.492432  8.230757  0.509766  7.942572
21   0.502930  8.057259  0.494873  8.180615
22   0.497803  8.130674  0.507935  7.966912
23   0.494507  8.182834  0.502319  8.052836
24   0.501709  8.062688  0.500732  8.076296
25   0.492920  8.198519  0.493652  8.185003
26   0.506470  7.974792  0.502319  8.044334
27   0.504150  8.012897  0.499268  8.090213
28   0.498901  8.094590  0.496948  8.124744
29   0.501221  8.054813  0.502686  8.026183
30   0.502686  8.025082  0.501953  8.037919
31   0.499756  8.073904  0.502197  8.034007
32   0.511719  7.875579  0.497314  8.108711
33   0.507324  7.946686  0.495728  8.130912
34   0.490845  8.212701  0.500977  8.048757
35   0.499512  8.069920  0.499390  8.073382
36   0.506104  7.960839  0.499023  8.070827
37   0.500732  8.046553  0.499878  8.058060
38   0.510254  7.896130  0.494629  8.147583
39   0.504517  7.988022  0.510620  7.887447
40   0.502197  8.024868  0.504639  7.979715
41   0.513062  7.849470  0.503418  8.004685
42   0.494263  8.152193  0.507446  7.937676
43   0.493896  8.152650  0.503540  7.993240
44   0.502563  8.018081  0.500122  8.048330
45   0.492432  8.183403  0.502563  8.002373
46   0.497192  8.079363  0.505615  7.937903
47   0.504517  7.951410  0.500610  8.010400
48   0.509155  7.870669  0.500732  8.004340
49   0.492065  8.140963  0.493530  8.114480
50   0.510498  7.844627  0.501343  7.985996
51   0.506470  7.906936  0.493042  8.114716
52   0.490601  8.158262  0.500977  7.984880
53   0.502808  7.960277  0.491089  8.146458
54   0.505615  7.915888  0.502808  7.959939
55   0.497925  8.037085  0.498413  8.026826
56   0.496704  8.053435  0.508179  7.869785
57   0.497314  8.036731  0.504395  7.923553
58   0.502319  7.962922  0.489014  8.166799
59   0.506714  7.891262  0.510498  7.826514
60   0.494751  8.080694  0.498047  8.020044
61   0.505859  7.901930  0.500488  7.985047
62   0.504028  7.924119  0.498291  8.014795
63   0.505981  7.897158  0.497314  8.032670
64   0.502075  7.955971  0.491577  8.120749
65   0.491333  8.124074  0.495728  8.054998
66   0.502075  7.955139  0.492554  8.106098
67   0.493652  8.087898  0.495361  8.054614
68   0.493042  8.096251  0.502930  7.936216
69   0.489502  8.151410  0.498901  7.993740
70   0.508545  7.844858  0.504395  7.909150
71   0.499634  7.985672  0.502075  7.944399
72   0.494019  8.075937  0.503784  7.912383
73   0.502197  7.942749  0.493164  8.080922
74   0.496948  8.027317  0.505859  7.882958
75   0.493042  8.088635  0.493652  8.074875
76   0.496216  8.035519  0.496094  8.036955
77   0.497803  8.011252  0.502441  7.935132
78   0.496826  8.022516  0.485962  8.191542
79   0.499390  7.982820  0.500366  7.969193
80   0.489014  8.149613  0.495483  8.044244
81   0.502563  7.932960  0.494141  8.066278
82   0.504517  7.899490  0.495728  8.036962
83   0.500732  7.961259  0.503662  7.910815
84   0.497803  8.002148  0.508057  7.842170
85   0.507080  7.857762  0.495239  8.046310
86   0.503052  7.923420  0.491211  8.108413
87   0.487427  8.174911  0.502075  7.943564
88   0.500000  7.974075  0.495605  8.043545
89   0.495850  8.041271  0.497192  8.008085
90   0.505859  7.881158  0.496948  8.017436
91   0.501587  7.948822  0.493286  8.077310
92   0.500977  7.956314  0.495361  8.045610
93   0.494629  8.059102  0.495850  8.035728
94   0.499634  7.998207  0.500488  8.087097
95   0.495239  8.167070  0.506836  7.976075
96   0.509033  7.938137  0.507690  7.957680
97   0.508789  7.938293  0.504883  7.999741
98   0.514771  7.839081  0.502075  8.040815
99   0.490723  8.221037  0.503784  8.011224
100  0.509766  7.915782  0.504272  8.002941
101  0.508667  7.932014  0.504272  7.996703
102  0.501221  8.050768  0.500854  8.056170
103  0.508911  7.925701  0.498657  8.090624
104  0.508057  7.938571  0.508423  7.930253
105  0.500854  8.053762  0.501221  8.045779
106  0.504517  7.993953  0.492554  8.186415
107  0.504395  7.995197  0.501831  8.036202
108  0.517944  7.772096  0.494385  8.151805
109  0.501221  8.045152  0.503784  8.001758
110  0.492798  8.180329  0.500244  8.060185
111  0.500122  8.059947  0.501465  8.040195
112  0.495605  8.134260  0.500854  8.047549
113  0.501831  8.031773  0.502563  8.019614
114  0.507446  7.942625  0.501221  8.040927
115  0.496216  8.123329  0.496216  8.123343
116  0.499146  8.075825  0.495728  8.129104
117  0.503784  8.000799  0.501465  8.034456
118  0.500366  7.984261  0.496338  8.036402
119  0.499512  7.985195  0.493042  8.080372
120  0.504883  7.898899  0.508789  7.834598
121  0.502319  7.950701  0.493164  8.098142
122  0.502319  7.953228  0.509277  7.839853
123  0.509766  7.833234  0.492920  8.101149
124  0.503174  7.937107  0.490356  8.140926
125  0.501587  7.961304  0.504395  7.916009
126  0.488770  8.164645  0.501953  7.952272
127  0.500366  7.978712  0.504272  7.914266

2018-02-25 11:54:44.019697 Finish.
Total elapsed time: 15:27:52.02.
