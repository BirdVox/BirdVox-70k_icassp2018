2018-02-24 20:28:03.115103: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:28:03.115882: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:28:03.115894: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:48.773257 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.968506  0.121523  0.853638  0.444022
1    0.965698  0.145747  0.852173  0.483377
2    0.966553  0.129279  0.764648  0.824524
3    0.797119  2.626115  0.667603  5.350289
4    0.652100  5.652576  0.640503  5.836033
5    0.621094  6.147769  0.656372  5.576095
6    0.703613  4.791572  0.757690  3.441757
7    0.829834  1.168693  0.777832  2.651717
8    0.875488  1.031560  0.767822  2.773830
9    0.585938  6.400945  0.495605  8.098593
10   0.509277  7.876146  0.497803  8.055175
11   0.498535  8.040469  0.493896  8.111670
12   0.507935  7.885593  0.501953  7.978829
13   0.507324  7.891365  0.496094  8.068663
14   0.504517  7.932828  0.506714  7.896304
15   0.496582  8.056471  0.502197  7.965635
16   0.497070  8.046160  0.503296  7.945731
17   0.505127  7.915448  0.501831  7.966926
18   0.496948  8.043778  0.489868  8.155682
19   0.494507  8.080826  0.503174  7.941769
20   0.498535  8.014894  0.504028  7.926510
21   0.500610  7.980243  0.498535  8.012584
22   0.513672  7.770573  0.496338  8.046236
23   0.505493  7.899639  0.505859  7.893172
24   0.507690  7.863390  0.504517  7.913407
25   0.503174  7.934267  0.498413  8.009626
26   0.493774  8.083067  0.501709  7.956069
27   0.503174  7.932240  0.495850  8.048536
28   0.491211  8.122040  0.497803  8.016510
29   0.499512  7.988843  0.496826  8.031241
30   0.488892  8.157339  0.500000  7.979852
31   0.501465  7.958948  0.505981  7.895324
32   0.494507  8.071474  0.499268  7.992393
33   0.504761  7.903664  0.494019  8.074051
34   0.504150  7.911948  0.504761  7.901708
35   0.501831  7.947997  0.507080  7.863925
36   0.497070  8.023159  0.508301  7.843787
37   0.496094  8.038095  0.515381  7.730320
38   0.497803  8.010288  0.507446  7.856284
39   0.500000  7.974753  0.498901  7.992031
40   0.486572  8.188366  0.498291  8.001325
41   0.503662  7.915497  0.503296  7.921140
42   0.502197  7.938473  0.498169  8.002517
43   0.506348  7.871964  0.503296  7.920456
44   0.500244  7.968960  0.501343  7.951301
45   0.502563  7.931708  0.497192  8.017207
46   0.503784  7.929561  0.506104  8.008780
47   0.499390  8.111353  0.500366  8.093806
48   0.502686  8.055720  0.495483  8.171146
49   0.503418  8.042641  0.500244  8.093187
50   0.505493  8.007991  0.502075  8.062490
51   0.503906  8.032396  0.502930  8.047553
52   0.501709  8.066653  0.501953  8.062140
53   0.508057  7.963188  0.499512  8.100337
54   0.508667  7.952196  0.499390  8.101148
55   0.500244  8.086796  0.502197  8.054729
56   0.490356  8.244995  0.493530  8.193246
57   0.498169  8.117886  0.502075  8.054324
58   0.504272  8.018305  0.502197  8.051143
59   0.497925  8.119394  0.509644  7.929889
60   0.498779  8.104376  0.498901  8.101775
61   0.506714  7.975217  0.501343  8.061144
62   0.500977  8.066399  0.492676  8.199535
63   0.498413  8.106402  0.511230  7.899142
64   0.500610  8.069647  0.489868  8.242112
65   0.501709  8.050581  0.505371  7.990866
66   0.512451  7.876061  0.504395  8.005224
67   0.493774  8.175709  0.492188  8.200589
68   0.495605  8.144805  0.508057  7.943418
69   0.501831  8.043074  0.500977  8.056156
70   0.496338  8.130244  0.497437  8.111856
71   0.504028  8.004946  0.500977  8.053473
72   0.498901  8.086280  0.493652  8.170248
73   0.511475  7.882375  0.504517  7.993920
74   0.507324  7.948091  0.501465  8.041965
75   0.495117  8.143743  0.500488  8.056648
76   0.498779  8.083705  0.500122  8.061587
77   0.502197  8.027702  0.498047  8.094175
78   0.498047  8.093790  0.502075  8.028491
79   0.495850  8.128504  0.497803  8.096708
80   0.515381  7.813103  0.508301  7.926957
81   0.509521  7.907052  0.499878  8.062273
82   0.505859  7.965681  0.503662  8.000927
83   0.513184  7.847317  0.498535  8.083292
84   0.494995  8.140243  0.494141  8.153919
85   0.496460  8.116458  0.492798  8.175414
86   0.495117  8.137976  0.498291  8.086772
87   0.500488  8.051319  0.494629  8.145728
88   0.498535  8.082743  0.503906  7.996150
89   0.498291  8.086642  0.495483  8.131883
90   0.500732  8.047270  0.488525  8.244016
91   0.499268  8.070867  0.512207  7.862304
92   0.508301  7.925262  0.503418  8.003961
93   0.495239  8.135785  0.497803  8.094466
94   0.489990  8.220388  0.509155  7.911483
95   0.505493  7.970509  0.500610  8.049210
96   0.500244  8.055113  0.499512  8.066918
97   0.504150  7.992151  0.501099  8.041340
98   0.498901  8.076756  0.498169  8.088561
99   0.494873  8.141684  0.497559  8.098399
100  0.501831  8.040929  0.505737  7.990417
101  0.493408  8.186191  0.497192  8.122828
102  0.503418  8.020799  0.490845  8.221957
103  0.496338  8.132201  0.511841  7.881192
104  0.493530  8.175341  0.497681  8.107508
105  0.499878  8.071250  0.500244  8.064534
106  0.504517  7.994926  0.495728  8.135867
107  0.503418  8.011248  0.501587  8.040116
108  0.502319  8.027719  0.500977  8.048789
109  0.496338  8.123032  0.503662  8.004474
110  0.498657  8.084686  0.498047  8.094084
111  0.503296  8.009086  0.496826  8.112990
112  0.503052  8.012312  0.503174  8.010029
113  0.486450  8.279307  0.493408  8.166897
114  0.499023  8.076168  0.507446  7.940199
115  0.492065  8.187934  0.503296  8.006758
116  0.499390  8.069584  0.507202  7.943538
117  0.503662  8.000497  0.497681  8.096815
118  0.498779  8.079036  0.495239  8.136030
119  0.498901  8.076954  0.499390  8.069039
120  0.492554  8.179188  0.492676  8.177192
121  0.492432  8.181106  0.498169  8.088613
122  0.498047  8.090568  0.501709  8.031531
123  0.509277  7.909535  0.503052  8.009874
124  0.505737  7.966584  0.500732  8.047249
125  0.510010  7.897714  0.498901  8.076759
126  0.500854  8.045277  0.490845  8.206614
127  0.497314  8.102335  0.501099  8.041341

2018-02-25 11:35:12.995547 Finish.
Total elapsed time: 15:08:25.00.
