2018-02-24 20:28:04.372506: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:28:04.372815: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:28:04.372836: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:28:04.372846: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:28:04.372855: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:52.525069 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.501953  7.957016  0.499512  7.993624
1    0.496094  8.046199  0.500732  7.970482
2    0.499512  8.058844  0.504761  8.016520
3    0.495361  8.163511  0.498047  8.116536
4    0.506348  7.980040  0.498291  8.107476
5    0.505615  7.987468  0.500977  8.060435
6    0.500977  8.058926  0.506348  7.970947
7    0.492310  8.196008  0.500610  8.061081
8    0.491943  8.199789  0.495850  8.135895
9    0.498047  8.099658  0.507935  7.939507
10   0.500610  8.056869  0.494751  8.150378
11   0.508789  7.923799  0.504028  7.999972
12   0.495850  8.131296  0.509644  7.907316
13   0.513306  7.849028  0.487915  8.257865
14   0.495850  8.129606  0.502319  8.024974
15   0.511353  7.879060  0.501709  8.034193
16   0.497070  8.108690  0.498169  8.090725
17   0.493286  8.169197  0.497314  8.101322
18   0.509644  7.905137  0.504761  7.982278
19   0.509521  7.906763  0.491821  8.191906
20   0.494629  8.146522  0.496216  8.120821
21   0.503784  7.998729  0.487671  8.258346
22   0.503296  8.006418  0.504150  7.992569
23   0.493042  8.171551  0.504883  7.980641
24   0.508301  7.925502  0.504028  7.994322
25   0.492920  8.173333  0.501099  8.041475
26   0.493408  8.165405  0.495483  8.130590
27   0.507080  7.982660  0.502441  8.050658
28   0.502441  8.040042  0.506592  7.964746
29   0.503052  8.014214  0.497070  8.103279
30   0.494751  8.135105  0.497437  8.087520
31   0.504639  7.968634  0.501587  8.013458
32   0.498413  8.060704  0.493408  8.137303
33   0.509521  7.877571  0.497192  8.070557
34   0.498657  8.045567  0.496704  8.074317
35   0.505371  7.933957  0.500366  8.011624
36   0.504639  7.941548  0.499756  8.017477
37   0.503296  7.959256  0.497192  8.054814
38   0.501587  7.983114  0.490967  8.150812
39   0.494873  8.087015  0.505859  7.910368
40   0.499146  8.015980  0.498535  8.024307
41   0.505981  7.904258  0.503784  7.936592
42   0.488770  8.176078  0.493286  8.102828
43   0.502930  7.947900  0.506958  7.882506
44   0.504272  7.924202  0.496948  8.039873
45   0.495483  8.062166  0.495850  8.055292
46   0.500854  7.974520  0.498047  8.018312
47   0.496582  8.040751  0.495850  8.051529
48   0.500732  7.972839  0.498169  8.011216
49   0.498901  8.000423  0.491455  8.118409
50   0.501953  7.950301  0.505859  7.887333
51   0.506104  7.882797  0.491943  8.107918
52   0.495850  8.045064  0.502686  7.934413
53   0.501343  7.956409  0.501953  7.946176
54   0.487427  8.177300  0.510132  7.814904
55   0.501099  7.958483  0.504395  7.905544
56   0.503174  7.924645  0.501831  7.945704
57   0.509155  7.828622  0.509277  7.824592
58   0.498901  7.991512  0.506714  7.866696
59   0.499634  7.979328  0.493286  8.080293
60   0.511719  7.786224  0.502930  7.926171
61   0.501831  7.944989  0.506958  7.863600
62   0.494873  8.055022  0.500488  7.964882
63   0.494263  8.063845  0.498413  7.997442
64   0.501221  7.952517  0.503418  7.917343
65   0.498047  8.002862  0.506226  7.872378
66   0.503662  7.913171  0.499390  7.981217
67   0.496826  8.022033  0.493896  8.068692
68   0.497192  8.016111  0.486938  8.179550
69   0.505249  7.887611  0.497559  8.010192
70   0.497437  8.012122  0.499023  7.986808
71   0.502075  7.938145  0.502075  7.938135
72   0.494873  8.052948  0.502686  7.928393
73   0.503296  7.918658  0.492798  8.086019
74   0.502441  7.932276  0.492432  8.090910
75   0.496216  8.031524  0.500244  7.967302
76   0.501709  7.943948  0.505493  7.883619
77   0.494019  8.105044  0.495972  8.095873
78   0.506104  7.908953  0.504883  7.914058
79   0.495483  8.057742  0.505981  7.885824
80   0.505493  7.890998  0.508789  7.836389
81   0.507202  7.860387  0.504272  7.906031
82   0.502197  7.938413  0.490845  8.118816
83   0.510986  7.797316  0.496216  8.032462
84   0.510132  7.810380  0.510620  7.802405
85   0.498169  8.000775  0.495605  8.041534
86   0.504883  7.893557  0.506104  7.874035
87   0.503540  7.914863  0.497681  8.008242
88   0.502075  7.938161  0.489136  8.144430
89   0.497192  8.015977  0.504272  7.903095
90   0.497314  8.014017  0.503662  7.912816
91   0.489258  8.142453  0.501465  7.947842
92   0.490723  8.119097  0.504761  7.895296
93   0.496460  8.027630  0.498291  7.998438
94   0.495850  8.037360  0.504150  7.905025
95   0.496460  8.027629  0.501343  7.949785
96   0.501709  7.944508  0.502197  7.936167
97   0.499512  7.979944  0.482178  8.255322
98   0.498169  8.000384  0.501587  7.945893
99   0.490112  8.128827  0.497437  8.012060
100  0.502197  8.108468  0.498535  8.291671
101  0.498413  8.288700  0.493408  8.364980
102  0.501221  8.235467  0.500732  8.239976
103  0.498779  8.268483  0.493896  8.344312
104  0.499634  8.249143  0.511841  8.049722
105  0.498169  8.267487  0.507568  8.113372
106  0.502686  8.189463  0.499390  8.239937
107  0.507080  8.113307  0.506348  8.122387
108  0.504761  8.145196  0.507080  8.104989
109  0.504395  8.145403  0.497925  8.246755
110  0.498657  8.231977  0.512207  8.010553
111  0.495117  8.282943  0.493774  8.301469
112  0.506714  8.089766  0.496826  8.245946
113  0.492432  8.313574  0.504028  8.123413
114  0.498657  8.206741  0.501709  8.154275
115  0.501343  8.156917  0.499756  8.179208
116  0.507080  8.057902  0.492432  8.290734
117  0.503540  8.108464  0.496338  8.221314
118  0.502197  8.123700  0.501221  8.136264
119  0.489746  8.318113  0.505005  8.069074
120  0.507812  8.020809  0.500977  8.127990
121  0.499023  8.156563  0.500732  8.126125
122  0.501099  8.117429  0.500977  8.116623
123  0.501587  8.104114  0.501587  8.101466
124  0.489624  8.291741  0.509277  7.972448
125  0.501343  8.097925  0.501709  8.089633
126  0.491211  8.256558  0.495728  8.181501
127  0.496704  8.163604  0.498535  8.131960

2018-02-25 11:49:40.558946 Finish.
Total elapsed time: 15:22:48.56.
