2018-02-24 20:26:46.393530: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:46.393912: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:46.393925: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:17.703503 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit07, unit10, unit01.
Validation set: unit02, unit03.
Test set: unit05.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.913330  0.254374  0.934814  0.198693
1    0.930298  0.215407  0.933472  0.197292
2    0.932495  0.222431  0.945190  0.183950
3    0.935303  0.202800  0.930786  0.214236
4    0.943359  0.182126  0.927246  0.222219
5    0.875610  0.367041  0.885864  0.319036
6    0.898804  0.286443  0.877930  0.371514
7    0.920654  0.233206  0.854614  0.399172
8    0.687500  4.512999  0.501831  7.961186
9    0.494629  8.073863  0.490601  8.136313
10   0.500000  7.985100  0.501343  7.962441
11   0.524536  7.573868  0.503296  7.953651
12   0.745117  1.585385  0.897339  0.325800
13   0.882690  0.372212  0.908325  0.297249
14   0.607422  5.955313  0.501465  7.999775
15   0.509766  7.860890  0.500000  8.011082
16   0.498413  8.032349  0.499023  8.019031
17   0.498535  8.023944  0.501709  7.970708
18   0.495850  8.061907  0.504517  7.921665
19   0.493408  8.096973  0.511475  7.807265
20   0.502808  7.943958  0.502930  7.940606
21   0.505493  7.898493  0.491699  8.117214
22   0.500122  7.981875  0.503418  7.928317
23   0.498047  8.013037  0.501221  7.961568
24   0.496338  8.038628  0.502686  7.936679
25   0.503418  7.924322  0.496338  8.036541
26   0.505493  7.889992  0.498169  8.006186
27   0.504761  7.900579  0.498779  7.995439
28   0.504517  7.949924  0.496704  8.176522
29   0.500000  8.119298  0.486572  8.334112
30   0.503784  8.056011  0.501343  8.094772
31   0.503540  8.058857  0.502808  8.070182
32   0.507202  7.998905  0.484741  8.360494
33   0.505737  8.021657  0.497559  8.153066
34   0.499023  8.129052  0.509644  7.957472
35   0.494995  8.193179  0.491821  8.243934
36   0.511719  7.922825  0.511475  7.926355
37   0.508179  7.979068  0.489990  8.271812
38   0.496094  8.173005  0.497559  8.148952
39   0.498901  8.126851  0.508179  7.976844
40   0.492798  8.224261  0.503540  8.050605
41   0.498779  8.126803  0.501221  8.086896
42   0.504639  8.031219  0.495605  8.176210
43   0.500244  8.100805  0.499146  8.117851
44   0.498413  8.128961  0.494629  8.189233
45   0.498779  8.121581  0.507690  7.977169
46   0.499023  8.116047  0.497559  8.138813
47   0.499390  8.108422  0.503296  8.044555
48   0.497192  8.141993  0.498291  8.123320
49   0.496948  8.143967  0.495850  8.160653
50   0.494995  8.173377  0.506226  7.991291
51   0.505615  8.000036  0.501587  8.063849
52   0.496826  8.139453  0.508789  7.945486
53   0.494873  8.168630  0.497681  8.122206
54   0.500366  8.077752  0.497803  8.117892
55   0.500000  8.081310  0.501343  8.058496
56   0.499878  8.080957  0.493286  8.186055
57   0.505981  7.980314  0.508545  7.937885
58   0.496338  8.133570  0.506836  7.963304
59   0.498413  8.098058  0.502319  8.034106
60   0.505859  7.976115  0.508789  7.927982
61   0.496460  8.125855  0.501587  8.042392
62   0.500610  8.057372  0.511841  7.875624
63   0.503540  8.008748  0.501465  8.041553
64   0.502319  8.027203  0.504028  7.999105
65   0.500000  8.063542  0.501465  8.039464
66   0.502686  8.019377  0.502563  8.020955
67   0.496094  8.124894  0.502319  8.024228
68   0.501465  8.037722  0.492432  8.183058
69   0.502197  8.025428  0.498169  8.090145
70   0.501221  8.040774  0.494141  8.154721
71   0.502319  8.022749  0.499268  8.071801
72   0.498657  8.081522  0.503174  8.008614
73   0.507812  7.933754  0.497925  8.093039
74   0.506958  7.947367  0.487549  8.260138
75   0.498047  8.090873  0.503174  8.008184
76   0.503662  8.000270  0.495850  8.126153
77   0.495483  8.132023  0.503052  8.010005
78   0.494141  8.153612  0.501221  8.039473
79   0.500732  8.047325  0.494995  8.139784
80   0.505127  7.976466  0.493530  8.163371
81   0.492920  8.173201  0.507080  7.944959
82   0.508667  7.919375  0.489380  8.230242
83   0.500854  8.045289  0.505249  7.974455
84   0.504761  7.982323  0.499390  8.068892
85   0.500610  8.049215  0.491577  8.194812
86   0.483765  8.320734  0.498047  8.090531
87   0.499756  8.062985  0.500854  8.045276
88   0.493286  8.167264  0.503418  8.003958
89   0.504761  7.982314  0.499146  8.072821
90   0.495605  8.129879  0.517212  7.781625
91   0.497314  8.102334  0.494263  8.151522
92   0.499634  8.062812  0.500244  8.055113
93   0.491211  8.192741  0.507446  7.930658
94   0.496094  8.107940  0.504517  7.970826
95   0.499634  8.046968  0.493530  8.142902
96   0.491211  8.178972  0.507446  7.919377
97   0.497070  8.084236  0.497437  8.077901
98   0.498657  8.058045  0.497925  8.069359
99   0.503906  7.973696  0.509521  7.883888
100  0.495728  8.103539  0.497681  8.072158
101  0.504883  7.957111  0.502197  7.999700
102  0.499390  8.044246  0.503296  7.981756
103  0.492432  8.154745  0.490845  8.179830
104  0.495483  8.105664  0.502319  7.996461
105  0.497681  8.070186  0.505737  7.941512
106  0.501099  8.015220  0.497437  8.073353
107  0.499146  8.045846  0.492310  8.154556
108  0.504517  7.959663  0.492432  8.152031
109  0.487183  8.235403  0.509033  7.886731
110  0.506714  7.923369  0.509033  7.886043
111  0.502441  7.990764  0.499634  8.035143
112  0.499634  8.034743  0.502930  7.981786
113  0.498169  8.057251  0.496826  8.078211
114  0.501831  7.997955  0.492310  8.149269
115  0.502197  7.991135  0.498413  8.050947
116  0.502075  7.992029  0.491821  8.154948
117  0.492554  8.142700  0.507935  7.896903
118  0.492310  8.145395  0.504517  7.950161
119  0.505737  7.930055  0.500488  8.013075
120  0.492798  8.134997  0.500488  8.011694
121  0.496338  8.077143  0.502686  7.975209
122  0.497925  8.050351  0.494385  8.106013
123  0.502197  7.980670  0.493896  8.112191
124  0.492554  8.132767  0.497559  8.052127
125  0.498779  8.031798  0.500732  7.999772
126  0.498169  8.039735  0.499878  8.011565
127  0.493530  8.111821  0.500000  8.007720

2018-02-25 12:10:26.595095 Finish.
Total elapsed time: 15:44:09.60.
