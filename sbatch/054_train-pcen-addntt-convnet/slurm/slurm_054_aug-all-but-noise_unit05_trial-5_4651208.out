2018-02-24 20:26:47.632730: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:47.633030: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:47.633042: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:17.619337 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit07, unit10, unit01.
Validation set: unit02, unit03.
Test set: unit05.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.907959  0.368584  0.936401  0.234218
1    0.767090  3.166807  0.501221  8.083657
2    0.502441  8.058163  0.496826  8.143948
3    0.494507  8.177708  0.500366  8.079967
4    0.493408  8.189405  0.502075  8.047211
5    0.507568  7.956585  0.497437  8.117956
6    0.494019  8.171416  0.506104  7.975111
7    0.505493  7.983657  0.510132  7.907682
8    0.490845  8.217516  0.504517  7.996178
9    0.495483  8.140933  0.506958  7.955189
10   0.502197  8.031230  0.507568  7.944001
11   0.492310  8.189366  0.491089  8.208494
12   0.499634  8.070281  0.497803  8.099333
13   0.501587  8.037931  0.491455  8.200847
14   0.494873  8.145409  0.504395  7.991611
15   0.497559  8.101500  0.506226  7.961524
16   0.498901  8.079328  0.500977  8.045642
17   0.493286  8.169387  0.497314  8.104258
18   0.502686  8.017509  0.503784  7.999632
19   0.501221  8.040801  0.500366  8.054433
20   0.500854  8.046437  0.491577  8.195852
21   0.505493  7.971448  0.504150  7.992992
22   0.497559  8.099152  0.497803  8.095134
23   0.496460  8.116705  0.506104  7.961201
24   0.504150  7.992622  0.493774  8.159806
25   0.494995  8.140082  0.513306  7.844905
26   0.504150  7.992430  0.497314  8.102575
27   0.497925  8.092705  0.503052  8.009381
28   0.494019  8.117976  0.502563  7.961598
29   0.498535  8.020187  0.498413  8.018266
30   0.496460  8.047157  0.507935  7.862335
31   0.510376  7.821995  0.484619  8.231340
32   0.492554  8.103787  0.504761  7.908193
33   0.491699  8.115574  0.500610  7.972704
34   0.494629  8.067348  0.494507  8.068609
35   0.508789  7.840299  0.498779  7.999282
36   0.495361  8.053229  0.503174  7.928152
37   0.504150  7.912098  0.494263  8.069261
38   0.497925  8.010445  0.498779  7.996401
39   0.501221  7.957092  0.507690  7.853572
40   0.493774  8.075082  0.493042  8.086424
41   0.500122  7.973245  0.486694  8.187019
42   0.511353  7.793641  0.494141  8.067781
43   0.496948  8.022788  0.508911  7.831847
44   0.494751  8.057392  0.500732  7.961841
45   0.498169  8.002537  0.507568  7.852523
46   0.504883  7.895190  0.498901  7.990409
47   0.500610  7.963039  0.506714  7.865615
48   0.502075  7.939461  0.494995  8.052233
49   0.495850  8.038520  0.496460  8.028703
50   0.492310  8.094792  0.498291  7.999359
51   0.493652  8.171785  0.497559  8.338312
52   0.510010  8.124190  0.494995  8.355660
53   0.496582  8.322326  0.499146  8.273809
54   0.506836  8.143435  0.504395  8.176542
55   0.502319  8.204124  0.496094  8.298684
56   0.507324  8.112140  0.493896  8.323086
57   0.498169  8.248938  0.496460  8.271230
58   0.503662  8.150062  0.504761  8.127291
59   0.503540  8.142058  0.498779  8.213900
60   0.511597  8.002564  0.500488  8.176883
61   0.492310  8.304130  0.499512  8.183488
62   0.501099  8.153506  0.495972  8.231765
63   0.494385  8.253127  0.503052  8.109249
64   0.494751  8.239029  0.499146  8.164225
65   0.495605  8.217488  0.504028  8.077978
66   0.491821  8.271167  0.505981  8.039417
67   0.485352  8.368600  0.490601  8.280719
68   0.503784  8.065129  0.497070  8.170303
69   0.494995  8.200886  0.515625  7.865559
70   0.490723  8.264290  0.508179  7.980334
71   0.494507  8.198253  0.502075  8.073863
72   0.499634  8.110950  0.496704  8.155945
73   0.499146  8.114490  0.485229  8.336718
74   0.500244  8.092745  0.506348  7.992430
75   0.500610  8.083059  0.503906  8.028114
76   0.505981  7.992928  0.502319  8.050236
77   0.503540  8.028920  0.504028  8.019428
78   0.504639  8.008040  0.492188  8.207199
79   0.505859  7.985376  0.496582  8.133470
80   0.500977  8.061273  0.494385  8.166176
81   0.504639  7.999634  0.485596  8.305326
82   0.499878  8.073959  0.494385  8.161359
83   0.495972  8.134723  0.500488  8.060896
84   0.503296  8.014697  0.499390  8.076745
85   0.489746  8.231350  0.508301  7.931485
86   0.500977  8.048823  0.500610  8.054042
87   0.503174  8.012121  0.495361  8.137473
88   0.505371  7.975639  0.504272  7.992882
89   0.499634  8.067251  0.500488  8.053110
90   0.506836  7.950489  0.505737  7.967913
91   0.490356  8.215591  0.494995  8.140614
92   0.493896  8.158154  0.502197  8.024210
93   0.500122  8.057541  0.499023  8.075145
94   0.502808  8.014073  0.499023  8.074998
95   0.502197  8.023792  0.499268  8.070971
96   0.501709  8.031590  0.508911  7.915480
97   0.500732  8.047288  0.511719  7.870195
98   0.511719  7.870186  0.498291  8.086608
99   0.498413  8.084636  0.504517  7.986255
100  0.512817  7.852460  0.506470  7.954771
101  0.497314  8.102335  0.494873  8.141685
102  0.493042  8.171198  0.494019  8.155458
103  0.501099  8.041340  0.508545  7.921229
104  0.496704  8.182382  0.493286  8.266983
105  0.495361  8.224972  0.491577  8.276878
106  0.491821  8.265423  0.498047  8.158833
107  0.505005  8.041055  0.499512  8.121893
108  0.488647  8.288694  0.491821  8.231766
109  0.504883  8.017475  0.505859  7.995906
110  0.500488  8.075783  0.493408  8.182961
111  0.492188  8.196976  0.500488  8.059258
112  0.499634  8.067750  0.504883  7.979007
113  0.502197  8.017021  0.496948  8.095979
114  0.510376  7.877452  0.500122  8.036549
115  0.494019  8.129747  0.505493  7.942795
116  0.505981  7.931257  0.501587  7.997650
117  0.499146  8.033165  0.499023  8.031793
118  0.503784  7.952825  0.496460  8.066607
119  0.503784  7.947092  0.505859  7.911342
120  0.500000  8.002309  0.497803  8.034970
121  0.496826  8.048373  0.506104  7.898376
122  0.495239  8.069671  0.502441  7.953011
123  0.497314  8.033075  0.490234  8.144338
124  0.500366  7.981354  0.495850  8.051957
125  0.496826  8.035123  0.495728  8.051424
126  0.505737  7.890753  0.495117  8.059017
127  0.510254  7.816766  0.501831  7.950152

2018-02-25 12:06:18.564267 Finish.
Total elapsed time: 15:40:01.56.
