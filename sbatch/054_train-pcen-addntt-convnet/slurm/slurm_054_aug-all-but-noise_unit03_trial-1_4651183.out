2018-02-24 20:26:44.025954: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:44.026187: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:44.026200: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:16.900473 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.489502  8.155164  0.497192  8.031225
1    0.497192  8.030082  0.501831  7.955056
2    0.504639  7.909367  0.505493  7.894867
3    0.501221  7.962216  0.494629  8.066582
4    0.500000  7.980320  0.495361  8.053670
5    0.490356  8.132929  0.508667  7.840510
6    0.492065  8.104730  0.500610  7.968075
7    0.501343  7.956015  0.497681  8.014030
8    0.504639  7.902772  0.487915  8.169068
9    0.492188  8.100666  0.498779  7.995298
10   0.494507  8.063156  0.510864  7.802133
11   0.498413  8.000408  0.491943  8.103330
12   0.499756  7.978576  0.492676  8.091250
13   0.503052  7.925647  0.503174  7.923520
14   0.497681  8.010924  0.503784  7.913454
15   0.496704  8.120397  0.500977  8.091615
16   0.497070  8.148986  0.501831  8.069462
17   0.488037  8.286906  0.485229  8.329416
18   0.504517  8.019940  0.498169  8.119184
19   0.497681  8.125082  0.500488  8.078428
20   0.499634  8.090162  0.503540  8.025972
21   0.501709  8.053233  0.500488  8.070728
22   0.493652  8.177719  0.511475  7.891562
23   0.492188  8.196989  0.493408  8.175435
24   0.508057  7.939785  0.496460  8.122507
25   0.499634  8.069730  0.494751  8.145358
26   0.504639  7.985488  0.496094  8.119444
27   0.501587  8.029583  0.509277  7.904659
28   0.494507  8.137806  0.495117  8.125712
29   0.498657  8.066908  0.495483  8.115110
30   0.510620  7.871399  0.507446  7.919576
31   0.499390  8.045604  0.497559  8.072359
32   0.500366  8.025179  0.510742  7.857324
33   0.504028  7.961947  0.505493  7.936169
34   0.498535  8.044706  0.501221  7.999495
35   0.488770  8.195646  0.505127  7.932518
36   0.493652  8.113156  0.500977  7.994102
37   0.507202  7.892630  0.502197  7.970212
38   0.512817  7.798773  0.492554  8.119714
39   0.502319  7.962008  0.517578  7.716752
40   0.500488  7.987310  0.496826  8.043830
41   0.507446  7.872765  0.492188  8.114308
42   0.509766  7.832468  0.504639  7.912641
43   0.501953  7.954013  0.500488  7.975968
44   0.490356  8.136217  0.501953  7.950109
45   0.501465  7.956785  0.491577  8.113357
46   0.504761  7.902236  0.501953  7.946100
47   0.501953  7.945316  0.504761  7.899816
48   0.504517  7.903073  0.500366  7.968646
49   0.504517  7.901977  0.489624  8.138936
50   0.499268  7.984811  0.511475  7.789851
51   0.496704  8.025044  0.497681  8.009218
52   0.499268  7.983717  0.500977  7.956290
53   0.501587  7.946421  0.502197  7.936568
54   0.508423  7.837227  0.496094  8.033703
55   0.503052  7.922719  0.498779  7.990784
56   0.491577  8.105569  0.495850  8.037427
57   0.502930  7.924535  0.494385  8.060746
58   0.496460  8.027653  0.496460  8.027644
59   0.503662  7.912820  0.500244  7.967307
60   0.503052  7.922544  0.490479  8.122990
61   0.495483  8.043199  0.500977  7.955625
62   0.502319  7.934217  0.503418  7.916702
63   0.509155  7.825236  0.497314  8.014006
64   0.500488  7.963408  0.496094  8.033467
65   0.509155  7.825235  0.489380  8.138289
66   0.504150  8.054848  0.498779  8.152163
67   0.503296  8.078785  0.503784  8.070311
68   0.503418  8.075575  0.491455  8.267735
69   0.501099  8.111613  0.489380  8.299786
70   0.498901  8.145572  0.496826  8.178249
71   0.497681  8.163674  0.508301  7.991670
72   0.503906  8.061642  0.503540  8.066659
73   0.504883  8.044100  0.508301  7.988068
74   0.501587  8.095312  0.495972  8.184825
75   0.496338  8.177901  0.501587  8.092251
76   0.502808  8.071508  0.493042  8.227820
77   0.494995  8.195230  0.495850  8.180327
78   0.498901  8.129993  0.510864  7.936009
79   0.499023  8.125686  0.493286  8.216968
80   0.498901  8.125264  0.503540  8.049284
81   0.501587  8.079551  0.504028  8.038973
82   0.494995  8.183347  0.499146  8.115214
83   0.506836  7.990030  0.500854  8.085200
84   0.500366  8.091839  0.497681  8.133885
85   0.500732  8.083468  0.493774  8.194380
86   0.510254  7.927538  0.503296  8.038455
87   0.508057  7.960502  0.508789  7.947470
88   0.511475  7.902973  0.486206  8.309035
89   0.511353  7.902519  0.496826  8.135447
90   0.500366  8.077197  0.493164  8.192086
91   0.499146  8.094500  0.500366  8.073645
92   0.501587  8.052815  0.499023  8.092977
93   0.496094  8.139072  0.497559  8.114337
94   0.498047  8.105379  0.487915  8.267604
95   0.500610  8.061942  0.501831  8.041240
96   0.493408  8.176025  0.492554  8.188839
97   0.497437  8.109237  0.514526  7.832902
98   0.505371  7.979654  0.493164  8.175620
99   0.499390  8.074557  0.502197  8.028613
100  0.499268  8.075217  0.505371  7.976253
101  0.498901  8.080018  0.499023  8.077565
102  0.502686  8.018124  0.507080  7.946906
103  0.496216  8.121693  0.499878  8.062370
104  0.493896  8.158537  0.503174  8.008784
105  0.499268  8.071572  0.502197  8.024195
106  0.499023  8.051644  0.512451  7.810172
107  0.503052  7.959874  0.497925  8.041469
108  0.497559  8.047176  0.494751  8.091811
109  0.496826  8.058606  0.492310  8.130491
110  0.498901  8.025283  0.497314  8.050462
111  0.496704  8.060068  0.498413  8.032694
112  0.489258  8.178515  0.495361  8.081070
113  0.500732  7.995293  0.500610  7.997083
114  0.516235  7.747816  0.505981  7.911112
115  0.495361  8.080235  0.494873  8.087824
116  0.507935  7.879383  0.500488  7.997875
117  0.505859  7.912013  0.498657  8.026587
118  0.505493  7.917345  0.494995  8.084435
119  0.495483  8.076360  0.497314  8.046865
120  0.512085  7.811065  0.491699  8.135725
121  0.493042  8.113961  0.495117  8.080505
122  0.503418  7.947777  0.495850  8.068025
123  0.501953  7.970287  0.507080  7.888102
124  0.502686  7.957688  0.494141  8.093423
125  0.504028  7.935275  0.504395  7.928903
126  0.504395  7.928345  0.494141  8.091241
127  0.505371  7.911600  0.512207  7.802001

2018-02-25 12:00:38.680852 Finish.
Total elapsed time: 15:34:22.68.
