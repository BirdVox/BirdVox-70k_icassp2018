2018-02-24 20:27:45.041052: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:45.041458: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:45.041480: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:45.041490: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:45.041499: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:52.567072 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit10, unit01, unit02.
Validation set: unit03, unit05.
Test set: unit07.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.926025  0.219586  0.951660  0.170951
1    0.930054  0.213618  0.966675  0.120170
2    0.939331  0.191914  0.958618  0.158130
3    0.937500  0.196219  0.967407  0.124228
4    0.938110  0.202215  0.956299  0.165419
5    0.943726  0.190453  0.971924  0.111768
6    0.943359  0.183811  0.965210  0.138656
7    0.935669  0.198722  0.973633  0.113754
8    0.948486  0.169616  0.965942  0.141653
9    0.947876  0.173413  0.910034  0.257404
10   0.609375  5.703092  0.497803  8.040386
11   0.495728  8.068864  0.498413  8.022145
12   0.494385  8.083384  0.492432  8.111822
13   0.505493  7.901385  0.505371  7.901299
14   0.504028  7.921010  0.504395  7.913598
15   0.487061  8.188618  0.498657  8.002507
16   0.503296  7.927513  0.509644  7.825346
17   0.501953  7.947128  0.499512  7.985284
18   0.500244  7.972959  0.504272  7.908134
19   0.503296  7.923190  0.502319  7.938280
20   0.499390  7.984581  0.499390  7.984202
21   0.492920  8.087023  0.491699  8.106183
22   0.501709  7.946346  0.501221  7.953891
23   0.506348  7.871949  0.498901  7.990467
24   0.500244  7.968893  0.496216  8.032958
25   0.492554  8.091205  0.498291  7.999611
26   0.499634  7.978093  0.507935  7.845654
27   0.493774  8.071309  0.500000  7.971971
28   0.499146  7.985518  0.495483  8.043828
29   0.498901  7.989275  0.509521  7.819905
30   0.506104  7.874343  0.489258  8.142854
31   0.503296  7.919010  0.502441  7.932592
32   0.509033  7.827467  0.494507  8.059018
33   0.500977  7.955846  0.491455  8.107614
34   0.498291  7.998609  0.500244  7.967449
35   0.488770  8.150363  0.494751  8.054987
36   0.489868  8.132815  0.502930  7.924569
37   0.500854  7.957641  0.498047  8.002391
38   0.515991  7.716306  0.513916  7.749381
39   0.490845  8.117186  0.495361  8.045174
40   0.493408  8.076306  0.505371  7.885585
41   0.503540  7.914773  0.507324  7.854441
42   0.499390  7.959585  0.505249  7.991601
43   0.495728  8.187873  0.491699  8.250186
44   0.496338  8.173295  0.496826  8.163477
45   0.494629  8.197166  0.511230  7.927904
46   0.497437  8.148667  0.500000  8.105806
47   0.495117  8.183040  0.498657  8.124531
48   0.508545  7.963769  0.507324  7.982065
49   0.492676  8.216843  0.502075  8.064022
50   0.505859  8.001756  0.504761  8.018198
51   0.498901  8.111417  0.501099  8.074785
52   0.490967  8.236916  0.497925  8.123596
53   0.496460  8.146077  0.503052  8.038707
54   0.507568  7.964823  0.502686  8.042447
55   0.502686  8.041407  0.505981  7.987250
56   0.500244  8.078729  0.500122  8.079709
57   0.495239  8.157460  0.504028  8.014855
58   0.496094  8.141840  0.497192  8.123235
59   0.494507  8.165661  0.506836  7.966089
60   0.508057  7.945599  0.484253  8.328464
61   0.500610  8.064045  0.497681  8.110506
62   0.486694  8.286861  0.500366  8.065782
63   0.504272  8.002143  0.504272  8.001474
64   0.515625  7.817859  0.500488  8.061210
65   0.503174  8.017334  0.499268  8.079716
66   0.498047  8.098845  0.505859  7.972386
67   0.493164  8.176506  0.499390  8.075668
68   0.501221  8.045693  0.501099  8.047209
69   0.498169  8.094008  0.496338  8.123110
70   0.497681  8.101085  0.495239  8.140064
71   0.499268  8.074792  0.495605  8.133485
72   0.510986  7.885269  0.505737  7.969576
73   0.500854  8.048008  0.498291  8.089065
74   0.493164  8.171466  0.498901  8.078765
75   0.506104  7.962476  0.506104  7.962281
76   0.496460  8.117544  0.503662  8.001293
77   0.509521  7.906705  0.500366  8.054133
78   0.504761  7.983181  0.501465  8.036191
79   0.490479  8.213172  0.496704  8.112736
80   0.507568  7.937547  0.500244  8.055527
81   0.493286  8.167616  0.499512  8.067215
82   0.502930  8.012077  0.500122  8.057288
83   0.499756  8.063156  0.505981  7.962780
84   0.512573  7.856508  0.499634  8.065045
85   0.495483  8.131923  0.505859  7.964667
86   0.502197  8.023681  0.500610  8.049248
87   0.492554  8.179098  0.500122  8.057103
88   0.511597  7.872149  0.504395  7.988230
89   0.507935  7.931168  0.492432  8.181043
90   0.503784  7.998059  0.513306  7.844590
91   0.504517  7.986252  0.505493  7.970510
92   0.487793  8.255803  0.504761  7.982315
93   0.493774  8.159393  0.504517  7.986249
94   0.498169  8.088561  0.496338  8.118074
95   0.509277  7.891615  0.493774  8.105812
96   0.504883  7.928101  0.502075  7.972236
97   0.499146  8.018331  0.503784  7.943764
98   0.498657  8.024887  0.503418  7.948368
99   0.516357  7.741458  0.492432  8.122258
100  0.496460  8.057400  0.489136  8.173519
101  0.493164  8.108649  0.502197  7.963979
102  0.505249  7.914666  0.496460  8.054115
103  0.497681  8.033984  0.492554  8.115042
104  0.502930  7.948947  0.503540  7.938532
105  0.505859  7.900875  0.497192  8.038359
106  0.500366  7.987078  0.492798  8.107046
107  0.505127  7.909808  0.510742  7.819599
108  0.496460  8.046611  0.498657  8.010896
109  0.502197  7.953781  0.501343  7.966723
110  0.509277  7.839555  0.503418  7.932292
111  0.501953  7.954979  0.496582  8.039939
112  0.499390  7.994521  0.507812  7.859582
113  0.500488  7.975700  0.505859  7.889424
114  0.501587  7.956903  0.502319  7.944593
115  0.496826  8.031549  0.505615  7.890815
116  0.499878  7.981686  0.490601  8.128997
117  0.507446  7.859867  0.499268  7.989692
118  0.499390  7.987210  0.495728  8.045065
119  0.498535  7.999807  0.499268  7.987644
120  0.491943  8.103956  0.504150  7.908907
121  0.504761  7.898772  0.503540  7.917843
122  0.507080  7.861054  0.504639  7.899638
123  0.495483  8.045295  0.499512  7.980790
124  0.513794  7.752850  0.499390  7.982256
125  0.500610  7.962596  0.514038  7.748341
126  0.496704  8.024532  0.497803  8.006876
127  0.499512  7.979515  0.492065  8.098122

2018-02-25 12:06:21.574824 Finish.
Total elapsed time: 15:39:29.57.
