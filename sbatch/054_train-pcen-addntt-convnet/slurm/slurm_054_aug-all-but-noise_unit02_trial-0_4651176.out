2018-02-24 20:26:37.723860: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:37.724200: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:37.724213: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:16.829170 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.973022  0.107480  0.903442  0.342710
1    0.838379  2.336944  0.502808  7.974448
2    0.505249  7.930189  0.497925  8.042963
3    0.498169  8.036156  0.509155  7.858352
4    0.498657  8.023477  0.502808  7.955203
5    0.501831  7.968920  0.499023  8.011911
6    0.492310  8.117361  0.498169  8.022424
7    0.494507  8.079426  0.490845  8.136475
8    0.499390  7.999035  0.497803  8.023159
9    0.498779  8.006517  0.504761  7.910119
10   0.500244  7.981173  0.503174  7.933544
11   0.499390  7.993027  0.500610  7.972747
12   0.494751  8.065407  0.498901  7.998511
13   0.503784  7.919998  0.503418  7.925188
14   0.504761  7.903185  0.498291  8.005750
15   0.501587  7.952675  0.497925  8.010543
16   0.496216  8.037317  0.498901  7.994046
17   0.498901  7.993627  0.503052  7.927055
18   0.506226  7.876086  0.505615  7.885458
19   0.504150  7.908485  0.496826  8.024936
20   0.492676  8.090818  0.498901  7.991294
21   0.495483  8.045537  0.501709  7.946049
22   0.504272  7.904970  0.495728  8.040994
23   0.498291  7.999947  0.499512  7.980316
24   0.499023  7.987950  0.493286  8.079275
25   0.500977  7.956549  0.501831  7.942810
26   0.499512  7.979687  0.503662  7.913427
27   0.495117  8.049574  0.497070  8.018364
28   0.501099  7.954081  0.495483  8.043544
29   0.501465  7.948138  0.493286  8.078483
30   0.511353  7.790425  0.501343  7.949972
31   0.501465  7.947999  0.495361  8.045279
32   0.497070  8.018013  0.503906  7.909014
33   0.502075  7.938190  0.501343  7.949854
34   0.503540  7.914813  0.497925  8.004324
35   0.501587  7.945933  0.502319  7.934249
36   0.503296  7.918675  0.505737  7.879748
37   0.500732  7.959534  0.501099  7.953692
38   0.497803  8.006234  0.506104  7.873897
39   0.498779  7.990661  0.508423  7.836918
40   0.493042  8.082124  0.506836  7.862215
41   0.506470  7.926776  0.513550  7.934452
42   0.496338  8.202728  0.503784  8.075664
43   0.512817  7.925134  0.494385  8.217803
44   0.504761  8.046876  0.494629  8.206714
45   0.498779  8.136754  0.500244  8.110206
46   0.499878  8.113440  0.498169  8.138403
47   0.494873  8.189144  0.494995  8.184855
48   0.501343  8.080381  0.500854  8.086137
49   0.500732  8.086123  0.494507  8.184524
50   0.503784  8.033161  0.499390  8.102196
51   0.498657  8.112307  0.505249  8.004394
52   0.504028  8.022497  0.510864  7.910770
53   0.492554  8.204444  0.496826  8.134149
54   0.496582  8.136737  0.504517  8.007526
55   0.495361  8.153852  0.502197  8.042455
56   0.499634  8.082637  0.502686  8.032338
57   0.495728  8.143453  0.500610  8.063741
58   0.507812  7.946719  0.510254  7.906456
59   0.506104  7.972508  0.498169  8.099578
60   0.493652  8.171622  0.497803  8.103992
61   0.488892  8.246949  0.503784  8.006257
62   0.497437  8.107972  0.498657  8.087719
63   0.501709  8.038003  0.506348  7.962727
64   0.499756  8.068509  0.494263  8.156600
65   0.514160  7.835483  0.506958  7.951174
66   0.499756  8.066902  0.498047  8.094104
67   0.502563  8.020994  0.490234  8.219416
68   0.500854  8.047971  0.501221  8.041809
69   0.500122  8.059284  0.499146  8.074801
70   0.499634  8.066732  0.490356  8.216074
71   0.506104  7.962092  0.495972  8.125237
72   0.497070  8.107387  0.496460  8.117090
73   0.501343  8.038271  0.506836  7.949621
74   0.500244  8.055772  0.501587  8.034039
75   0.500122  8.057572  0.500854  8.045696
76   0.497803  8.094823  0.494629  8.145924
77   0.498291  8.086851  0.498535  8.082873
78   0.507812  7.933305  0.514893  7.819155
79   0.499634  8.065072  0.501587  8.033568
80   0.500854  8.045355  0.502808  8.013858
81   0.496826  8.110255  0.503784  7.998094
82   0.497437  8.100398  0.496094  8.122033
83   0.503662  8.000040  0.500610  8.049224
84   0.504028  7.994130  0.483521  8.324673
85   0.497314  8.087160  0.500854  8.012044
86   0.499023  8.040022  0.508179  7.893382
87   0.509155  7.877342  0.507446  7.904120
88   0.495483  8.094365  0.507324  7.905108
89   0.499390  8.031102  0.500610  8.011124
90   0.500488  8.012534  0.498657  8.041174
91   0.502808  7.974437  0.506470  7.915467
92   0.493042  8.128930  0.493286  8.124415
93   0.486206  8.236644  0.494019  8.111434
94   0.497070  8.062102  0.507202  7.899880
95   0.495605  8.084044  0.504883  7.935410
96   0.500854  7.998885  0.495117  8.089588
97   0.490234  8.166655  0.507935  7.883680
98   0.508301  7.877038  0.503052  7.959902
99   0.494507  8.095303  0.503662  7.948507
100  0.492432  8.126704  0.507080  7.892318
101  0.507690  7.881731  0.502686  7.960654
102  0.502441  7.963683  0.499023  8.017301
103  0.494019  8.096225  0.508179  7.869606
104  0.510254  7.835659  0.500122  7.996318
105  0.502441  7.958488  0.496338  8.054935
106  0.508911  7.853646  0.501831  7.965677
107  0.504517  7.922038  0.502441  7.954298
108  0.507080  7.879543  0.499512  7.999399
109  0.500000  7.990836  0.496704  8.042605
110  0.492798  8.104128  0.500977  7.972993
111  0.503052  7.939188  0.506104  7.889820
112  0.495972  8.050656  0.495605  8.055812
113  0.502930  7.938393  0.493652  8.085649
114  0.497314  8.026649  0.492188  8.107775
115  0.490723  8.130549  0.503418  7.927584
116  0.498413  8.006834  0.507080  7.868130
117  0.500977  7.964934  0.498169  8.009202
118  0.491577  8.113828  0.505127  7.897359
119  0.492065  8.105166  0.504150  7.912088
120  0.495972  8.042087  0.487427  8.177934
121  0.498291  8.004377  0.500610  7.967055
122  0.504395  7.906403  0.493408  8.081236
123  0.496704  8.028398  0.495728  8.043679
124  0.502441  7.936378  0.501831  7.945848
125  0.503174  7.924199  0.495972  8.038782
126  0.506226  7.875091  0.502441  7.935206
127  0.501587  7.948631  0.505249  7.890055

2018-02-25 11:29:56.885021 Finish.
Total elapsed time: 15:03:40.89.
