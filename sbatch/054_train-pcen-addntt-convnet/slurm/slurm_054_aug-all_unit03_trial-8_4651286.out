2018-02-24 20:27:54.401643: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:54.414369: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:54.414388: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:54.414397: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:54.414405: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:52.367660 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.497314  8.131237  0.501831  8.053068
1    0.502563  8.037072  0.501221  8.054978
2    0.488525  8.255686  0.495361  8.143820
3    0.500488  8.054584  0.502075  8.031881
4    0.492798  8.180209  0.499512  8.070968
5    0.504883  7.923430  0.500244  7.976514
6    0.494873  8.061213  0.500244  7.975219
7    0.495728  8.046372  0.489868  8.137162
8    0.491699  8.103860  0.504517  7.904456
9    0.493652  8.077161  0.497559  8.014577
10   0.501099  7.955906  0.497925  8.008060
11   0.492920  8.101265  0.506836  7.892410
12   0.499268  8.010777  0.496216  8.057450
13   0.504272  7.927615  0.492920  8.107341
14   0.499023  8.009037  0.504883  7.914569
15   0.494141  8.084993  0.494263  8.082156
16   0.508545  7.853731  0.504517  7.917186
17   0.498779  8.008011  0.488525  8.170794
18   0.503784  7.926925  0.509644  7.831174
19   0.498047  8.017247  0.507812  7.861019
20   0.494507  8.072671  0.494385  8.074100
21   0.497314  8.026914  0.502686  7.940828
22   0.488525  8.166141  0.499756  7.986675
23   0.499023  7.994322  0.492554  8.098846
24   0.496094  8.043878  0.503906  7.919014
25   0.492065  8.105551  0.500488  7.972772
26   0.493896  8.077541  0.508301  7.847513
27   0.506104  7.882222  0.500000  7.977485
28   0.492798  8.093781  0.497681  8.015571
29   0.493408  8.083391  0.503052  7.929364
30   0.499146  7.991351  0.499634  7.983286
31   0.505493  7.889603  0.500610  7.967177
32   0.501343  7.955241  0.493530  8.079533
33   0.497681  8.013117  0.503784  7.915566
34   0.495972  8.039884  0.502197  7.940392
35   0.498657  7.993114  0.503784  7.914643
36   0.495972  8.039032  0.504639  7.900595
37   0.509766  7.818681  0.550903  7.243607
38   0.578369  6.797073  0.543457  7.367153
39   0.580566  6.761012  0.545044  7.340895
40   0.570068  6.928632  0.542969  7.374341
41   0.579712  6.771442  0.546265  7.319808
42   0.563354  7.039123  0.496582  8.155281
43   0.491455  8.211312  0.492920  8.176828
44   0.499390  8.067543  0.501709  8.025494
45   0.500000  8.048979  0.496216  8.105938
46   0.499268  8.054552  0.498535  8.063708
47   0.518311  7.746314  0.507324  7.919473
48   0.508545  7.898309  0.500977  8.017333
49   0.511963  7.840772  0.492188  8.154681
50   0.501343  8.007528  0.509766  7.872084
51   0.501343  8.005330  0.497070  8.072441
52   0.499634  8.030683  0.495728  8.092051
53   0.507812  7.898572  0.499512  8.030110
54   0.502441  7.982663  0.497559  8.059783
55   0.493286  8.127215  0.486816  8.229690
56   0.499023  8.034448  0.495117  8.096101
57   0.498901  8.035179  0.499268  8.028754
58   0.493896  8.112386  0.491577  8.150242
59   0.499268  8.027103  0.487915  8.207558
60   0.506470  7.911238  0.495605  8.083962
61   0.506592  7.908337  0.496582  8.067368
62   0.501099  7.994867  0.492432  8.132549
63   0.497681  8.046705  0.503296  7.958378
64   0.493652  8.111636  0.497437  8.050820
65   0.503662  7.951083  0.510986  7.832043
66   0.501831  7.979298  0.484619  8.253184
67   0.494995  8.086825  0.500977  7.991379
68   0.503052  7.957765  0.511230  7.826859
69   0.509644  7.849811  0.506348  7.903591
70   0.488525  8.187156  0.497681  8.038788
71   0.495239  8.077197  0.493042  8.113384
72   0.500977  7.986304  0.489990  8.160805
73   0.504028  7.934622  0.502197  7.964888
74   0.498779  8.016858  0.502319  7.961593
75   0.495117  8.075719  0.505493  7.909594
76   0.486816  8.206672  0.511841  7.806964
77   0.507568  7.874347  0.505493  7.906656
78   0.502075  7.960389  0.503662  7.934330
79   0.505737  7.900467  0.505493  7.903578
80   0.504395  7.918602  0.498169  8.018775
81   0.507324  7.872034  0.500244  7.984118
82   0.507080  7.874355  0.512939  7.780157
83   0.505737  7.894204  0.500244  7.979150
84   0.491699  8.116474  0.502808  7.938624
85   0.504761  7.906753  0.498779  8.001382
86   0.504517  7.909213  0.497192  8.025281
87   0.494507  8.065596  0.490845  8.125155
88   0.495239  8.052769  0.492920  8.090924
89   0.499878  7.979339  0.503174  7.926235
90   0.508789  7.836253  0.501465  7.952458
91   0.489258  8.144944  0.504150  7.908735
92   0.497437  8.015432  0.498047  8.005252
93   0.503174  7.923222  0.504272  7.905330
94   0.500122  7.971211  0.494629  8.058514
95   0.505859  7.879240  0.503906  7.910159
96   0.497681  8.007420  0.498535  7.995431
97   0.503784  7.911607  0.493896  8.069110
98   0.506226  7.872449  0.494141  8.065016
99   0.492676  8.088292  0.504761  7.895560
100  0.507080  7.856688  0.497437  8.012226
101  0.497437  8.012234  0.503174  7.920706
102  0.509644  7.820660  0.493164  8.254155
103  0.493408  8.238618  0.507812  7.991947
104  0.497070  8.157630  0.499878  8.112523
105  0.497925  8.142321  0.492188  8.233505
106  0.517578  7.823548  0.507202  7.990118
107  0.500732  8.092278  0.494019  8.201885
108  0.504028  8.040303  0.500366  8.099111
109  0.495361  8.179632  0.502930  8.057431
110  0.500854  8.090713  0.489624  8.271581
111  0.499023  8.119919  0.490845  8.251547
112  0.493286  8.212044  0.497437  8.144942
113  0.507935  7.975543  0.491455  8.240964
114  0.511963  7.910228  0.499512  8.110685
115  0.491089  8.240994  0.497803  8.137771
116  0.509277  7.950719  0.496338  8.160880
117  0.504028  8.034788  0.496094  8.164266
118  0.489990  8.262349  0.506348  7.998393
119  0.499390  8.110221  0.500122  8.098082
120  0.499512  8.107637  0.512573  7.896677
121  0.492065  8.226958  0.510498  7.929346
122  0.500854  8.084407  0.498657  8.119369
123  0.501587  8.071678  0.492676  8.214860
124  0.503662  8.037304  0.502686  8.052511
125  0.495605  8.166121  0.509399  7.943230
126  0.504517  8.021369  0.503540  8.036529
127  0.513672  7.872633  0.497559  8.131721

2018-02-25 11:42:52.528322 Finish.
Total elapsed time: 15:16:00.53.
