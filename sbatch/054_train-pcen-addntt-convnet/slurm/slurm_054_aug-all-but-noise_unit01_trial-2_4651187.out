2018-02-24 20:26:38.420715: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:38.420978: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:38.420990: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:16.792315 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit02, unit03, unit05.
Validation set: unit07, unit10.
Test set: unit01.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.494141  8.069327  0.499634  7.980751
1    0.494507  8.061756  0.506226  7.874298
2    0.495483  8.045087  0.494385  8.062197
3    0.502441  7.933456  0.500488  7.964333
4    0.498535  7.995279  0.499268  7.983436
5    0.496704  8.024183  0.488159  8.159592
6    0.502686  7.928643  0.502197  7.935598
7    0.502441  7.960785  0.508667  7.922657
8    0.496460  8.085685  0.487915  8.204379
9    0.499268  8.015196  0.499512  8.004925
10   0.510132  7.831496  0.505127  7.907813
11   0.504639  7.913075  0.491455  8.121036
12   0.504395  7.913032  0.506226  7.882291
13   0.503540  7.923856  0.499878  7.981094
14   0.497803  8.013230  0.498413  8.002623
15   0.505249  7.892905  0.498535  7.999255
16   0.504883  7.897477  0.499023  7.990348
17   0.491333  8.112492  0.506226  7.874640
18   0.494751  8.057210  0.488770  8.152231
19   0.493896  8.070212  0.498901  7.990160
20   0.498291  7.999672  0.501709  7.944981
21   0.498047  8.003199  0.492432  8.092568
22   0.504395  7.901729  0.500122  7.969731
23   0.496094  8.033864  0.496460  8.027947
24   0.492065  8.097944  0.516113  7.714509
25   0.499146  7.984974  0.496582  8.025806
26   0.498779  7.990748  0.487549  8.169765
27   0.497925  8.004330  0.499390  7.980963
28   0.507446  7.852510  0.497803  8.006243
29   0.499756  7.975100  0.490479  8.122998
30   0.489258  8.142456  0.492065  8.097693
31   0.497437  8.012064  0.503296  7.918650
32   0.498291  8.086570  0.501221  8.070551
33   0.499390  8.090107  0.508179  7.942408
34   0.497070  8.113563  0.495972  8.125548
35   0.505737  7.965091  0.497070  8.098753
36   0.508911  7.906023  0.502197  8.009299
37   0.494141  8.134437  0.501343  8.016486
38   0.501587  8.009861  0.498047  8.063722
39   0.494507  8.117941  0.500000  8.028293
40   0.499146  8.040156  0.502686  7.982090
41   0.501465  8.000200  0.500977  8.006742
42   0.499023  8.036870  0.498291  8.047631
43   0.486938  8.227884  0.502197  7.983964
44   0.496582  8.072968  0.498169  8.047210
45   0.507690  7.895055  0.509644  7.863596
46   0.504517  7.945079  0.510498  7.849492
47   0.514282  7.788975  0.491211  8.156611
48   0.500122  8.014395  0.503174  7.965596
49   0.509521  7.864259  0.508667  7.877742
50   0.494263  8.107241  0.504028  7.951410
51   0.492554  8.134193  0.497314  8.058140
52   0.504883  7.937316  0.498901  8.032501
53   0.508545  7.878575  0.500732  8.002931
54   0.498901  8.031916  0.507080  7.901311
55   0.492554  8.132664  0.496094  8.075984
56   0.496826  8.064049  0.500000  8.013179
57   0.490845  8.158847  0.498413  8.037887
58   0.490479  8.164061  0.495239  8.087827
59   0.502808  7.966812  0.508057  7.882757
60   0.499023  8.026373  0.495728  8.078505
61   0.500244  8.006063  0.498779  8.028962
62   0.508179  7.878633  0.500122  8.006577
63   0.508057  7.879556  0.500854  7.993831
64   0.494995  8.086671  0.500366  8.000451
65   0.497314  8.048483  0.496704  8.057572
66   0.491821  8.134747  0.505981  7.908311
67   0.489990  8.162532  0.498291  8.029460
68   0.502441  7.962530  0.506348  7.899472
69   0.506348  7.898665  0.494385  8.088556
70   0.492188  8.122739  0.500732  7.985648
71   0.498779  8.015906  0.494263  8.087015
72   0.496826  8.045240  0.508179  7.863332
73   0.504272  7.924680  0.488403  8.176734
74   0.501099  7.973404  0.496948  8.038627
75   0.500732  7.977361  0.491699  8.120431
76   0.510132  7.825645  0.507324  7.869478
77   0.509277  7.837437  0.492920  8.097311
78   0.499512  7.991350  0.498169  8.011892
79   0.497559  8.020793  0.506104  7.883747
80   0.503174  7.929675  0.507080  7.866636
81   0.496094  8.041067  0.494263  8.069558
82   0.500854  7.963818  0.499878  7.978755
83   0.501465  7.952877  0.495361  8.049623
84   0.502075  7.942084  0.506958  7.863757
85   0.511353  7.793268  0.496948  8.022499
86   0.496948  8.022143  0.505737  7.881688
87   0.498413  7.998166  0.497803  8.007628
88   0.499878  7.974319  0.510742  7.800910
89   0.512207  7.777387  0.497437  8.012709
90   0.503174  7.921119  0.505005  7.891817
91   0.503296  7.918976  0.495972  8.035665
92   0.491577  8.105668  0.504028  7.907117
93   0.503052  7.922650  0.499146  7.984894
94   0.497925  8.004334  0.482056  8.257308
95   0.505493  7.883647  0.495483  8.043217
96   0.492676  8.087971  0.500122  7.969255
97   0.504761  7.895300  0.496826  8.021794
98   0.498657  7.992601  0.511597  7.786315
99   0.500854  7.957570  0.493164  8.080174
100  0.506470  7.868050  0.498169  8.000384
101  0.505005  7.891403  0.498779  7.990653
102  0.493042  8.082120  0.496460  8.027629
103  0.492188  8.095742  0.493652  8.072389
104  0.497681  8.008168  0.495239  8.047090
105  0.495117  8.240798  0.497314  8.219839
106  0.503052  8.106854  0.492432  8.258992
107  0.511230  7.948187  0.502808  8.073262
108  0.500244  8.107806  0.502075  8.073238
109  0.506592  7.997330  0.486694  8.311130
110  0.495239  8.172260  0.501465  8.070607
111  0.499268  8.103636  0.496826  8.140668
112  0.493774  8.187619  0.503052  8.038056
113  0.503906  8.022846  0.493652  8.184730
114  0.506714  7.974927  0.499268  8.092043
115  0.512451  7.880251  0.495117  8.154952
116  0.497681  8.112405  0.495239  8.149619
117  0.506592  7.966885  0.491699  8.202526
118  0.499146  8.082010  0.497314  8.109359
119  0.494263  8.156154  0.500854  8.049176
120  0.500610  8.051174  0.492188  8.183536
121  0.496216  8.117401  0.495972  8.119362
122  0.503906  7.990953  0.508545  7.915079
123  0.499878  8.051359  0.510864  7.874315
124  0.502319  8.008688  0.500366  8.037974
125  0.498047  8.073151  0.505371  7.954595
126  0.511963  7.847775  0.513794  7.816863
127  0.503418  7.980625  0.504883  7.955630

2018-02-25 11:39:13.096314 Finish.
Total elapsed time: 15:12:57.10.
