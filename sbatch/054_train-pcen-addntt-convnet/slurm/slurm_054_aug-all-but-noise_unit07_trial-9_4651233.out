2018-02-24 20:26:35.453550: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:35.453803: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:35.453815: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:17.661114 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit10, unit01, unit02.
Validation set: unit03, unit05.
Test set: unit07.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.916626  0.259019  0.939941  0.184709
1    0.930542  0.228450  0.939819  0.189016
2    0.928711  0.230151  0.956665  0.160525
3    0.934937  0.214181  0.949951  0.173364
4    0.939453  0.202144  0.969360  0.136160
5    0.944824  0.180482  0.966675  0.115716
6    0.896484  0.317314  0.932983  0.250515
7    0.921875  0.260187  0.954712  0.169440
8    0.923950  0.263095  0.960449  0.166898
9    0.584839  6.526472  0.509033  7.892568
10   0.497070  8.076464  0.497803  8.058780
11   0.489014  8.194144  0.493652  8.115842
12   0.493164  8.119997  0.505005  7.927844
13   0.501099  7.987217  0.491821  8.132385
14   0.496338  8.057991  0.505371  7.911714
15   0.502808  7.950579  0.497437  8.034297
16   0.491699  8.124062  0.485229  8.225577
17   0.504395  7.918582  0.495117  8.065087
18   0.496216  8.046316  0.492310  8.107384
19   0.497070  8.030400  0.499878  7.984596
20   0.507812  7.857159  0.500854  7.967183
21   0.488892  8.157086  0.495728  8.047323
22   0.490723  8.126411  0.500366  7.971996
23   0.501953  7.946093  0.492310  8.099256
24   0.493408  8.081224  0.496948  8.024294
25   0.503296  7.922657  0.498535  7.998136
26   0.506226  7.875162  0.490967  8.118071
27   0.503296  7.921205  0.499390  7.983186
28   0.500122  7.971253  0.494263  8.064422
29   0.497803  8.007775  0.490479  8.124343
30   0.500000  7.972377  0.495605  8.042277
31   0.496704  8.024626  0.499878  7.973901
32   0.512573  7.771400  0.493896  8.069054
33   0.489746  8.135138  0.507324  7.854825
34   0.496460  8.027965  0.504639  7.897519
35   0.492920  8.084298  0.501465  7.948030
36   0.503662  7.912966  0.506836  7.862338
37   0.498901  7.988810  0.504272  7.903161
38   0.494873  8.052994  0.499390  7.980974
39   0.487915  8.163896  0.498413  7.996523
40   0.500122  7.969271  0.506348  7.870014
41   0.505127  7.889471  0.492065  8.097699
42   0.500366  7.965362  0.493164  8.080179
43   0.499023  7.986765  0.501831  7.942004
44   0.495483  8.043200  0.501953  7.940056
45   0.500000  8.032587  0.500488  8.084130
46   0.505615  7.986567  0.492920  8.178105
47   0.497070  8.105698  0.499268  8.065493
48   0.494873  8.131807  0.492920  8.159621
49   0.507446  7.925390  0.495605  8.111733
50   0.503296  7.987092  0.496582  8.092220
51   0.497803  8.071108  0.506226  7.935264
52   0.511719  7.846305  0.501465  8.008453
53   0.507202  7.915797  0.496460  8.085909
54   0.507690  7.905829  0.500366  8.021589
55   0.505127  7.944771  0.506836  7.916632
56   0.494873  8.106522  0.497559  8.062903
57   0.504272  7.955121  0.498779  8.041964
58   0.498047  8.052955  0.502808  7.976386
59   0.499268  8.032188  0.492554  8.138599
60   0.496826  8.069892  0.495728  8.086820
61   0.498535  8.041497  0.499023  8.033154
62   0.501221  7.997584  0.497803  8.051537
63   0.491821  8.146372  0.496948  8.064112
64   0.493774  8.114197  0.500977  7.998863
65   0.501099  7.996409  0.499634  8.019251
66   0.504272  7.944794  0.495972  8.076618
67   0.507080  7.899016  0.502319  7.974401
68   0.504761  7.934968  0.497192  8.055108
69   0.492554  8.128542  0.498169  8.038497
70   0.498413  8.034079  0.506470  7.905104
71   0.508057  7.879268  0.495483  8.079171
72   0.496948  8.055270  0.496094  8.068336
73   0.494629  8.091128  0.502930  7.958223
74   0.498779  8.023814  0.510376  7.838350
75   0.505249  7.919494  0.510864  7.829372
76   0.498779  8.021426  0.494019  8.096705
77   0.509888  7.843088  0.503052  7.951433
78   0.500366  7.993605  0.505371  7.913162
79   0.499268  8.009808  0.486816  8.207641
80   0.498657  8.018197  0.505859  7.902694
81   0.500122  7.993474  0.511719  7.807899
82   0.502319  7.957051  0.511841  7.804550
83   0.500610  7.982885  0.496948  8.040555
84   0.512329  7.794637  0.505737  7.899009
85   0.488159  8.178535  0.497192  8.033808
86   0.501587  7.963040  0.506592  7.882538
87   0.508301  7.854592  0.507812  7.861674
88   0.496948  8.034187  0.503784  7.924517
89   0.495117  8.062018  0.503906  7.921229
90   0.506592  7.877766  0.502075  7.949125
91   0.496826  8.032184  0.496704  8.033512
92   0.499390  7.990106  0.501221  7.960329
93   0.498169  8.008425  0.503174  7.928087
94   0.501953  7.947030  0.504028  7.913438
95   0.502441  7.938260  0.511230  7.797676
96   0.493652  8.077479  0.510742  7.804605
97   0.489624  8.140889  0.502319  7.938118
98   0.500732  7.963073  0.497314  8.017230
99   0.490601  8.123963  0.493408  8.078912
100  0.504639  7.899611  0.505493  7.885739
101  0.503174  7.922493  0.505737  7.881413
102  0.502075  7.939610  0.493286  8.079552
103  0.513428  7.758292  0.492065  8.098713
104  0.505249  7.888409  0.496216  8.032301
105  0.503418  7.917381  0.507935  7.845281
106  0.497803  8.006727  0.489990  8.131202
107  0.498047  8.002698  0.495117  8.049347
108  0.510132  7.809930  0.509033  7.827401
109  0.493652  8.072573  0.500977  7.955775
110  0.502197  7.936288  0.498901  7.988809
111  0.504639  7.897324  0.492676  8.088025
112  0.498169  8.000438  0.499634  7.977073
113  0.496948  8.019878  0.500366  7.965380
114  0.497192  8.015973  0.490723  8.119111
115  0.496582  8.025694  0.509033  7.827190
116  0.494385  8.060719  0.500000  7.971197
117  0.507568  7.928138  0.489136  8.252528
118  0.507812  7.951410  0.496338  8.136272
119  0.500610  8.067317  0.497925  8.110508
120  0.494019  8.173369  0.502686  8.033569
121  0.505005  7.996074  0.495117  8.155329
122  0.494751  8.161108  0.497437  8.117694
123  0.506592  7.969993  0.509766  7.918695
124  0.504517  8.003150  0.503540  8.018735
125  0.511108  7.896583  0.504150  8.008561
126  0.499146  8.089051  0.500244  8.071156
127  0.503906  8.011933  0.505981  7.978281

2018-02-25 12:12:23.386714 Finish.
Total elapsed time: 15:46:06.39.
