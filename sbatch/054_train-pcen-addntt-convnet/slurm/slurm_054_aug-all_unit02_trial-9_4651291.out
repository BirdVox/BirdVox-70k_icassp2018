2018-02-24 20:27:54.070484: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:54.070753: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:54.070765: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:54.070771: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:54.070776: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:27:07.828792 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.975708  0.117532  0.804199  1.111691
1    0.979492  0.095759  0.817749  1.256900
2    0.975342  0.096995  0.848877  0.765852
3    0.980469  0.085526  0.808838  1.104109
4    0.975708  0.131264  0.800415  1.561805
5    0.976685  0.099508  0.856201  0.797171
6    0.979248  0.085413  0.768433  1.463102
7    0.981934  0.076374  0.842651  0.880824
8    0.981689  0.075607  0.809937  0.888723
9    0.982666  0.075265  0.797363  1.185727
10   0.979492  0.088931  0.864746  0.831155
11   0.984375  0.065195  0.834717  1.133554
12   0.984497  0.060069  0.859497  0.858351
13   0.969604  0.118031  0.823364  1.344480
14   0.972046  0.107156  0.858765  0.568267
15   0.980957  0.078273  0.868530  0.646388
16   0.979980  0.081330  0.875000  0.600690
17   0.982544  0.071371  0.869629  0.739623
18   0.985107  0.061590  0.878784  0.713079
19   0.982788  0.068073  0.856689  0.757778
20   0.986328  0.056632  0.875854  0.739657
21   0.982544  0.073256  0.878906  0.577013
22   0.984009  0.067271  0.873535  0.506627
23   0.981567  0.077800  0.861816  0.511149
24   0.980347  0.076921  0.865234  0.615573
25   0.983032  0.071462  0.860352  0.608082
26   0.983032  0.070185  0.865112  0.622508
27   0.982666  0.073631  0.826294  0.579096
28   0.982788  0.075826  0.878174  0.478444
29   0.952393  0.168119  0.797119  0.676435
30   0.673096  5.159220  0.499756  8.086393
31   0.492188  8.205456  0.486450  8.295484
32   0.489868  8.238625  0.510010  7.912408
33   0.494873  8.155123  0.504272  8.002465
34   0.497437  8.111674  0.501221  8.049771
35   0.502441  8.029310  0.500732  8.056113
36   0.500488  8.059396  0.493408  8.172894
37   0.496460  8.123155  0.498169  8.095084
38   0.500244  8.061167  0.500977  8.048913
39   0.503052  8.015062  0.492676  8.181917
40   0.496338  8.122544  0.504272  7.994321
41   0.508423  7.927125  0.504883  7.983895
42   0.504028  7.997409  0.490479  8.215556
43   0.497559  8.101214  0.498047  8.093127
44   0.505127  7.978815  0.506714  7.953049
45   0.492188  8.187017  0.500122  8.058965
46   0.498047  8.092265  0.491821  8.192469
47   0.499512  8.068386  0.500366  8.054490
48   0.505859  7.965840  0.490967  8.205773
49   0.512085  7.865293  0.503418  8.004896
50   0.505859  7.965462  0.491699  8.193618
51   0.503906  7.996792  0.499512  8.067556
52   0.502319  8.022242  0.501221  8.039891
53   0.499634  8.065418  0.502319  8.022083
54   0.503784  7.998429  0.499634  8.065285
55   0.497681  8.096730  0.504761  7.982578
56   0.495972  8.124212  0.501343  8.037612
57   0.494141  8.153673  0.497070  8.106430
58   0.506226  7.958845  0.516113  7.799456
59   0.500854  8.045383  0.501831  8.029629
60   0.508423  7.923370  0.511230  7.878106
61   0.495117  8.137812  0.493042  8.171252
62   0.499146  8.072868  0.502319  8.021705
63   0.500488  7.994391  0.495972  8.043418
64   0.502808  7.933665  0.492432  8.098484
65   0.500000  7.977408  0.496338  8.035421
66   0.499878  7.978690  0.495728  8.044588
67   0.495483  8.048253  0.505615  7.886515
68   0.495117  8.053694  0.499268  7.987351
69   0.494141  8.068931  0.501709  7.948123
70   0.499268  7.986909  0.496216  8.035429
71   0.499756  7.978872  0.493286  8.081897
72   0.500854  7.961130  0.501099  7.957131
73   0.494385  8.064066  0.497925  8.007531
74   0.496338  8.032737  0.500244  7.970371
75   0.497314  8.016990  0.498657  7.995498
76   0.497925  8.007093  0.487183  8.178268
77   0.494019  8.069209  0.504272  7.905661
78   0.484253  8.224746  0.498901  7.991141
79   0.490967  8.117566  0.492188  8.098035
80   0.505981  7.878058  0.503418  7.918858
81   0.500732  7.961606  0.498535  7.996570
82   0.508179  7.842765  0.489624  8.138507
83   0.483765  8.231858  0.500122  7.971020
84   0.493286  8.079941  0.496704  8.025390
85   0.495117  8.050632  0.506104  7.875425
86   0.504883  7.894830  0.505127  7.890881
87   0.506348  7.871366  0.498657  7.993915
88   0.502075  7.939372  0.502075  7.939320
89   0.501953  7.941216  0.508545  7.836076
90   0.499390  7.981985  0.490479  8.124002
91   0.503418  7.917670  0.492554  8.090826
92   0.495361  8.046022  0.506348  7.870831
93   0.495117  8.049830  0.489624  8.137363
94   0.503174  7.921308  0.503296  7.919324
95   0.493774  8.071083  0.500366  7.965959
96   0.504761  7.895866  0.496216  8.032059
97   0.500610  7.961969  0.498291  7.998914
98   0.497437  8.012508  0.504395  7.901554
99   0.502075  7.938503  0.487671  8.168117
100  0.497925  8.004622  0.492554  8.090227
101  0.491333  8.109667  0.496582  8.025964
102  0.502686  7.928641  0.502319  7.934460
103  0.495239  8.047317  0.497437  8.012271
104  0.510010  7.811808  0.500854  7.957751
105  0.499390  7.981090  0.495117  8.049191
106  0.507812  7.846786  0.503784  7.910995
107  0.501831  7.942123  0.500366  7.965466
108  0.485718  8.198988  0.505005  7.891497
109  0.496948  8.019931  0.505859  7.877859
110  0.496582  8.025755  0.497803  8.006288
111  0.495850  8.037420  0.499268  7.982924
112  0.486328  8.189204  0.501221  7.951776
113  0.500122  7.969287  0.503662  7.912847
114  0.508667  7.833053  0.495361  8.045173
115  0.508789  7.831100  0.500000  7.971216
116  0.492188  8.095763  0.504150  7.905044
117  0.509521  7.819414  0.507080  7.858334
118  0.509277  7.823302  0.500366  7.965366
119  0.511230  7.792162  0.499390  7.980931
120  0.508789  7.831081  0.496338  8.029582
121  0.490112  8.128831  0.501587  7.945898
122  0.506470  7.868053  0.502686  7.928381
123  0.508789  7.831076  0.495972  8.035416
124  0.496704  8.023739  0.497925  8.004278
125  0.498169  8.000385  0.499390  7.980924
126  0.505005  7.891403  0.501587  7.945894
127  0.503662  7.912810  0.506104  7.873888

2018-02-25 11:53:23.453216 Finish.
Total elapsed time: 15:26:16.45.
