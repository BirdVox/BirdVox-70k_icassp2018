2018-02-24 20:26:40.193857: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:40.194037: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:40.194050: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:17.661180 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit01, unit02, unit03.
Validation set: unit05, unit07.
Test set: unit10.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.938110  0.205583  0.930908  0.212858
1    0.947754  0.181151  0.958130  0.156178
2    0.947144  0.173235  0.957397  0.158944
3    0.947876  0.177077  0.954590  0.165116
4    0.949707  0.164360  0.944824  0.201770
5    0.947266  0.165136  0.962158  0.157220
6    0.951294  0.160222  0.965210  0.135253
7    0.948853  0.162950  0.943970  0.197018
8    0.951294  0.155349  0.957275  0.163585
9    0.956421  0.147862  0.935303  0.210563
10   0.957031  0.143632  0.963989  0.144498
11   0.958130  0.137523  0.967407  0.121685
12   0.960571  0.137536  0.960693  0.139117
13   0.953735  0.153863  0.956055  0.146602
14   0.956787  0.137956  0.963623  0.129317
15   0.958252  0.132292  0.958008  0.149636
16   0.958496  0.129324  0.820801  0.429605
17   0.828857  2.247362  0.493774  8.119702
18   0.588623  6.607094  0.634888  5.854505
19   0.532227  7.527391  0.489014  8.219147
20   0.508301  7.905965  0.500854  8.019706
21   0.493896  8.126685  0.490234  8.181426
22   0.507324  7.905872  0.501953  7.988575
23   0.495483  8.089141  0.499146  8.028298
24   0.510254  7.848992  0.507935  7.883840
25   0.510010  7.848819  0.501587  7.981225
26   0.504517  7.932798  0.498901  8.020646
27   0.500732  7.989909  0.498291  8.027324
28   0.485229  8.234157  0.512451  7.798812
29   0.493164  8.105020  0.496826  8.045391
30   0.500854  7.980004  0.512329  7.795930
31   0.502808  7.946655  0.498901  8.007882
32   0.496338  8.047765  0.500000  7.988418
33   0.504883  7.909667  0.499512  7.994406
34   0.498291  8.013032  0.504150  7.918801
35   0.502441  7.945277  0.496216  8.043774
36   0.498535  8.006090  0.490723  8.129947
37   0.503784  7.921065  0.502563  7.939890
38   0.504883  7.902319  0.498047  8.010718
39   0.500610  7.969306  0.495605  8.048565
40   0.506714  7.870977  0.510986  7.802381
41   0.501343  7.955675  0.493530  8.079789
42   0.505005  7.896453  0.495361  8.049803
43   0.499756  7.979383  0.496948  8.023794
44   0.494751  8.058504  0.491577  8.108792
45   0.499023  7.989799  0.490845  8.119915
46   0.493896  8.071016  0.498291  8.000720
47   0.494995  8.053051  0.509155  7.827099
48   0.499634  7.978711  0.493530  8.075841
49   0.507446  7.853831  0.511353  7.791408
50   0.506958  7.861337  0.498047  8.003278
51   0.494507  8.059606  0.496948  8.020582
52   0.502563  7.930973  0.502686  7.928943
53   0.488525  8.154618  0.499756  7.975510
54   0.491211  8.111679  0.495972  8.035728
55   0.508789  7.831344  0.509766  7.815734
56   0.491943  8.099828  0.499512  7.979139
57   0.503906  7.909054  0.501465  7.947952
58   0.496216  8.031615  0.501587  7.945969
59   0.505127  7.889519  0.494141  8.064655
60   0.487427  8.171680  0.503784  7.910896
61   0.498169  8.000409  0.500610  7.961482
62   0.505005  7.900899  0.506836  7.977385
63   0.495605  8.120119  0.488647  8.216355
64   0.502075  7.996530  0.500488  8.017237
65   0.494263  8.113248  0.502197  7.983871
66   0.508789  7.876452  0.503174  7.963814
67   0.494019  8.107920  0.501099  7.993300
68   0.498657  8.030680  0.501099  7.990286
69   0.495361  8.080433  0.502197  7.970186
70   0.502197  7.969040  0.499756  8.006858
71   0.505493  7.914388  0.503784  7.940663
72   0.501831  7.970915  0.496460  8.055686
73   0.501221  7.979003  0.498779  8.017163
74   0.507080  7.884128  0.497070  8.043028
75   0.502441  7.956771  0.504028  7.930861
76   0.507568  7.873856  0.490723  8.141862
77   0.499268  8.005117  0.502197  7.957903
78   0.488770  8.171495  0.501953  7.960847
79   0.495728  8.059652  0.485596  8.220736
80   0.501465  7.967323  0.503418  7.935767
81   0.502197  7.954824  0.496704  8.041996
82   0.510620  7.819751  0.502197  7.953641
83   0.495850  8.054457  0.497314  8.030721
84   0.508545  7.851307  0.500977  7.971589
85   0.497192  8.031548  0.507202  7.871597
86   0.495483  8.058057  0.493896  8.082990
87   0.501343  7.963918  0.494507  8.072538
88   0.493408  8.089697  0.499512  7.992037
89   0.501343  7.962498  0.507324  7.866792
90   0.500366  7.977381  0.500000  7.982881
91   0.504761  7.906656  0.496460  8.038664
92   0.495972  8.046134  0.492676  8.098367
93   0.500244  7.977410  0.506714  7.873971
94   0.493286  8.087759  0.499023  7.996014
95   0.500366  7.974343  0.500977  7.964353
96   0.504761  7.903779  0.493530  8.082579
97   0.508301  7.846875  0.503662  7.920605
98   0.500488  7.970995  0.514404  7.748937
99   0.499023  7.993953  0.508423  7.843916
100  0.502319  7.941044  0.509399  7.827997
101  0.503418  7.923192  0.500366  7.971683
102  0.500000  7.977368  0.501099  7.959701
103  0.503296  7.924526  0.498779  7.996387
104  0.490967  8.120796  0.492432  8.097303
105  0.503052  7.901794  0.502197  7.959000
106  0.503296  7.954804  0.502319  7.969073
107  0.497803  8.039728  0.501587  7.978251
108  0.502686  7.959897  0.502441  7.963047
109  0.503174  7.950782  0.504272  7.932725
110  0.497559  8.039293  0.500610  7.990194
111  0.491089  8.141579  0.497437  8.039980
112  0.499634  8.004565  0.494385  8.087862
113  0.498657  8.019374  0.505371  7.911963
114  0.501343  7.975813  0.497681  8.033824
115  0.509399  7.846632  0.498657  8.017519
116  0.505249  7.912067  0.499390  8.005115
117  0.509155  7.849071  0.499878  7.996617
118  0.505371  7.908696  0.501465  7.970624
119  0.501465  7.970288  0.504639  7.919356
120  0.493652  8.094184  0.501465  7.969316
121  0.504517  7.920360  0.502441  7.953145
122  0.502563  7.950915  0.508545  7.855278
123  0.502441  7.952320  0.503662  7.932603
124  0.497314  8.033560  0.494141  8.083925
125  0.499390  8.000027  0.494629  8.075714
126  0.497314  8.032707  0.506104  7.892400
127  0.501465  7.966180  0.488892  8.166461

2018-02-25 12:11:07.499211 Finish.
Total elapsed time: 15:44:50.50.
