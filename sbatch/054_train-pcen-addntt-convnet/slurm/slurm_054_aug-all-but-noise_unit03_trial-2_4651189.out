2018-02-24 20:26:48.262102: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:48.262392: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:48.262406: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:16.851306 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.494995  8.053041  0.501465  7.949342
1    0.492310  8.094938  0.510010  7.812459
2    0.503052  7.923188  0.505981  7.876318
3    0.499512  7.979349  0.500000  7.971471
4    0.497070  8.018112  0.502075  7.938268
5    0.499390  7.981045  0.508179  7.840894
6    0.493774  8.070511  0.502563  7.930374
7    0.508179  7.956327  0.506226  7.996282
8    0.497681  8.121178  0.506470  7.971153
9    0.506958  7.958963  0.506714  7.959495
10   0.499878  8.067498  0.507568  7.941744
11   0.488525  8.247475  0.504639  7.986564
12   0.492310  8.184811  0.500122  8.052154
13   0.501343  8.038302  0.490479  8.153181
14   0.501587  8.033891  0.502441  8.008678
15   0.489624  8.226475  0.512329  7.718028
16   0.507935  8.001489  0.497803  8.196151
17   0.498657  8.166658  0.496216  8.191857
18   0.500977  8.105343  0.500732  8.099570
19   0.502808  8.058431  0.506958  7.984735
20   0.496704  8.141673  0.496582  8.137417
21   0.494507  8.164985  0.496582  8.126614
22   0.499878  8.069286  0.501221  8.043259
23   0.503174  8.007889  0.507935  7.927882
24   0.500244  8.046684  0.495117  8.124712
25   0.498169  8.072603  0.507080  7.927157
26   0.493042  8.147788  0.500854  8.020130
27   0.500732  8.019151  0.500244  8.024061
28   0.495972  8.089462  0.495850  8.088741
29   0.497681  8.057028  0.496460  8.074006
30   0.494507  8.102794  0.498169  8.042097
31   0.502075  7.977630  0.497559  8.047477
32   0.502930  7.959803  0.493774  8.103746
33   0.495483  8.074593  0.506470  7.897567
34   0.499512  8.006716  0.508911  7.855118
35   0.494141  8.088943  0.491455  8.130133
36   0.500244  7.988485  0.500366  7.985037
37   0.508667  7.851292  0.504395  7.918024
38   0.501343  7.965385  0.493408  8.090620
39   0.501343  7.962952  0.513916  7.761364
40   0.501953  7.951028  0.495728  8.049258
41   0.507324  7.863445  0.493164  8.088290
42   0.505737  7.887024  0.504272  7.909592
43   0.493652  8.078197  0.494873  8.058063
44   0.498657  7.997137  0.492065  8.101658
45   0.492798  8.089485  0.502441  7.935276
46   0.498535  7.997147  0.504761  7.897518
47   0.508667  7.834921  0.499756  7.976687
48   0.507446  7.853834  0.500244  7.968424
49   0.498169  8.001319  0.497681  8.008932
50   0.500610  7.962088  0.507568  7.851036
51   0.496338  8.029979  0.495728  8.039623
52   0.501587  7.946144  0.504272  7.903272
53   0.498535  7.994695  0.497559  8.010226
54   0.496948  8.019929  0.505127  7.889518
55   0.496460  8.027674  0.493286  8.078259
56   0.506348  7.870019  0.485229  8.206685
57   0.496216  8.031532  0.496948  8.019852
58   0.499878  7.973143  0.493164  8.080176
59   0.491943  8.099636  0.505493  7.883619
60   0.495850  8.037360  0.507080  7.858319
61   0.499512  7.978977  0.499756  7.975085
62   0.489258  8.142448  0.502075  7.937208
63   0.491577  8.265140  0.497437  8.236973
64   0.497314  8.228801  0.494873  8.261072
65   0.500000  8.173584  0.506226  8.068731
66   0.488403  8.351911  0.502319  8.123601
67   0.509155  8.009580  0.504028  8.088399
68   0.507935  8.021738  0.510376  7.978699
69   0.497192  8.187613  0.508301  8.005000
70   0.501343  8.113694  0.493164  8.242080
71   0.504028  8.063645  0.504761  8.048538
72   0.494629  8.208664  0.490845  8.266503
73   0.503052  8.066722  0.503906  8.049951
74   0.501587  8.084469  0.513306  7.892752
75   0.492798  8.220602  0.502441  8.062505
76   0.505737  8.006859  0.512451  7.896160
77   0.509644  7.939069  0.493774  8.192546
78   0.510620  7.918861  0.504395  8.017085
79   0.495850  8.152829  0.504761  8.007261
80   0.495483  8.154991  0.496948  8.129623
81   0.506714  7.970594  0.497070  8.124450
82   0.501587  8.050198  0.491821  8.206193
83   0.509521  7.919612  0.492432  8.193825
84   0.494019  8.167118  0.497070  8.116843
85   0.492676  8.186694  0.502075  8.034253
86   0.497192  8.112112  0.500366  8.060151
87   0.496460  8.122396  0.490234  8.222058
88   0.505615  7.973546  0.492676  8.181534
89   0.497437  8.104300  0.493042  8.174658
90   0.499512  8.069968  0.502075  8.028263
91   0.502808  8.016125  0.495728  8.129930
92   0.498413  8.086378  0.499756  8.064487
93   0.490112  8.219713  0.500488  8.052277
94   0.492310  8.183939  0.493164  8.170017
95   0.505493  7.971171  0.496582  8.114688
96   0.494507  8.148044  0.503540  8.002363
97   0.500854  8.045581  0.503418  8.004203
98   0.511963  7.866427  0.499390  8.069042
99   0.498901  8.076880  0.495361  8.133910
100  0.507568  7.937135  0.493164  8.169286
101  0.503174  8.007934  0.497681  8.096462
102  0.485718  8.289273  0.505493  7.970525
103  0.501831  8.029546  0.486938  8.269582
104  0.494019  8.155463  0.493896  8.157429
105  0.502197  8.023635  0.489258  8.232193
106  0.496338  8.118075  0.514038  7.832781
107  0.498779  8.078723  0.496094  8.122009
108  0.494995  8.139717  0.497192  8.104301
109  0.497314  8.102334  0.492676  8.177100
110  0.500488  8.051178  0.500732  8.047243
111  0.500488  8.033405  0.499023  8.042496
112  0.489746  8.190290  0.497803  8.061745
113  0.503296  7.974054  0.493286  8.133518
114  0.503540  7.969922  0.497192  8.070990
115  0.503662  7.967707  0.492798  8.140762
116  0.489868  8.187310  0.502197  7.990590
117  0.502563  7.984572  0.504395  7.955195
118  0.496704  8.077599  0.502686  7.982031
119  0.495361  8.098574  0.507446  7.905675
120  0.500244  8.020245  0.504395  7.953816
121  0.509888  7.865963  0.498413  8.048603
122  0.495361  8.096944  0.502808  7.977908
123  0.500366  8.016483  0.492920  8.134834
124  0.502808  7.976817  0.514160  7.795431
125  0.507202  7.905935  0.490723  8.168216
126  0.493042  8.130774  0.499878  8.021308
127  0.501343  7.997444  0.506714  7.911285

2018-02-25 11:39:59.542109 Finish.
Total elapsed time: 15:13:43.54.
