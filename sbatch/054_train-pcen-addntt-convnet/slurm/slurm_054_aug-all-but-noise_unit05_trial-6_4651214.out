2018-02-24 20:26:36.915713: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:36.915996: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:36.916009: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:17.216670 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit07, unit10, unit01.
Validation set: unit02, unit03.
Test set: unit05.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.509155  7.834092  0.497681  8.015524
1    0.498535  8.000732  0.505493  7.888758
2    0.498779  7.994969  0.503540  7.918331
3    0.499634  7.980021  0.501587  7.948358
4    0.489990  8.132823  0.506470  7.869729
5    0.498901  8.116285  0.494873  8.175739
6    0.501587  8.061046  0.502686  8.038026
7    0.505859  7.983123  0.503296  8.021182
8    0.492432  8.193828  0.503784  8.008659
9    0.505127  7.985313  0.498169  8.095936
10   0.505371  7.978650  0.493652  8.166454
11   0.507568  7.941303  0.499756  8.066460
12   0.498657  8.064388  0.502686  7.989608
13   0.493652  8.126297  0.489990  8.178778
14   0.490234  8.170785  0.502197  7.976489
15   0.508057  7.880299  0.488037  8.196936
16   0.499878  8.006074  0.505493  7.914603
17   0.492798  8.115303  0.498657  8.020279
18   0.500732  7.985753  0.503052  7.947390
19   0.501465  7.971426  0.498535  8.016910
20   0.501709  7.965190  0.499634  7.997185
21   0.498901  8.007858  0.494873  8.071105
22   0.506714  7.881435  0.497559  8.026518
23   0.502319  7.949812  0.495239  8.061898
24   0.495361  8.059220  0.508911  7.842491
25   0.507690  7.861286  0.499268  7.994916
26   0.506958  7.871701  0.511230  7.802989
27   0.511597  7.796586  0.502441  7.941987
28   0.499878  7.982328  0.505737  7.888396
29   0.496948  8.028020  0.511353  7.797893
30   0.499512  7.986199  0.503784  7.917627
31   0.499512  7.985303  0.500488  7.969303
32   0.498291  8.003923  0.504150  7.910107
33   0.504395  7.905834  0.503052  7.926868
34   0.505127  7.893433  0.500122  7.972880
35   0.510010  7.814928  0.499146  7.987819
36   0.498779  7.993372  0.497437  8.014502
37   0.493896  8.070687  0.495972  8.037361
38   0.492310  8.095526  0.496582  8.027204
39   0.496582  8.027020  0.502075  7.939271
40   0.495605  8.042261  0.494751  8.055740
41   0.503540  7.915498  0.496582  8.026310
42   0.496948  8.020375  0.503906  7.909358
43   0.496094  8.033834  0.503906  7.909217
44   0.502686  7.928623  0.499390  7.981118
45   0.502197  7.936320  0.503906  7.909040
46   0.508301  7.838954  0.499634  7.977104
47   0.493408  8.076337  0.498047  8.002371
48   0.502197  7.936193  0.499634  7.977053
49   0.494385  8.060728  0.496704  8.023748
50   0.488525  8.154133  0.495361  8.045149
51   0.498047  8.092745  0.505859  8.013493
52   0.498413  8.123493  0.506470  7.987616
53   0.499512  8.097337  0.504761  8.011033
54   0.495483  8.159662  0.500610  8.076313
55   0.500610  8.075830  0.495361  8.160008
56   0.494141  8.179330  0.500977  8.068807
57   0.493408  8.190471  0.502808  8.038646
58   0.492676  8.201624  0.498535  8.106847
59   0.503906  8.019931  0.507568  7.960549
60   0.502686  8.038883  0.489136  8.256901
61   0.496826  8.132554  0.500244  8.077060
62   0.503906  8.017618  0.494629  8.166724
63   0.496704  8.132837  0.509277  7.929732
64   0.508301  7.945013  0.502930  8.031115
65   0.512207  7.881104  0.493774  8.177716
66   0.491089  8.220508  0.495850  8.143272
67   0.501587  8.050291  0.509521  7.921887
68   0.498047  8.106321  0.500000  8.074318
69   0.498291  8.101343  0.495117  8.151973
70   0.495850  8.139645  0.497559  8.111572
71   0.490967  8.217298  0.501953  8.039696
72   0.510132  7.907355  0.491699  8.203936
73   0.495483  8.142436  0.492676  8.187182
74   0.499146  8.082407  0.496216  8.129133
75   0.505859  7.973217  0.499878  8.069147
76   0.512695  7.862093  0.495850  8.133153
77   0.499390  8.075651  0.498779  8.085049
78   0.504517  7.992154  0.498047  8.096019
79   0.500732  8.052337  0.499146  8.077525
80   0.501953  8.031904  0.499634  8.068926
81   0.501831  8.033173  0.496826  8.113512
82   0.494507  8.150590  0.498291  8.089299
83   0.491943  8.191339  0.498413  8.086797
84   0.487915  8.255768  0.496582  8.115845
85   0.495972  8.125480  0.497437  8.101677
86   0.502563  8.018870  0.507812  7.934106
87   0.501465  8.036281  0.489624  8.227003
88   0.495728  8.128518  0.498657  8.081196
89   0.503662  8.000443  0.503418  8.004302
90   0.497925  8.092779  0.490967  8.204874
91   0.505005  7.978562  0.503540  8.002133
92   0.492554  8.179181  0.494629  8.145706
93   0.492798  8.175200  0.504150  7.992201
94   0.490479  8.212554  0.502441  8.019724
95   0.501953  8.027587  0.498901  8.076769
96   0.508545  7.921330  0.506226  7.958710
97   0.506348  7.956740  0.489746  8.224324
98   0.496460  8.116108  0.493774  8.159393
99   0.503052  8.009860  0.491455  8.196776
100  0.496826  8.110204  0.511230  7.878034
101  0.488159  8.251008  0.497314  8.111695
102  0.498779  8.085325  0.499878  8.065061
103  0.506714  7.953820  0.494751  8.142452
104  0.504395  7.986937  0.489990  8.214899
105  0.498047  8.084967  0.495361  8.126350
106  0.502563  8.010215  0.504883  7.971959
107  0.500122  8.046653  0.498047  8.078549
108  0.509033  7.902263  0.493286  8.152179
109  0.496582  8.098543  0.506836  7.933983
110  0.497681  8.078877  0.505127  7.959103
111  0.493652  8.140994  0.496094  8.101028
112  0.492798  8.152547  0.486328  8.254660
113  0.505615  7.946162  0.492798  8.149482
114  0.496582  8.088148  0.498047  8.063785
115  0.492310  8.154255  0.487671  8.227205
116  0.501587  8.004363  0.506836  7.919689
117  0.498169  8.056883  0.494995  8.106497
118  0.496338  8.084118  0.502197  7.989728
119  0.501709  7.996543  0.505005  7.943022
120  0.499756  8.025733  0.499268  8.032538
121  0.499268  8.031560  0.497559  8.057815
122  0.494141  8.111314  0.492798  8.131715
123  0.505005  7.936093  0.498657  8.036262
124  0.502197  7.978787  0.489624  8.178180
125  0.497803  8.046725  0.503296  7.958065
126  0.501099  7.991999  0.505493  7.920824
127  0.501709  7.980026  0.493896  8.103431

2018-02-25 11:30:09.509757 Finish.
Total elapsed time: 15:03:52.51.
