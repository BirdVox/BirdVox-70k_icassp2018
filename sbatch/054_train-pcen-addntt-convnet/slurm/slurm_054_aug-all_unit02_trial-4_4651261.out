2018-02-24 20:27:44.005608: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:44.005867: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:44.005885: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:44.005893: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:44.005901: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:50.248482 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.975220  0.111461  0.872192  0.456033
1    0.976929  0.102765  0.813599  0.805826
2    0.978271  0.093610  0.837769  0.523124
3    0.975098  0.105434  0.844238  0.415948
4    0.979858  0.092814  0.858154  0.386994
5    0.980225  0.083425  0.813354  0.596837
6    0.979370  0.081187  0.850220  0.378741
7    0.978882  0.085563  0.819824  0.445327
8    0.979736  0.081432  0.875488  0.379907
9    0.981812  0.086162  0.867920  0.380935
10   0.982788  0.073735  0.882202  0.346783
11   0.959839  0.158841  0.870728  0.362031
12   0.979004  0.091209  0.879883  0.358663
13   0.982422  0.081029  0.872192  0.324263
14   0.980591  0.087500  0.862061  0.360110
15   0.980225  0.087380  0.873413  0.343969
16   0.960449  0.178974  0.749512  0.586016
17   0.976318  0.105268  0.824463  0.425643
18   0.979980  0.090602  0.519043  2.542411
19   0.534058  7.526333  0.494019  8.210717
20   0.494873  8.191313  0.501099  8.086117
21   0.489624  8.267309  0.506714  7.988427
22   0.506958  7.981633  0.497437  8.132430
23   0.500122  8.086833  0.504272  8.017748
24   0.508301  7.950887  0.497070  8.130056
25   0.493042  8.193335  0.504028  8.014673
26   0.497437  8.119491  0.499878  8.078763
27   0.486938  8.286074  0.500244  8.070407
28   0.495605  8.144075  0.494507  8.160722
29   0.505615  7.980707  0.497314  8.113564
30   0.494751  8.154028  0.500488  8.060729
31   0.500977  8.052106  0.495850  8.134017
32   0.494141  8.160903  0.498657  8.087468
33   0.497681  8.102632  0.496826  8.115851
34   0.498901  8.081904  0.505005  7.983048
35   0.501709  8.035742  0.502930  8.015657
36   0.497314  8.105798  0.495605  8.132995
37   0.508667  7.922160  0.503052  8.012373
38   0.492554  8.181324  0.499512  8.068929
39   0.503784  7.999852  0.494751  8.145248
40   0.502563  8.019149  0.509521  7.906834
41   0.498291  8.087705  0.508667  7.920330
42   0.503662  8.000884  0.506592  7.953555
43   0.502808  8.014456  0.502197  8.024208
44   0.499512  8.067420  0.496582  8.114573
45   0.504639  7.984658  0.496094  8.122332
46   0.497070  8.106547  0.501221  8.039610
47   0.494995  8.139920  0.498901  8.076927
48   0.501709  8.031647  0.501587  8.033591
49   0.504517  7.986351  0.493042  8.171282
50   0.495361  8.133884  0.508789  7.917442
51   0.503296  8.005971  0.498779  8.078761
52   0.500610  8.049241  0.495605  8.129904
53   0.493408  8.165315  0.498901  8.076771
54   0.497070  8.106281  0.496948  8.108246
55   0.507568  7.937067  0.495972  8.123982
56   0.499634  8.064955  0.505981  7.962641
57   0.491089  8.202681  0.502686  8.015764
58   0.492432  8.181036  0.501465  8.035438
59   0.500122  8.052815  0.502686  7.947978
60   0.492798  8.113326  0.496338  8.055833
61   0.512329  7.799807  0.490723  8.143269
62   0.506226  7.895270  0.498779  8.013184
63   0.502808  7.948254  0.499268  8.004010
64   0.499023  8.007280  0.500610  7.981377
65   0.491455  8.126772  0.498169  8.019185
66   0.501831  7.960280  0.506592  7.883867
67   0.496704  8.041007  0.497192  8.032732
68   0.487305  8.189892  0.503540  7.930591
69   0.497803  8.021599  0.502686  7.943298
70   0.502686  7.942851  0.498291  8.012464
71   0.498169  8.013974  0.503052  7.935693
72   0.499634  7.989756  0.492554  8.102202
73   0.494507  8.070645  0.500366  7.976814
74   0.504761  7.906345  0.497314  8.024647
75   0.497803  8.016461  0.506470  7.877889
76   0.502563  7.939773  0.509521  7.828456
77   0.497559  8.018794  0.497314  8.022308
78   0.507568  7.858469  0.512939  7.772476
79   0.492310  8.101012  0.507446  7.859346
80   0.506104  7.880415  0.496216  8.037713
81   0.494141  8.070475  0.492798  8.091564
82   0.497681  8.013417  0.494751  8.059825
83   0.501465  7.952506  0.499878  7.977527
84   0.495728  8.043431  0.501831  7.945869
85   0.494873  8.056555  0.497681  8.011559
86   0.498901  7.991879  0.505737  7.882685
87   0.499512  7.981738  0.511719  7.786937
88   0.487793  8.168195  0.502441  7.934494
89   0.493896  8.070565  0.492920  8.085984
90   0.492188  8.097524  0.498901  7.990358
91   0.502075  7.939640  0.510620  7.803300
92   0.498779  7.991968  0.499878  7.974354
93   0.503662  7.913936  0.503906  7.909958
94   0.505127  7.890421  0.507935  7.845587
95   0.494385  8.061537  0.503540  7.915516
96   0.496216  8.032224  0.496704  8.024385
97   0.502686  7.928977  0.506836  7.862761
98   0.489502  8.139063  0.507935  7.845162
99   0.502197  7.936591  0.499634  7.977423
100  0.496216  8.031880  0.497314  8.014334
101  0.498535  7.994845  0.497803  8.006494
102  0.498047  8.002577  0.499756  7.975308
103  0.492188  8.095944  0.505005  7.891584
104  0.506714  7.864320  0.502197  7.936308
105  0.496094  8.034695  0.498901  7.988950
106  0.497559  8.049043  0.504395  7.930466
107  0.498047  8.026768  0.500610  7.981989
108  0.493652  8.090096  0.497925  8.019459
109  0.490601  8.134192  0.501709  7.955237
110  0.504761  7.905053  0.494873  8.061278
111  0.493408  8.083469  0.499390  7.987043
112  0.506592  7.871347  0.503784  7.915304
113  0.500122  7.973031  0.499512  7.982164
114  0.495850  8.040062  0.504150  7.907287
115  0.496216  8.033430  0.505005  7.892991
116  0.505371  7.886898  0.490723  8.120198
117  0.506226  7.872863  0.499390  7.981681
118  0.497681  8.008798  0.501343  7.950300
119  0.489868  8.133144  0.495605  8.041598
120  0.495361  8.045428  0.496704  8.023967
121  0.495972  8.035601  0.494019  8.066702
122  0.500000  7.971315  0.512817  7.766950
123  0.493408  8.076360  0.506836  7.862273
124  0.496948  8.019894  0.508179  7.840843
125  0.492065  8.097719  0.496338  8.029599
126  0.497314  8.014025  0.495117  8.049050
127  0.493896  8.068508  0.507324  7.854435

2018-02-25 11:23:10.342020 Finish.
Total elapsed time: 14:56:20.34.
