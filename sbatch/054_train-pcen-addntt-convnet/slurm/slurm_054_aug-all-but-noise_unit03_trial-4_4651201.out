2018-02-24 20:26:39.192116: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:39.192324: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:39.192336: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:16.916075 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.801636  0.712819  0.754883  0.615040
1    0.920532  0.265361  0.748535  0.525661
2    0.924316  0.256943  0.723633  0.578254
3    0.940674  0.204924  0.742188  0.656925
4    0.948975  0.182961  0.775635  0.714956
5    0.957764  0.162436  0.763916  0.574700
6    0.951050  0.173979  0.765381  0.544686
7    0.966064  0.132110  0.835571  0.467929
8    0.965576  0.137147  0.750000  0.680473
9    0.578613  6.672057  0.503174  7.945903
10   0.499634  7.999623  0.495483  8.063444
11   0.499756  7.993458  0.497803  8.022860
12   0.500610  7.976611  0.492310  8.107541
13   0.503296  7.931164  0.499512  7.990326
14   0.497437  8.022381  0.504272  7.912422
15   0.496338  8.038055  0.496338  8.037236
16   0.502075  7.945048  0.495728  8.040742
17   0.726562  1.252841  0.544067  1.708968
18   0.552734  6.773180  0.506958  8.032722
19   0.497314  8.180165  0.499268  8.142340
20   0.504639  8.051408  0.502319  8.084980
21   0.503784  8.058382  0.495361  8.191414
22   0.501953  8.082886  0.511230  7.931221
23   0.498169  8.139903  0.509766  7.951238
24   0.508057  7.977237  0.510376  7.938374
25   0.502319  8.066898  0.490601  8.254497
26   0.492798  8.217905  0.502319  8.063294
27   0.501343  8.077973  0.498413  8.124154
28   0.505371  8.011024  0.498535  8.120241
29   0.504028  8.030779  0.506348  7.992481
30   0.501953  8.062430  0.490112  8.252401
31   0.489502  8.261383  0.507324  7.973266
32   0.501221  8.070804  0.506226  7.989293
33   0.492920  8.202928  0.496582  8.143072
34   0.494751  8.171766  0.498169  8.115853
35   0.514160  7.857295  0.497925  8.118163
36   0.499756  8.087848  0.490967  8.228705
37   0.503906  8.019354  0.499512  8.089391
38   0.504517  8.007942  0.505859  7.985518
39   0.519653  7.762422  0.509033  7.932833
40   0.494141  8.172127  0.503784  8.015946
41   0.494385  8.166722  0.500366  8.069589
42   0.511353  7.891809  0.503906  8.011130
43   0.497437  8.114734  0.499878  8.074711
44   0.493530  8.176374  0.501343  8.049807
45   0.496704  8.123954  0.502319  8.032833
46   0.493896  8.168003  0.495117  8.147745
47   0.497559  8.107836  0.500732  8.056129
48   0.498657  8.089052  0.506836  7.956709
49   0.504517  7.993600  0.505615  7.975409
50   0.505615  7.974952  0.497925  8.098458
51   0.503784  8.003595  0.497070  8.111397
52   0.509766  7.906387  0.504272  7.994550
53   0.501831  8.033551  0.511475  7.877775
54   0.509155  7.914844  0.498169  8.091619
55   0.491089  8.205458  0.510742  7.888414
56   0.490112  8.220685  0.500366  8.055177
57   0.494507  8.149408  0.507446  7.940646
58   0.505859  7.966044  0.496216  8.121309
59   0.495605  8.130995  0.494507  8.148560
60   0.495850  8.126793  0.507446  7.939759
61   0.494751  8.144283  0.490601  8.211085
62   0.504395  7.988674  0.507935  7.931541
63   0.502808  8.014117  0.497437  8.100632
64   0.505615  7.968761  0.491943  8.189084
65   0.496948  8.108382  0.501587  8.033585
66   0.489868  8.222446  0.503052  8.009931
67   0.496826  8.110260  0.501709  8.031545
68   0.496216  8.120074  0.495483  8.131871
69   0.498901  8.076774  0.498535  8.082671
70   0.501587  8.033479  0.495239  8.135789
71   0.498657  8.080695  0.496460  8.116109
72   0.500610  8.049212  0.499268  8.070854
73   0.503174  8.007893  0.505127  7.976412
74   0.502197  8.023632  0.498535  8.082658
75   0.495972  8.123977  0.505981  7.962638
76   0.504761  7.982314  0.493530  8.163327
77   0.504761  7.970654  0.497559  8.201833
78   0.493286  8.244853  0.501343  8.107858
79   0.500122  8.125514  0.494873  8.208661
80   0.497437  8.166582  0.492676  8.242750
81   0.490601  8.275884  0.491821  8.255969
82   0.506714  8.015782  0.496582  8.178967
83   0.499268  8.135593  0.487671  8.322431
84   0.498901  8.141348  0.505005  8.042901
85   0.497559  8.162851  0.502930  8.076207
86   0.496826  8.174510  0.502686  8.079992
87   0.498169  8.152709  0.497803  8.158526
88   0.500244  8.119083  0.501343  8.101278
89   0.500366  8.116913  0.493774  8.223050
90   0.497070  8.169808  0.506470  8.018184
91   0.500244  8.118394  0.507690  7.998234
92   0.492310  8.245993  0.498901  8.139587
93   0.498291  8.149257  0.499268  8.133342
94   0.500977  8.105611  0.501709  8.093611
95   0.500610  8.111112  0.499023  8.136474
96   0.491211  8.262166  0.497925  8.153711
97   0.489258  8.293150  0.505005  8.039071
98   0.496948  8.168647  0.505127  8.036527
99   0.494141  8.213294  0.494019  8.214938
100  0.508667  7.978491  0.521606  7.769576
101  0.503540  8.060396  0.487427  8.319722
102  0.500366  8.110754  0.501587  8.090655
103  0.489624  8.283032  0.504517  8.042535
104  0.488525  8.299806  0.495483  8.187165
105  0.500244  8.109919  0.497925  8.146776
106  0.495605  8.183615  0.499268  8.124030
107  0.493652  8.213962  0.502808  8.065807
108  0.485718  8.340659  0.499878  8.111807
109  0.502808  8.063957  0.503418  8.053478
110  0.501709  8.080373  0.506958  7.995107
111  0.500488  8.098720  0.501709  8.078368
112  0.499146  8.119008  0.519409  7.791709
113  0.492065  8.231753  0.495972  8.168100
114  0.502930  8.055263  0.508179  7.969967
115  0.498535  8.124719  0.496948  8.149610
116  0.499023  8.115485  0.489990  8.260404
117  0.500122  8.096431  0.491333  8.237425
118  0.504761  8.020342  0.510864  7.921309
119  0.506348  7.993467  0.486572  8.311566
120  0.495972  8.159437  0.506104  7.995502
121  0.504272  8.024398  0.498413  8.118222
122  0.503174  8.040882  0.507202  7.975345
123  0.499634  8.096734  0.500000  8.090231
124  0.497192  8.134891  0.505371  8.002469
125  0.507568  7.966462  0.493652  8.190167
126  0.505127  8.004627  0.499756  8.090602
127  0.493530  8.190353  0.489502  8.254682

2018-02-25 11:51:14.489557 Finish.
Total elapsed time: 15:24:58.49.
