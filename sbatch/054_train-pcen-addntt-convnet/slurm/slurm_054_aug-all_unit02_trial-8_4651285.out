2018-02-24 20:28:03.666668: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:28:03.667054: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:28:03.667076: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:28:03.667086: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:28:03.667097: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:52.367661 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.495728  8.049956  0.509766  7.824610
1    0.497314  8.021838  0.487549  8.176356
2    0.503662  7.918509  0.495972  8.040229
3    0.491699  8.107618  0.497314  8.017436
4    0.500000  7.974083  0.501465  7.950240
5    0.488281  8.160023  0.504272  7.904727
6    0.493530  8.075699  0.499023  7.987869
7    0.498169  8.001291  0.508789  7.831802
8    0.498779  7.991242  0.499023  7.987227
9    0.491943  8.100007  0.498657  7.992889
10   0.500732  7.959744  0.504395  7.901308
11   0.507812  7.846778  0.506470  7.868151
12   0.503540  7.914833  0.505615  7.881729
13   0.503174  8.054070  0.502441  8.080240
14   0.506470  7.995636  0.502441  8.046237
15   0.502197  8.042223  0.503540  8.014423
16   0.495239  8.144546  0.501953  8.033439
17   0.505737  7.970689  0.501221  8.042101
18   0.493774  8.161282  0.507812  7.934355
19   0.505005  7.979216  0.500977  8.043840
20   0.500977  8.043663  0.500977  8.043527
21   0.499268  8.070996  0.496094  8.122094
22   0.502319  8.021719  0.500122  8.057111
23   0.506714  7.950852  0.508789  7.917395
24   0.506348  7.968268  0.506470  8.008188
25   0.502686  8.040111  0.510864  7.888522
26   0.490601  8.197676  0.505493  7.948252
27   0.496704  8.079253  0.500488  8.010759
28   0.503052  7.963391  0.499146  8.019761
29   0.500366  7.995506  0.500610  7.987231
30   0.493164  8.102358  0.501343  7.968687
31   0.502441  7.948485  0.498901  8.002462
32   0.491455  8.119166  0.491089  8.123170
33   0.497803  8.014645  0.497681  8.015236
34   0.503906  7.914890  0.502197  7.941142
35   0.504395  7.905317  0.498779  7.994117
36   0.494385  8.063604  0.494141  8.066980
37   0.489990  8.132740  0.512939  7.766509
38   0.490234  8.128197  0.499634  7.978092
39   0.499878  7.974002  0.501343  7.950473
40   0.501099  7.954230  0.493774  8.070876
41   0.499756  7.975428  0.495483  8.043463
42   0.508301  7.839064  0.492188  8.095898
43   0.509399  7.821462  0.499512  7.979064
44   0.494873  8.052993  0.505981  7.875881
45   0.498657  7.992633  0.500732  7.959539
46   0.504517  7.899203  0.497559  8.010125
47   0.499634  7.977038  0.500366  7.965359
48   0.496826  8.021794  0.498901  7.988709
49   0.503052  7.922541  0.510132  7.809667
50   0.508667  7.833020  0.491943  8.099085
51   0.491333  8.376470  0.501831  8.243624
52   0.504028  8.202871  0.503906  8.199666
53   0.509033  8.112262  0.500854  8.239442
54   0.493896  8.347246  0.490356  8.400043
55   0.504028  8.175644  0.489624  8.403836
56   0.495117  8.311510  0.503052  8.179881
57   0.507202  8.109388  0.492920  8.336033
58   0.502197  8.183076  0.491333  8.354791
59   0.498169  8.241334  0.500244  8.204628
60   0.505371  8.118835  0.509766  8.044863
61   0.494141  8.293653  0.498657  8.217800
62   0.499146  8.206949  0.488770  8.371202
63   0.502075  8.153803  0.502197  8.148886
64   0.498291  8.208932  0.495728  8.247315
65   0.490356  8.330971  0.505005  8.091924
66   0.500854  8.155892  0.490601  8.318206
67   0.501221  8.144079  0.497192  8.206026
68   0.503662  8.098772  0.500610  8.144957
69   0.497925  8.185252  0.485840  8.377021
70   0.494385  8.236296  0.504272  8.073906
71   0.493896  8.238157  0.496948  8.185962
72   0.493896  8.232183  0.498047  8.162309
73   0.496094  8.190860  0.504761  8.048231
74   0.505005  8.041421  0.494629  8.205787
75   0.504028  8.051482  0.507080  7.999493
76   0.504150  8.043991  0.500366  8.102273
77   0.503662  8.046525  0.493042  8.215092
78   0.491821  8.232253  0.505859  8.003491
79   0.508423  7.959782  0.499268  8.104982
80   0.493530  8.195201  0.502563  8.047378
81   0.498413  8.112168  0.504395  8.013687
82   0.496216  8.143564  0.505371  7.994089
83   0.507446  7.958859  0.492065  8.205030
84   0.504761  7.998797  0.504272  8.005105
85   0.491699  8.206329  0.503418  8.016062
86   0.505249  7.985293  0.506104  7.970313
87   0.496704  8.120732  0.500488  8.058705
88   0.500366  8.059758  0.501221  8.045117
89   0.499756  8.067972  0.491333  8.203020
90   0.504150  7.995818  0.493530  8.166424
91   0.494141  8.156106  0.494995  8.141890
92   0.499756  8.064790  0.502930  8.013300
93   0.508545  7.922524  0.492920  8.174126
94   0.504883  7.981116  0.498535  8.083259
95   0.504639  7.984752  0.494141  8.153847
96   0.487305  8.263946  0.501343  8.037607
97   0.496216  8.120192  0.487671  8.257876
98   0.500488  8.051255  0.497314  8.102387
99   0.497192  8.104339  0.498779  8.078748
100  0.492676  8.177117  0.500000  8.059058
101  0.498657  8.080697  0.499756  8.062987
102  0.507446  7.939030  0.504272  7.990185
103  0.494873  8.141685  0.500000  8.059048
104  0.499512  8.066918  0.490723  8.208581
105  0.493530  8.163327  0.499878  8.061015
106  0.486450  8.277445  0.494751  8.143652
107  0.500610  8.232497  0.501221  8.279212
108  0.493408  8.404194  0.505005  8.216305
109  0.491577  8.431686  0.501831  8.265311
110  0.500854  8.279880  0.500122  8.290463
111  0.500244  8.287186  0.497681  8.327139
112  0.504883  8.209604  0.490234  8.444198
113  0.504272  8.216325  0.494263  8.375988
114  0.501099  8.264034  0.506958  8.167744
115  0.500244  8.274007  0.504028  8.210981
116  0.505615  8.183260  0.499512  8.279409
117  0.498901  8.286902  0.503052  8.217572
118  0.503174  8.213050  0.502686  8.218275
119  0.509277  8.109252  0.496826  8.307073
120  0.504639  8.178154  0.504150  8.182927
121  0.504639  8.171835  0.500000  8.243278
122  0.509277  8.090296  0.494385  8.326785
123  0.502563  8.191289  0.497070  8.276054
124  0.501221  8.205272  0.493042  8.333111
125  0.493408  8.323119  0.491577  8.348447
126  0.505493  8.119873  0.497070  8.251267
127  0.508911  8.055978  0.497925  8.228535

2018-02-25 11:47:31.772777 Finish.
Total elapsed time: 15:20:39.77.
