2018-02-24 20:27:54.483988: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:54.484400: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:54.484424: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:54.484435: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:54.484445: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:51.519853 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.969971  0.130536  0.774170  0.556706
1    0.759644  3.713498  0.500488  8.043569
2    0.512329  7.846962  0.492554  8.156585
3    0.580933  6.746737  0.506836  7.927625
4    0.673950  5.259682  0.505005  7.949181
5    0.636108  5.896744  0.579468  6.842922
6    0.590210  6.667050  0.595337  6.584572
7    0.592651  6.631891  0.588501  6.698119
8    0.585205  6.748476  0.576294  6.889519
9    0.586060  6.730103  0.578857  6.843911
10   0.581299  6.802912  0.572754  6.938667
11   0.575317  6.895793  0.588379  6.682077
12   0.578735  6.837569  0.575439  6.889213
13   0.578125  6.844602  0.579834  6.815629
14   0.573608  6.914729  0.582397  6.771651
15   0.584229  6.741091  0.573730  6.909034
16   0.580811  6.793904  0.582886  6.759179
17   0.580811  6.791725  0.576172  6.865346
18   0.566650  7.017974  0.572266  6.926290
19   0.574463  6.890083  0.579956  6.800632
20   0.651978  5.624771  0.520752  7.695922
21   0.682129  5.119776  0.525024  7.620501
22   0.690063  4.987824  0.521851  7.667127
23   0.690063  4.984519  0.519531  7.700935
24   0.531616  7.627303  0.528564  7.670490
25   0.522827  7.759388  0.534790  7.562306
26   0.520752  7.788146  0.529297  7.647394
27   0.532471  7.596231  0.534546  7.561434
28   0.521118  7.776842  0.518799  7.813034
29   0.525513  7.703970  0.539185  7.482586
30   0.527344  7.672598  0.536865  7.518172
31   0.526978  7.676747  0.537231  7.510595
32   0.526123  7.688828  0.545654  7.373186
33   0.515625  7.856381  0.540771  7.450224
34   0.537109  7.508459  0.534790  7.544983
35   0.532959  7.573731  0.528931  7.637782
36   0.522705  7.737384  0.543457  7.402003
37   0.539551  7.464243  0.543823  7.394458
38   0.529053  7.631829  0.528198  7.643122
39   0.525391  7.689230  0.526367  7.672629
40   0.523926  7.711212  0.540405  7.444647
41   0.520386  7.766639  0.539185  7.461744
42   0.529541  7.617419  0.544312  7.378484
43   0.528564  7.631549  0.547363  7.326692
44   0.525513  7.679107  0.541260  7.424415
45   0.531372  7.583039  0.530151  7.601857
46   0.526489  7.660132  0.549683  7.284483
47   0.545288  7.355510  0.554321  7.209021
48   0.539795  7.441815  0.568726  6.977053
49   0.554443  7.206934  0.569458  6.963671
50   0.553345  7.222992  0.562744  7.070321
51   0.558228  7.142480  0.557617  7.150306
52   0.548584  7.296306  0.562256  7.074539
53   0.566040  7.013342  0.571045  6.931462
54   0.540894  7.417005  0.562378  7.069592
55   0.550049  7.266516  0.628296  6.012714
56   0.612671  6.266959  0.650146  5.659931
57   0.626465  6.042976  0.650269  5.655405
58   0.619873  6.147615  0.655762  5.564338
59   0.618530  6.150937  0.508789  7.859535
60   0.649048  5.622169  0.538086  7.385918
61   0.684448  5.056965  0.552002  7.162162
62   0.564819  6.971749  0.494629  8.097441
63   0.502930  7.964563  0.491333  8.148930
64   0.499878  8.012245  0.496582  8.064345
65   0.498047  8.040576  0.506836  7.900048
66   0.504028  7.944416  0.507690  7.885644
67   0.497437  8.048736  0.497314  8.050303
68   0.510742  7.835860  0.495483  8.078747
69   0.498291  8.033617  0.491211  8.146117
70   0.501465  7.982274  0.505249  7.921571
71   0.491455  8.141106  0.501709  7.977256
72   0.504028  7.939903  0.504272  7.935629
73   0.499390  8.013090  0.509521  7.851176
74   0.498901  8.020098  0.500366  7.996351
75   0.505493  7.914221  0.503540  7.944959
76   0.494995  8.080784  0.510132  7.839063
77   0.507935  7.873685  0.503540  7.943333
78   0.495605  8.069415  0.493164  8.107919
79   0.500854  7.984897  0.496826  8.048695
80   0.510254  7.834201  0.498779  8.016706
81   0.500488  7.989033  0.500854  7.982762
82   0.496216  8.056282  0.504028  7.931296
83   0.505615  7.905563  0.496094  8.056919
84   0.501587  7.968908  0.502930  7.947060
85   0.498413  8.018627  0.496460  8.049323
86   0.507568  7.871790  0.493042  8.102932
87   0.501587  7.966267  0.491699  8.123459
88   0.500000  7.990687  0.503662  7.931864
89   0.507812  7.865261  0.493042  8.100299
90   0.504028  7.924718  0.495728  8.056617
91   0.503296  7.935530  0.490845  8.133599
92   0.494141  8.080629  0.504150  7.920623
93   0.492310  8.108973  0.502930  7.939241
94   0.504272  7.917419  0.510864  7.811914
95   0.491211  8.124826  0.499390  7.994028
96   0.502563  7.943029  0.496826  8.034093
97   0.493286  8.090136  0.506592  7.877618
98   0.494873  8.064058  0.493042  8.092865
99   0.500610  7.971831  0.509521  7.829391
100  0.500854  7.967197  0.488770  8.159495
101  0.510132  7.818574  0.510254  7.816273
102  0.494019  8.074758  0.499512  7.986841
103  0.499023  7.994293  0.492798  8.093212
104  0.503906  7.915797  0.501709  7.950508
105  0.504883  7.899602  0.498535  8.000493
106  0.497192  8.021606  0.505737  7.885088
107  0.502075  7.943191  0.497559  8.014920
108  0.501831  7.946542  0.496826  8.026071
109  0.495361  8.049174  0.501953  7.943840
110  0.504761  7.898847  0.508301  7.842181
111  0.506348  7.873102  0.498047  8.005224
112  0.494995  8.053677  0.502441  7.934769
113  0.489502  8.140872  0.490723  8.121233
114  0.500610  7.963433  0.496216  8.033331
115  0.502197  7.937823  0.504517  7.900701
116  0.507202  7.857754  0.502930  7.925738
117  0.498657  7.993733  0.488159  8.160984
118  0.497070  8.018816  0.505859  7.878598
119  0.503540  7.915486  0.493896  8.069142
120  0.497192  8.016522  0.501221  7.952230
121  0.507812  7.847078  0.493774  8.070820
122  0.505981  7.876160  0.495605  8.041529
123  0.500610  7.961699  0.501953  7.940254
124  0.495361  8.045311  0.500122  7.969385
125  0.499512  7.979091  0.494019  8.066644
126  0.509644  7.817527  0.511230  7.792212
127  0.496338  8.029623  0.502075  7.938146

2018-02-25 11:29:20.990045 Finish.
Total elapsed time: 15:02:29.99.
