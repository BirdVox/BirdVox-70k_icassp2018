2018-02-24 20:27:41.332791: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:41.333112: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:41.333135: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:41.333145: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:41.333155: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:50.248621 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit10, unit01, unit02.
Validation set: unit03, unit05.
Test set: unit07.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.950439  0.175038  0.969360  0.128161
1    0.949219  0.182529  0.971313  0.119113
2    0.953003  0.166258  0.974365  0.108488
3    0.953369  0.162737  0.970215  0.142533
4    0.951538  0.163598  0.974854  0.119570
5    0.954956  0.156075  0.966431  0.128770
6    0.955811  0.154779  0.973145  0.111612
7    0.954346  0.156084  0.971802  0.134355
8    0.806763  0.496233  0.885498  0.388905
9    0.858765  0.371890  0.950439  0.236099
10   0.936646  0.212626  0.948364  0.209180
11   0.917725  0.255858  0.494995  0.711017
12   0.799194  0.516925  0.919189  0.365662
13   0.916382  0.300891  0.952881  0.239919
14   0.930176  0.251726  0.966309  0.187639
15   0.863281  1.530778  0.499756  8.016701
16   0.525757  7.538142  0.882446  0.317770
17   0.678589  4.786168  0.500366  8.099945
18   0.494263  8.192698  0.500366  8.089393
19   0.505859  7.996995  0.498413  8.113512
20   0.504272  8.016208  0.502686  8.039152
21   0.495605  8.151073  0.505127  7.995567
22   0.496704  8.129604  0.494751  8.159477
23   0.499023  8.089238  0.502930  8.024988
24   0.502319  8.033713  0.503296  8.016925
25   0.494507  8.157676  0.494995  8.148943
26   0.510498  7.898308  0.495972  8.131727
27   0.489990  8.227502  0.505615  7.975052
28   0.492188  8.190944  0.501099  8.046800
29   0.499268  8.075855  0.496826  8.114767
30   0.502808  8.017965  0.490845  8.210408
31   0.490845  8.210070  0.502075  8.028733
32   0.504028  7.996964  0.501709  8.034070
33   0.510376  7.894127  0.492188  8.187054
34   0.502441  8.021571  0.498291  8.088267
35   0.501587  8.034967  0.506348  7.958065
36   0.499634  8.066132  0.500610  8.050253
37   0.497192  8.105223  0.493530  8.164134
38   0.504272  7.990893  0.497925  8.093113
39   0.500977  8.043845  0.488281  8.248396
40   0.502319  8.022067  0.488770  8.240407
41   0.501953  8.027864  0.488281  8.248184
42   0.514160  7.831029  0.498047  8.090711
43   0.502441  8.019851  0.495117  8.137879
44   0.504150  7.992260  0.491577  8.194899
45   0.500000  8.059123  0.494629  8.145682
46   0.505493  7.970560  0.496460  8.116149
47   0.496948  8.108271  0.493286  8.167291
48   0.502319  8.021688  0.495117  8.137768
49   0.500366  8.053160  0.507080  7.944942
50   0.501099  8.041349  0.499878  8.061023
51   0.489868  8.220527  0.500244  8.021113
52   0.502930  8.017599  0.500977  8.025271
53   0.499634  8.039589  0.497681  8.065717
54   0.498901  8.043316  0.498169  8.052535
55   0.499023  8.037095  0.494751  8.103574
56   0.497925  8.051631  0.497681  8.054269
57   0.495972  8.080426  0.499878  8.017117
58   0.497681  8.051221  0.501343  7.991948
59   0.486328  8.230506  0.501587  7.986456
60   0.505127  7.929287  0.509277  7.862403
61   0.501831  7.980439  0.501709  7.981718
62   0.502319  7.971350  0.505371  7.922063
63   0.503784  7.946746  0.499390  8.016189
64   0.510010  7.846272  0.505371  7.919614
65   0.501099  7.987123  0.501953  7.972889
66   0.496094  8.065691  0.507446  7.884087
67   0.488892  8.179273  0.495239  8.077447
68   0.506226  7.901666  0.501587  7.974976
69   0.500488  7.991845  0.491333  8.137147
70   0.494751  8.081997  0.505493  7.910071
71   0.502686  7.954159  0.498291  8.023536
72   0.502563  7.954738  0.499512  8.002696
73   0.502197  7.959186  0.494873  8.075247
74   0.498169  8.021997  0.494507  8.079667
75   0.494385  8.080900  0.496948  8.039312
76   0.498291  8.017188  0.502808  7.944458
77   0.501343  7.967091  0.510864  7.814571
78   0.498413  8.012354  0.494019  8.081690
79   0.496582  8.040110  0.489624  8.150321
80   0.497803  8.019231  0.500000  7.983498
81   0.497314  8.025626  0.496338  8.040510
82   0.500366  7.975625  0.493896  8.078108
83   0.502808  7.935409  0.501709  7.952295
84   0.502319  7.941965  0.500000  7.978350
85   0.498291  8.005037  0.502075  7.944160
86   0.493286  8.083767  0.502319  7.939257
87   0.494995  8.055561  0.497314  8.018139
88   0.502686  7.932102  0.500488  7.966738
89   0.502197  7.939139  0.509277  7.825926
90   0.492065  8.100024  0.487061  8.179527
91   0.490479  8.124786  0.495483  8.044759
92   0.490112  8.130183  0.505005  7.892569
93   0.496582  8.026687  0.500366  7.966207
94   0.506714  7.864884  0.490356  8.125544
95   0.502808  7.926947  0.496826  8.022219
96   0.494873  8.053285  0.495239  8.047383
97   0.503784  7.911105  0.495972  8.035610
98   0.498901  7.988867  0.502319  7.934344
99   0.483276  8.237910  0.503052  7.922621
100  0.502808  7.926497  0.499512  7.979027
101  0.500977  7.955663  0.502441  7.932301
102  0.498779  7.990677  0.500854  7.957587
103  0.492188  8.095755  0.500610  7.961471
104  0.506226  7.871949  0.499146  7.984820
105  0.494507  8.058770  0.493042  8.082122
106  0.502441  7.932272  0.503540  7.914757
107  0.498169  8.000385  0.495850  8.037360
108  0.502686  7.928379  0.504150  7.905025
109  0.506958  7.860265  0.507812  7.846642
110  0.507202  7.856373  0.499390  7.980923
111  0.496704  8.023737  0.498169  8.000384
112  0.502686  7.928378  0.516846  7.702632
113  0.503296  8.047857  0.497803  8.275433
114  0.491211  8.365009  0.502930  8.164905
115  0.506714  8.096804  0.504272  8.129838
116  0.494263  8.285996  0.492798  8.304778
117  0.503784  8.123545  0.507690  8.056664
118  0.494141  8.271636  0.499146  8.187720
119  0.507080  8.056969  0.502075  8.134915
120  0.509644  8.010507  0.502075  8.130181
121  0.496582  8.216642  0.506470  8.055274
122  0.506958  8.045588  0.496338  8.215006
123  0.494629  8.240931  0.499146  8.166551
124  0.506592  8.045053  0.499878  8.151815
125  0.510986  7.971389  0.495361  8.221870
126  0.503906  8.082831  0.497925  8.177935
127  0.492798  8.259303  0.499512  8.149821

2018-02-25 11:48:53.646805 Finish.
Total elapsed time: 15:22:03.65.
