2018-02-24 20:26:39.842065: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:39.842204: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:39.842216: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:17.784289 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit10, unit01, unit02.
Validation set: unit03, unit05.
Test set: unit07.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.497681  8.016453  0.498291  8.005372
1    0.501099  7.959573  0.504761  7.900263
2    0.488281  8.162258  0.496948  8.023430
3    0.513184  7.764078  0.497437  8.014651
4    0.493042  8.084331  0.502197  7.938029
5    0.494629  8.058408  0.493530  8.075670
6    0.500977  7.956755  0.495605  8.042197
7    0.503906  7.909714  0.509277  7.823951
8    0.496948  8.022496  0.497803  8.089783
9    0.503174  7.970220  0.504395  7.933946
10   0.506836  7.888108  0.503540  7.935421
11   0.490723  8.136544  0.512207  7.791378
12   0.498291  8.011381  0.514038  7.758739
13   0.508789  7.841230  0.502808  7.935536
14   0.504761  7.903582  0.497437  8.019613
15   0.499146  7.991780  0.501221  7.958160
16   0.501831  7.947986  0.507446  7.858054
17   0.502563  7.935545  0.498291  8.003323
18   0.500488  7.967996  0.504883  7.897649
19   0.498535  7.998581  0.504272  7.906856
20   0.498535  7.998078  0.499634  7.980323
21   0.511963  7.783540  0.501465  7.950678
22   0.504639  7.899865  0.503784  7.913276
23   0.499146  7.987028  0.500732  7.961532
24   0.501099  7.955510  0.494263  8.064311
25   0.507324  7.855914  0.502197  7.937489
26   0.505615  7.882851  0.496582  8.026721
27   0.501343  7.950698  0.492676  8.088750
28   0.505737  7.880413  0.495850  8.037946
29   0.494385  8.061214  0.498291  7.998859
30   0.495605  8.041606  0.504028  7.907263
31   0.504028  7.907212  0.492676  8.088152
32   0.493652  8.072547  0.496948  8.019969
33   0.503296  7.918747  0.499390  7.980999
34   0.500000  7.971252  0.495361  8.045188
35   0.504272  7.903113  0.502319  7.934241
36   0.505371  7.885582  0.498047  8.002342
37   0.489868  8.132727  0.497192  8.015958
38   0.507690  7.848593  0.503784  7.910866
39   0.497437  8.012062  0.497681  8.008169
40   0.491089  8.113258  0.496582  8.025683
41   0.495361  8.045144  0.487915  8.163855
42   0.510864  7.797990  0.495239  8.047090
43   0.499512  8.095569  0.495361  8.192104
44   0.505981  8.017215  0.497314  8.153534
45   0.496826  8.158643  0.492920  8.219061
46   0.510742  7.929670  0.508667  7.961134
47   0.496216  8.160120  0.501221  8.077844
48   0.499756  8.100040  0.502808  8.049501
49   0.501709  8.065988  0.500244  8.088417
50   0.497314  8.134545  0.498169  8.119700
51   0.513794  7.866841  0.507446  7.968149
52   0.507690  7.963246  0.499268  8.098040
53   0.501831  8.055777  0.503052  8.035153
54   0.500244  8.079469  0.500610  8.072623
55   0.509766  7.924119  0.512573  7.877919
56   0.495483  8.152433  0.496216  8.139677
57   0.498535  8.101348  0.491821  8.208609
58   0.495728  8.144705  0.502441  8.035541
59   0.505127  7.991322  0.495972  8.137953
60   0.509033  7.926513  0.501465  8.047590
61   0.501953  8.038840  0.490845  8.217014
62   0.492432  8.190602  0.499268  8.079598
63   0.505493  7.978478  0.494995  8.146927
64   0.506836  7.955369  0.500610  8.055027
65   0.508301  7.930444  0.490967  8.209229
66   0.504517  7.990286  0.502563  8.021246
67   0.506958  7.949954  0.503174  8.010513
68   0.492554  8.181312  0.501831  8.031427
69   0.500732  8.048837  0.500488  8.052497
70   0.486328  8.280504  0.497925  8.093380
71   0.494263  8.152240  0.500488  8.051746
72   0.509277  7.909966  0.501099  8.041688
73   0.497803  8.094734  0.497192  8.104504
74   0.506348  7.956889  0.506348  7.956847
75   0.495483  8.131929  0.504272  7.990241
76   0.503906  7.996128  0.511841  7.868224
77   0.496216  8.120061  0.506592  7.952813
78   0.501221  8.039381  0.491211  8.200716
79   0.503906  7.996090  0.500000  8.059050
80   0.492798  8.175134  0.502808  8.013795
81   0.505615  7.968541  0.501831  8.029535
82   0.503418  8.003957  0.501709  8.031502
83   0.498413  8.089527  0.498413  8.169577
84   0.509033  7.996868  0.500488  8.134253
85   0.506348  8.039635  0.492798  8.257844
86   0.503174  8.090391  0.495239  8.218061
87   0.506348  8.038772  0.498657  8.162474
88   0.496460  8.197633  0.503906  8.077336
89   0.505859  8.045563  0.503296  8.086576
90   0.493042  8.251529  0.510498  7.969837
91   0.498047  8.170158  0.487061  8.346866
92   0.505371  8.051337  0.503906  8.074533
93   0.502563  8.095743  0.497192  8.181864
94   0.495850  8.203025  0.506836  8.025452
95   0.497803  8.170528  0.500732  8.122763
96   0.491089  8.277634  0.500854  8.119641
97   0.494385  8.223307  0.502808  8.086912
98   0.491943  8.261357  0.489624  8.298054
99   0.493896  8.228476  0.501465  8.105752
100  0.507568  8.006609  0.502441  8.088456
101  0.497192  8.172244  0.506958  8.013999
102  0.502563  8.083960  0.495728  8.193247
103  0.497681  8.160842  0.498779  8.142185
104  0.506226  8.021185  0.501587  8.094945
105  0.500488  8.111617  0.493896  8.216801
106  0.498535  8.140939  0.500366  8.110302
107  0.496582  8.170140  0.497681  8.151246
108  0.489990  8.273982  0.495483  8.184193
109  0.510986  7.933031  0.500854  8.095021
110  0.502930  8.060224  0.500732  8.094259
111  0.500244  8.100716  0.492920  8.217323
112  0.505127  8.019095  0.505249  8.015623
113  0.501221  8.079024  0.500854  8.083371
114  0.519287  7.784701  0.495361  8.168743
115  0.503784  8.031380  0.493408  8.197000
116  0.492432  8.211125  0.501953  8.056028
117  0.500977  8.070159  0.492432  8.206272
118  0.500488  8.074834  0.496094  8.144088
119  0.500000  8.079597  0.500000  8.078078
120  0.491211  8.218285  0.497192  8.120439
121  0.504272  8.004961  0.499512  8.080360
122  0.499634  8.077146  0.493896  8.168405
123  0.500488  8.061039  0.502563  8.026509
124  0.495239  8.143580  0.499023  8.081645
125  0.500000  8.065065  0.499756  8.068201
126  0.498657  8.085209  0.501465  8.039297
127  0.488647  8.245323  0.498047  8.093295

2018-02-25 12:13:07.677470 Finish.
Total elapsed time: 15:46:50.68.
