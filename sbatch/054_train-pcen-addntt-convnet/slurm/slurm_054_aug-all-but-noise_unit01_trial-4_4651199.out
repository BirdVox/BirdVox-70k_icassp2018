2018-02-24 20:26:41.673125: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:41.673318: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:41.673331: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:16.795227 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit02, unit03, unit05.
Validation set: unit07, unit10.
Test set: unit01.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.502563  8.024739  0.494751  8.149298
1    0.494995  8.144337  0.500244  8.058833
2    0.499756  8.066026  0.501831  8.031984
3    0.507202  7.944966  0.502686  8.017374
4    0.500732  8.048561  0.493652  8.162276
5    0.502319  8.022531  0.497803  8.093958
6    0.499390  8.069454  0.495972  8.123329
7    0.491455  8.197147  0.498535  8.080802
8    0.506714  7.972513  0.503296  8.055656
9    0.500610  8.087071  0.496338  8.147375
10   0.512451  7.882478  0.496338  8.137870
11   0.504761  7.999017  0.501709  8.045506
12   0.500244  8.067060  0.493896  8.167539
13   0.505493  7.979182  0.493408  8.172670
14   0.500366  8.059481  0.501221  8.044765
15   0.490723  8.213212  0.502441  8.023634
16   0.495972  8.127349  0.495728  8.130770
17   0.508179  7.929661  0.501221  8.041428
18   0.490967  8.206390  0.505615  7.970001
19   0.502197  8.024862  0.506104  7.961691
20   0.504639  7.985133  0.494873  8.142384
21   0.492798  8.175711  0.502319  8.022133
22   0.504761  7.982696  0.499878  8.061321
23   0.498047  8.090775  0.500366  8.053339
24   0.504517  7.986403  0.486328  8.279532
25   0.502930  8.011920  0.503418  8.004027
26   0.500488  8.051232  0.498291  8.086634
27   0.499878  8.061046  0.493652  8.160454
28   0.494141  8.153506  0.501831  8.029546
29   0.501831  8.029543  0.500854  8.041600
30   0.503784  7.998058  0.496338  8.115919
31   0.498047  8.090530  0.496216  8.117122
32   0.507568  7.937061  0.499634  8.060030
33   0.500244  8.109797  0.499390  8.137849
34   0.497681  8.145840  0.494507  8.181007
35   0.489746  8.246012  0.501709  8.045655
36   0.494995  8.144988  0.494263  8.149577
37   0.498169  8.081298  0.498169  8.075653
38   0.510620  7.872207  0.499390  8.046536
39   0.492188  8.157139  0.501465  8.005185
40   0.504761  7.948966  0.502930  7.974607
41   0.500610  8.008335  0.499390  8.024648
42   0.505493  7.924447  0.505737  7.917742
43   0.497070  8.053320  0.499878  8.006038
44   0.502075  7.968680  0.497192  8.044260
45   0.494263  8.088879  0.502075  7.962302
46   0.489624  8.158938  0.494141  8.085123
47   0.496704  8.042597  0.499146  8.002069
48   0.493408  8.092070  0.497314  8.028380
49   0.496094  8.046557  0.496826  8.033644
50   0.501343  7.960523  0.500732  7.969184
51   0.496948  8.028555  0.502930  7.932281
52   0.498291  8.005418  0.494385  8.066918
53   0.500854  7.963093  0.488403  8.160947
54   0.498047  8.006641  0.501831  7.945779
55   0.507568  7.853851  0.512451  7.775574
56   0.506104  7.876400  0.505005  7.893568
57   0.498657  7.994471  0.506836  7.863810
58   0.506348  7.871365  0.494995  8.052140
59   0.499878  7.974121  0.505371  7.886386
60   0.501709  7.944637  0.496338  8.030144
61   0.501343  7.950258  0.509888  7.813944
62   0.500488  7.963723  0.494019  8.066803
63   0.504639  7.897445  0.504028  7.907132
64   0.499390  7.981050  0.505371  7.885663
65   0.496094  8.033544  0.503906  7.908975
66   0.503174  7.920638  0.493774  8.070475
67   0.511353  7.790230  0.498291  7.998152
68   0.499023  7.986774  0.505127  7.887073
69   0.495361  8.045150  0.507690  7.848147
70   0.502441  7.932273  0.500122  7.965933
71   0.500488  8.133762  0.517212  8.068040
72   0.495728  8.413279  0.501099  8.325714
73   0.500122  8.340509  0.512695  8.136890
74   0.490112  8.499894  0.510254  8.174228
75   0.496704  8.391555  0.498779  8.356998
76   0.498657  8.357806  0.501831  8.305448
77   0.493652  8.436014  0.498413  8.357970
78   0.510498  8.161811  0.506348  8.227285
79   0.495605  8.398933  0.501587  8.300975
80   0.492310  8.448883  0.493530  8.427524
81   0.494141  8.415924  0.507202  8.203572
82   0.495361  8.392517  0.487183  8.522370
83   0.499512  8.321590  0.506104  8.213215
84   0.512817  8.102784  0.509521  8.153621
85   0.500366  8.298808  0.506470  8.197978
86   0.500732  8.287906  0.491943  8.426947
87   0.494141  8.388814  0.499878  8.293544
88   0.493896  8.387062  0.499146  8.299485
89   0.505615  8.192137  0.503662  8.220465
90   0.501465  8.252634  0.497803  8.308328
91   0.502441  8.230134  0.496094  8.328932
92   0.496338  8.321390  0.502686  8.215383
93   0.500854  8.241112  0.489258  8.424156
94   0.504395  8.176221  0.495483  8.315805
95   0.496948  8.288069  0.503296  8.181546
96   0.490967  8.375985  0.502563  8.184703
97   0.507202  8.105513  0.502686  8.173811
98   0.504517  8.139749  0.501587  8.182351
99   0.491943  8.333140  0.494385  8.289077
100  0.494507  8.282389  0.490112  8.348446
101  0.501099  8.166605  0.498169  8.209020
102  0.500244  8.170800  0.505493  8.081391
103  0.506714  8.056971  0.505249  8.075815
104  0.497559  8.195086  0.501587  8.125466
105  0.504761  8.069724  0.510010  7.980539
106  0.495728  8.206288  0.493652  8.235301
107  0.500244  8.124768  0.500977  8.108707
108  0.493042  8.232510  0.494507  8.204853
109  0.502930  8.065234  0.495483  8.181447
110  0.499634  8.110946  0.505249  8.016896
111  0.496704  8.151295  0.502930  8.047693
112  0.495483  8.164679  0.495483  8.161723
113  0.498657  8.107840  0.494141  8.177995
114  0.497437  8.122458  0.504639  8.004046
115  0.504272  8.007849  0.493896  8.173077
116  0.500854  8.059134  0.494263  8.163674
117  0.492554  8.189719  0.510132  7.904975
118  0.497803  8.102469  0.501221  8.046227
119  0.502563  8.023604  0.502441  8.024660
120  0.502563  8.021930  0.499756  8.066480
121  0.504639  7.987202  0.501953  8.029962
122  0.491943  8.190877  0.506958  7.948487
123  0.497192  8.105590  0.493164  8.170250
124  0.493652  8.162174  0.499634  8.065582
125  0.503296  8.006420  0.499390  8.069262
126  0.500732  8.047533  0.498657  8.080906
127  0.496338  8.118237  0.497314  8.102452

2018-02-25 11:53:43.036964 Finish.
Total elapsed time: 15:27:27.04.
