2018-02-24 20:27:40.998851: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:41.000019: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:41.000038: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:41.000047: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:41.000055: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:52.525068 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit07, unit10, unit01.
Validation set: unit02, unit03.
Test set: unit05.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.501099  8.057683  0.491821  8.203862
1    0.508301  7.828015  0.503418  8.029481
2    0.501221  8.062094  0.504517  8.006436
3    0.502808  8.031916  0.497559  8.114623
4    0.498169  8.103211  0.496094  8.135208
5    0.498291  8.098582  0.494507  8.158459
6    0.490967  8.214585  0.500000  8.068125
7    0.517212  7.789979  0.494995  8.147401
8    0.497192  8.111420  0.499634  8.071542
9    0.496338  8.124216  0.495361  8.139533
10   0.502930  8.017179  0.504150  7.997155
11   0.490845  8.211309  0.493042  8.175599
12   0.503906  8.000222  0.502930  8.015705
13   0.496826  8.113847  0.506226  7.962117
14   0.490845  8.209815  0.499146  8.075814
15   0.507324  7.943795  0.500122  8.059690
16   0.501953  8.029999  0.503296  8.008182
17   0.511841  7.870290  0.503418  8.005891
18   0.495605  8.131664  0.500488  8.052816
19   0.498291  8.088097  0.506836  7.950237
20   0.498169  8.089810  0.502075  8.026731
21   0.501099  8.042363  0.506104  7.961590
22   0.499268  8.071678  0.505493  7.971243
23   0.499756  8.063636  0.499878  8.061592
24   0.506714  7.951341  0.497925  8.092940
25   0.494995  8.140105  0.495239  8.136117
26   0.497192  8.104591  0.493774  8.159640
27   0.494385  8.149766  0.492920  8.173344
28   0.494995  8.139868  0.487061  8.267733
29   0.499756  8.063088  0.493408  8.165382
30   0.506226  7.958775  0.499390  8.068944
31   0.506714  7.950881  0.506836  7.948904
32   0.499512  8.066949  0.504272  7.990208
33   0.496338  8.118093  0.508179  7.927238
34   0.511475  7.874111  0.491455  8.196785
35   0.502930  8.011834  0.492065  8.186943
36   0.496704  8.112176  0.498047  8.090531
37   0.495972  8.123979  0.504028  7.994121
38   0.496582  8.114140  0.507202  7.942964
39   0.505859  7.964607  0.489380  8.230202
40   0.492554  8.187192  0.496948  8.084762
41   0.496094  8.081444  0.501099  7.992441
42   0.508789  7.865996  0.495239  8.079033
43   0.504150  7.934911  0.499390  8.008970
44   0.503296  7.945166  0.495117  8.074109
45   0.502319  7.957992  0.505737  7.902243
46   0.492920  8.105412  0.495239  8.067290
47   0.494507  8.077884  0.492920  8.102118
48   0.504395  8.131019  0.502686  8.185048
49   0.492065  8.337831  0.494751  8.280602
50   0.504028  8.121602  0.497681  8.212779
51   0.503540  8.111165  0.491089  8.302059
52   0.505249  8.069772  0.505493  8.059678
53   0.502563  8.100836  0.498291  8.163599
54   0.500366  8.125592  0.513916  7.904771
55   0.502686  8.079303  0.494995  8.197473
56   0.496216  8.173807  0.495605  8.179385
57   0.499268  8.117038  0.511353  7.920450
58   0.488770  8.276715  0.490967  8.237959
59   0.498901  8.107886  0.497925  8.119907
60   0.497192  8.128179  0.506592  7.974953
61   0.494995  8.156586  0.511597  7.888699
62   0.506836  7.961505  0.495361  8.141371
63   0.502563  8.023606  0.499756  8.065444
64   0.499756  8.062635  0.498413  8.081255
65   0.499634  8.059115  0.495361  8.124568
66   0.494141  8.141469  0.499756  8.049406
67   0.502197  8.008033  0.504395  7.970568
68   0.506836  7.929295  0.496948  8.084591
69   0.501099  8.016164  0.493530  8.134575
70   0.499268  8.040933  0.502563  7.986222
71   0.492920  8.137866  0.497192  8.067663
72   0.490112  8.178512  0.492188  8.143411
73   0.509888  7.859273  0.496338  8.073343
74   0.505493  7.925500  0.494141  8.104609
75   0.499146  8.023004  0.492432  8.128233
76   0.501953  7.974692  0.501343  7.982689
77   0.503662  7.944043  0.519409  7.691340
78   0.491455  8.135403  0.502441  7.958678
79   0.495239  8.071989  0.495361  8.068552
80   0.500000  7.993182  0.495605  8.061841
81   0.495850  8.056623  0.508545  7.852926
82   0.499268  7.999602  0.488403  8.171600
83   0.510254  7.822123  0.501099  7.966979
84   0.508667  7.845297  0.501221  7.963012
85   0.503662  7.923168  0.503296  7.928112
86   0.496338  8.038219  0.501343  7.957635
87   0.505981  7.882960  0.496826  8.028219
88   0.504639  7.903038  0.490112  8.134017
89   0.507324  7.859074  0.496094  8.037594
90   0.499756  7.978748  0.502197  7.939385
91   0.510742  7.802770  0.509644  7.819917
92   0.518311  7.681424  0.495483  8.045042
93   0.504150  7.906611  0.507446  7.853825
94   0.501343  7.950926  0.498047  8.003282
95   0.493530  8.075131  0.500977  7.956276
96   0.500366  7.965890  0.496704  8.024168
97   0.493774  8.070791  0.497803  8.006495
98   0.497803  8.006438  0.496948  8.020010
99   0.491577  8.105601  0.499023  7.986856
100  0.505249  7.887583  0.503052  7.922592
101  0.501587  7.945932  0.502319  7.934243
102  0.494629  8.056840  0.496826  8.021803
103  0.507812  7.846651  0.505005  7.891408
104  0.501221  7.951735  0.499023  7.986763
105  0.496216  8.031523  0.502441  7.932271
106  0.504761  8.096759  0.492188  8.389358
107  0.506958  8.145146  0.492310  8.375474
108  0.505493  8.157936  0.499756  8.245625
109  0.504028  8.172529  0.506348  8.131112
110  0.500854  8.216063  0.499756  8.230337
111  0.501831  8.193824  0.501343  8.198765
112  0.507202  8.101696  0.505249  8.130658
113  0.506836  8.102810  0.503540  8.153751
114  0.505859  8.114386  0.499390  8.216748
115  0.497803  8.240559  0.504395  8.132590
116  0.497192  8.247066  0.500854  8.186458
117  0.490356  8.354164  0.503418  8.142151
118  0.494629  8.282373  0.509644  8.038929
119  0.493286  8.301168  0.503662  8.132502
120  0.502563  8.148797  0.499634  8.194585
121  0.494263  8.279714  0.500610  8.175932
122  0.496826  8.235438  0.503784  8.121767
123  0.503052  8.132023  0.501709  8.152081
124  0.492554  8.298025  0.504517  8.103545
125  0.488281  8.363530  0.496094  8.235868
126  0.487793  8.367881  0.499268  8.181110
127  0.501343  8.145800  0.498901  8.183247

2018-02-25 10:44:36.815896 Finish.
Total elapsed time: 14:17:44.82.
