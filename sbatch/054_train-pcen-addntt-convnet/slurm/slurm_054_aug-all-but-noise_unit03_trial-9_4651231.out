2018-02-24 20:26:37.169031: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:37.169307: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:37.169321: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:17.661114 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.968628  0.137280  0.803833  0.596890
1    0.973022  0.117366  0.842773  0.508767
2    0.971680  0.114102  0.829224  0.442210
3    0.972290  0.119041  0.842407  0.544702
4    0.972778  0.116415  0.846191  0.540420
5    0.971191  0.121320  0.827515  0.444140
6    0.975830  0.104474  0.831543  0.546752
7    0.978271  0.091213  0.838379  0.429430
8    0.975708  0.094577  0.833496  0.440758
9    0.977295  0.098460  0.839111  0.461639
10   0.975586  0.097924  0.854370  0.361838
11   0.947754  0.229867  0.614380  0.852093
12   0.902222  0.298440  0.693726  0.940768
13   0.668823  5.001365  0.490601  8.248332
14   0.505615  8.002749  0.497559  8.129561
15   0.500977  8.072073  0.495850  8.152508
16   0.495361  8.158524  0.502075  8.048576
17   0.499268  8.092334  0.498901  8.096824
18   0.506470  7.973597  0.500854  8.062926
19   0.496826  8.126809  0.495361  8.149421
20   0.501587  8.048181  0.496094  8.135861
21   0.503540  8.015065  0.494873  8.154013
22   0.503784  8.009701  0.499634  8.075940
23   0.492065  8.197324  0.493774  8.169193
24   0.486084  8.292609  0.496582  8.122877
25   0.500000  8.067301  0.502319  8.029445
26   0.503662  8.007363  0.494385  8.156468
27   0.507202  7.949477  0.496948  8.114361
28   0.505005  7.984141  0.507324  7.946403
29   0.502686  8.020839  0.509399  7.912301
30   0.500122  8.061534  0.503906  8.000246
31   0.507935  7.935044  0.498413  8.088246
32   0.497559  8.101770  0.496216  8.123172
33   0.499023  8.077694  0.494873  8.144372
34   0.502563  8.020215  0.501587  8.035759
35   0.499512  8.069026  0.494385  8.151487
36   0.500244  8.056883  0.498901  8.078369
37   0.497559  8.099870  0.501953  8.028900
38   0.498169  8.089768  0.498291  8.087680
39   0.495728  8.128890  0.499878  8.061889
40   0.506958  7.907359  0.497681  8.042671
41   0.496826  8.053522  0.490723  8.148936
42   0.499023  8.015420  0.497681  8.035786
43   0.499634  8.003796  0.500122  7.995211
44   0.492676  8.113209  0.498413  8.021053
45   0.494507  8.082688  0.496582  8.048975
46   0.501343  7.972477  0.496826  8.043885
47   0.499146  8.006331  0.504150  7.925962
48   0.498779  8.011021  0.502441  7.952066
49   0.496094  8.052698  0.504028  7.925632
50   0.493530  8.092431  0.498779  8.008179
51   0.501099  7.970638  0.505127  7.905846
52   0.504639  7.913065  0.502808  7.941687
53   0.501343  7.964477  0.495117  8.063163
54   0.496948  8.033415  0.497803  8.019235
55   0.500000  7.983660  0.501587  7.957816
56   0.487183  8.186926  0.496948  8.030712
57   0.503906  7.919277  0.493042  8.091975
58   0.490967  8.124576  0.498413  8.005388
59   0.504272  7.911523  0.488770  8.158231
60   0.506470  7.875629  0.502808  7.933600
61   0.494507  8.065550  0.493164  8.086582
62   0.503784  7.916924  0.501343  7.955507
63   0.501709  7.949357  0.503296  7.923756
64   0.505859  7.882611  0.506592  7.870666
65   0.501709  7.948266  0.489624  8.140693
66   0.506714  7.868025  0.504639  7.900901
67   0.508057  7.846222  0.502075  7.941397
68   0.495605  8.044374  0.500854  7.960531
69   0.502319  7.937029  0.504517  7.901856
70   0.505127  7.891992  0.501221  7.954138
71   0.493652  8.074675  0.493530  8.076503
72   0.509033  7.829238  0.494507  8.060714
73   0.498291  8.000282  0.497070  8.019641
74   0.498047  8.003976  0.497803  8.007773
75   0.497192  8.017413  0.501709  7.945319
76   0.499878  7.974425  0.500732  7.960719
77   0.508179  7.841928  0.513306  7.760114
78   0.501831  7.942973  0.508911  7.830028
79   0.495972  8.036245  0.502930  7.925252
80   0.498413  7.997195  0.490479  8.123631
81   0.505493  7.884206  0.495972  8.035946
82   0.498535  7.995029  0.502930  7.924922
83   0.494019  8.066943  0.498657  7.992949
84   0.491699  8.103839  0.495850  8.037636
85   0.510864  7.798235  0.511230  7.792366
86   0.498413  7.996679  0.503418  7.916865
87   0.489868  8.132859  0.499390  7.981044
88   0.505005  7.891506  0.498047  8.002417
89   0.509888  7.813633  0.503418  7.916764
90   0.500244  7.967352  0.497681  8.008210
91   0.498779  7.990688  0.503052  7.922568
92   0.502563  7.930347  0.506226  7.871960
93   0.509888  7.813573  0.497803  8.006233
94   0.501831  7.942010  0.498413  7.996498
95   0.494507  8.058771  0.504395  7.901137
96   0.494263  8.062661  0.492188  8.095744
97   0.493042  8.082121  0.493408  8.076282
98   0.500854  7.957571  0.488281  8.158017
99   0.504761  7.895295  0.495483  8.043198
100  0.493408  8.076281  0.507080  7.858319
101  0.502319  7.934217  0.493652  8.072389
102  0.507812  7.846642  0.507324  7.853736
103  0.496582  8.184222  0.503540  8.073814
104  0.507690  8.002473  0.502808  8.078786
105  0.501709  8.095665  0.500000  8.122622
106  0.488770  8.303254  0.496826  8.173045
107  0.495361  8.196332  0.516724  7.851690
108  0.507568  7.998934  0.496582  8.175687
109  0.496826  8.171423  0.504517  8.047134
110  0.510376  7.952355  0.497681  8.156637
111  0.500610  8.109071  0.500854  8.104784
112  0.497192  8.163457  0.500488  8.109973
113  0.499634  8.123383  0.496216  8.178106
114  0.485840  8.344975  0.496216  8.177358
115  0.507690  7.992030  0.502197  8.080186
116  0.511841  7.924364  0.515869  7.859044
117  0.496338  8.173459  0.506226  8.013690
118  0.496826  8.164793  0.490356  8.268668
119  0.510498  7.943620  0.494995  8.193087
120  0.510498  7.942799  0.494751  8.196195
121  0.499878  8.113139  0.498291  8.138292
122  0.507324  7.992266  0.503418  8.054791
123  0.499268  8.121247  0.495483  8.181792
124  0.495117  8.187238  0.494385  8.198577
125  0.494141  8.202037  0.496582  8.162200
126  0.496460  8.163668  0.499512  8.113967
127  0.495239  8.182305  0.488647  8.288010

2018-02-25 12:13:55.116673 Finish.
Total elapsed time: 15:47:38.12.
