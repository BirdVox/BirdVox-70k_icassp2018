2018-02-24 20:26:35.144972: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:35.145277: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:35.145290: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:16.810760 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit10, unit01, unit02.
Validation set: unit03, unit05.
Test set: unit07.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.492920  8.089726  0.487549  8.174174
1    0.494019  8.070159  0.494263  8.065507
2    0.504028  7.886670  0.502441  8.024177
3    0.506470  8.034408  0.489380  8.293833
4    0.508789  7.972881  0.495850  8.175191
5    0.497192  8.149302  0.497070  8.147513
6    0.500610  8.087396  0.504517  8.021575
7    0.498779  8.111536  0.497437  8.130770
8    0.495850  8.154160  0.497681  8.122526
9    0.492432  8.205174  0.502563  8.039966
10   0.501953  8.048041  0.502808  8.032553
11   0.498779  8.095894  0.501709  8.047130
12   0.506592  7.967007  0.506104  7.973499
13   0.491089  8.214247  0.498657  8.091043
14   0.512573  7.865642  0.494995  8.147910
15   0.499023  8.082034  0.497559  8.104741
16   0.503418  8.009502  0.500244  8.059904
17   0.496338  8.122209  0.502930  8.015347
18   0.495728  8.130907  0.491211  8.203219
19   0.492920  8.175266  0.497070  8.107996
20   0.497192  8.105723  0.508423  7.924432
21   0.504272  7.991109  0.510132  7.896470
22   0.493896  8.158001  0.494751  8.144094
23   0.498535  8.083000  0.488892  8.238349
24   0.499268  8.071045  0.499023  8.074927
25   0.505371  7.972578  0.500610  8.049281
26   0.504761  7.982364  0.499146  8.072855
27   0.497070  8.106292  0.499023  8.074803
28   0.496216  8.120052  0.508545  7.921326
29   0.501099  8.041344  0.497803  8.094466
30   0.498047  8.090530  0.501099  8.041341
31   0.499390  8.068886  0.496460  8.116106
32   0.501587  8.033470  0.497559  8.098399
33   0.514893  7.799968  0.500854  8.123230
34   0.502319  8.137060  0.503052  8.113773
35   0.515137  7.912485  0.485474  8.378313
36   0.502808  8.097080  0.507202  8.022791
37   0.496338  8.192745  0.499878  8.133357
38   0.491943  8.257357  0.510742  7.955292
39   0.496094  8.186660  0.504028  8.058042
40   0.497803  8.155246  0.497925  8.151248
41   0.492432  8.236776  0.498413  8.139340
42   0.503296  8.059392  0.495117  8.187631
43   0.497925  8.140675  0.490723  8.253250
44   0.513794  7.883142  0.499023  8.116270
45   0.510376  7.932883  0.487305  8.298240
46   0.490723  8.241246  0.494873  8.172522
47   0.504517  8.016179  0.506836  7.976550
48   0.494995  8.162626  0.505127  7.998354
49   0.498047  8.108444  0.495728  8.142587
50   0.512939  7.865324  0.510620  7.899387
51   0.504517  7.993752  0.506836  7.953791
52   0.501587  8.034467  0.501709  8.029472
53   0.506958  7.942729  0.492065  8.177052
54   0.501099  8.029940  0.512695  7.841926
55   0.498657  8.062602  0.497559  8.076965
56   0.504150  7.968750  0.498657  8.053180
57   0.511475  7.845741  0.497681  8.062540
58   0.498291  8.049765  0.491943  8.147918
59   0.492676  8.133284  0.492798  8.128393
60   0.500977  7.995166  0.502808  7.963162
61   0.491699  8.137569  0.498291  8.029830
62   0.506104  7.902775  0.497192  8.042382
63   0.498535  8.018676  0.498535  8.016434
64   0.503174  7.940411  0.496582  8.043491
65   0.504883  7.909325  0.491211  8.125524
66   0.497925  8.016903  0.497314  8.025117
67   0.498535  8.004313  0.491821  8.110075
68   0.494995  8.058366  0.503052  7.928879
69   0.507446  7.857925  0.497925  8.008887
70   0.502441  7.936180  0.506836  7.865473
71   0.500366  7.968082  0.496826  8.024030
72   0.503784  7.912709  0.500977  7.957112
73   0.500000  7.972400  0.497192  8.016908
74   0.497681  8.008930  0.508057  7.843341
75   0.490479  8.123449  0.511353  7.790556
76   0.497559  8.010382  0.505127  7.889654
77   0.503662  7.912958  0.506592  7.866209
78   0.501953  7.940132  0.492798  8.086065
79   0.506958  7.860303  0.500854  7.957595
80   0.502930  7.924503  0.482300  8.253386
81   0.491211  8.111318  0.495361  8.045148
82   0.495117  8.049039  0.492676  8.087960
83   0.500122  7.969247  0.502930  7.924487
84   0.512207  7.776583  0.497437  8.012060
85   0.494019  8.066551  0.489746  8.134664
86   0.499390  7.980923  0.503052  7.922540
87   0.498779  7.990653  0.501831  7.942001
88   0.497559  8.010114  0.498291  7.998438
89   0.493530  8.148290  0.498047  8.087374
90   0.499146  8.059223  0.501831  8.009286
91   0.495605  8.104822  0.502930  7.985215
92   0.512695  7.827889  0.499268  8.040692
93   0.499512  8.036065  0.494385  8.117237
94   0.500000  8.027401  0.499878  8.029104
95   0.497925  8.060105  0.506470  7.923777
96   0.499146  8.040484  0.506104  7.929508
97   0.502686  7.983967  0.501953  7.995614
98   0.500610  8.016993  0.501953  7.995556
99   0.497803  8.061698  0.509644  7.872902
100  0.501343  8.005207  0.495850  8.092751
101  0.500488  8.018757  0.508057  7.898066
102  0.502075  7.993386  0.506592  7.921338
103  0.500000  8.026387  0.492920  8.139214
104  0.493286  8.133324  0.499146  8.039860
105  0.506226  7.926930  0.497070  8.072825
106  0.507935  7.899562  0.498657  8.047402
107  0.497192  8.070677  0.488159  8.214613
108  0.497681  8.062736  0.481445  8.321481
109  0.494141  8.118992  0.496948  8.074138
110  0.498047  8.056514  0.503662  7.966883
111  0.499878  8.027095  0.505981  7.929661
112  0.507690  7.902284  0.504517  7.952744
113  0.505615  7.935074  0.499878  8.026379
114  0.495972  8.088484  0.506104  7.926777
115  0.510254  7.860418  0.503784  7.963358
116  0.490601  8.173318  0.494019  8.118599
117  0.501099  8.005479  0.506592  7.917649
118  0.499878  8.024407  0.505371  7.936545
119  0.503296  7.969319  0.500366  8.015702
120  0.516602  7.756525  0.489624  8.186250
121  0.504395  7.950385  0.506714  7.913005
122  0.507080  7.906735  0.500366  8.013319
123  0.504028  7.954457  0.496338  8.076561
124  0.490234  8.173333  0.491943  8.145533
125  0.503540  7.960066  0.501465  7.992537
126  0.494995  8.095032  0.494995  8.094359
127  0.502441  7.974936  0.499268  8.024797

2018-02-25 11:40:07.382692 Finish.
Total elapsed time: 15:13:51.38.
