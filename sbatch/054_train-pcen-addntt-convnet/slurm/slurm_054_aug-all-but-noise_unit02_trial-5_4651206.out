2018-02-24 20:26:46.775836: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:46.776156: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:46.776169: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:17.668413 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.973145  0.115092  0.820312  0.483657
1    0.974365  0.105694  0.774292  0.419871
2    0.805298  2.956538  0.496094  8.062736
3    0.501953  7.964608  0.501221  7.973036
4    0.499512  7.998244  0.504395  7.918616
5    0.507324  7.870473  0.500122  7.983957
6    0.504150  7.918581  0.504272  7.915540
7    0.509155  7.836726  0.501465  7.958402
8    0.500610  7.971195  0.500122  7.978185
9    0.494629  8.065045  0.499756  7.982625
10   0.499756  7.982010  0.499146  7.991151
11   0.494629  8.062628  0.506592  7.871404
12   0.501709  7.948794  0.505127  7.893869
13   0.499268  7.986894  0.503296  7.922303
14   0.501831  7.945327  0.494873  8.055940
15   0.511108  7.796831  0.501709  7.946415
16   0.502197  7.938397  0.492676  8.089970
17   0.504883  7.895166  0.493408  8.077914
18   0.491455  8.108890  0.500366  7.966673
19   0.500122  7.970433  0.493408  8.077344
20   0.490479  8.123943  0.499634  7.977886
21   0.497803  8.006991  0.502930  7.925174
22   0.503418  7.917321  0.499756  7.975639
23   0.496094  8.048072  0.504395  7.945626
24   0.499634  8.139514  0.492554  8.235530
25   0.502563  8.060328  0.501221  8.068054
26   0.502930  8.030123  0.506470  7.963991
27   0.508057  7.930761  0.504272  7.983778
28   0.494507  8.133325  0.496826  8.090624
29   0.506226  7.935872  0.511597  7.845633
30   0.496094  8.088773  0.504272  7.954581
31   0.494263  8.110805  0.505737  7.924672
32   0.504272  7.945168  0.500854  7.996920
33   0.498047  8.039213  0.499878  8.007648
34   0.502563  7.962679  0.500610  7.991736
35   0.508911  7.857505  0.505493  7.910161
36   0.491821  8.126445  0.501953  7.963295
37   0.501099  7.975430  0.503540  7.935068
38   0.501465  7.966834  0.498047  8.020049
39   0.505371  7.902117  0.501465  7.963263
40   0.507690  7.862983  0.499634  7.990431
41   0.497681  8.020663  0.502563  7.941945
42   0.505127  7.900283  0.498413  8.006552
43   0.501343  7.959151  0.489380  8.149199
44   0.504028  7.915061  0.503906  7.916423
45   0.497925  8.011252  0.499512  7.985443
46   0.507812  7.852647  0.498291  8.003998
47   0.498779  7.995810  0.503418  7.921470
48   0.493286  8.082644  0.493164  8.084252
49   0.496338  8.033346  0.506104  7.877363
50   0.506348  7.873203  0.508911  7.832077
51   0.501709  7.946663  0.504028  7.909462
52   0.496826  8.024078  0.512573  7.772837
53   0.499756  7.977000  0.507446  7.854227
54   0.503296  7.920242  0.494385  8.062160
55   0.495972  8.036730  0.486938  8.180615
56   0.493652  8.073467  0.490967  8.116174
57   0.494995  8.051857  0.500732  7.960300
58   0.495728  8.040008  0.497192  8.016578
59   0.502197  7.936721  0.497314  8.014500
60   0.498901  7.989145  0.502563  7.930709
61   0.502686  7.928717  0.504639  7.897536
62   0.495361  8.045401  0.504639  7.897463
63   0.499390  7.981115  0.498901  7.988871
64   0.494873  8.053069  0.491943  8.099754
65   0.499878  7.973239  0.504028  7.907055
66   0.495728  8.039376  0.493408  8.076339
67   0.506104  7.873936  0.497803  8.006260
68   0.504272  7.903110  0.498901  7.988732
69   0.504272  7.903099  0.496582  8.025698
70   0.501221  7.951744  0.492920  8.084075
71   0.504517  7.899194  0.496094  8.033472
72   0.503906  7.908921  0.495361  8.045147
73   0.496704  8.023739  0.506226  7.871943
74   0.497070  8.017900  0.499023  7.986762
75   0.494507  8.058767  0.497070  8.017899
76   0.503662  7.912810  0.497681  8.008168
77   0.499268  7.984014  0.498291  7.998442
78   0.507690  7.848589  0.498779  7.990653
79   0.504761  7.895295  0.505371  7.885564
80   0.504761  7.901399  0.501343  7.950476
81   0.501831  7.942215  0.503052  7.922580
82   0.499512  7.978992  0.505859  7.877784
83   0.501831  7.942003  0.500610  7.961463
84   0.496094  8.033467  0.496460  8.027629
85   0.499268  7.982869  0.508667  7.833020
86   0.497925  8.004369  0.502930  7.924486
87   0.486084  8.193047  0.507935  7.840036
88   0.507935  7.844997  0.493652  8.072390
89   0.496826  8.021791  0.505127  7.889458
90   0.501953  7.940101  0.498779  7.990653
91   0.496094  8.018525  0.495361  8.093115
92   0.498291  8.044913  0.501221  7.997875
93   0.503540  7.960743  0.501953  7.985878
94   0.499023  8.032408  0.511597  7.831777
95   0.499878  8.018406  0.506470  7.913112
96   0.512451  7.817535  0.500732  8.004131
97   0.509033  7.871554  0.510986  7.840163
98   0.498169  8.044234  0.500488  8.006978
99   0.503418  7.959974  0.496826  8.064753
100  0.493286  8.120862  0.498291  8.040731
101  0.500610  8.003396  0.507324  7.895988
102  0.501343  7.990953  0.503784  7.951625
103  0.498779  8.030986  0.495972  8.075304
104  0.502075  7.977536  0.495483  8.082147
105  0.508667  7.871470  0.505371  7.923500
106  0.495850  8.074760  0.512939  7.801757
107  0.492676  8.124238  0.510986  7.831739
108  0.491699  8.138618  0.505737  7.914199
109  0.497803  8.040060  0.503662  7.945997
110  0.501465  7.980363  0.493164  8.112019
111  0.514893  7.764924  0.499634  8.007482
112  0.496460  8.057368  0.485840  8.225953
113  0.501221  7.980014  0.490967  8.142743
114  0.503052  7.949335  0.498413  8.022531
115  0.498779  8.015937  0.500000  7.995712
116  0.502686  7.952136  0.500977  7.978612
117  0.492310  8.116021  0.497070  8.039354
118  0.500488  7.984102  0.494263  8.082587
119  0.511475  7.807432  0.505127  7.907870
120  0.498047  8.019998  0.509033  7.844103
121  0.494995  8.067171  0.494141  8.080062
122  0.506104  7.888630  0.498047  8.016358
123  0.504028  7.920305  0.499390  7.993565
124  0.502319  7.946189  0.497192  8.027259
125  0.506836  7.872877  0.496826  8.031820
126  0.487061  8.186897  0.506104  7.882704
127  0.500854  7.965810  0.501343  7.957459

2018-02-25 12:05:52.852708 Finish.
Total elapsed time: 15:39:35.85.
