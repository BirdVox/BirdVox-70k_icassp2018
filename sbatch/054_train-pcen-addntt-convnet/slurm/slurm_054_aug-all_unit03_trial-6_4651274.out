2018-02-24 20:28:05.125483: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:28:05.125778: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:28:05.125793: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:28:05.125799: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:28:05.125805: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:52.525156 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.495728  8.045863  0.489624  8.141879
1    0.498657  7.996894  0.495483  8.046634
2    0.502686  8.055018  0.508789  7.953395
3    0.503784  8.027385  0.493408  8.189159
4    0.504761  8.002319  0.506836  7.965517
5    0.495972  8.138098  0.491943  8.200783
6    0.498047  8.100659  0.489624  8.234851
7    0.500732  8.054560  0.498779  8.084915
8    0.501343  8.042692  0.498535  8.087123
9    0.498291  8.090394  0.496094  8.125205
10   0.508301  7.927962  0.499268  8.073114
11   0.494019  8.157360  0.495483  8.133423
12   0.497803  8.095779  0.502319  8.022744
13   0.501831  8.030427  0.509155  7.912207
14   0.493164  8.169823  0.495850  8.126131
15   0.506714  7.953240  0.499756  8.036900
16   0.507080  7.911338  0.505493  7.931547
17   0.496582  8.071395  0.487549  8.213689
18   0.492554  8.132709  0.500000  8.012924
19   0.501465  7.988641  0.511719  7.824265
20   0.495361  8.084170  0.496338  8.067724
21   0.492432  8.129118  0.497437  8.048430
22   0.499878  8.008592  0.507568  7.885052
23   0.502686  7.961938  0.493042  8.114702
24   0.500977  7.987209  0.496704  8.054305
25   0.495605  8.070787  0.501709  7.972432
26   0.497559  8.037538  0.503174  7.946941
27   0.500488  7.988675  0.497437  8.036233
28   0.500244  7.990382  0.499756  7.997065
29   0.497681  8.029058  0.503662  7.932603
30   0.491089  8.131972  0.498779  8.008289
31   0.501953  7.956637  0.503784  7.926394
32   0.500977  7.970138  0.501709  7.957451
33   0.499390  7.993459  0.503296  7.930228
34   0.497437  8.022732  0.509155  7.835015
35   0.505859  7.886721  0.498047  8.010452
36   0.506714  7.871519  0.495239  8.053712
37   0.507690  7.854534  0.502441  7.937563
38   0.488037  8.166612  0.499268  7.987006
39   0.496582  8.029318  0.508301  7.842015
40   0.502075  7.940848  0.505249  7.889857
41   0.490356  8.126942  0.496704  8.025431
42   0.496460  8.029058  0.508667  7.834205
43   0.492920  8.085050  0.493042  8.082921
44   0.500488  7.964062  0.491943  8.100156
45   0.491821  8.101998  0.499268  7.983195
46   0.500366  7.965610  0.496826  8.021985
47   0.504517  7.899337  0.503784  7.910974
48   0.495483  8.043281  0.502930  7.924546
49   0.510132  7.809711  0.502930  7.924517
50   0.493896  8.068519  0.506714  7.864172
51   0.500122  7.969256  0.488770  8.150239
52   0.500366  7.965358  0.505859  7.877783
53   0.501099  7.953679  0.500977  7.955625
54   0.499146  7.984816  0.496338  8.029575
55   0.502441  7.932271  0.498657  7.992202
56   0.500854  8.170218  0.497437  8.263436
57   0.501831  8.189980  0.501709  8.189291
58   0.495239  8.290896  0.505981  8.115030
59   0.503662  8.149669  0.502319  8.168531
60   0.500244  8.199201  0.513184  7.987832
61   0.503174  8.146369  0.499390  8.204541
62   0.489990  8.353243  0.509766  8.031682
63   0.502808  8.141039  0.497803  8.218901
64   0.506714  8.072506  0.497559  8.217300
65   0.491821  8.307050  0.498169  8.202008
66   0.500488  8.161959  0.502930  8.119940
67   0.505371  8.077990  0.501465  8.138360
68   0.505127  8.076816  0.497925  8.190394
69   0.499268  8.166325  0.499023  8.167848
70   0.492920  8.263898  0.500000  8.147471
71   0.506104  8.046870  0.497803  8.178457
72   0.483521  8.406539  0.502563  8.097500
73   0.504761  8.060065  0.499512  8.142666
74   0.492310  8.256825  0.500366  8.125057
75   0.496826  8.180276  0.500122  8.125326
76   0.508789  7.983870  0.493286  8.231997
77   0.507690  7.998135  0.507202  8.004320
78   0.500122  8.116808  0.498413  8.142730
79   0.508789  7.973915  0.493774  8.214352
80   0.496704  8.165605  0.506470  8.006678
81   0.498169  8.138987  0.494995  8.188660
82   0.495728  8.175406  0.499634  8.110995
83   0.499146  8.117443  0.503296  8.049122
84   0.501221  8.081168  0.498657  8.121078
85   0.497437  8.139362  0.497803  8.132060
86   0.503174  8.044101  0.513062  7.883333
87   0.493408  8.198720  0.498901  8.108783
88   0.504761  8.012952  0.504639  8.013520
89   0.506226  7.986553  0.499634  8.091402
90   0.501831  8.054605  0.501587  8.057153
91   0.497437  8.122685  0.497803  8.115416
92   0.501221  8.058991  0.502930  8.030115
93   0.497559  8.115399  0.507690  7.950814
94   0.499634  8.079447  0.506836  7.962152
95   0.502563  8.029871  0.499023  8.085806
96   0.494873  8.151653  0.501221  8.048318
97   0.498291  8.094597  0.502686  8.022855
98   0.510498  7.896109  0.508667  7.924832
99   0.500977  8.048086  0.505371  7.976589
100  0.503174  8.011425  0.503906  7.999076
101  0.500854  8.047803  0.509766  7.903743
102  0.494873  8.143427  0.502686  8.017179
103  0.497437  8.101521  0.505859  7.965523
104  0.506226  7.959435  0.500854  8.045842
105  0.504639  7.984723  0.497070  8.106602
106  0.504395  7.988469  0.500732  8.047427
107  0.498413  8.084762  0.502441  8.019793
108  0.496460  8.116176  0.501831  8.029581
109  0.499390  8.068918  0.495850  8.125965
110  0.499756  8.062997  0.500122  8.057089
111  0.499756  8.062989  0.506592  7.952804
112  0.502075  8.025602  0.507080  7.944931
113  0.489258  8.232192  0.506470  7.954768
114  0.505737  7.966573  0.506470  7.954768
115  0.505737  7.966573  0.499878  8.061015
116  0.498047  8.090528  0.495972  8.123977
117  0.497925  8.092496  0.495728  8.127912
118  0.505737  7.966573  0.505737  7.966517
119  0.506226  7.956759  0.498657  8.080691
120  0.512451  7.858359  0.497925  8.092353
121  0.502319  8.021670  0.503784  7.998054
122  0.498657  8.080696  0.511597  7.872131
123  0.490479  8.212516  0.492920  8.173165
124  0.500488  8.051182  0.496460  8.116106
125  0.502563  8.017729  0.506104  7.960671
126  0.499023  8.016914  0.508545  7.890668
127  0.502197  7.991780  0.499634  8.032564

2018-02-25 11:53:01.464260 Finish.
Total elapsed time: 15:26:09.46.
