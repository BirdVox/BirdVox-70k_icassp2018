2018-02-24 20:27:50.416435: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:50.416751: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:50.416773: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:50.416783: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:50.416792: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:52.094572 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.494141  8.080922  0.495728  8.054484
1    0.490234  8.141022  0.506226  7.885082
2    0.494385  8.072937  0.502319  7.945555
3    0.508789  7.841604  0.499634  7.986782
4    0.504028  7.916015  0.513184  7.769378
5    0.501831  7.949751  0.502441  7.939432
6    0.497803  8.012854  0.502686  7.934504
7    0.492920  8.089738  0.500488  7.968647
8    0.490601  8.125893  0.505005  7.895883
9    0.497437  8.016209  0.497437  8.015891
10   0.502930  7.928032  0.490845  8.120422
11   0.506592  7.869130  0.497192  8.018743
12   0.496826  8.024368  0.503052  7.924911
13   0.504150  7.907209  0.505859  7.879784
14   0.497681  8.105383  0.501343  8.083992
15   0.499023  8.103892  0.496460  8.131101
16   0.499634  8.070706  0.500244  8.052220
17   0.503174  7.998391  0.503662  7.983997
18   0.493896  8.134003  0.501831  8.002130
19   0.502319  7.989589  0.504028  7.957795
20   0.513184  7.807750  0.498047  8.045136
21   0.495239  8.086341  0.497437  8.047886
22   0.505859  7.910501  0.498657  8.022329
23   0.504395  7.928156  0.503906  7.933337
24   0.489990  8.152845  0.501709  7.963769
25   0.507812  7.864447  0.508179  7.856681
26   0.494995  8.065144  0.496582  8.038212
27   0.507324  7.865518  0.498047  8.012058
28   0.497925  8.012815  0.503784  7.918283
29   0.494995  8.057435  0.498779  7.996202
30   0.492798  8.090790  0.497437  8.016122
31   0.504761  7.898755  0.495483  8.046102
32   0.509888  7.816004  0.499390  7.982947
33   0.499268  7.984551  0.500244  7.968672
34   0.500610  7.962586  0.495728  8.040207
35   0.497070  8.018626  0.496094  8.034039
36   0.502686  7.928832  0.493408  8.076631
37   0.499023  7.987033  0.509644  7.817656
38   0.498779  7.990810  0.499756  7.975199
39   0.495728  8.039391  0.494995  8.051043
40   0.502563  7.930369  0.506714  7.864188
41   0.493164  8.080195  0.507690  7.848603
42   0.514282  7.743510  0.492432  8.091856
43   0.506104  7.873892  0.502808  7.926435
44   0.501343  8.023973  0.502319  8.159931
45   0.495972  8.235937  0.500122  8.155454
46   0.506226  8.051260  0.497070  8.194407
47   0.506226  8.044149  0.505005  8.061621
48   0.504028  8.075834  0.502563  8.098113
49   0.490234  8.295778  0.495850  8.204285
50   0.497925  8.169950  0.495728  8.204494
51   0.496704  8.187904  0.500366  8.128019
52   0.495972  8.197978  0.492065  8.260046
53   0.507690  8.007279  0.505127  8.047649
54   0.510132  7.965997  0.504883  8.049589
55   0.491577  8.263002  0.501587  8.100584
56   0.504883  8.046342  0.484985  8.365902
57   0.494263  8.215184  0.496094  8.184453
58   0.494995  8.200909  0.493774  8.219300
59   0.499634  8.123541  0.504150  8.049394
60   0.496094  8.177875  0.494263  8.205981
61   0.491821  8.243898  0.501709  8.083065
62   0.506592  8.002881  0.501343  8.085975
63   0.502686  8.062805  0.507324  7.986488
64   0.491577  8.238739  0.496460  8.158454
65   0.501831  8.070295  0.485474  8.332339
66   0.493652  8.198912  0.502808  8.049729
67   0.495605  8.164207  0.497681  8.129140
68   0.500000  8.090160  0.495117  8.167258
69   0.501343  8.065339  0.488770  8.266420
70   0.498413  8.109448  0.498779  8.102013
71   0.496826  8.132011  0.494873  8.162019
72   0.501221  8.058294  0.500977  8.060832
73   0.500366  8.069342  0.501343  8.052294
74   0.491333  8.212402  0.493652  8.173815
75   0.500977  8.054642  0.506348  7.966980
76   0.501709  8.040744  0.500000  8.067320
77   0.503296  8.013317  0.504272  7.996732
78   0.500244  8.060905  0.497559  8.103471
79   0.494507  8.152026  0.497192  8.108140
80   0.501465  8.038757  0.505249  7.977276
81   0.494873  8.144103  0.508423  7.925321
82   0.503052  8.011572  0.496826  8.111622
83   0.503784  7.999231  0.504517  7.987206
84   0.504517  7.987031  0.508789  7.918009
85   0.502441  8.020199  0.507080  7.945323
86   0.496948  8.108546  0.509155  7.911719
87   0.503540  8.002172  0.500610  8.049347
88   0.508545  7.921423  0.504028  7.994194
89   0.503662  8.000077  0.505615  7.968580
90   0.498535  8.082686  0.506104  7.960690
91   0.496460  8.116120  0.501465  8.035446
92   0.498047  8.090534  0.497681  8.096435
93   0.495117  8.137752  0.502930  8.011828
94   0.494751  8.143653  0.504883  7.980347
95   0.501343  8.037405  0.510864  7.883937
96   0.506714  7.938437  0.499146  8.056043
97   0.497314  8.082270  0.505005  7.956696
98   0.495728  8.101691  0.492920  8.143545
99   0.502441  7.988920  0.498657  8.046427
100  0.504639  7.948335  0.500122  8.017621
101  0.497925  8.050031  0.497192  8.059109
102  0.493774  8.111109  0.494629  8.095024
103  0.491455  8.143275  0.503296  7.952191
104  0.488770  8.181585  0.502075  7.967308
105  0.496826  8.048966  0.499634  8.002223
106  0.491943  8.122977  0.494629  8.078358
107  0.506104  7.893754  0.487183  8.193774
108  0.502563  7.947074  0.499268  7.998174
109  0.503906  7.922906  0.505493  7.896339
110  0.504028  7.918545  0.495728  8.049778
111  0.495361  8.054628  0.500977  7.964163
112  0.498291  8.006135  0.487671  8.174643
113  0.500977  7.961810  0.495850  8.042871
114  0.508301  7.843777  0.505005  7.895760
115  0.500610  7.965329  0.498535  7.997949
116  0.501953  7.943056  0.499634  7.979653
117  0.494873  8.055224  0.504639  7.899230
118  0.500244  7.969027  0.494385  8.062194
119  0.504883  7.894622  0.503784  7.911943
120  0.508789  7.831992  0.486694  8.184085
121  0.498047  8.002975  0.500488  7.963939
122  0.500610  7.961902  0.500977  7.955980
123  0.499756  7.975374  0.503174  7.920824
124  0.491821  8.101764  0.497559  8.010257
125  0.502075  7.938220  0.496216  8.031606
126  0.500488  7.963473  0.502319  7.934264
127  0.502930  7.924522  0.498535  7.994571

2018-02-25 12:04:10.740123 Finish.
Total elapsed time: 15:37:18.74.
