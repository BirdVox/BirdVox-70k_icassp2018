2018-02-24 20:26:31.438983: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:31.439273: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:31.439286: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:17.934319 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit01, unit02, unit03.
Validation set: unit05, unit07.
Test set: unit10.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.502808  7.934989  0.497559  8.017203
1    0.505371  7.891515  0.505249  7.892446
2    0.501831  7.946143  0.497192  8.019384
3    0.502563  7.933199  0.496704  8.026111
4    0.491943  8.101617  0.498901  7.990338
5    0.507446  7.853836  0.496338  8.030683
6    0.503418  7.933927  0.494141  8.168541
7    0.491943  8.194485  0.501343  8.039525
8    0.503052  8.011134  0.509399  7.908293
9    0.508667  7.893102  0.499390  8.022467
10   0.495972  8.066996  0.512573  7.794633
11   0.504395  7.920137  0.509033  7.842114
12   0.490723  8.131171  0.506958  7.869885
13   0.496948  8.027662  0.503296  7.924892
14   0.502686  7.933445  0.504028  7.911003
15   0.497803  8.009474  0.498901  7.991274
16   0.505859  7.879831  0.505249  7.889110
17   0.495605  8.042515  0.495117  8.050007
18   0.503052  7.923296  0.498901  7.989278
19   0.490723  8.119533  0.497437  8.012384
20   0.489136  8.144638  0.491455  8.107595
21   0.502197  7.936293  0.507446  7.852573
22   0.499512  7.979044  0.508545  7.835012
23   0.501465  7.947872  0.497925  8.004298
24   0.508057  7.842765  0.495972  8.035423
25   0.505371  7.885571  0.507324  7.854431
26   0.495117  8.049039  0.518311  7.679281
27   0.494263  8.062660  0.506226  7.871942
28   0.501709  7.943947  0.489258  8.142448
29   0.496948  8.224069  0.491821  8.337933
30   0.488647  8.380821  0.503662  8.133932
31   0.505615  8.095780  0.496216  8.238768
32   0.495239  8.247908  0.495239  8.241613
33   0.496948  8.208458  0.505493  8.066438
34   0.506226  8.049317  0.501709  8.115980
35   0.505127  8.056463  0.510864  7.960061
36   0.500366  8.122775  0.499146  8.137667
37   0.499023  8.135302  0.500854  8.101872
38   0.504639  8.037538  0.493408  8.212636
39   0.500854  8.090189  0.502808  8.055372
40   0.500854  8.083011  0.496094  8.155455
41   0.506714  7.982848  0.492798  8.201439
42   0.497559  8.122408  0.495728  8.148488
43   0.496826  8.127967  0.497192  8.119133
44   0.500366  8.065621  0.496460  8.124983
45   0.502319  8.028719  0.499146  8.076457
46   0.504883  7.982176  0.504150  7.991024
47   0.493896  8.151702  0.496338  8.109968
48   0.501587  8.023505  0.497559  8.084925
49   0.504761  7.967335  0.501831  8.011254
50   0.503418  7.983201  0.499268  8.046600
51   0.509277  7.884295  0.495972  8.093686
52   0.501953  7.995645  0.490723  8.172000
53   0.503540  7.965040  0.494019  8.114219
54   0.507568  7.895662  0.498535  8.037144
55   0.506470  7.908207  0.498291  8.036172
56   0.493896  8.103908  0.494019  8.099664
57   0.504517  7.930112  0.495972  8.064181
58   0.501343  7.976514  0.490356  8.149660
59   0.502563  7.953172  0.505371  7.906574
60   0.498413  8.015792  0.493652  8.090025
61   0.487793  8.181902  0.499268  7.997480
62   0.506104  7.887138  0.498047  8.014266
63   0.495850  8.048106  0.505615  7.891278
64   0.506104  7.882470  0.492310  8.101401
65   0.501953  7.946794  0.503052  7.928458
66   0.504150  7.910223  0.504883  7.897867
67   0.510498  7.807760  0.485596  8.204212
68   0.492920  8.086978  0.489136  8.146870
69   0.505127  7.891566  0.494263  8.064429
70   0.503296  7.920139  0.494141  8.065840
71   0.494385  8.061739  0.502197  7.937000
72   0.506348  7.870683  0.496094  8.034019
73   0.493896  8.068943  0.505859  7.878132
74   0.508179  7.841084  0.497070  8.018115
75   0.493530  8.074504  0.502197  7.936290
76   0.494629  8.056918  0.502930  7.924559
77   0.498169  8.000438  0.504639  7.897280
78   0.491699  8.103555  0.499634  7.977051
79   0.495972  8.035427  0.496948  8.019854
80   0.492798  8.086018  0.503784  7.910868
81   0.501953  7.940058  0.499634  7.977033
82   0.501221  7.951733  0.495972  8.035414
83   0.497192  8.015953  0.496094  8.033467
84   0.493408  8.076281  0.509766  7.815505
85   0.494995  8.050982  0.499146  7.984815
86   0.501831  7.942001  0.500977  7.955624
87   0.501221  7.951731  0.503906  7.908917
88   0.503906  7.908922  0.502808  7.926432
89   0.507080  7.874619  0.504883  8.053461
90   0.500122  8.116729  0.495483  8.183270
91   0.500366  8.102118  0.506592  8.000276
92   0.513062  7.895412  0.504272  8.036682
93   0.488403  8.292271  0.496704  8.158332
94   0.503906  8.042150  0.501343  8.083379
95   0.500000  8.104939  0.488281  8.293740
96   0.490601  8.256270  0.500366  8.098777
97   0.501953  8.073103  0.500000  8.104483
98   0.494751  8.188979  0.498291  8.131805
99   0.493774  8.204483  0.501953  8.072529
100  0.512573  7.901215  0.494019  8.200135
101  0.506836  7.993388  0.500488  8.095536
102  0.495728  8.172095  0.512817  7.896455
103  0.500488  8.094978  0.499268  8.114446
104  0.493408  8.208666  0.501099  8.084478
105  0.486694  8.316398  0.497070  8.148895
106  0.511597  7.914478  0.500488  8.093231
107  0.497192  8.146042  0.503540  8.043402
108  0.505005  8.019442  0.498657  8.121389
109  0.500488  8.091486  0.493774  8.199295
110  0.501343  8.076875  0.500488  8.090197
111  0.492065  8.225479  0.505981  8.000680
112  0.516724  7.827009  0.499023  8.111753
113  0.497437  8.136750  0.507812  7.968907
114  0.500610  8.084356  0.492920  8.207653
115  0.501587  8.067266  0.495239  8.168862
116  0.493042  8.203530  0.504883  8.011905
117  0.491455  8.227529  0.502319  8.051587
118  0.504272  8.019246  0.497070  8.134446
119  0.501953  8.054832  0.506470  7.981097
120  0.498779  8.104094  0.508423  7.947678
121  0.498169  8.111954  0.493408  8.187672
122  0.493652  8.182710  0.493408  8.185603
123  0.493652  8.180623  0.491943  8.207114
124  0.499146  8.089981  0.496704  8.128278
125  0.492798  8.190204  0.510742  7.899940
126  0.503784  8.011083  0.506714  7.962862
127  0.498657  8.091759  0.499878  8.071135

2018-02-25 12:12:46.915425 Finish.
Total elapsed time: 15:46:29.92.
