2018-02-24 20:27:45.388616: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:45.388934: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:45.388955: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:45.388963: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:45.388971: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:49.921765 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit10, unit01, unit02.
Validation set: unit03, unit05.
Test set: unit07.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.499390  8.076579  0.500488  8.057605
1    0.501831  8.034974  0.496094  8.126563
2    0.491211  8.204565  0.495605  8.133103
3    0.494019  8.158181  0.499390  8.071157
4    0.500000  8.060959  0.507324  7.942581
5    0.495117  8.139076  0.495972  8.125070
6    0.500732  8.048151  0.497925  8.093238
7    0.501831  8.018201  0.500366  7.988919
8    0.493286  8.085895  0.493286  8.080542
9    0.497070  8.019191  0.497314  8.014563
10   0.493774  8.215638  0.501099  8.085269
11   0.498657  8.112143  0.504639  8.006495
12   0.499756  8.079813  0.498901  8.089275
13   0.494385  8.159276  0.500000  8.066451
14   0.503540  8.007812  0.502808  8.018285
15   0.493042  8.174756  0.498169  8.091327
16   0.495605  8.132081  0.494263  8.153240
17   0.500366  8.054515  0.505981  7.963709
18   0.505737  7.967427  0.505737  7.967239
19   0.500732  8.047772  0.494751  8.144063
20   0.509033  7.913774  0.497192  8.104551
21   0.500610  8.049406  0.506836  7.949015
22   0.505737  7.966689  0.498901  8.076842
23   0.517456  7.777756  0.504761  7.982362
24   0.496948  8.108273  0.487793  8.255828
25   0.504761  7.982333  0.506348  7.956749
26   0.499146  8.072830  0.499634  8.064957
27   0.502930  8.011832  0.495605  8.129882
28   0.500122  8.057082  0.494019  8.155458
29   0.502808  8.013795  0.505737  7.966574
30   0.503296  8.129283  0.492065  8.399096
31   0.501343  8.248441  0.495361  8.343723
32   0.508301  8.134027  0.489380  8.437825
33   0.502686  8.222148  0.507812  8.138261
34   0.490234  8.420295  0.508911  8.117930
35   0.504883  8.181483  0.487671  8.457492
36   0.506714  8.149092  0.497681  8.293185
37   0.490234  8.411652  0.498779  8.272328
38   0.497314  8.294296  0.507324  8.131272
39   0.494507  8.336130  0.491943  8.375671
40   0.497314  8.287276  0.502441  8.202771
41   0.494263  8.332684  0.496094  8.301212
42   0.504395  8.165417  0.504395  8.163367
43   0.502319  8.194722  0.508301  8.096171
44   0.492554  8.347796  0.499756  8.229475
45   0.499146  8.237030  0.494385  8.311429
46   0.499756  8.222473  0.497192  8.261354
47   0.499268  8.225416  0.503418  8.155974
48   0.508423  8.072704  0.508667  8.066109
49   0.492310  8.327044  0.504761  8.123578
50   0.508423  8.061717  0.504883  8.115879
51   0.504883  8.112925  0.497681  8.225994
52   0.505615  8.095034  0.494751  8.267013
53   0.507324  8.061176  0.506836  8.065806
54   0.493896  8.271084  0.495728  8.238234
55   0.495850  8.232900  0.505493  8.074047
56   0.500732  8.147348  0.500732  8.143870
57   0.495972  8.217125  0.492310  8.272636
58   0.503784  8.084187  0.503906  8.078692
59   0.501221  8.118486  0.495239  8.211384
60   0.497925  8.164640  0.505249  8.043121
61   0.500000  8.124332  0.499512  8.128810
62   0.507812  7.991716  0.507080  8.000231
63   0.502563  8.069848  0.492310  8.231961
64   0.494751  8.189572  0.487915  8.296746
65   0.499146  8.112860  0.502930  8.049032
66   0.511963  7.900747  0.489502  8.260133
67   0.491943  8.218295  0.508301  7.952209
68   0.500732  8.071923  0.507446  7.961491
69   0.490845  8.227024  0.500732  8.065661
70   0.496704  8.128765  0.499512  8.081749
71   0.495239  8.149018  0.490112  8.230123
72   0.508545  7.931655  0.506592  7.961831
73   0.497192  8.112182  0.493042  8.177991
74   0.496094  8.127861  0.494751  8.148622
75   0.502686  8.019982  0.507568  7.940585
76   0.487305  8.266618  0.495361  8.136230
77   0.496704  8.114156  0.501343  8.038998
78   0.500854  8.046559  0.487427  8.262713
79   0.490234  8.217247  0.499634  8.065559
80   0.495239  8.136252  0.507324  7.941345
81   0.506226  7.958966  0.508179  7.927412
82   0.505005  7.978517  0.503296  8.006021
83   0.499268  8.070921  0.495605  8.129925
84   0.502808  8.013826  0.489624  8.226309
85   0.504639  7.984295  0.499268  8.070861
86   0.492676  8.177105  0.503296  8.005927
87   0.492188  8.184972  0.493896  8.157426
88   0.499878  8.061016  0.501587  8.033470
89   0.493652  8.161360  0.506470  7.954768
90   0.495361  8.133814  0.497070  8.106269
91   0.505493  7.970508  0.502686  8.015228
92   0.496704  8.256800  0.500732  8.316687
93   0.501953  8.296747  0.500854  8.314180
94   0.505005  8.246994  0.496704  8.380485
95   0.493286  8.435249  0.495483  8.399487
96   0.497314  8.369595  0.504639  8.251152
97   0.497925  8.358946  0.487793  8.521805
98   0.491455  8.462304  0.500977  8.308344
99   0.503296  8.270430  0.504639  8.248233
100  0.501099  8.304700  0.497070  8.369004
101  0.501953  8.289636  0.501099  8.302713
102  0.500366  8.313771  0.495850  8.385790
103  0.496094  8.381027  0.488525  8.502150
104  0.506104  8.217898  0.498779  8.334987
105  0.502930  8.267065  0.503784  8.252226
106  0.494751  8.396692  0.502441  8.271557
107  0.500610  8.299822  0.506348  8.206049
108  0.507202  8.190903  0.500122  8.303596
109  0.509399  8.152554  0.490967  8.448089
110  0.499756  8.304781  0.505371  8.212568
111  0.495483  8.370147  0.506836  8.185308
112  0.501221  8.273870  0.497559  8.330884
113  0.498047  8.320907  0.506714  8.179036
114  0.496460  8.342040  0.501831  8.253126
115  0.511963  8.087383  0.496582  8.332776
116  0.495972  8.340004  0.498291  8.299932
117  0.503052  8.220409  0.506470  8.162450
118  0.502808  8.218509  0.497803  8.296127
119  0.489746  8.422839  0.502930  8.207111
120  0.501099  8.233296  0.497681  8.284972
121  0.501099  8.226370  0.496216  8.301474
122  0.505127  8.154156  0.497437  8.274336
123  0.503784  8.168164  0.498413  8.250789
124  0.501465  8.197578  0.508667  8.077387
125  0.490479  8.366378  0.492554  8.328679
126  0.497681  8.241740  0.494507  8.288520
127  0.500122  8.193605  0.499390  8.200937

2018-02-25 12:13:28.778673 Finish.
Total elapsed time: 15:46:39.78.
