2018-02-24 20:27:44.228638: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:44.228925: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:44.228946: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:44.228956: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:44.228964: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:49.270534 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.967163  0.133110  0.844482  0.681969
1    0.942383  0.216803  0.843628  0.442883
2    0.949341  0.186167  0.852661  0.471469
3    0.963867  0.157766  0.868652  0.455113
4    0.960083  0.156309  0.851929  0.436144
5    0.952393  0.274728  0.792725  0.666038
6    0.916016  0.426585  0.689453  0.819302
7    0.943848  0.213624  0.798096  1.178728
8    0.954468  0.179660  0.789062  1.014479
9    0.779907  3.253667  0.493164  8.114688
10   0.503662  7.943746  0.493530  8.102307
11   0.504639  7.922869  0.504272  7.926530
12   0.502441  7.953855  0.486572  8.205089
13   0.506348  7.724342  0.501465  8.084556
14   0.500122  8.103309  0.506348  8.000360
15   0.513794  7.878157  0.509888  7.939065
16   0.488403  8.283552  0.500122  8.092953
17   0.508179  7.961562  0.496826  8.143072
18   0.499878  8.092554  0.500244  8.085370
19   0.495850  8.155032  0.495972  8.151933
20   0.501221  8.066290  0.499634  8.090859
21   0.506348  7.981712  0.498291  8.110661
22   0.490234  8.239676  0.496216  8.142442
23   0.494263  8.173153  0.501465  8.056315
24   0.497803  8.114634  0.495972  8.143453
25   0.508789  7.936206  0.496826  8.128382
26   0.493408  8.182863  0.495483  8.148813
27   0.495239  8.152177  0.499512  8.082748
28   0.497925  8.107787  0.495972  8.138734
29   0.501221  8.053619  0.499878  8.074755
30   0.507446  7.952281  0.497803  8.107233
31   0.490723  8.220885  0.500122  8.068922
32   0.500732  8.058637  0.497925  8.103445
33   0.496704  8.122690  0.508667  7.929442
34   0.503296  8.015599  0.509033  7.922711
35   0.496826  8.119064  0.502075  8.034060
36   0.498169  8.096635  0.490234  8.224140
37   0.498779  8.086039  0.508057  7.936135
38   0.496460  8.122694  0.507202  7.949196
39   0.503662  8.005913  0.494873  8.147239
40   0.498047  8.095759  0.499146  8.077732
41   0.504761  7.986921  0.490723  8.212890
42   0.499390  8.072912  0.500854  8.049025
43   0.495361  8.137304  0.493530  8.166563
44   0.493042  8.174198  0.497070  8.109039
45   0.497070  8.108827  0.500366  8.055499
46   0.500488  8.053343  0.495605  8.131864
47   0.504395  7.990037  0.507812  7.934787
48   0.503296  8.007444  0.508789  7.918769
49   0.500854  8.046537  0.503540  8.003134
50   0.493164  8.170272  0.501343  8.038348
51   0.494751  8.144508  0.491333  8.199516
52   0.500732  8.047941  0.498657  8.081320
53   0.494507  8.148155  0.498657  8.081200
54   0.498413  8.085084  0.496460  8.116516
55   0.503906  7.996453  0.504272  7.990510
56   0.502563  8.018020  0.502319  8.021922
57   0.492310  8.183231  0.500977  8.043508
58   0.500366  8.053322  0.508301  7.925409
59   0.500122  8.057215  0.495117  8.137867
60   0.499878  8.061117  0.497803  8.094550
61   0.496582  8.114214  0.496216  8.120105
62   0.499268  8.070907  0.498779  8.078768
63   0.508057  7.929228  0.497803  8.094495
64   0.500244  8.052617  0.494995  8.078838
65   0.500488  8.035665  0.510864  7.872205
66   0.494629  8.130962  0.499512  8.053042
67   0.490967  8.189183  0.501221  8.025621
68   0.504517  7.972980  0.504272  7.976772
69   0.498901  8.062292  0.492188  8.169213
70   0.496704  8.097084  0.497437  8.085279
71   0.506836  7.935291  0.503418  7.989635
72   0.501343  8.022563  0.499634  8.049643
73   0.506104  7.946325  0.494751  8.127126
74   0.500122  8.041300  0.500000  8.043037
75   0.503784  7.982484  0.487549  8.241080
76   0.498901  8.059840  0.500854  8.028438
77   0.504517  7.969772  0.496826  8.092078
78   0.502563  8.000294  0.504150  7.974662
79   0.499023  8.056040  0.503540  7.983661
80   0.493774  8.138949  0.499512  8.047064
81   0.503052  7.990181  0.502930  7.991661
82   0.503296  7.985325  0.504761  7.961452
83   0.503662  7.978414  0.493042  8.147146
84   0.503540  7.979169  0.494751  8.118647
85   0.501343  8.012880  0.492554  8.152292
86   0.500122  8.030887  0.495605  8.102116
87   0.499390  8.040966  0.502686  7.987569
88   0.495483  8.101492  0.504883  7.950713
89   0.492310  8.150186  0.495239  8.102471
90   0.499512  8.033302  0.508545  7.888201
91   0.505005  7.943503  0.501343  8.000715
92   0.500610  8.011179  0.502686  7.976848
93   0.513062  7.810144  0.494629  8.102683
94   0.501953  7.984564  0.492188  8.138865
95   0.498169  8.042093  0.513062  7.803228
96   0.501709  7.982754  0.501221  7.989053
97   0.506470  7.903876  0.505127  7.923769
98   0.491943  8.132433  0.499878  8.004410
99   0.499512  8.008735  0.501099  7.981914
100  0.511597  7.813055  0.505615  7.906918
101  0.492676  8.111746  0.512207  7.798919
102  0.502075  7.959043  0.498657  8.012144
103  0.497559  8.028331  0.493896  8.085404
104  0.492310  8.109463  0.498779  8.005103
105  0.500488  7.976718  0.509399  7.833540
106  0.500732  7.970681  0.498901  7.998870
107  0.503052  7.931783  0.496826  8.030146
108  0.498291  8.005986  0.501709  7.950721
109  0.494019  8.072629  0.500977  7.961037
110  0.503540  7.919578  0.499023  7.991022
111  0.489624  8.140378  0.505005  7.894704
112  0.496094  8.036364  0.496338  8.032092
113  0.511841  7.784612  0.501587  7.947780
114  0.505493  7.885247  0.504639  7.898630
115  0.505737  7.880915  0.500854  7.958575
116  0.493652  8.073242  0.493896  8.069211
117  0.505249  7.888111  0.503418  7.917200
118  0.495361  8.045559  0.498169  8.000724
119  0.499268  7.983151  0.505615  7.881901
120  0.511108  7.794286  0.507568  7.850686
121  0.499512  7.979100  0.508423  7.837010
122  0.487061  8.177557  0.502319  7.934279
123  0.510620  7.801932  0.488281  8.158056
124  0.495239  8.047120  0.501831  7.942024
125  0.494629  8.056838  0.499390  7.980936
126  0.493164  8.080184  0.500977  7.955631
127  0.500244  7.967306  0.502441  7.932275

2018-02-25 11:27:01.524022 Finish.
Total elapsed time: 15:00:12.52.
