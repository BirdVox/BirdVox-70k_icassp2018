2018-02-24 20:26:35.128008: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:35.128268: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:35.128280: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:16.816994 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit10, unit01, unit02.
Validation set: unit03, unit05.
Test set: unit07.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.505615  7.974341  0.500122  8.061975
1    0.505615  7.972759  0.502686  8.019381
2    0.497681  8.099587  0.498535  8.085398
3    0.502808  8.016204  0.503540  8.004098
4    0.497925  8.094360  0.500610  8.050850
5    0.499634  8.066404  0.507690  7.936373
6    0.494141  8.154625  0.498291  8.087537
7    0.508423  5.352596  0.512329  7.940577
8    0.501709  8.092154  0.498169  8.135669
9    0.504028  8.034000  0.506592  7.987026
10   0.510376  7.922434  0.503296  8.033557
11   0.493652  8.186881  0.496948  8.131925
12   0.488647  8.264329  0.498291  8.107651
13   0.502563  8.037795  0.502808  8.032954
14   0.496094  8.140415  0.498291  8.104298
15   0.508179  7.944326  0.496582  8.130675
16   0.497681  8.112466  0.491577  8.210363
17   0.494141  8.168611  0.508057  7.943891
18   0.503296  8.020237  0.498779  8.092656
19   0.500854  8.058849  0.504395  8.001437
20   0.487549  8.272620  0.500000  8.071595
21   0.518311  7.776140  0.505615  7.980440
22   0.500854  8.056858  0.485107  8.310354
23   0.505493  7.981464  0.496948  8.118880
24   0.499512  8.077256  0.495728  8.137941
25   0.506592  7.962527  0.504395  7.997639
26   0.508423  7.932410  0.500977  8.052130
27   0.507446  7.947555  0.504639  7.992512
28   0.502930  8.017989  0.507935  7.855632
29   0.504761  7.929056  0.502075  7.972473
30   0.505859  7.911594  0.500854  7.990842
31   0.510132  7.842420  0.505981  7.908070
32   0.492065  8.129421  0.501465  7.979068
33   0.507202  7.887107  0.504395  7.931369
34   0.508423  7.866657  0.505127  7.918706
35   0.504028  7.935729  0.495728  8.067568
36   0.494263  8.090428  0.499512  8.006249
37   0.500122  7.996024  0.503174  7.946872
38   0.496704  8.049517  0.503296  7.943926
39   0.499634  8.001810  0.503418  7.940978
40   0.507446  7.876257  0.488037  8.185182
41   0.497803  8.028996  0.499634  7.999303
42   0.494507  8.080543  0.501831  7.963279
43   0.499512  7.999765  0.491333  8.129662
44   0.495605  8.061066  0.502930  7.943817
45   0.495117  8.067893  0.495117  8.067419
46   0.500488  7.981328  0.502930  7.941943
47   0.495728  8.056310  0.497925  8.020828
48   0.499512  7.995088  0.498169  8.016054
49   0.497192  8.031191  0.499756  7.989892
50   0.503418  7.931088  0.500000  7.985156
51   0.495117  8.062586  0.515503  7.737174
52   0.500366  7.978081  0.495117  8.061354
53   0.499023  7.998677  0.507446  7.863993
54   0.503662  7.923923  0.500610  7.972175
55   0.505981  7.886153  0.512451  7.782613
56   0.507935  7.854228  0.491577  8.114611
57   0.502075  7.946861  0.502686  7.936742
58   0.506104  7.881870  0.495483  8.050798
59   0.503784  7.918090  0.490356  8.131786
60   0.505371  7.892052  0.499634  7.983154
61   0.495850  8.043130  0.494141  8.070024
62   0.499268  7.987951  0.503174  7.925341
63   0.495605  8.045678  0.496460  8.031739
64   0.503540  7.918565  0.498657  7.996114
65   0.503662  7.916047  0.499756  7.978050
66   0.508423  7.839625  0.502197  7.938630
67   0.499146  7.987055  0.503052  7.924561
68   0.503052  7.924361  0.492798  8.087640
69   0.499512  7.980432  0.501831  7.943290
70   0.498413  7.997633  0.495361  8.046144
71   0.495850  8.038237  0.503662  7.913570
72   0.505249  7.888170  0.510498  7.804393
73   0.494873  8.053412  0.496460  8.028038
74   0.496216  8.031867  0.494141  8.064892
75   0.493164  8.080413  0.501709  7.944143
76   0.499512  7.979137  0.496582  8.025811
77   0.497314  8.014110  0.493164  8.080254
78   0.514282  7.743563  0.495728  8.039354
79   0.513672  7.753267  0.500488  7.963436
80   0.501709  7.943968  0.503784  7.910878
81   0.496704  8.023748  0.499146  7.984823
82   0.495361  8.045149  0.500122  7.969250
83   0.506714  7.864160  0.506836  7.862213
84   0.502075  7.938114  0.499634  7.977031
85   0.494995  8.050982  0.498657  7.992599
86   0.498291  7.998567  0.495483  8.043199
87   0.493896  8.068151  0.499268  0.775516
88   0.502930  8.031128  0.495605  8.182899
89   0.501221  8.090953  0.500854  8.095423
90   0.497803  8.143231  0.492065  8.234337
91   0.499512  8.113013  0.496460  8.160916
92   0.494751  8.187251  0.502686  8.058176
93   0.501343  8.078715  0.501953  8.067804
94   0.498901  8.116003  0.489380  8.268515
95   0.509644  7.941034  0.494019  8.192042
96   0.500366  8.088976  0.498657  8.115800
97   0.496460  8.150573  0.500244  8.088967
98   0.502930  8.045142  0.500732  8.080045
99   0.496460  8.148461  0.509033  7.945380
100  0.497192  8.135862  0.492432  8.212246
101  0.504883  8.011252  0.505371  8.003093
102  0.498413  8.114989  0.490234  8.246574
103  0.502686  8.045671  0.501221  8.069078
104  0.504028  8.023641  0.500732  8.076587
105  0.495605  8.159062  0.501831  8.058560
106  0.503296  8.034802  0.498413  8.113358
107  0.497559  8.126994  0.495972  8.152435
108  0.492065  8.215265  0.503174  8.036087
109  0.499878  8.089082  0.499634  8.092889
110  0.492676  8.204911  0.497314  8.130017
111  0.496460  8.143663  0.496948  8.135664
112  0.497192  8.131602  0.491089  8.229850
113  0.508789  7.944428  0.496582  8.141052
114  0.500366  8.079929  0.489868  8.249005
115  0.496948  8.134756  0.504272  8.016569
116  0.496948  8.134486  0.511597  7.898244
117  0.505249  8.000417  0.492676  8.202931
118  0.497681  8.122118  0.510010  7.923248
119  0.501709  8.056890  0.499634  8.090183
120  0.493652  8.186432  0.498047  8.115436
121  0.497681  8.121169  0.502319  8.046227
122  0.504639  8.008661  0.503540  8.026180
123  0.499634  8.088945  0.498413  8.108416
124  0.501709  8.055080  0.494751  8.167010
125  0.504395  8.011344  0.490234  8.239341
126  0.492432  8.203676  0.502563  8.040112
127  0.507324  7.963109  0.493896  8.179260

2018-02-25 11:52:55.954200 Finish.
Total elapsed time: 15:26:39.95.
