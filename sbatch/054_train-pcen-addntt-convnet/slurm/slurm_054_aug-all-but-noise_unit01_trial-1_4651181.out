2018-02-24 20:26:38.448646: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:38.448962: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:38.448975: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:16.869952 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit02, unit03, unit05.
Validation set: unit07, unit10.
Test set: unit01.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.965576  0.142276  0.956177  0.182318
1    0.974121  0.115414  0.956543  0.165139
2    0.971069  0.119117  0.951294  0.183166
3    0.960327  0.222963  0.958130  0.206759
4    0.973022  0.126788  0.963623  0.169166
5    0.974609  0.106834  0.963501  0.146307
6    0.977051  0.102858  0.907349  0.272271
7    0.721924  4.341015  0.508057  7.950178
8    0.493896  8.174694  0.499268  8.085094
9    0.503296  8.017990  0.492432  8.191187
10   0.506226  7.967380  0.495117  8.145106
11   0.492920  8.179484  0.498657  8.086076
12   0.503296  8.010571  0.496216  8.124022
13   0.495850  8.129396  0.508423  7.926262
14   0.499756  8.065577  0.499268  8.073103
15   0.496338  8.120049  0.510864  7.885662
16   0.510010  7.899234  0.503174  8.009234
17   0.496338  8.112705  0.501221  7.986178
18   0.501831  7.969124  0.496582  8.046084
19   0.501221  7.969791  0.503540  7.930937
20   0.501709  7.958799  0.498291  8.012112
21   0.500732  7.972253  0.500366  7.977230
22   0.509155  7.836389  0.504517  7.909664
23   0.498779  8.000545  0.495361  8.054481
24   0.491455  8.116267  0.500854  7.965951
25   0.503540  7.922718  0.498901  7.996268
26   0.505249  7.894707  0.500366  7.972200
27   0.504517  7.905712  0.500488  7.969622
28   0.498901  7.994635  0.500366  7.971003
29   0.505005  7.896794  0.501831  7.947141
30   0.505249  7.892416  0.498901  7.993383
31   0.495361  8.049606  0.506226  7.876193
32   0.494629  8.060874  0.492432  8.095710
33   0.503662  7.916487  0.496826  8.025289
34   0.500732  7.962844  0.510498  7.806990
35   0.499878  7.976142  0.511353  7.793054
36   0.498535  7.997247  0.517456  7.695458
37   0.497437  8.014480  0.503296  7.920933
38   0.513062  7.765118  0.496948  8.021877
39   0.509155  7.827150  0.496460  8.029428
40   0.501709  8.028561  0.492188  8.246172
41   0.514526  7.876823  0.504639  8.030407
42   0.505859  8.007671  0.500122  8.097671
43   0.503906  8.034930  0.491333  8.236029
44   0.496216  8.156040  0.494507  8.182371
45   0.498779  8.112418  0.495117  8.170388
46   0.506592  7.984453  0.497437  8.131049
47   0.502319  8.051426  0.505249  8.003293
48   0.503418  8.031928  0.500244  8.082210
49   0.502563  8.043979  0.501465  8.060841
50   0.494629  8.170197  0.502075  8.049349
51   0.494629  8.168557  0.492310  8.205126
52   0.496704  8.133495  0.499512  8.087437
53   0.493286  8.186992  0.499512  8.085854
54   0.494141  8.171646  0.501953  8.044942
55   0.506226  7.975312  0.497314  8.118176
56   0.498779  8.093818  0.494263  8.165870
57   0.510132  7.909364  0.498901  8.089655
58   0.505005  7.990581  0.502563  8.029239
59   0.497559  8.109243  0.501953  8.037754
60   0.503662  8.009581  0.501587  8.042410
61   0.499023  8.083142  0.502197  8.031409
62   0.493652  8.168594  0.495728  8.134614
63   0.494995  8.145923  0.505249  7.980165
64   0.507568  7.942331  0.504639  7.989114
65   0.499268  8.075283  0.487671  8.261808
66   0.502441  8.023378  0.500977  8.046643
67   0.496460  8.119129  0.502563  8.020451
68   0.500488  8.053628  0.495605  8.132071
69   0.493042  8.173157  0.494019  8.157197
70   0.506958  7.948443  0.501953  8.028927
71   0.498413  8.085824  0.498657  8.081737
72   0.501221  8.040287  0.507080  7.945722
73   0.496094  8.122696  0.496582  8.114728
74   0.496460  8.116613  0.504883  7.980776
75   0.502930  8.012193  0.498047  8.090836
76   0.497925  8.092756  0.500732  8.047459
77   0.504395  7.988398  0.503052  8.010009
78   0.500732  8.047367  0.502319  8.021766
79   0.498413  8.084710  0.499023  8.074856
80   0.496338  8.118129  0.501221  8.039416
81   0.498657  8.080727  0.505859  7.964634
82   0.502563  8.017752  0.497681  8.096449
83   0.497192  8.104315  0.492188  8.184981
84   0.501099  8.041348  0.503540  8.001996
85   0.506470  7.954773  0.497803  8.094467
86   0.486694  8.273512  0.501587  8.033472
87   0.499512  8.066919  0.505981  7.962639
88   0.509888  7.899678  0.497803  8.094464
89   0.495972  8.123977  0.495239  8.135782
90   0.502319  8.021665  0.504028  7.994119
91   0.495850  8.125944  0.498657  8.080691
92   0.508545  7.921320  0.503418  8.003957
93   0.499878  8.067735  0.498779  8.136183
94   0.506348  8.000331  0.497070  8.139126
95   0.494995  8.165865  0.498779  8.099278
96   0.498779  8.095344  0.501709  8.044740
97   0.489624  8.237050  0.499023  8.083391
98   0.500122  8.064077  0.502319  8.027254
99   0.494385  8.154089  0.505981  7.966248
100  0.502930  8.014742  0.506714  7.953141
101  0.498901  8.078609  0.498291  8.088049
102  0.508301  7.926415  0.504639  7.985184
103  0.498413  8.085339  0.504395  7.988765
104  0.494751  8.144081  0.497681  8.096757
105  0.503784  7.998305  0.500000  8.059236
106  0.511841  7.868339  0.502075  8.025704
107  0.495361  8.133893  0.495239  8.135838
108  0.501709  8.031543  0.494629  8.145648
109  0.490356  8.214504  0.505493  7.970523
110  0.499390  8.068895  0.496582  8.114145
111  0.497681  8.096435  0.502563  8.017732
112  0.495850  8.125946  0.503906  7.996087
113  0.504883  7.980347  0.504028  7.994119
114  0.496582  8.114139  0.510254  7.893774
115  0.499268  8.070853  0.499390  8.068886
116  0.498657  8.115543  0.491577  8.294131
117  0.503784  8.096257  0.504028  8.089140
118  0.494019  8.245641  0.502686  8.104424
119  0.493408  8.249413  0.496948  8.190089
120  0.503662  8.080278  0.496094  8.198179
121  0.499512  8.141029  0.499512  8.138382
122  0.503662  8.069651  0.504028  8.061257
123  0.498535  8.146346  0.508057  7.992070
124  0.500000  8.118093  0.510132  7.954149
125  0.495850  8.179480  0.502563  8.070082
126  0.500366  8.102800  0.495972  8.170546
127  0.492554  8.222771  0.502075  8.068708

2018-02-25 11:23:21.875218 Finish.
Total elapsed time: 14:57:05.88.
