2018-02-24 20:26:43.209815: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:43.210107: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:43.210120: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:17.497837 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit10, unit01, unit02.
Validation set: unit03, unit05.
Test set: unit07.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.922729  0.258323  0.961792  0.167318
1    0.916504  0.314213  0.927002  0.265453
2    0.925781  0.269943  0.962891  0.156420
3    0.938232  0.232156  0.960449  0.178528
4    0.936279  0.263320  0.928345  0.346991
5    0.802368  2.116533  0.509766  7.970527
6    0.495483  8.192300  0.497681  8.149473
7    0.493652  8.208537  0.504517  8.028068
8    0.495850  8.163338  0.498413  8.117931
9    0.501831  8.059409  0.494019  8.182144
10   0.500854  8.069266  0.502808  8.035274
11   0.505127  7.995756  0.493896  8.174777
12   0.490845  8.222267  0.507202  7.957027
13   0.503174  8.020598  0.495728  8.139346
14   0.498657  8.091034  0.501953  8.036888
15   0.497803  8.102905  0.500977  8.050924
16   0.503296  8.012829  0.487915  8.260070
17   0.493164  8.174886  0.500488  8.056288
18   0.506226  7.963340  0.495728  8.132102
19   0.495972  8.127777  0.498291  8.090027
20   0.494629  8.148732  0.507568  7.939869
21   0.498779  8.081266  0.496460  8.118397
22   0.496948  8.110307  0.505493  7.972371
23   0.497803  8.096143  0.499756  8.064490
24   0.494385  8.150911  0.503052  8.011072
25   0.496582  8.115227  0.493652  8.162331
26   0.496094  8.122877  0.501343  8.038176
27   0.513794  7.837403  0.499268  8.071461
28   0.496338  8.118614  0.509521  7.906055
29   0.496216  8.120462  0.495728  8.128281
30   0.492798  8.175457  0.491089  8.202961
31   0.501221  8.039620  0.503906  7.996301
32   0.503418  8.004143  0.496094  8.122170
33   0.503296  8.003120  0.496704  8.098281
34   0.502441  7.988515  0.495972  8.081037
35   0.499756  8.015399  0.509399  7.857411
36   0.500366  7.998519  0.502930  7.955132
37   0.495361  8.073853  0.503052  7.949499
38   0.491089  8.138782  0.498413  8.020689
39   0.488770  8.173299  0.504639  7.919244
40   0.496094  8.054541  0.500366  7.985543
41   0.503052  7.941940  0.496216  8.050164
42   0.500122  7.987202  0.494019  8.083842
43   0.499146  8.001495  0.495850  8.053445
44   0.503906  7.924450  0.508301  7.853851
45   0.493774  8.084929  0.502808  7.940421
46   0.505005  7.904922  0.500854  7.970628
47   0.499268  7.995490  0.501587  7.958083
48   0.496704  8.035518  0.497192  8.027330
49   0.496826  8.032785  0.503052  7.933156
50   0.497314  8.024264  0.497925  8.014179
51   0.504395  7.910699  0.501221  7.960966
52   0.507690  7.857508  0.501953  7.948664
53   0.510498  7.812143  0.508667  7.841044
54   0.498535  8.002294  0.502075  7.945585
55   0.502930  7.931704  0.505981  7.882797
56   0.497559  8.016834  0.500854  7.964050
57   0.503662  7.919061  0.498047  8.008355
58   0.500000  7.977001  0.499634  7.982626
59   0.506104  7.879277  0.501587  7.951078
60   0.501343  7.954774  0.495728  8.044100
61   0.489014  8.150947  0.499878  7.977559
62   0.506958  7.864505  0.505005  7.895464
63   0.504639  7.901129  0.496338  8.033292
64   0.504883  7.896900  0.504639  7.900627
65   0.502563  7.933552  0.494019  8.069622
66   0.500488  7.966328  0.499390  7.983693
67   0.500366  7.967981  0.509399  7.823828
68   0.505493  7.885968  0.500977  7.957840
69   0.497803  8.008311  0.498169  8.002348
70   0.499512  7.980823  0.494385  8.062443
71   0.507080  7.859939  0.500488  7.964921
72   0.505371  7.886977  0.501831  7.943315
73   0.497803  8.007444  0.499390  7.982055
74   0.498657  7.993648  0.499756  7.976052
75   0.506714  7.865049  0.499756  7.975903
76   0.504761  7.896046  0.502197  7.936848
77   0.502686  7.929004  0.506714  7.864725
78   0.487305  8.174101  0.505859  7.878243
79   0.501465  7.948257  0.493652  8.072762
80   0.496704  8.024070  0.496460  8.027925
81   0.497559  8.010376  0.490479  8.123217
82   0.499268  7.983070  0.505859  7.877954
83   0.501343  7.949937  0.499146  7.984945
84   0.505127  7.889567  0.504517  7.899281
85   0.503540  7.914835  0.489258  8.142514
86   0.511108  7.794153  0.500244  7.967345
87   0.495972  8.035450  0.508667  7.833049
88   0.496826  8.021815  0.492310  8.093815
89   0.489624  8.136625  0.501831  7.942012
90   0.489624  8.136619  0.500366  7.965361
91   0.497070  8.017904  0.489014  8.146344
92   0.512207  7.776586  0.505371  7.885566
93   0.500122  7.969248  0.500977  7.955625
94   0.505371  7.885565  0.490723  8.119096
95   0.506836  7.890194  0.501465  8.061829
96   0.498901  8.060173  0.500610  8.012034
97   0.501221  7.994486  0.505127  7.926652
98   0.502441  7.966435  0.500854  7.989354
99   0.501831  7.972250  0.493652  8.101345
100  0.498413  8.024502  0.497803  8.033391
101  0.502930  7.950977  0.496460  8.053492
102  0.503174  7.945914  0.513062  7.787763
103  0.498535  8.018879  0.494263  8.086536
104  0.491943  8.123086  0.497559  8.033146
105  0.497192  8.038584  0.500000  7.993427
106  0.500854  7.979419  0.506104  7.895353
107  0.502075  7.959198  0.504761  7.916006
108  0.495483  8.063537  0.505737  7.899691
109  0.506348  7.889590  0.507812  7.865862
110  0.507080  7.877167  0.495605  8.059724
111  0.498535  8.012643  0.502197  7.953882
112  0.492920  8.101408  0.498657  8.009561
113  0.493530  8.090918  0.508911  7.845327
114  0.502563  7.946143  0.497437  8.027494
115  0.500244  7.982352  0.488037  8.176576
116  0.504639  7.911526  0.498413  8.010392
117  0.504395  7.914653  0.502441  7.945408
118  0.498413  8.009252  0.497925  8.016657
119  0.503174  7.932601  0.498779  8.002285
120  0.497803  8.017484  0.503418  7.927594
121  0.498413  8.007020  0.498413  8.006655
122  0.513428  7.766927  0.498901  7.998153
123  0.490479  8.132082  0.488037  8.170652
124  0.503906  7.917315  0.499756  7.983137
125  0.497803  8.013938  0.514771  7.743094
126  0.505737  7.886777  0.505493  7.890341
127  0.499878  7.979542  0.511597  7.792400

2018-02-25 12:05:27.612992 Finish.
Total elapsed time: 15:39:10.61.
