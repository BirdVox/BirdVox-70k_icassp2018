2018-02-24 20:27:44.971570: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:44.971882: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:44.971899: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:44.971906: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:44.971913: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:52.567129 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit01, unit02, unit03.
Validation set: unit05, unit07.
Test set: unit10.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.948608  0.169097  0.959961  0.143198
1    0.949585  0.170612  0.957275  0.161811
2    0.951660  0.164525  0.952881  0.194669
3    0.958618  0.145018  0.967896  0.124299
4    0.961426  0.126612  0.952759  0.151427
5    0.953003  0.156479  0.970337  0.125268
6    0.956421  0.162570  0.959961  0.158502
7    0.959473  0.138696  0.975830  0.104006
8    0.961060  0.128276  0.970093  0.119224
9    0.965210  0.121969  0.970337  0.120167
10   0.959473  0.136987  0.964111  0.148809
11   0.957520  0.157824  0.953491  0.215233
12   0.965332  0.139772  0.963013  0.135253
13   0.963623  0.130335  0.967651  0.130850
14   0.883545  1.486808  0.500854  8.059289
15   0.791504  0.602328  0.885254  0.357708
16   0.721191  3.871558  0.488892  8.194973
17   0.501953  7.981158  0.503662  7.949128
18   0.498535  8.027197  0.499634  8.006361
19   0.504395  7.927732  0.493652  8.096459
20   0.501587  7.967815  0.510986  7.815954
21   0.494629  8.074994  0.509155  7.841773
22   0.505615  7.896787  0.501099  7.967448
23   0.504517  7.911784  0.499146  7.996303
24   0.507812  7.857162  0.501465  7.957443
25   0.496460  8.036434  0.494751  8.062924
26   0.501343  7.957177  0.500854  7.964341
27   0.502319  7.940447  0.510498  7.809549
28   0.505249  7.892786  0.500732  7.964372
29   0.491211  8.115801  0.497192  8.020096
30   0.496094  8.037307  0.499268  7.986421
31   0.510132  7.812965  0.508301  7.841915
32   0.497192  8.018794  0.493652  8.075026
33   0.501221  7.954185  0.497314  8.016284
34   0.506836  7.864330  0.501465  7.949806
35   0.501465  7.949667  0.498779  7.992348
36   0.504883  7.894921  0.499146  7.986270
37   0.504639  7.898588  0.497314  8.015250
38   0.505127  7.890605  0.490967  8.116260
39   0.506348  7.870968  0.505615  7.882563
40   0.507690  7.849405  0.500854  7.958315
41   0.505249  7.888191  0.505981  7.876452
42   0.499878  7.973700  0.500244  7.967807
43   0.493408  8.076740  0.504272  7.903490
44   0.492676  8.088328  0.499512  7.979307
45   0.504883  7.893643  0.496582  8.025944
46   0.499146  7.985047  0.497559  8.010318
47   0.502075  7.938288  0.501587  7.946050
48   0.506714  7.864294  0.497070  8.018017
49   0.496948  8.019947  0.498413  7.996579
50   0.500000  8.032548  0.498169  8.142913
51   0.496582  8.164646  0.501831  8.077180
52   0.501221  8.084742  0.501221  8.082574
53   0.497437  8.141602  0.497070  8.145598
54   0.505005  8.015949  0.492676  8.212957
55   0.485474  8.327448  0.504395  8.020924
56   0.500854  8.076530  0.508301  7.955090
57   0.491089  8.231185  0.500244  8.082319
58   0.492798  8.201120  0.495483  8.156639
59   0.507568  7.960730  0.497681  8.119001
60   0.506348  7.978270  0.503418  8.024474
61   0.498901  8.096316  0.495728  8.146531
62   0.502441  8.037429  0.498535  8.099518
63   0.510498  7.905877  0.502686  8.030990
64   0.495972  8.138441  0.495850  8.139658
65   0.499878  8.074022  0.510498  7.902150
66   0.501587  8.045125  0.499146  8.083832
67   0.498779  8.089129  0.503052  8.019671
68   0.495361  8.143069  0.500488  8.059885
69   0.499390  8.077081  0.508301  7.932950
70   0.499268  8.078080  0.498657  8.087460
71   0.492554  8.185411  0.497070  8.112196
72   0.495972  8.129517  0.502075  8.030763
73   0.498779  8.083537  0.493774  8.163867
74   0.486572  8.279637  0.502686  8.019617
75   0.504883  7.983920  0.498169  8.091862
76   0.496094  8.125059  0.498779  8.081530
77   0.500122  8.059665  0.499146  8.075190
78   0.501953  8.029741  0.510742  7.887889
79   0.505615  7.970354  0.511230  7.879682
80   0.502441  8.021197  0.498169  8.089918
81   0.499023  8.076017  0.499390  8.069993
82   0.502075  8.026598  0.494873  8.142579
83   0.501221  8.040176  0.503906  7.996803
84   0.502808  8.014435  0.496948  8.108805
85   0.501587  8.033975  0.505615  7.968987
86   0.504639  7.984677  0.489136  8.234507
87   0.503296  8.006231  0.502930  8.012095
88   0.489380  8.230459  0.501099  8.041544
89   0.496826  8.110382  0.492065  8.187091
90   0.495483  8.131979  0.508423  7.923401
91   0.496704  8.112269  0.508911  7.915499
92   0.492676  8.177170  0.494019  8.155515
93   0.494629  8.145668  0.496582  8.114179
94   0.501221  8.039406  0.509033  7.913476
95   0.500854  8.045297  0.495239  8.135799
96   0.503418  8.003971  0.493408  8.165306
97   0.509399  7.907556  0.490479  8.212522
98   0.499268  8.070858  0.499756  8.062987
99   0.502197  8.024861  0.497803  8.101259
100  0.500854  8.046495  0.508545  7.921416
101  0.507080  7.944963  0.498657  8.080698
102  0.496582  8.109086  0.502441  7.971453
103  0.502563  7.976484  0.489746  8.179455
104  0.503540  7.958427  0.506836  7.904968
105  0.510010  7.853765  0.502563  7.971973
106  0.500122  8.010545  0.498901  8.029704
107  0.509766  7.856277  0.495605  8.081821
108  0.495483  8.083601  0.497437  8.052307
109  0.498657  8.032707  0.503662  7.952781
110  0.499268  8.022710  0.504150  7.944736
111  0.505371  7.925146  0.502441  7.971720
112  0.500488  8.002722  0.494385  8.099888
113  0.505493  7.922649  0.501221  7.990612
114  0.499756  8.013807  0.508179  7.879362
115  0.503174  7.958978  0.497925  8.042479
116  0.492432  8.129860  0.495361  8.082952
117  0.504761  7.932889  0.496338  8.066945
118  0.516968  7.737817  0.501099  7.990559
119  0.503052  7.959155  0.500854  7.993907
120  0.493652  8.108431  0.499878  8.008872
121  0.501465  7.983245  0.510376  7.840838
122  0.496826  8.056492  0.499634  8.011354
123  0.503052  7.956464  0.501465  7.981347
124  0.491821  8.134649  0.502563  7.962937
125  0.493164  8.112306  0.496826  8.053425
126  0.503662  7.943921  0.492310  8.124366
127  0.501587  7.975897  0.496338  8.058994

2018-02-25 12:04:24.463639 Finish.
Total elapsed time: 15:37:32.46.
