2018-02-24 20:26:42.941873: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:42.942115: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:42.942128: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:17.920480 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.973877  0.106173  0.830566  0.864513
1    0.976440  0.103052  0.825562  0.776947
2    0.975464  0.100024  0.852905  0.607247
3    0.972900  0.112520  0.841431  0.601507
4    0.976562  0.107959  0.857544  0.602805
5    0.976685  0.093090  0.833130  0.888665
6    0.973877  0.113350  0.853149  0.624144
7    0.977539  0.102669  0.823853  0.640750
8    0.979614  0.093865  0.819580  0.652545
9    0.977295  0.096648  0.845581  0.556055
10   0.976685  0.088818  0.833618  0.680275
11   0.944702  0.191085  0.565063  1.397940
12   0.897461  0.354656  0.610107  1.369323
13   0.941895  0.240491  0.707275  1.062198
14   0.955566  0.194175  0.653931  1.649338
15   0.964355  0.155209  0.770752  1.007960
16   0.970703  0.145891  0.737671  1.193016
17   0.966309  0.153284  0.763428  0.994548
18   0.970215  0.130114  0.807007  0.765746
19   0.705933  4.524767  0.497070  8.044065
20   0.505615  7.904739  0.489014  8.166590
21   0.510132  7.827602  0.485596  8.216632
22   0.498535  8.008565  0.490601  8.133412
23   0.498291  8.009426  0.495972  8.045120
24   0.495972  8.044042  0.493286  8.085856
25   0.500610  7.968248  0.493408  8.082285
26   0.512207  7.781926  0.499146  7.989544
27   0.500122  7.973457  0.492798  8.089739
28   0.504517  7.902505  0.497437  8.014997
29   0.498291  8.001050  0.503296  7.920958
30   0.494385  8.062766  0.508667  7.834833
31   0.505127  7.891066  0.502075  7.939528
32   0.494629  8.058077  0.500610  7.962567
33   0.506714  7.865134  0.498291  7.999295
34   0.507202  7.857128  0.504395  7.901794
35   0.496094  8.034048  0.497681  8.008674
36   0.486572  8.185706  0.500732  7.959901
37   0.498047  8.002666  0.507812  7.846933
38   0.499756  7.975338  0.498047  8.002548
39   0.504150  7.905214  0.499390  7.981085
40   0.489380  8.140642  0.506470  7.868169
41   0.497070  8.018001  0.499512  7.979064
42   0.499023  7.986836  0.506958  7.860328
43   0.493652  8.072443  0.504639  7.897287
44   0.499512  7.979641  0.501099  8.042669
45   0.503906  8.002245  0.497681  8.101358
46   0.497314  8.106705  0.500488  8.055133
47   0.509888  7.903359  0.500732  8.050686
48   0.506348  7.959984  0.493774  8.162456
49   0.498413  8.074298  0.508301  7.913705
50   0.494263  8.133887  0.497314  8.189421
51   0.502441  8.105803  0.493896  8.240346
52   0.509155  7.992361  0.505249  8.053458
53   0.506348  8.034175  0.495117  8.213698
54   0.504028  8.068736  0.502808  8.087124
55   0.493530  8.235465  0.506226  8.029672
56   0.500610  8.119072  0.499756  8.131750
57   0.499634  8.132664  0.509277  7.976180
58   0.500610  8.114857  0.504395  8.052845
59   0.495117  8.201381  0.496582  8.176771
60   0.485596  8.352866  0.501099  8.101999
61   0.503662  8.059703  0.500977  8.102003
62   0.504272  8.047901  0.506592  8.009531
63   0.501709  8.087248  0.504761  8.037065
64   0.502075  8.079357  0.494385  8.202306
65   0.504028  8.045862  0.505615  8.019263
66   0.497314  8.152031  0.502197  8.072290
67   0.489624  8.273903  0.495728  8.174467
68   0.505249  8.019936  0.505249  8.018857
69   0.498413  8.127956  0.494507  8.189819
70   0.495483  8.172976  0.500122  8.097094
71   0.500000  8.097943  0.491455  8.234539
72   0.503906  8.032720  0.494263  8.187014
73   0.496460  8.150461  0.497192  8.137507
74   0.500610  8.081277  0.501587  8.064389
75   0.505249  8.004228  0.503418  8.032600
76   0.507324  7.968516  0.486450  8.303840
77   0.501465  8.060729  0.496826  8.134391
78   0.501465  8.058548  0.494507  8.169624
79   0.501221  8.060367  0.498535  8.102617
80   0.503662  8.018979  0.498413  8.102590
81   0.500610  8.066221  0.507690  7.951161
82   0.491821  8.206041  0.511230  7.892314
83   0.497559  8.111836  0.507935  7.943766
84   0.496338  8.129901  0.504028  8.005180
85   0.507812  7.943468  0.496338  8.127714
86   0.500732  8.056228  0.502441  8.028044
87   0.499390  8.076642  0.521484  7.719943
88   0.499756  8.069637  0.490967  8.210788
89   0.504395  7.993890  0.501221  8.044593
90   0.495605  8.134688  0.502930  8.016237
91   0.504028  7.998168  0.493408  8.168997
92   0.504639  7.987669  0.494263  8.154609
93   0.497437  8.103181  0.499634  8.067505
94   0.496338  8.120395  0.503296  8.008020
95   0.498901  8.078651  0.501709  8.033206
96   0.502197  8.025165  0.502197  8.025002
97   0.497192  8.105526  0.496826  8.111291
98   0.505859  7.965572  0.493164  8.170081
99   0.497314  8.103084  0.505615  7.969197
100  0.500000  8.059621  0.499512  8.067414
101  0.505493  7.970939  0.489746  8.224689
102  0.504639  7.984597  0.501587  8.033738
103  0.498413  8.084853  0.498901  8.076945
104  0.507812  7.933284  0.494507  8.147719
105  0.498535  8.082767  0.511475  7.874187
106  0.506836  7.948937  0.501343  8.037462
107  0.489502  8.228302  0.499268  8.070889
108  0.512207  7.862322  0.498535  8.082680
109  0.504150  7.992168  0.493286  8.167275
110  0.503174  8.007901  0.508545  7.921327
111  0.492065  8.186943  0.496094  8.122013
112  0.500610  8.049213  0.496460  8.116108
113  0.504272  7.990185  0.494507  8.147588
114  0.494995  8.139718  0.499878  8.061016
115  0.502441  8.020646  0.498535  8.081356
116  0.492920  8.164614  0.502319  8.009548
117  0.499756  8.046842  0.507446  7.921221
118  0.504883  7.959963  0.499390  8.045713
119  0.491699  8.166972  0.508423  7.899170
120  0.497192  8.077272  0.496948  8.080300
121  0.489624  8.196320  0.506958  7.919259
122  0.494751  8.113205  0.498779  8.048329
123  0.497559  8.067151  0.501343  8.006180
124  0.494995  8.106736  0.493286  8.133332
125  0.500366  8.019805  0.498169  8.054170
126  0.504761  7.948410  0.504272  7.955513
127  0.503052  7.974285  0.502197  7.987208

2018-02-25 12:07:28.525966 Finish.
Total elapsed time: 15:41:11.53.
