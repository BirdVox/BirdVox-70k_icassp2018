2018-02-24 20:27:54.746689: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:54.747056: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:54.747077: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:54.747087: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:54.747097: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:52.525076 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit10, unit01, unit02.
Validation set: unit03, unit05.
Test set: unit07.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.945679  0.176741  0.958862  0.144203
1    0.952393  0.164474  0.969849  0.110960
2    0.943359  0.202676  0.873169  0.522052
3    0.557251  7.076913  0.515503  7.772277
4    0.500854  8.002037  0.503418  7.957870
5    0.515869  7.756735  0.493896  8.104598
6    0.486938  8.213445  0.506348  7.902051
7    0.504395  7.931453  0.497314  8.042666
8    0.498413  8.023653  0.499390  8.006639
9    0.503174  7.944987  0.496704  8.046846
10   0.499023  8.008683  0.497925  8.025041
11   0.503296  7.938336  0.497681  8.026803
12   0.495972  8.053064  0.492798  8.102698
13   0.498047  8.018113  0.497437  8.026958
14   0.500244  7.981368  0.489014  8.159596
15   0.499512  7.991471  0.496460  8.039379
16   0.501099  7.964731  0.500610  7.971835
17   0.489990  8.140511  0.492920  8.093186
18   0.508789  7.839618  0.510986  7.804028
19   0.495239  8.054554  0.500732  7.966475
20   0.499023  7.993254  0.494141  8.070644
21   0.507935  7.850319  0.499878  7.978357
22   0.500977  7.960472  0.505737  7.884216
23   0.496338  8.033737  0.502686  7.932224
24   0.498657  7.996156  0.502563  7.933602
25   0.501587  7.948918  0.510498  7.806609
26   0.499512  7.981536  0.503174  7.922940
27   0.499146  7.986969  0.497681  8.010137
28   0.500732  7.961318  0.511719  7.786009
29   0.502319  7.935714  0.497314  8.015366
30   0.497925  8.005512  0.498291  7.999555
31   0.494507  8.059779  0.502686  7.929290
32   0.493286  8.079049  0.506226  7.872678
33   0.501831  7.942661  0.495117  8.049624
34   0.502686  7.928903  0.495361  8.045609
35   0.492798  8.086424  0.489624  8.136973
36   0.495728  8.130681  0.505981  8.013993
37   0.501709  8.069507  0.515991  7.831613
38   0.498169  8.108378  0.495361  8.146557
39   0.497803  8.102284  0.503174  8.011697
40   0.497925  8.091126  0.500366  8.048185
41   0.502075  8.017384  0.500000  8.047065
42   0.500488  8.036209  0.491943  8.169472
43   0.498901  8.055831  0.499390  8.045413
44   0.501953  8.002109  0.494873  8.112608
45   0.490845  8.174617  0.506714  7.919461
46   0.502319  7.987494  0.490479  8.174278
47   0.505859  7.927202  0.505737  7.927313
48   0.506958  7.906122  0.497681  8.052324
49   0.504883  7.935897  0.510986  7.837010
50   0.512207  7.816052  0.496338  8.067570
51   0.498901  8.025306  0.501953  7.975279
52   0.496826  8.055711  0.491211  8.143948
53   0.496582  8.057103  0.497192  8.046173
54   0.497314  8.043091  0.494751  8.082839
55   0.503540  7.941659  0.500000  7.997051
56   0.506348  7.894863  0.495850  8.061250
57   0.501221  7.974696  0.497192  8.038005
58   0.505859  7.898967  0.491821  8.121915
59   0.507690  7.868113  0.492554  8.108631
60   0.507446  7.870450  0.501587  7.963115
61   0.500732  7.976027  0.498901  8.004517
62   0.500488  7.978551  0.502563  7.944810
63   0.495728  8.053164  0.508789  7.844313
64   0.490723  8.131745  0.504517  7.911254
65   0.494019  8.078063  0.496460  8.038592
66   0.502563  7.940765  0.488403  8.165995
67   0.492065  8.107119  0.501099  7.962622
68   0.503906  7.917398  0.503906  7.916940
69   0.497314  8.021593  0.503906  7.916075
70   0.501831  7.948750  0.501953  7.946401
71   0.512085  7.784493  0.492432  8.097439
72   0.504272  7.908312  0.502075  7.942993
73   0.493774  8.074998  0.501587  7.950126
74   0.498169  8.004313  0.496582  8.029316
75   0.515869  7.721558  0.495483  8.046286
76   0.501831  7.944839  0.510254  7.810315
77   0.493286  8.080598  0.506348  7.872149
78   0.491821  8.103536  0.488403  8.157834
79   0.498047  8.003920  0.497681  8.009591
80   0.504395  7.902406  0.496948  8.020975
81   0.490601  8.122044  0.499023  7.987643
82   0.484985  8.211337  0.499878  7.973813
83   0.498657  7.993187  0.501831  7.942507
84   0.494751  8.055310  0.487671  8.168118
85   0.495605  8.041567  0.490479  8.123253
86   0.501587  7.946116  0.499023  7.986945
87   0.495361  8.045297  0.505005  7.891528
88   0.502808  7.926534  0.507568  7.850617
89   0.500977  7.955690  0.501221  7.951783
90   0.504272  7.903120  0.499023  7.986793
91   0.498047  8.002355  0.503296  7.918667
92   0.485596  8.200846  0.498047  8.002340
93   0.504150  7.905033  0.499146  7.984821
94   0.499268  7.982873  0.500122  7.969249
95   0.500610  7.961464  0.505493  7.883620
96   0.500854  7.957571  0.487549  8.169695
97   0.501221  7.951732  0.499512  7.978977
98   0.501099  8.067018  0.493774  8.213753
99   0.494019  8.208883  0.493164  8.221701
100  0.506592  8.004301  0.498169  8.139071
101  0.495117  8.187253  0.497314  8.150813
102  0.496582  8.161581  0.502686  8.062150
103  0.487793  8.301126  0.489624  8.270533
104  0.497437  8.143526  0.494385  8.191615
105  0.494263  8.192481  0.505005  8.018223
106  0.503296  8.044657  0.503906  8.033697
107  0.511475  7.910592  0.500854  8.080641
108  0.499756  8.097231  0.503662  8.033145
109  0.501587  8.065480  0.497681  8.127322
110  0.497559  8.128186  0.510132  7.924421
111  0.491943  8.216493  0.505615  7.995034
112  0.496704  8.137591  0.503052  8.034203
113  0.497314  8.125623  0.501221  8.061608
114  0.504883  8.001553  0.493896  8.177604
115  0.497192  8.123480  0.504517  8.004431
116  0.505615  7.985757  0.494507  8.163841
117  0.496216  8.135367  0.503174  8.022296
118  0.501709  8.045022  0.499878  8.073659
119  0.503906  8.007895  0.509766  7.912628
120  0.500000  8.069250  0.499878  8.070448
121  0.508179  7.935933  0.506104  7.968673
122  0.482666  8.345781  0.497437  8.107065
123  0.502930  8.017931  0.501953  8.033096
124  0.492432  8.186037  0.498535  8.087152
125  0.496826  8.114239  0.493896  8.161021
126  0.508179  7.930426  0.501465  8.038268
127  0.499023  8.077290  0.505249  7.976634

2018-02-25 10:44:30.022798 Finish.
Total elapsed time: 14:17:38.02.
