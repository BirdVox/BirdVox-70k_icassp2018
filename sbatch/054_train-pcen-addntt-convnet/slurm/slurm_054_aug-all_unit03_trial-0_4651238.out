2018-02-24 20:27:39.794241: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:39.794514: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:39.794527: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:48.493923 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.498901  8.005863  0.498291  8.014233
1    0.501343  7.964390  0.502319  7.947690
2    0.504150  7.917495  0.496948  8.031355
3    0.505981  7.886487  0.489990  8.140604
4    0.500000  7.980286  0.502441  7.940655
5    0.502686  7.936124  0.501099  7.960808
6    0.496460  8.034204  0.504150  7.911065
7    0.513428  7.762678  0.508911  7.834216
8    0.487671  8.215174  0.500732  8.088324
9    0.512207  7.888257  0.501709  8.049012
10   0.504761  7.995894  0.497559  8.108914
11   0.495361  8.142350  0.495972  8.130850
12   0.503418  8.009645  0.508057  7.933853
13   0.499023  8.078684  0.497437  8.103586
14   0.498535  8.085358  0.503296  8.008161
15   0.502808  8.015668  0.498779  8.080272
16   0.496338  8.119366  0.498901  8.077818
17   0.501953  8.028447  0.510620  7.888589
18   0.504517  7.986838  0.492676  8.177574
19   0.498413  8.085011  0.501587  8.033776
20   0.496948  8.108481  0.493652  8.161551
21   0.505005  7.978530  0.500122  8.057196
22   0.501709  8.031592  0.501953  8.027635
23   0.508789  7.917436  0.485718  8.289287
24   0.507202  7.942991  0.492676  8.177120
25   0.505005  7.978393  0.500244  8.055122
26   0.503296  8.005931  0.502686  8.015767
27   0.498047  8.069231  0.497925  8.050921
28   0.501831  7.977949  0.503296  7.946555
29   0.496460  8.050576  0.505005  7.910235
30   0.500244  7.983240  0.501221  7.965168
31   0.493164  8.091731  0.493530  8.084226
32   0.495728  8.047892  0.491089  8.120667
33   0.502686  7.934843  0.506348  7.875597
34   0.494385  8.065606  0.491455  8.111661
35   0.501343  7.953487  0.501587  7.949096
36   0.499756  7.977870  0.500366  7.967754
37   0.500610  7.963538  0.496948  8.021622
38   0.497192  8.017480  0.508301  7.840155
39   0.496338  8.030681  0.498657  7.991926
40   0.504028  8.047578  0.497559  8.197228
41   0.504150  8.087927  0.500732  8.139950
42   0.493408  8.254981  0.491577  8.281462
43   0.508789  8.001058  0.492920  8.253850
44   0.497925  8.170251  0.510620  7.962692
45   0.507080  8.016880  0.510132  7.964820
46   0.495972  8.190251  0.499756  8.126457
47   0.495117  8.198495  0.503662  8.058043
48   0.501709  8.086876  0.494263  8.204257
49   0.492065  8.237111  0.488037  8.299489
50   0.497559  8.143551  0.510010  7.940404
51   0.500732  8.087565  0.497070  8.144235
52   0.506836  7.984564  0.502075  8.059051
53   0.490112  8.249717  0.496094  8.151177
54   0.509033  7.940590  0.505615  7.993682
55   0.504028  8.017369  0.503174  8.029285
56   0.500854  8.064926  0.488892  8.256043
57   0.503662  8.016390  0.497070  8.121100
58   0.508911  7.928837  0.499878  8.073071
59   0.508423  7.934105  0.501221  8.049001
60   0.508423  7.931852  0.496948  8.115787
61   0.501953  8.034221  0.497437  8.106172
62   0.501953  8.032636  0.500732  8.051618
63   0.506470  7.958550  0.489136  8.237386
64   0.511475  7.876860  0.504761  7.984642
65   0.493774  8.161362  0.502808  8.015435
66   0.499512  8.068290  0.489624  8.227416
67   0.504639  7.985211  0.508667  7.920105
68   0.505493  7.971121  0.498779  8.079211
69   0.494019  8.155848  0.494995  8.140022
70   0.496948  8.108477  0.494263  8.151706
71   0.496460  8.116248  0.498779  8.078829
72   0.507202  7.943043  0.499268  8.070911
73   0.501465  8.035480  0.501831  8.029564
74   0.504639  7.984303  0.492676  8.177115
75   0.511108  7.880012  0.498169  8.088568
76   0.502441  8.019701  0.499756  8.062986
77   0.493652  8.161362  0.504517  7.986250
78   0.501831  8.029535  0.505371  7.972476
79   0.499634  8.064950  0.496338  8.118074
80   0.491821  8.204059  0.498901  8.179260
81   0.505859  8.064661  0.492798  8.273449
82   0.487183  8.362310  0.503174  8.102902
83   0.502319  8.115019  0.501831  8.121213
84   0.501465  8.125440  0.504639  8.072587
85   0.506592  8.039404  0.501709  8.116383
86   0.496826  8.193361  0.500977  8.124718
87   0.488647  8.321692  0.502686  8.093658
88   0.500122  8.133210  0.500122  8.131427
89   0.499878  8.133581  0.501709  8.102268
90   0.505005  8.047355  0.503052  8.077028
91   0.497437  8.165739  0.493896  8.220987
92   0.497437  8.162134  0.499756  8.122943
93   0.501709  8.089675  0.503662  8.056398
94   0.497314  8.156940  0.498291  8.139423
95   0.500732  8.098328  0.484497  8.358265
96   0.506104  8.008304  0.494629  8.191549
97   0.501587  8.077742  0.503418  8.046578
98   0.493774  8.200419  0.503174  8.047333
99   0.511963  7.904147  0.501099  8.077749
100  0.499878  8.095983  0.505859  7.998149
101  0.500732  8.079433  0.492920  8.204023
102  0.500610  8.078808  0.502319  8.050024
103  0.495483  8.159041  0.504028  8.020169
104  0.506104  7.985646  0.498413  8.108548
105  0.502441  8.042631  0.497437  8.122332
106  0.506226  7.979761  0.494019  8.175625
107  0.489990  8.239716  0.496704  8.130680
108  0.501099  8.059074  0.500854  8.062248
109  0.498169  8.104814  0.488647  8.257575
110  0.510010  7.912584  0.508057  7.943404
111  0.501221  8.052958  0.504028  8.007087
112  0.505493  7.982889  0.501221  8.051174
113  0.503052  8.021112  0.497437  8.111078
114  0.500732  8.057442  0.505127  7.986107
115  0.500000  8.068268  0.496094  8.130761
116  0.499512  8.075229  0.502930  8.019705
117  0.494385  8.157026  0.502930  8.018899
118  0.496094  8.128706  0.494995  8.146046
119  0.504883  7.986330  0.500122  8.062725
120  0.498535  8.087984  0.504883  7.985359
121  0.506714  7.955550  0.502441  8.024124
122  0.499878  8.065168  0.499634  8.068834
123  0.495117  8.141377  0.501709  8.034877
124  0.505859  7.967743  0.502441  8.022599
125  0.503174  8.010571  0.497192  8.106762
126  0.497437  8.102621  0.492676  8.179153
127  0.499756  8.064847  0.503662  8.001701

2018-02-25 11:35:30.614839 Finish.
Total elapsed time: 15:08:42.61.
