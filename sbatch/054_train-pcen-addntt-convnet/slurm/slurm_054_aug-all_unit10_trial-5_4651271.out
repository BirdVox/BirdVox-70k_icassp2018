2018-02-24 20:27:51.525045: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:51.525339: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:51.525364: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:51.525379: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:51.525393: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:52.525067 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit01, unit02, unit03.
Validation set: unit05, unit07.
Test set: unit10.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.495361  8.053763  0.501221  7.958875
1    0.503662  7.918792  0.501221  7.956670
2    0.505737  7.883842  0.506470  7.870775
3    0.513672  7.797104  0.491211  8.223672
4    0.497070  8.119619  0.503784  8.006246
5    0.492065  8.192842  0.496216  8.124205
6    0.499878  8.064128  0.498535  8.084923
7    0.499268  8.072566  0.501221  8.040631
8    0.495483  8.130260  0.489014  8.236246
9    0.501099  8.020760  0.503174  7.972384
10   0.501587  7.988690  0.503296  7.953890
11   0.500488  7.993151  0.503052  7.947443
12   0.506226  7.893111  0.499878  7.990965
13   0.498657  8.007783  0.503906  7.921716
14   0.498291  8.009330  0.496948  8.029010
15   0.503174  7.928374  0.496948  8.026368
16   0.495117  8.054552  0.501831  7.946603
17   0.503540  7.918628  0.495239  8.050303
18   0.494751  8.057562  0.500854  7.959784
19   0.502563  7.932166  0.501953  7.941561
20   0.504028  7.908215  0.508545  7.835975
21   0.503174  7.921421  0.496460  8.028294
22   0.489868  8.133258  0.501465  7.948269
23   0.500854  7.957916  0.501221  7.952004
24   0.501099  7.953895  0.502441  7.932440
25   0.490601  8.121175  0.500366  7.965457
26   0.504517  7.899267  0.498413  7.996553
27   0.501831  7.942048  0.512207  7.776618
28   0.501953  7.940082  0.494507  8.058786
29   0.505737  7.879741  0.512817  7.766863
30   0.501709  7.943955  0.500244  7.967306
31   0.503662  7.912814  0.502930  7.924489
32   0.487671  8.167750  0.509033  7.827183
33   0.497192  8.015953  0.495972  8.035414
34   0.502441  7.932271  0.502319  7.934217
35   0.498413  7.996492  0.500488  7.963408
36   0.493896  8.059451  0.491211  8.224291
37   0.505737  8.098390  0.497925  8.205116
38   0.502075  8.127083  0.497681  8.189077
39   0.501465  8.122109  0.504639  8.065802
40   0.491089  8.280286  0.505005  8.052459
41   0.495239  8.206969  0.495361  8.202306
42   0.489502  8.294404  0.500854  8.109185
43   0.505249  8.036320  0.508545  7.981220
44   0.498657  8.138742  0.506592  8.009033
45   0.496460  8.170607  0.498047  8.143313
46   0.507568  7.988193  0.499390  8.118376
47   0.499634  8.112854  0.495972  8.170299
48   0.499878  8.105807  0.493652  8.204628
49   0.496582  8.155933  0.501587  8.073797
50   0.501709  8.070414  0.505371  8.009981
51   0.500488  8.087329  0.504272  8.024993
52   0.498291  8.120115  0.497070  8.138516
53   0.501099  8.072369  0.499390  8.098711
54   0.496094  8.150687  0.495605  8.157425
55   0.503174  8.034361  0.495605  8.155288
56   0.498291  8.110995  0.511108  7.903411
57   0.500854  8.067744  0.487915  8.275376
58   0.510376  7.912468  0.492798  8.194926
59   0.495239  8.154750  0.496216  8.138195
60   0.504517  8.003625  0.505737  7.983181
61   0.507690  7.950965  0.509399  7.922691
62   0.495239  8.150227  0.512207  7.876045
63   0.501465  8.048520  0.493652  8.173779
64   0.505981  7.974418  0.501465  8.046581
65   0.496948  8.118767  0.493652  8.171282
66   0.496338  8.127410  0.507446  7.947782
67   0.504272  7.998380  0.497192  8.111945
68   0.507080  7.952047  0.504272  7.996780
69   0.502197  8.029735  0.507690  7.940711
70   0.502686  8.020923  0.499878  8.065730
71   0.486816  8.275840  0.503662  8.003914
72   0.500000  8.062566  0.507935  7.934314
73   0.498535  8.085485  0.495117  8.140259
74   0.503418  8.006181  0.493164  8.171182
75   0.504883  7.982058  0.498657  8.082174
76   0.489136  8.235443  0.500122  8.058177
77   0.497070  8.107206  0.499023  8.075577
78   0.500122  8.057744  0.498657  8.081240
79   0.502808  8.014249  0.499634  8.065319
80   0.505371  7.972776  0.492920  8.173404
81   0.496460  8.116297  0.505005  7.978526
82   0.498657  8.080806  0.498413  8.084713
83   0.507080  7.944997  0.496216  8.120091
84   0.510864  7.883973  0.494507  8.147613
85   0.496094  8.122028  0.501465  8.035450
86   0.501709  8.031511  0.496460  8.116112
87   0.496948  8.108241  0.500977  8.043310
88   0.503174  8.007893  0.497192  8.104302
89   0.498047  8.090529  0.502808  8.013795
90   0.504883  7.980346  0.502808  8.013794
91   0.499756  8.083028  0.501831  8.254033
92   0.501709  8.255446  0.495728  8.345923
93   0.500000  8.272132  0.498047  8.298973
94   0.502197  8.227822  0.493042  8.371182
95   0.503662  8.195905  0.499634  8.256707
96   0.496460  8.303758  0.495605  8.313380
97   0.494141  8.332845  0.494019  8.330622
98   0.504517  8.157235  0.503296  8.172693
99   0.507812  8.095704  0.500366  8.211503
100  0.500854  8.199454  0.497803  8.244440
101  0.509888  8.045509  0.500977  8.184978
102  0.501587  8.171049  0.503784  8.131531
103  0.497192  8.233755  0.490967  8.330071
104  0.490112  8.339899  0.510010  8.015244
105  0.494263  8.265199  0.497925  8.202314
106  0.502441  8.125748  0.508057  8.031475
107  0.497803  8.193073  0.500488  8.146115
108  0.506714  8.042187  0.495117  8.225525
109  0.503418  8.088242  0.500000  8.139848
110  0.501221  8.116779  0.497192  8.178319
111  0.496948  8.178957  0.500732  8.114674
112  0.491699  8.257078  0.492676  8.238154
113  0.505005  8.036348  0.501831  8.084434
114  0.502563  8.069664  0.500610  8.098199
115  0.495972  8.170131  0.499390  8.112230
116  0.493286  8.207915  0.495117  8.175741
117  0.499023  8.110245  0.490356  8.247442
118  0.504272  8.020777  0.484009  8.345066
119  0.490112  8.244506  0.492432  8.204987
120  0.507690  7.957054  0.498413  8.104648
121  0.495239  8.154014  0.491211  8.217206
122  0.501465  8.050346  0.491089  8.216056
123  0.503296  8.017918  0.500488  8.061844
124  0.497192  8.113782  0.510498  7.898190
125  0.497559  8.105752  0.491577  8.201217
126  0.503174  8.013479  0.496704  8.116986
127  0.506470  7.958921  0.507202  7.946498

2018-02-25 11:58:25.610831 Finish.
Total elapsed time: 15:31:33.61.
