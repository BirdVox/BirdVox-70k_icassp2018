2018-02-24 20:26:38.326087: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:38.326375: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:38.326388: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:17.588019 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.963623  0.149601  0.880249  0.336045
1    0.780518  3.307000  0.495239  8.090697
2    0.490479  8.161171  0.500732  7.993232
3    0.501709  7.974283  0.497314  8.041270
4    0.498901  8.013423  0.494873  8.075273
5    0.495239  8.067406  0.497314  8.032414
6    0.505493  7.900368  0.500610  7.976642
7    0.498047  8.016134  0.503296  7.931146
8    0.510864  7.809337  0.498047  8.012583
9    0.499268  7.992156  0.500610  7.969830
10   0.497192  8.023509  0.494751  8.061660
11   0.488525  8.160230  0.506104  7.879348
12   0.499512  7.983869  0.494629  8.061174
13   0.489136  8.148277  0.494141  8.068042
14   0.507202  7.859422  0.507080  7.861003
15   0.491455  8.109786  0.498413  7.998562
16   0.500366  7.967169  0.505127  7.891033
17   0.498413  7.997865  0.502686  7.929563
18   0.485962  8.196018  0.504395  7.902010
19   0.503418  7.917455  0.502930  7.925126
20   0.495361  8.045690  0.500977  7.956083
21   0.500732  7.959905  0.496826  8.022116
22   0.507202  7.856646  0.509644  7.817677
23   0.499878  7.973327  0.489380  8.140658
24   0.508057  7.842879  0.498901  7.988811
25   0.486938  8.179510  0.505371  7.885633
26   0.509033  7.827237  0.505005  7.891447
27   0.508057  7.886793  0.491699  8.216324
28   0.493164  8.190144  0.495728  8.146693
29   0.501587  8.050580  0.502930  8.027402
30   0.499634  8.079234  0.500366  8.066218
31   0.485229  8.309146  0.504761  7.993349
32   0.507202  7.953126  0.501709  8.040835
33   0.502808  8.022388  0.505615  7.976428
34   0.501831  8.036788  0.492798  8.181777
35   0.495361  8.139910  0.496582  8.119707
36   0.498169  8.093653  0.505005  7.983012
37   0.507690  7.939311  0.491089  8.206498
38   0.505005  7.981838  0.498657  8.083804
39   0.502686  8.018564  0.506592  7.955305
40   0.515015  7.819278  0.494751  8.145636
41   0.502441  8.021455  0.494141  8.155034
42   0.511719  7.871520  0.506104  7.961849
43   0.496704  8.113196  0.499512  8.067798
44   0.490723  8.209338  0.499878  8.061657
45   0.503662  8.000567  0.499390  8.069342
46   0.491699  8.193223  0.496338  8.118389
47   0.507324  7.941256  0.498535  8.082869
48   0.499390  8.069057  0.504639  7.984418
49   0.494995  8.139826  0.499634  8.065035
50   0.491211  8.200777  0.495605  8.129930
51   0.491943  8.188945  0.485840  8.287311
52   0.491699  8.192863  0.504150  7.992167
53   0.497559  8.098410  0.501343  8.037413
54   0.501709  8.031508  0.511475  7.874103
55   0.501953  8.027570  0.503174  8.007894
56   0.498291  8.086595  0.507568  7.937061
57   0.504517  7.986249  0.494995  8.139717
58   0.498779  8.078723  0.504883  7.980346
59   0.502808  8.013794  0.500366  8.053145
60   0.495972  8.123977  0.485962  8.285096
61   0.506470  7.962813  0.501465  8.020911
62   0.495239  8.110448  0.498779  8.047717
63   0.502686  7.982473  0.501221  8.003618
64   0.495728  8.089907  0.499146  8.034384
65   0.494873  8.101803  0.503662  7.961088
66   0.494019  8.114372  0.510132  7.857069
67   0.500366  8.012396  0.494751  8.101569
68   0.508789  7.877444  0.496948  8.065892
69   0.490967  8.160932  0.500366  8.010760
70   0.503540  7.959834  0.504150  7.949768
71   0.496338  8.073973  0.497314  8.058049
72   0.502319  7.977891  0.499756  8.018380
73   0.493652  8.115290  0.504639  7.939735
74   0.489990  8.172843  0.496704  8.065370
75   0.500122  8.010426  0.505859  7.918490
76   0.498169  8.040607  0.496094  8.073188
77   0.507202  7.895572  0.504761  7.933956
78   0.497192  8.054057  0.497314  8.051538
79   0.493164  8.117113  0.508423  7.873242
80   0.496826  8.057493  0.496704  8.058795
81   0.495483  8.077592  0.492798  8.119727
82   0.492554  8.122923  0.506226  7.904247
83   0.494141  8.096182  0.494507  8.089600
84   0.505615  7.911748  0.492432  8.121154
85   0.502197  7.964683  0.494751  8.082598
86   0.503296  7.945567  0.499268  8.008971
87   0.493774  8.095724  0.500732  7.983965
88   0.502075  7.961726  0.505737  7.902503
89   0.501221  7.973671  0.497070  8.038995
90   0.502563  7.950585  0.498291  8.017858
91   0.502808  7.945025  0.515137  7.747641
92   0.502319  7.951168  0.504395  7.917272
93   0.501343  7.965133  0.510010  7.826172
94   0.489502  8.152352  0.494019  8.079588
95   0.497070  8.030205  0.507446  7.864063
96   0.503418  7.927592  0.498657  8.002806
97   0.501343  7.959343  0.500732  7.968434
98   0.500244  7.975614  0.503052  7.930261
99   0.509399  7.828509  0.502930  7.931108
100  0.499390  7.987038  0.495728  8.044927
101  0.499634  7.982197  0.488647  8.156902
102  0.504028  7.911288  0.492432  8.095773
103  0.497070  8.021464  0.502686  7.931598
104  0.509521  7.822306  0.496826  8.024401
105  0.494507  8.061109  0.490601  8.123128
106  0.501953  7.941916  0.504272  7.904725
107  0.497437  8.013517  0.510620  7.803161
108  0.496216  8.032645  0.506592  7.867081
109  0.513062  7.763813  0.514282  7.744235
110  0.489380  8.141138  0.500000  7.971736
111  0.500122  7.969712  0.499634  7.977425
112  0.506104  7.874223  0.504639  7.897522
113  0.509155  7.825472  0.484497  8.218542
114  0.500000  7.971356  0.505249  7.887644
115  0.496948  8.019955  0.499756  7.975174
116  0.495361  8.045216  0.496338  8.029633
117  0.510254  7.807767  0.500000  7.971229
118  0.504395  7.901162  0.507935  7.844718
119  0.496094  8.033484  0.502319  7.934229
120  0.491943  8.099644  0.496582  8.025690
121  0.499023  7.986766  0.501953  7.940059
122  0.498535  7.994548  0.490234  8.126882
123  0.505737  7.879727  0.503052  7.922541
124  0.499390  7.980923  0.494019  8.066551
125  0.505737  7.879726  0.507202  7.856373
126  0.509155  7.825235  0.501099  7.953678
127  0.498169  8.000384  0.498779  7.990653

2018-02-25 12:06:26.683677 Finish.
Total elapsed time: 15:40:09.68.
