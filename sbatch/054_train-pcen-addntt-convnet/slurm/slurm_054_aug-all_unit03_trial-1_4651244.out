2018-02-24 20:27:40.941438: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:40.941686: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:40.941705: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:40.941714: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:40.941723: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:47.104808 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.972412  0.116850  0.868286  0.444391
1    0.968994  0.122806  0.852295  0.446124
2    0.972900  0.103896  0.846924  0.494496
3    0.968750  0.121064  0.855835  0.421830
4    0.970093  0.120365  0.834229  0.458799
5    0.957275  0.153836  0.506592  1.003481
6    0.635376  0.676224  0.592407  0.852115
7    0.836304  1.170109  0.504517  7.949680
8    0.502197  7.973943  0.497070  8.048190
9    0.500244  7.993232  0.492554  8.112120
10   0.510010  7.785006  0.493408  8.107605
11   0.500000  7.997317  0.504395  7.923302
12   0.502808  7.945964  0.489868  8.149947
13   0.500366  7.980773  0.499268  7.996631
14   0.491211  8.123692  0.494263  8.073753
15   0.499268  7.992864  0.499268  7.991832
16   0.502075  7.946178  0.506104  7.881116
17   0.502197  7.942658  0.500244  7.973104
18   0.505859  7.882980  0.498169  8.005013
19   0.507935  7.848828  0.504761  7.898956
20   0.498657  7.995852  0.500122  7.972113
21   0.499512  7.981509  0.496094  8.035686
22   0.498901  7.990656  0.494873  8.054624
23   0.498291  7.999918  0.499878  7.974417
24   0.533813  6.170460  0.496582  0.937192
25   0.700806  0.630244  0.506104  1.706286
26   0.737549  0.582600  0.505127  1.595383
27   0.729126  0.581512  0.500122  1.427834
28   0.735840  0.572508  0.495728  1.520486
29   0.744995  0.567142  0.503052  1.531952
30   0.748169  0.559700  0.500610  1.542998
31   0.744507  0.554595  0.501099  1.439321
32   0.644775  3.635935  0.515869  7.767210
33   0.500854  8.005355  0.504761  7.941867
34   0.502319  7.979621  0.498779  8.034900
35   0.502686  7.971508  0.503540  7.956776
36   0.497925  8.045224  0.497314  8.053888
37   0.497559  8.048964  0.497192  8.053776
38   0.505005  7.928231  0.495850  8.073198
39   0.497192  8.050830  0.511475  7.822181
40   0.499878  8.006134  0.501831  7.974074
41   0.490356  8.156115  0.501343  7.980079
42   0.496094  8.062905  0.506592  7.894691
43   0.496338  8.057345  0.493896  8.095455
44   0.502319  7.960398  0.497070  8.043311
45   0.510620  7.826561  0.493774  8.094396
46   0.497803  8.029485  0.500854  7.980153
47   0.502075  7.960048  0.503296  7.939953
48   0.494751  8.075580  0.507080  7.878435
49   0.491333  8.128924  0.507324  7.873440
50   0.495117  8.067533  0.491943  8.117625
51   0.492554  8.107416  0.501343  7.966827
52   0.501465  7.964435  0.496826  8.037948
53   0.491333  8.125104  0.511963  7.795802
54   0.493164  8.095105  0.498169  8.014924
55   0.493408  8.090444  0.505371  7.899352
56   0.495605  8.054674  0.510132  7.822725
57   0.504028  7.919673  0.506226  7.884285
58   0.494019  8.078541  0.496338  8.041211
59   0.494019  8.077834  0.509277  7.834218
60   0.504028  7.917547  0.507324  7.864645
61   0.493652  8.082253  0.494751  8.064379
62   0.505859  7.886927  0.493042  8.090906
63   0.487671  8.176175  0.501831  7.950067
64   0.502686  7.936087  0.504639  7.904590
65   0.499878  7.980133  0.497314  8.020646
66   0.501953  7.946347  0.498169  8.006329
67   0.501343  7.955394  0.494141  8.069878
68   0.505493  7.888568  0.504883  7.897978
69   0.501953  7.944377  0.512817  7.770872
70   0.502441  7.936004  0.504272  7.906530
71   0.493530  8.077522  0.493286  8.081155
72   0.492798  8.088699  0.499878  7.975591
73   0.510254  7.809958  0.500732  7.961543
74   0.497192  8.017789  0.490479  8.124640
75   0.504395  7.902619  0.497559  8.011441
76   0.497314  8.015191  0.500366  7.966403
77   0.500488  7.964336  0.499878  7.973953
78   0.499756  7.975799  0.503296  7.919269
79   0.511963  7.781015  0.505615  7.882136
80   0.497925  8.004675  0.498291  7.998777
81   0.500366  7.965643  0.497559  8.010356
82   0.489624  8.136813  0.502930  7.924654
83   0.504395  7.901272  0.504028  7.907085
84   0.496094  8.033560  0.505981  7.875908
85   0.494995  8.051042  0.500977  7.955670
86   0.501343  7.949822  0.500610  7.961490
87   0.492676  8.087980  0.497681  8.008184
88   0.511108  7.794111  0.502686  7.928388
89   0.500977  7.955630  0.507690  7.848593
90   0.500000  7.971196  0.499878  7.973141
91   0.497925  8.004278  0.507935  7.844697
92   0.501221  7.951732  0.496704  8.023738
93   0.501099  7.953678  0.490723  8.119095
94   0.493896  8.068497  0.489136  8.144395
95   0.510376  7.805775  0.499390  7.980923
96   0.500977  7.971989  0.504028  8.029944
97   0.497314  8.139382  0.493408  8.202308
98   0.500122  8.094000  0.501465  8.072259
99   0.508179  7.963938  0.500610  8.085815
100  0.505249  8.010929  0.497681  8.132792
101  0.498901  8.112983  0.507812  7.969212
102  0.497803  8.130402  0.496704  8.147953
103  0.504517  8.021864  0.497925  8.127937
104  0.493774  8.194647  0.501831  8.064595
105  0.503052  8.044713  0.504883  8.014985
106  0.503662  8.034431  0.504761  8.016484
107  0.491699  8.226758  0.494507  8.181240
108  0.504272  8.023558  0.503540  8.035073
109  0.507568  7.969839  0.494263  8.183983
110  0.488770  8.272187  0.508057  7.960969
111  0.510010  7.929124  0.500366  8.084183
112  0.501465  8.066081  0.496582  8.144376
113  0.493774  8.189205  0.496582  8.143513
114  0.497681  8.125351  0.504517  8.014701
115  0.498657  8.108660  0.508911  7.942890
116  0.506226  7.985664  0.509766  7.928081
117  0.494507  8.173487  0.489624  8.251639
118  0.502319  8.046455  0.501221  8.063592
119  0.503784  8.021694  0.510376  7.914857
120  0.500244  8.077567  0.501221  8.061222
121  0.496582  8.135382  0.498291  8.107221
122  0.492310  8.203015  0.502686  8.035152
123  0.498169  8.107333  0.494629  8.163767
124  0.500977  8.060837  0.495605  8.146787
125  0.499268  8.087146  0.498413  8.100302
126  0.488403  8.261033  0.497925  8.106957
127  0.500122  8.070944  0.497070  8.119535

2018-02-25 11:13:30.917782 Finish.
Total elapsed time: 14:46:43.92.
