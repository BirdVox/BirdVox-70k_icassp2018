2018-02-24 20:27:49.578404: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:49.578637: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:49.578650: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:48.442714 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit07, unit10, unit01.
Validation set: unit02, unit03.
Test set: unit05.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.937622  0.215882  0.907837  0.530702
1    0.939697  0.211686  0.931641  0.374713
2    0.942749  0.199617  0.953125  0.216169
3    0.939941  0.195819  0.943726  0.296151
4    0.906616  0.306075  0.943848  0.299164
5    0.935669  0.217662  0.911865  0.948521
6    0.939941  0.190990  0.940918  0.358455
7    0.947388  0.177684  0.946533  0.374225
8    0.948975  0.165810  0.923828  0.593127
9    0.951416  0.168879  0.935181  0.661653
10   0.651245  5.449886  0.498413  8.139262
11   0.500610  8.096929  0.502197  8.065352
12   0.507690  7.972148  0.500854  8.078093
13   0.499512  8.096255  0.502441  8.045821
14   0.503174  8.031315  0.503052  8.030769
15   0.504639  8.003053  0.504761  7.999086
16   0.497803  8.109525  0.506348  7.970195
17   0.491455  8.208858  0.508789  7.928177
18   0.499023  8.084471  0.494263  8.160164
19   0.496338  8.125821  0.511353  7.882975
20   0.508057  7.935378  0.510620  7.893385
21   0.503174  8.012826  0.499023  8.079180
22   0.502319  8.025592  0.494507  8.151080
23   0.493774  8.162514  0.496582  8.116913
24   0.500488  8.053656  0.513184  7.848754
25   0.501343  8.039370  0.506592  7.954545
26   0.495728  8.129469  0.499634  8.066333
27   0.516235  7.798599  0.503052  8.010954
28   0.500732  8.048220  0.503174  8.008759
29   0.496704  8.112946  0.501465  8.036125
30   0.497437  8.100980  0.488770  8.240607
31   0.504150  8.039891  0.490723  8.326868
32   0.507446  8.033984  0.497559  8.176802
33   0.499390  8.137143  0.491699  8.252538
34   0.499512  8.120322  0.496704  8.159990
35   0.503296  8.049321  0.498169  8.127926
36   0.499146  8.108856  0.503418  8.036906
37   0.494507  8.177916  0.494995  8.167589
38   0.506226  7.984449  0.498657  8.104428
39   0.503174  8.029865  0.501465  8.055733
40   0.494141  8.172296  0.496704  8.129554
41   0.503296  8.022031  0.498535  8.097542
42   0.495850  8.139725  0.501343  8.050125
43   0.491699  8.204600  0.496826  8.121039
44   0.494629  8.155615  0.493164  8.178416
45   0.496704  8.120623  0.497192  8.112045
46   0.494873  8.148787  0.505981  7.969123
47   0.499023  8.080716  0.494995  8.145109
48   0.513306  7.849496  0.501343  8.041854
49   0.495361  8.137850  0.499878  8.064656
50   0.506958  7.950189  0.497681  8.099387
51   0.494873  8.144346  0.504395  7.990598
52   0.495972  8.126113  0.503784  7.999959
53   0.497314  8.104037  0.494751  8.145165
54   0.501587  8.034818  0.493530  8.164522
55   0.495483  8.132909  0.503418  8.004896
56   0.496338  8.118908  0.498901  8.077492
57   0.502930  8.012480  0.495483  8.132423
58   0.501587  8.033980  0.499023  8.075238
59   0.500366  8.053544  0.496826  8.110556
60   0.500732  8.047554  0.497192  8.104576
61   0.496460  8.116350  0.504150  7.992366
62   0.509766  7.901834  0.495605  8.130046
63   0.497192  8.104448  0.497681  8.096560
64   0.487549  8.296835  0.499023  8.111877
65   0.490723  8.242564  0.496216  8.151270
66   0.496582  8.143150  0.501221  8.066335
67   0.502441  8.044913  0.500977  8.066874
68   0.500366  8.075254  0.498779  8.099436
69   0.492676  8.196552  0.505371  7.990710
70   0.502930  8.028945  0.500000  8.075084
71   0.496582  8.129177  0.500977  8.057375
72   0.496704  8.125341  0.510864  7.896233
73   0.492798  8.186622  0.504272  8.000888
74   0.504395  7.998196  0.496948  8.117513
75   0.506714  7.959462  0.506592  7.960802
76   0.496826  8.117629  0.493042  8.178065
77   0.501587  8.039828  0.494507  8.153453
78   0.496948  8.113654  0.498779  8.083709
79   0.500000  8.063641  0.494751  8.147867
80   0.506958  7.950771  0.499634  8.068494
81   0.503052  8.013106  0.499146  8.075782
82   0.492920  8.175870  0.502930  8.014285
83   0.494873  8.143921  0.499023  8.076813
84   0.494507  8.149422  0.502441  8.021350
85   0.491089  8.204169  0.500854  8.046611
86   0.498169  8.089759  0.505371  7.973543
87   0.504150  7.993102  0.505615  7.969381
88   0.501709  8.032246  0.499146  8.073473
89   0.505981  7.963210  0.493286  8.167759
90   0.498047  8.090960  0.501343  8.037776
91   0.499512  8.067237  0.504272  7.990455
92   0.497314  8.102564  0.501953  8.027760
93   0.502930  8.011989  0.500122  8.057214
94   0.488525  8.244107  0.501465  8.035527
95   0.497803  8.094536  0.497681  8.096489
96   0.493774  8.159439  0.497437  8.100402
97   0.502563  8.017758  0.498901  8.076777
98   0.504639  8.002101  0.496338  8.174957
99   0.500000  8.110096  0.493042  8.217484
100  0.502319  8.067992  0.503906  8.041418
101  0.510620  7.933455  0.500854  8.088297
102  0.497803  8.136223  0.498657  8.121903
103  0.495972  8.164077  0.498169  8.128419
104  0.497070  8.145332  0.502075  8.064941
105  0.510376  7.932013  0.496826  8.147429
106  0.496948  8.144879  0.500122  8.093665
107  0.499756  8.098874  0.494751  8.178019
108  0.500366  8.087833  0.490356  8.246728
109  0.513428  7.878204  0.509521  7.939742
110  0.500000  8.090768  0.484009  8.344911
111  0.498291  8.116385  0.506836  7.979298
112  0.502197  8.052347  0.503296  8.033900
113  0.502686  8.042655  0.506958  7.973533
114  0.500244  8.079516  0.503662  8.023939
115  0.503418  8.026704  0.491943  8.208473
116  0.500488  8.071042  0.505615  7.988066
117  0.500977  8.060737  0.504761  7.999094
118  0.500977  8.058071  0.501221  8.052793
119  0.502808  8.026074  0.506470  7.966238
120  0.493408  8.172988  0.505737  7.974919
121  0.501221  8.045385  0.503296  8.010734
122  0.497070  8.108393  0.502441  8.021147
123  0.498657  8.079844  0.496338  8.115162
124  0.515503  7.807957  0.503540  7.996981
125  0.495972  8.115944  0.493652  8.151203
126  0.500244  8.044398  0.498779  8.066016
127  0.507568  7.924169  0.506958  7.932155

2018-02-25 11:21:22.784427 Finish.
Total elapsed time: 14:54:34.78.
