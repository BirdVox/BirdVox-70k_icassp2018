2018-02-24 20:26:46.600920: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:46.601148: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:46.601161: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:17.597544 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit02, unit03, unit05.
Validation set: unit07, unit10.
Test set: unit01.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.972656  0.114721  0.959351  0.164405
1    0.975220  0.112077  0.956909  0.190314
2    0.977295  0.093347  0.958862  0.155229
3    0.951416  0.395327  0.590210  4.176786
4    0.847778  0.426561  0.914917  0.309205
5    0.957031  0.210162  0.944946  0.241375
6    0.963989  0.164712  0.946655  0.222789
7    0.968384  0.144163  0.955322  0.199674
8    0.974121  0.117575  0.957153  0.201760
9    0.971924  0.146048  0.955566  0.209551
10   0.973877  0.117310  0.948608  0.206141
11   0.594482  6.493603  0.497803  8.114605
12   0.502808  8.030889  0.499756  8.074695
13   0.517090  7.684531  0.501099  7.989685
14   0.497559  8.042312  0.504517  7.928235
15   0.494019  8.093257  0.499634  8.001625
16   0.508423  7.859778  0.501099  7.974942
17   0.505371  7.905470  0.505127  7.908087
18   0.501221  7.969258  0.502319  7.950701
19   0.498535  8.010114  0.503906  7.923617
20   0.493652  8.086318  0.505615  7.894866
21   0.503540  7.927291  0.499878  7.985042
22   0.503418  7.928036  0.504150  7.915811
23   0.503784  7.921150  0.506104  7.883691
24   0.495117  8.058396  0.499146  7.993744
25   0.505859  7.886311  0.498901  7.996851
26   0.498779  7.998436  0.491943  8.107064
27   0.497192  8.023052  0.495728  8.046081
28   0.497070  8.024369  0.497192  8.022124
29   0.496948  8.025734  0.500854  7.963181
30   0.506348  7.875344  0.508667  7.838109
31   0.501221  7.956575  0.505859  7.882382
32   0.511475  7.792632  0.501953  7.944201
33   0.495239  8.051022  0.498047  8.006051
34   0.504272  7.906600  0.503052  7.925865
35   0.498169  8.003523  0.504150  7.907983
36   0.495117  8.051822  0.492554  8.092522
37   0.504028  7.909431  0.494995  8.053287
38   0.503052  7.924701  0.492065  8.099708
39   0.502197  7.938051  0.491699  8.105287
40   0.496094  8.035109  0.500977  7.957150
41   0.503174  7.922013  0.503906  7.910233
42   0.503784  7.912083  0.498169  8.001511
43   0.497314  8.015049  0.502808  7.927393
44   0.498291  7.999323  0.500488  7.964221
45   0.493530  8.075083  0.494141  8.065289
46   0.500732  7.960143  0.499878  7.973711
47   0.499023  7.987284  0.498657  7.993074
48   0.493774  8.070875  0.506348  7.870387
49   0.501709  7.944301  0.496582  8.026002
50   0.503784  7.911151  0.487671  8.168006
51   0.485962  8.195225  0.496948  8.020051
52   0.501221  7.951916  0.510864  7.798154
53   0.498413  7.996637  0.497192  8.016080
54   0.497925  8.004389  0.501709  7.944045
55   0.503662  7.912896  0.500610  7.961537
56   0.504517  7.899252  0.505615  7.881728
57   0.494385  8.060761  0.504150  7.905066
58   0.505615  7.881707  0.499146  7.984845
59   0.503540  7.914781  0.495239  8.047111
60   0.496460  8.027646  0.498535  7.994560
61   0.498657  7.992611  0.501709  7.943957
62   0.509277  7.823297  0.507935  7.844703
63   0.493164  8.080179  0.489990  8.130776
64   0.498535  7.994549  0.497925  8.004279
65   0.504639  7.897243  0.497070  8.017900
66   0.491333  8.109366  0.504761  7.895296
67   0.504639  7.897242  0.491821  8.101581
68   0.502686  7.928379  0.496094  8.033468
69   0.489746  8.134664  0.503784  7.910864
70   0.499390  7.967946  0.499756  8.019455
71   0.494385  8.104626  0.502930  7.967937
72   0.497681  8.051186  0.487061  8.220069
73   0.509277  7.865469  0.500000  8.012960
74   0.494019  8.107915  0.501099  7.994636
75   0.502930  7.965046  0.511719  7.824528
76   0.499390  8.020691  0.499512  8.018350
77   0.506714  7.903143  0.506104  7.912484
78   0.498413  8.034706  0.499756  8.012916
79   0.502197  7.973618  0.492188  8.132820
80   0.496826  8.058498  0.504883  7.929685
81   0.495117  8.085009  0.502563  7.965933
82   0.500610  7.996713  0.492432  8.126743
83   0.507202  7.890914  0.493652  8.106578
84   0.512329  7.808481  0.500000  8.004689
85   0.503052  7.955695  0.493896  8.101309
86   0.502441  7.964745  0.499756  8.007220
87   0.506592  7.897902  0.514893  7.765230
88   0.498779  8.021778  0.502563  7.961111
89   0.488403  8.186520  0.504272  7.933187
90   0.495850  8.067126  0.496826  8.051213
91   0.503784  7.939939  0.492432  8.120574
92   0.494263  8.091029  0.499023  8.014772
93   0.500122  7.996894  0.504028  7.934251
94   0.496460  8.054535  0.487427  8.198167
95   0.506836  7.888354  0.504761  7.921047
96   0.497803  8.031578  0.504272  7.928032
97   0.497070  8.042443  0.508545  7.859095
98   0.500366  7.989063  0.486694  8.206597
99   0.504395  7.923982  0.499268  8.005278
100  0.499878  7.995104  0.502441  7.953784
101  0.498413  8.017549  0.494751  8.075469
102  0.494263  8.082786  0.499390  8.000575
103  0.501953  7.959229  0.498047  8.021018
104  0.507935  7.862896  0.498169  8.018087
105  0.496094  8.050672  0.499756  7.991783
106  0.500732  7.975706  0.499390  7.996597
107  0.491821  8.116738  0.503052  7.937174
108  0.502563  7.944435  0.496094  8.047048
109  0.497192  8.029005  0.494873  8.065447
110  0.502197  7.948151  0.497925  8.015729
111  0.497314  8.024930  0.500366  7.975746
112  0.494507  8.068634  0.498901  7.998049
113  0.505493  7.892446  0.494873  8.061242
114  0.500366  7.973169  0.503662  7.920127
115  0.498535  8.001383  0.510742  7.806299
116  0.492432  8.097758  0.503662  7.918270
117  0.508911  7.834163  0.501343  7.954405
118  0.497314  8.018235  0.499634  7.980879
119  0.503174  7.924089  0.500244  7.970453
120  0.503052  7.925379  0.502197  7.938699
121  0.508545  7.837230  0.496338  8.031577
122  0.496338  8.031344  0.504150  7.906573
123  0.496826  8.023144  0.496948  8.021015
124  0.494751  8.055886  0.507935  7.845560
125  0.486206  8.191838  0.501099  7.954298
126  0.497681  8.008692  0.500854  7.958004
127  0.491455  8.107780  0.501465  7.948134

2018-02-25 12:01:09.151708 Finish.
Total elapsed time: 15:34:52.15.
