2018-02-24 20:26:42.865239: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:42.865476: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:42.865489: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:17.166811 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit07, unit10, unit01.
Validation set: unit02, unit03.
Test set: unit05.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.921143  0.237966  0.921753  0.314455
1    0.911133  0.271407  0.895996  0.388538
2    0.913452  0.260186  0.911377  0.529223
3    0.921265  0.221189  0.944946  0.348884
4    0.864258  1.387609  0.501587  7.979656
5    0.506348  7.899629  0.505005  7.917523
6    0.490967  8.138651  0.499146  8.005815
7    0.498901  8.007647  0.494629  8.073830
8    0.501953  7.955397  0.496460  8.041398
9    0.501221  7.964130  0.495850  8.048463
10   0.502197  7.946136  0.490112  8.137732
11   0.489868  8.140694  0.505371  7.892664
12   0.501587  7.952232  0.498047  8.007953
13   0.507812  7.851648  0.491333  8.113791
14   0.504150  7.908954  0.502319  7.937680
15   0.501221  7.954798  0.508423  7.839608
16   0.501831  7.944382  0.496948  8.021933
17   0.504761  7.897136  0.498779  7.992264
18   0.494141  8.066022  0.506104  7.875126
19   0.514771  7.736803  0.490234  8.127828
20   0.497925  8.005108  0.501587  7.946617
21   0.498291  7.999072  0.494873  8.053479
22   0.494019  8.067032  0.497437  8.012477
23   0.499878  7.973501  0.502686  7.928692
24   0.507812  7.846915  0.496826  8.022025
25   0.492188  8.095945  0.506104  7.874061
26   0.493408  8.076430  0.507812  7.846769
27   0.501831  7.942110  0.507080  7.858411
28   0.498413  7.996569  0.506348  7.870061
29   0.502686  7.928433  0.500488  7.963454
30   0.500488  7.963446  0.493164  8.080205
31   0.495117  8.049062  0.490723  8.118513
32   0.500732  8.049377  0.497559  8.113033
33   0.503540  8.014848  0.504272  8.001542
34   0.503418  8.014117  0.501709  8.040550
35   0.506348  7.964829  0.497803  8.101659
36   0.509155  7.917898  0.493164  8.174910
37   0.489624  8.231329  0.500366  8.057582
38   0.498291  8.090507  0.493042  8.174619
39   0.491577  8.197806  0.499634  8.067552
40   0.498779  8.080987  0.507690  7.937041
41   0.505493  7.972191  0.497314  8.103771
42   0.490601  8.211780  0.502686  8.016804
43   0.505981  7.963525  0.502930  8.012572
44   0.504517  7.986878  0.498901  8.077280
45   0.508057  7.929630  0.492554  8.179432
46   0.503906  7.996390  0.500732  8.047492
47   0.506348  7.956944  0.498413  8.084796
48   0.503540  8.002130  0.507202  7.943078
49   0.502197  8.023728  0.503784  7.998132
50   0.499390  8.068950  0.493042  8.171250
51   0.498413  8.084669  0.498901  8.076791
52   0.496704  8.112201  0.492554  8.179092
53   0.505005  7.978398  0.498169  8.088577
54   0.508423  7.923301  0.505737  7.966584
55   0.510254  7.893783  0.507446  7.939034
56   0.503296  8.005930  0.494751  8.143657
57   0.495850  8.125948  0.496826  8.110207
58   0.509521  7.905582  0.494141  8.153491
59   0.506348  7.956737  0.495483  8.131848
60   0.497314  8.102335  0.501465  8.035438
61   0.502930  8.007293  0.500488  8.031039
62   0.505493  7.941595  0.496338  8.080348
63   0.502930  7.970819  0.500488  8.006065
64   0.495972  8.075509  0.496460  8.065519
65   0.497925  8.040523  0.494629  8.091611
66   0.507690  7.882233  0.495239  8.079690
67   0.496094  8.065200  0.504761  7.926215
68   0.501953  7.970263  0.497192  8.045477
69   0.511597  7.815211  0.495972  8.063695
70   0.506714  7.891851  0.503906  7.936026
71   0.494141  8.091139  0.493408  8.102237
72   0.499390  8.006303  0.504395  7.925929
73   0.509155  7.849443  0.488892  8.171897
74   0.494873  8.075934  0.494751  8.077265
75   0.510376  7.827542  0.482300  8.274509
76   0.499268  8.003362  0.496216  8.051363
77   0.509155  7.844420  0.504639  7.915759
78   0.494995  8.068830  0.506104  7.891058
79   0.493896  8.084989  0.501099  7.969484
80   0.499268  7.997994  0.497192  8.030391
81   0.488037  8.175669  0.505127  7.902535
82   0.493164  8.092582  0.505493  7.895356
83   0.499512  7.990858  0.501343  8.047903
84   0.496826  8.105011  0.494873  8.141520
85   0.497803  8.091523  0.495605  8.124207
86   0.500854  8.036611  0.490356  8.202086
87   0.500000  8.044873  0.499634  8.048548
88   0.498169  8.068703  0.491699  8.169721
89   0.503784  7.974128  0.500366  8.026289
90   0.495972  8.093904  0.495972  8.091740
91   0.495239  8.101009  0.499634  8.028866
92   0.498657  8.042259  0.503540  7.962437
93   0.508911  7.874802  0.493896  8.112321
94   0.500610  8.003406  0.501221  7.991888
95   0.500854  7.996036  0.491089  8.150061
96   0.504761  7.930531  0.494751  8.088571
97   0.497925  8.036524  0.494141  8.095431
98   0.499756  8.004575  0.501465  7.976020
99   0.502563  7.957276  0.509033  7.852929
100  0.502686  7.952997  0.509155  7.848749
101  0.499512  8.001454  0.503784  7.932327
102  0.500244  7.987813  0.502319  7.953798
103  0.499512  7.997685  0.489868  8.150571
104  0.493530  8.091385  0.504150  7.921287
105  0.506104  7.889410  0.500244  7.982095
106  0.503418  7.930812  0.497559  8.023552
107  0.491577  8.118276  0.500732  7.971694
108  0.501709  7.955535  0.507812  7.857648
109  0.492065  8.108142  0.505981  7.885743
110  0.496948  8.029238  0.499756  7.983968
111  0.494629  8.065220  0.509277  7.831211
112  0.494507  8.066234  0.499268  7.989889
113  0.500610  7.968058  0.500000  7.977370
114  0.495850  8.043141  0.497559  8.015506
115  0.499878  7.978164  0.508423  7.841577
116  0.498535  7.998873  0.492432  8.095847
117  0.505493  7.887307  0.500732  7.962905
118  0.503052  7.925652  0.507935  7.847538
119  0.496948  8.022440  0.489624  8.138967
120  0.495239  8.049230  0.493286  8.080159
121  0.508179  7.842549  0.503906  7.910483
122  0.500732  7.960922  0.498047  8.003584
123  0.502808  7.927552  0.504150  7.906018
124  0.503540  7.915638  0.498413  7.997269
125  0.502197  7.936850  0.497070  8.018501
126  0.505371  7.886093  0.502930  7.924947
127  0.504272  7.903481  0.494263  8.063006

2018-02-25 11:29:13.309407 Finish.
Total elapsed time: 15:02:56.31.
