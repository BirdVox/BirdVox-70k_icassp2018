2018-02-24 20:26:41.388260: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:41.388454: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:41.388467: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:16.821602 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.494019  8.072619  0.500366  7.970304
1    0.485352  8.208846  0.503540  7.918154
2    0.500122  7.972095  0.505127  7.891819
3    0.491699  8.105512  0.500610  7.963111
4    0.503052  7.923924  0.499146  7.985961
5    0.489990  8.131730  0.488770  8.151021
6    0.489990  8.131426  0.502319  7.933622
7    0.496948  8.106012  0.505127  7.989703
8    0.499878  8.067650  0.497681  8.099373
9    0.508179  7.928928  0.498169  8.089482
10   0.504639  7.984872  0.512817  7.817333
11   0.489868  8.226432  0.494629  8.143768
12   0.498901  8.059271  0.499878  8.030987
13   0.500977  8.005014  0.499268  8.024980
14   0.497070  8.054523  0.494995  8.082708
15   0.507812  7.874471  0.499390  8.005202
16   0.502075  7.959477  0.498657  8.011286
17   0.506226  7.888387  0.490479  8.137354
18   0.490356  8.137545  0.496338  8.040552
19   0.498413  8.006083  0.511597  7.794615
20   0.497192  8.023161  0.501221  7.957923
21   0.509155  7.830571  0.507690  7.853130
22   0.492310  8.097674  0.505615  7.884938
23   0.496704  8.026497  0.500854  7.959866
24   0.497314  8.015925  0.494629  8.058395
25   0.509766  7.816804  0.501953  7.941105
26   0.499023  7.987616  0.501709  7.944627
27   0.507690  7.849133  0.504028  7.907397
28   0.497070  8.018234  0.504028  7.907229
29   0.495239  8.047289  0.506470  7.868199
30   0.505737  7.879840  0.494873  8.053011
31   0.506470  7.868112  0.500000  7.971236
32   0.493164  8.080206  0.507080  7.858342
33   0.503662  7.912826  0.497314  8.014017
34   0.494629  8.056828  0.498657  7.992604
35   0.495850  8.037363  0.511719  7.784369
36   0.495605  8.041253  0.497070  8.017900
37   0.509766  7.815506  0.504395  7.901133
38   0.506470  7.868050  0.500000  7.971192
39   0.500366  7.965354  0.497803  8.006222
40   0.493042  8.082120  0.506836  7.862211
41   0.499390  8.207149  0.497437  8.282411
42   0.503052  8.168413  0.500244  8.196795
43   0.497437  8.231812  0.500366  8.175830
44   0.509521  8.021367  0.504761  8.091689
45   0.500244  8.158885  0.489990  8.318798
46   0.505737  8.060154  0.501831  8.118462
47   0.498413  8.169333  0.495117  8.218391
48   0.505981  8.039600  0.494995  8.213139
49   0.500244  8.125345  0.508667  7.986522
50   0.492065  8.251355  0.505737  8.028350
51   0.500977  8.102714  0.490967  8.261779
52   0.505249  8.029531  0.510132  7.948865
53   0.513916  7.886094  0.507812  7.982758
54   0.496216  8.168112  0.498779  8.125278
55   0.512451  7.903514  0.508789  7.961174
56   0.501709  8.074009  0.503906  8.037332
57   0.500610  8.089256  0.489502  8.267112
58   0.499878  8.098722  0.502319  8.058226
59   0.519043  7.787554  0.498169  8.122882
60   0.501831  8.062751  0.505981  7.994745
61   0.502563  8.048738  0.489014  8.266029
62   0.495117  8.166557  0.503906  8.023790
63   0.493042  8.197809  0.499023  8.100300
64   0.488770  8.264487  0.497314  8.125667
65   0.492920  8.195424  0.499512  8.088098
66   0.500977  8.063430  0.498291  8.105657
67   0.500122  8.075111  0.508667  7.936352
68   0.507690  7.951090  0.502808  8.028795
69   0.501587  8.047508  0.501587  8.046553
70   0.495850  8.138113  0.493408  8.176559
71   0.493530  8.173731  0.500610  8.058767
72   0.508423  7.932045  0.496094  8.129984
73   0.496216  8.127287  0.500610  8.055744
74   0.500488  8.057055  0.502319  8.026908
75   0.504150  7.996817  0.503174  8.012002
76   0.507690  7.938706  0.500610  8.052350
77   0.501709  8.034226  0.499146  8.075150
78   0.488037  8.253858  0.494019  8.157131
79   0.498413  8.086033  0.504517  7.987409
80   0.506470  7.955726  0.499390  8.069658
81   0.491699  8.193465  0.505493  7.971001
82   0.504395  7.988605  0.500732  8.047541
83   0.479492  8.389825  0.495361  8.133985
84   0.497681  8.096559  0.499756  8.063075
85   0.499146  8.072888  0.505371  7.972522
86   0.513062  7.848553  0.497192  8.104322
87   0.509155  7.911497  0.497559  8.098408
88   0.499146  8.072826  0.494141  8.153493
89   0.500244  8.055115  0.493164  8.169231
90   0.489746  8.224322  0.505249  7.974444
91   0.495850  8.125944  0.497925  8.092496
92   0.484009  8.307246  0.510132  8.049554
93   0.506226  8.112459  0.500244  8.202451
94   0.501221  8.182075  0.509277  8.048052
95   0.498169  8.223630  0.501099  8.173145
96   0.497925  8.221407  0.501221  8.165498
97   0.499146  8.196390  0.499634  8.186026
98   0.506348  8.075471  0.497192  8.220732
99   0.491211  8.314946  0.499512  8.178975
100  0.497314  8.212294  0.492432  8.288907
101  0.501343  8.143247  0.501709  8.135317
102  0.504761  8.084146  0.506592  8.052646
103  0.510376  7.989703  0.500732  8.143181
104  0.504150  8.086161  0.495239  8.227852
105  0.492310  8.273157  0.507324  8.029222
106  0.493408  8.251613  0.489624  8.310686
107  0.497070  8.188763  0.511475  7.954676
108  0.499390  8.147563  0.493774  8.236156
109  0.504639  8.059150  0.498901  8.149715
110  0.487427  8.332775  0.488770  8.309232
111  0.493774  8.226684  0.500244  8.120516
112  0.498779  8.142264  0.497437  8.162037
113  0.500977  8.103140  0.495728  8.185902
114  0.507568  7.993244  0.496460  8.170482
115  0.497192  8.156912  0.503296  8.056771
116  0.495605  8.179012  0.499512  8.114342
117  0.496338  8.163843  0.496216  8.164164
118  0.501343  8.079941  0.502686  8.056722
119  0.498291  8.126040  0.500610  8.087157
120  0.504395  8.024726  0.504150  8.027241
121  0.494507  8.181319  0.499146  8.105211
122  0.500610  8.080322  0.498657  8.110540
123  0.503662  8.028667  0.512329  7.887783
124  0.510620  7.914195  0.492432  8.206239
125  0.508789  7.941520  0.495361  8.156893
126  0.494141  8.175558  0.487915  8.274902
127  0.495361  8.153924  0.495483  8.151007

2018-02-25 12:00:53.403964 Finish.
Total elapsed time: 15:34:37.40.
