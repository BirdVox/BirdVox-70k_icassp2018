2018-02-24 20:28:00.690889: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:28:00.691176: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:28:00.691196: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:28:00.691206: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:28:00.691216: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:27:08.003788 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.889160  0.332707  0.622925  1.284456
1    0.668213  4.738388  0.502197  7.973948
2    0.499146  8.018089  0.495605  8.070583
3    0.501953  7.966305  0.497070  8.041343
4    0.505005  7.912520  0.501343  7.968737
5    0.505737  7.896825  0.504761  7.910651
6    0.502197  7.950005  0.508057  7.855161
7    0.511475  7.799419  0.509888  7.823534
8    0.500122  7.978185  0.505127  7.897415
9    0.492065  8.104791  0.500977  7.961920
10   0.508057  7.848345  0.503540  7.919690
11   0.499268  7.987234  0.504883  7.897179
12   0.508057  7.846122  0.497070  8.020841
13   0.502930  7.927063  0.500732  7.961752
14   0.490601  8.122989  0.504272  7.904758
15   0.500366  7.966808  0.491577  8.106717
16   0.499390  7.981993  0.501343  7.950695
17   0.486450  8.187985  0.496338  8.030229
18   0.500732  7.960069  0.496704  8.024198
19   0.494629  8.057207  0.497070  8.018217
20   0.495850  8.037623  0.501221  7.951946
21   0.492554  8.090079  0.495361  8.045284
22   0.495117  8.049149  0.505615  7.881761
23   0.507324  8.010127  0.501465  8.170573
24   0.498291  8.199597  0.500000  8.155530
25   0.503906  8.082249  0.489746  8.301740
26   0.507446  8.009876  0.498047  8.155441
27   0.508667  7.979349  0.497925  8.147892
28   0.496704  8.163549  0.493896  8.204971
29   0.501709  8.075610  0.506714  7.991631
30   0.512939  7.888276  0.500122  8.091959
31   0.490112  8.250641  0.505371  8.002128
32   0.495483  8.159152  0.497070  8.131307
33   0.505859  7.987580  0.508667  7.940336
34   0.501221  8.058553  0.499756  8.080430
35   0.502930  8.027712  0.495728  8.142300
36   0.500000  8.072096  0.501343  8.049172
37   0.489990  8.231015  0.493164  8.178774
38   0.497803  8.103049  0.510864  7.891612
39   0.499878  8.067891  0.511963  7.872347
40   0.504395  7.993674  0.500977  8.048141
41   0.501465  8.039729  0.510132  7.899523
42   0.510620  7.891212  0.483765  8.323657
43   0.505737  7.969143  0.495728  8.130148
44   0.508789  7.919336  0.512207  7.863979
45   0.504272  7.991642  0.493164  8.170478
46   0.498291  8.087664  0.491577  8.195714
47   0.504517  7.987018  0.504517  7.986892
48   0.497314  8.102873  0.491211  8.201155
49   0.496094  8.122377  0.500244  8.055411
50   0.492920  8.173408  0.507568  7.937254
51   0.493896  8.157580  0.499268  8.070974
52   0.496460  8.116202  0.505127  7.976484
53   0.501831  8.029591  0.503052  8.009901
54   0.493042  8.209783  0.499146  8.154153
55   0.492676  8.250441  0.506348  8.025810
56   0.504761  8.044918  0.506470  8.011631
57   0.493408  8.214223  0.495605  8.173673
58   0.494873  8.180170  0.487793  8.287960
59   0.504272  8.020443  0.493408  8.188930
60   0.504883  8.001532  0.505981  7.979617
61   0.502441  8.031870  0.503296  8.014118
62   0.497070  8.109431  0.499878  8.060777
63   0.494385  8.144630  0.494751  8.135108
64   0.494019  8.143257  0.494751  8.128087
65   0.496582  8.095546  0.509277  7.889834
66   0.501099  8.017040  0.494995  8.111191
67   0.499390  8.038111  0.508911  7.883324
68   0.501343  8.001120  0.516113  7.762811
69   0.499023  8.032562  0.498291  8.041570
70   0.500977  7.996218  0.490601  8.159133
71   0.489502  8.174277  0.504639  7.930630
72   0.501221  7.982922  0.500366  7.994388
73   0.495850  8.064370  0.495483  8.068230
74   0.495239  8.070278  0.503296  7.940039
75   0.506958  7.879991  0.504272  7.921187
76   0.503784  7.927484  0.500488  7.978588
77   0.502563  7.944191  0.502686  7.940977
78   0.495361  8.056595  0.500854  7.967917
79   0.498413  8.005849  0.498047  8.010739
80   0.505249  7.895074  0.499023  7.993520
81   0.499146  7.990862  0.491943  8.105006
82   0.500000  7.975972  0.505005  7.895623
83   0.498291  8.002171  0.501221  7.955007
84   0.499023  7.989642  0.503174  7.923105
85   0.504272  7.905274  0.501831  7.943901
86   0.517578  7.692605  0.494751  8.056292
87   0.494873  8.054151  0.499512  7.980020
88   0.487183  8.176426  0.503418  7.917458
89   0.500244  7.967943  0.491211  8.111850
90   0.499390  7.981378  0.498657  7.992977
91   0.501465  7.948155  0.493042  8.082379
92   0.504150  7.905240  0.506348  7.870170
93   0.494995  8.051125  0.504883  7.893464
94   0.507202  7.856466  0.507935  7.844770
95   0.495728  8.039364  0.500610  7.961507
96   0.499390  8.000576  0.496826  8.214266
97   0.491577  8.280312  0.499268  8.144290
98   0.510376  7.959730  0.516968  7.849174
99   0.497192  8.164889  0.508179  7.985070
100  0.502930  8.067334  0.499512  8.120189
101  0.515869  7.854507  0.503784  8.047321
102  0.499268  8.118293  0.494629  8.191277
103  0.494629  8.189614  0.496704  8.154539
104  0.499512  8.107765  0.490601  8.249906
105  0.502441  8.057659  0.489014  8.272722
106  0.500244  8.090427  0.492554  8.213127
107  0.500977  8.076191  0.496216  8.151773
108  0.495605  8.160531  0.505371  8.002069
109  0.503174  8.036492  0.491821  8.218499
110  0.495850  8.152655  0.500000  8.084860
111  0.491699  8.217807  0.489014  8.260260
112  0.505127  7.999758  0.503418  8.026530
113  0.502319  8.043503  0.501221  8.060486
114  0.495850  8.146368  0.510132  7.915484
115  0.503662  8.019112  0.500977  8.061753
116  0.496704  8.129999  0.503174  8.025107
117  0.493408  8.181921  0.503906  8.012128
118  0.493530  8.178807  0.505249  7.989364
119  0.506958  7.961280  0.489868  8.236202
120  0.502930  8.025161  0.507690  7.947917
121  0.502686  8.028096  0.511475  7.885948
122  0.503662  8.011407  0.502441  8.030623
123  0.501221  8.049861  0.496216  8.130098
124  0.497192  8.113948  0.505493  7.979752
125  0.495972  8.132839  0.499756  8.071470
126  0.511475  7.882232  0.494263  8.159307
127  0.505615  7.975998  0.489502  8.235392

2018-02-25 11:52:21.214675 Finish.
Total elapsed time: 15:25:13.21.
