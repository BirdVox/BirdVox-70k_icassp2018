2018-02-24 20:26:35.280995: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:35.281200: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:35.281213: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:16.934354 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit01, unit02, unit03.
Validation set: unit05, unit07.
Test set: unit10.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.960571  0.136393  0.949585  0.161567
1    0.962280  0.126246  0.932251  0.243625
2    0.954956  0.157748  0.957031  0.146872
3    0.956177  0.156651  0.922729  0.292437
4    0.966431  0.131137  0.895142  0.411002
5    0.963013  0.125873  0.956787  0.151619
6    0.959717  0.162184  0.951904  0.169948
7    0.948730  0.251535  0.918823  0.487330
8    0.956787  0.171255  0.930054  0.243462
9    0.971558  0.142416  0.957397  0.197800
10   0.967896  0.151703  0.942261  0.239087
11   0.965942  0.141126  0.861450  0.373512
12   0.932373  0.270243  0.908936  0.364263
13   0.605835  6.193608  0.894897  0.409101
14   0.829956  2.083768  0.502075  7.992312
15   0.498047  8.048645  0.499878  8.013188
16   0.496948  8.055314  0.503540  7.946105
17   0.496216  8.059526  0.503784  7.935791
18   0.504517  7.921547  0.499756  7.995064
19   0.498901  8.006675  0.487305  8.189679
20   0.503784  7.925362  0.501465  7.960846
21   0.506104  7.885618  0.505737  7.890260
22   0.508667  7.842525  0.502808  7.934971
23   0.499268  7.990573  0.502930  7.931405
24   0.505371  7.953122  0.499146  8.079416
25   0.486206  8.277216  0.504639  7.976575
26   0.499512  8.053578  0.490723  8.189536
27   0.503540  7.981915  0.502075  8.002267
28   0.489624  8.198251  0.495850  8.096642
29   0.500122  8.026470  0.510376  7.861036
30   0.500244  8.020799  0.497681  8.059966
31   0.504150  7.955261  0.498169  8.049098
32   0.506226  7.919236  0.503784  7.956764
33   0.495483  8.087782  0.499878  8.016425
34   0.506470  7.910100  0.503906  7.949745
35   0.506836  7.901868  0.497559  8.048610
36   0.496582  8.063065  0.494507  8.095042
37   0.496582  8.060895  0.501099  7.987833
38   0.493042  8.115260  0.501465  7.979971
39   0.492188  8.126905  0.498413  8.026693
40   0.497559  8.039392  0.500488  7.991770
41   0.495972  8.062897  0.495605  8.067866
42   0.497681  8.033950  0.500000  7.996150
43   0.490234  8.151049  0.506836  7.885601
44   0.501099  7.976324  0.499023  8.008672
45   0.504150  7.926235  0.492310  8.114313
46   0.496582  8.045540  0.500854  7.976776
47   0.491333  8.127950  0.504517  7.917160
48   0.496216  8.048910  0.497925  8.021089
49   0.501099  7.969941  0.504639  7.912962
50   0.501953  7.955258  0.503174  7.935285
51   0.506958  7.874467  0.493652  8.086106
52   0.496460  8.040882  0.505493  7.896411
53   0.496216  8.043873  0.504639  7.909154
54   0.503174  7.932085  0.493530  8.085406
55   0.485840  8.207605  0.494019  8.076814
56   0.500854  7.967443  0.497559  8.019599
57   0.508667  7.842128  0.488647  8.160912
58   0.496460  8.035998  0.498779  7.998658
59   0.508057  7.850403  0.512939  7.772207
60   0.502319  7.941175  0.491333  8.115983
61   0.506104  7.880175  0.502686  7.934337
62   0.504150  7.910665  0.507324  7.859751
63   0.497437  8.017079  0.505249  7.892226
64   0.506104  7.878313  0.498901  7.992844
65   0.509521  7.823259  0.500000  7.974783
66   0.501099  7.957011  0.498413  7.999571
67   0.501221  7.954573  0.505615  7.884280
68   0.498535  7.996935  0.506714  7.866335
69   0.489258  8.144429  0.498169  8.002174
70   0.495728  8.040921  0.499756  7.976531
71   0.493042  8.083414  0.500488  7.964556
72   0.503662  7.913828  0.507446  7.853375
73   0.489624  8.137395  0.504883  7.894030
74   0.497314  8.014598  0.501953  7.940563
75   0.495483  8.043634  0.499512  7.979346
76   0.501831  7.942314  0.501831  7.942262
77   0.498779  7.990872  0.502686  7.928558
78   0.496460  8.027778  0.502441  7.932391
79   0.500610  7.961560  0.505493  7.883696
80   0.499512  7.979039  0.506470  7.868098
81   0.500977  7.955662  0.496094  8.033496
82   0.501465  7.947862  0.515137  7.729894
83   0.495117  8.049049  0.504272  7.903088
84   0.501709  7.943954  0.493530  8.074340
85   0.502319  7.934220  0.497314  8.014009
86   0.496704  8.023738  0.510254  7.807722
87   0.499023  7.986762  0.499512  7.978977
88   0.500000  7.971193  0.506958  7.860265
89   0.499146  7.984815  0.507202  7.856373
90   0.506958  7.908195  0.491699  8.367450
91   0.496704  8.254227  0.500977  8.167884
92   0.494141  8.270468  0.487549  8.370885
93   0.497070  8.213690  0.499390  8.173152
94   0.502319  8.123589  0.497314  8.202160
95   0.495361  8.231922  0.502808  8.110298
96   0.497437  8.195459  0.499146  8.166557
97   0.511230  7.970516  0.496704  8.203420
98   0.494385  8.239623  0.498535  8.171549
99   0.513184  7.934292  0.494263  8.238102
100  0.502808  8.099222  0.504639  8.068545
101  0.490723  8.291677  0.501953  8.109479
102  0.497681  8.177150  0.493164  8.248737
103  0.494507  8.225869  0.504028  8.071156
104  0.489014  8.311905  0.500732  8.121741
105  0.502441  8.092901  0.491333  8.270631
106  0.494995  8.210275  0.490234  8.285656
107  0.499878  8.128855  0.507446  8.005479
108  0.495972  8.189027  0.503784  8.061680
109  0.503296  8.068115  0.500366  8.113879
110  0.497559  8.157664  0.505005  8.036153
111  0.504028  8.050393  0.500977  8.098061
112  0.505249  8.027668  0.499390  8.120560
113  0.498169  8.138682  0.499512  8.115466
114  0.499268  8.117828  0.506470  8.000152
115  0.505371  8.016274  0.502441  8.061895
116  0.491211  8.241320  0.496338  8.157084
117  0.511841  7.905624  0.501465  8.071276
118  0.501587  8.067747  0.504517  8.018961
119  0.493896  8.188607  0.504150  8.021805
120  0.498047  8.118697  0.502808  8.040484
121  0.498535  8.107920  0.497559  8.122244
122  0.508179  7.949708  0.505127  7.997552
123  0.500732  8.067103  0.491089  8.221276
124  0.510620  7.905274  0.503906  8.012314
125  0.499390  8.084008  0.499756  8.077023
126  0.505249  7.987472  0.498657  8.092731
127  0.493896  8.168547  0.497192  8.114529

2018-02-25 11:53:51.716402 Finish.
Total elapsed time: 15:27:35.72.
