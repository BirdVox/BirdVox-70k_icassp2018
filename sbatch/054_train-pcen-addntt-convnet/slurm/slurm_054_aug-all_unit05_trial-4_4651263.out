2018-02-24 20:27:45.837411: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:45.837703: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:45.837717: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:45.837723: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:45.837730: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:50.483740 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit07, unit10, unit01.
Validation set: unit02, unit03.
Test set: unit05.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.932251  0.204516  0.943726  0.404504
1    0.939087  0.186684  0.946045  0.256268
2    0.941895  0.175078  0.956177  0.294541
3    0.946167  0.178889  0.948486  0.282229
4    0.947266  0.172485  0.945679  0.319639
5    0.947144  0.155578  0.951660  0.239776
6    0.949585  0.148389  0.955322  0.233265
7    0.708740  0.647910  0.711914  0.736956
8    0.760864  0.520661  0.720947  0.789807
9    0.641968  3.752557  0.497681  8.125418
10   0.495117  8.164307  0.502808  8.038268
11   0.502075  8.048437  0.497803  8.115787
12   0.506592  7.972806  0.501587  8.052215
13   0.491943  8.206506  0.497314  8.118824
14   0.488525  8.259462  0.496948  8.122705
15   0.488403  8.259511  0.502563  8.030378
16   0.494873  8.153503  0.505615  7.979553
17   0.502075  8.035870  0.504272  7.999735
18   0.507080  7.953822  0.505005  7.986633
19   0.502808  8.021468  0.505249  7.981557
20   0.504517  7.992854  0.494995  8.145833
21   0.497192  8.109976  0.502075  8.030851
22   0.500366  8.058015  0.494995  8.144221
23   0.499756  8.067159  0.503174  8.011753
24   0.508667  7.922933  0.502808  8.017106
25   0.501221  8.042442  0.496338  8.120913
26   0.500122  8.059713  0.502563  8.020164
27   0.504761  7.984571  0.497925  8.094583
28   0.496094  8.123943  0.507202  7.944751
29   0.507446  7.940684  0.503784  7.999583
30   0.492798  8.176548  0.496216  8.121347
31   0.500122  8.058286  0.506226  7.959814
32   0.503662  8.001046  0.507935  7.932099
33   0.508179  7.928089  0.497681  8.097225
34   0.497070  8.106998  0.498413  8.085292
35   0.498291  8.087203  0.501587  8.034025
36   0.497925  8.093002  0.499756  8.063442
37   0.500977  8.043724  0.497925  8.092872
38   0.509521  7.905919  0.494751  8.143956
39   0.501953  8.027840  0.495239  8.136025
40   0.503906  7.996303  0.494263  8.151713
41   0.499512  8.067087  0.505005  7.978526
42   0.492310  8.183132  0.503418  8.004068
43   0.494751  8.143749  0.499512  8.067001
44   0.506958  7.946969  0.502563  8.017789
45   0.499878  8.061066  0.494507  8.147630
46   0.497192  8.104337  0.499023  8.074818
47   0.504883  7.980370  0.508179  7.927242
48   0.513672  7.838699  0.503052  8.009872
49   0.498657  8.080701  0.507324  7.941004
50   0.499390  8.068892  0.490967  8.204651
51   0.500854  8.045279  0.495850  8.125947
52   0.494751  8.143654  0.486694  8.273512
53   0.493774  8.159394  0.495728  8.127913
54   0.502197  8.023633  0.498169  8.082402
55   0.506348  7.942133  0.505493  7.943688
56   0.500122  8.026613  0.501343  8.005379
57   0.501465  8.002419  0.500244  8.021005
58   0.497070  8.070889  0.496216  8.083831
59   0.505493  7.935293  0.494751  8.105916
60   0.506958  7.910680  0.507080  7.908098
61   0.509766  7.864637  0.505249  7.935980
62   0.498535  8.042334  0.500610  8.008550
63   0.493286  8.124592  0.505493  7.929239
64   0.506226  7.916793  0.489136  8.188455
65   0.495972  8.078660  0.505249  7.929920
66   0.498901  8.030257  0.501709  7.984616
67   0.501099  7.993444  0.502197  7.975007
68   0.496582  8.063586  0.507202  7.893315
69   0.499268  8.018836  0.501709  7.978922
70   0.503296  7.952619  0.494141  8.097557
71   0.503662  7.944737  0.497192  8.046842
72   0.508545  7.864817  0.496582  8.054484
73   0.506958  7.888022  0.493652  8.099094
74   0.502563  7.955989  0.505371  7.910182
75   0.503784  7.934454  0.498535  8.017106
76   0.508545  7.856520  0.505371  7.906114
77   0.497803  8.025797  0.506348  7.888601
78   0.501709  7.961619  0.499878  7.989884
79   0.497192  8.031813  0.489746  8.149651
80   0.497314  8.028164  0.499756  7.988426
81   0.497192  8.028526  0.495605  8.053073
82   0.495483  8.054316  0.509766  7.825937
83   0.493408  8.086076  0.508179  7.849979
84   0.495605  8.049854  0.502441  7.940318
85   0.497070  8.025436  0.506836  7.869254
86   0.503052  7.929131  0.507812  7.852795
87   0.498779  7.996405  0.495361  8.050507
88   0.510742  7.804944  0.495728  8.043969
89   0.494873  8.057276  0.501099  7.957719
90   0.489746  8.138423  0.509888  7.817045
91   0.498047  8.005563  0.503662  7.915797
92   0.504028  7.909732  0.499146  7.987355
93   0.499512  7.981313  0.500000  7.973330
94   0.506836  7.864167  0.490601  8.122820
95   0.499268  7.984485  0.499512  7.980437
96   0.489380  8.141819  0.498657  7.993779
97   0.491943  8.100690  0.505981  7.876770
98   0.501099  7.954508  0.503540  7.915485
99   0.506958  7.860905  0.505981  7.876390
100  0.494141  8.065087  0.499023  7.987175
101  0.496704  8.024091  0.500732  7.959816
102  0.502930  7.924740  0.500977  7.955835
103  0.500488  7.963584  0.495972  8.035558
104  0.501099  7.953796  0.496460  8.027724
105  0.504272  7.903156  0.489990  8.130833
106  0.494263  8.062707  0.499268  7.982906
107  0.486572  8.185291  0.498413  7.996513
108  0.494263  8.062675  0.509399  7.821355
109  0.508789  7.831083  0.495605  8.041258
110  0.492676  8.087962  0.500610  7.961465
111  0.500488  7.963410  0.510986  7.796046
112  0.492676  8.087959  0.497803  8.006223
113  0.508057  7.842751  0.507935  7.844697
114  0.496094  8.033467  0.491089  8.113257
115  0.490479  8.122988  0.502563  7.930324
116  0.501831  7.942001  0.500977  7.955624
117  0.483887  8.228076  0.496826  8.021791
118  0.502319  7.934217  0.503174  7.920594
119  0.500977  7.955624  0.505981  7.875834
120  0.498413  7.996492  0.493408  8.076027
121  0.501709  7.943947  0.511230  7.736664
122  0.498657  8.102090  0.503174  8.108935
123  0.509644  7.998557  0.502930  8.102239
124  0.499268  8.158040  0.503906  8.080355
125  0.503662  8.081880  0.497925  8.172118
126  0.499146  8.150548  0.504150  8.068106
127  0.499512  8.141348  0.500244  8.128109

2018-02-25 11:51:16.064197 Finish.
Total elapsed time: 15:24:26.06.
