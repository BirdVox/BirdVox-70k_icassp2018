2018-02-24 20:26:41.406139: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:41.406346: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:41.406359: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:17.920480 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit10, unit01, unit02.
Validation set: unit03, unit05.
Test set: unit07.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.915894  0.260729  0.962524  0.160704
1    0.927246  0.224304  0.968750  0.129888
2    0.927002  0.230393  0.957275  0.184685
3    0.905640  0.301278  0.963257  0.162112
4    0.920654  0.254790  0.959229  0.168240
5    0.874268  0.510702  0.930176  0.321312
6    0.902954  0.303673  0.952026  0.194828
7    0.924561  0.248886  0.952271  0.192675
8    0.889648  0.951931  0.499023  8.051353
9    0.506714  7.919671  0.498291  8.045015
10   0.508423  7.877336  0.502930  7.959515
11   0.501099  7.984497  0.508179  7.867812
12   0.503906  7.932825  0.504272  7.924143
13   0.503174  7.939302  0.494873  8.069461
14   0.500977  7.970339  0.502808  7.939464
15   0.506958  7.871885  0.499878  7.983448
16   0.511353  7.799414  0.503540  7.922942
17   0.507812  7.853970  0.503662  7.919339
18   0.500488  7.969265  0.501953  7.945288
19   0.501953  7.944762  0.500244  7.971519
20   0.497437  8.015866  0.498291  8.001858
21   0.503540  7.917850  0.500854  7.960359
22   0.491333  8.111895  0.496216  8.033808
23   0.503296  7.920726  0.505493  7.885501
24   0.485840  8.198653  0.502319  7.935771
25   0.498657  7.994015  0.504150  7.906311
26   0.502808  7.934527  0.490723  8.305366
27   0.502808  8.119930  0.510498  7.970321
28   0.494507  8.214231  0.504639  8.039859
29   0.504395  8.036265  0.498657  8.122241
30   0.506226  7.995332  0.504883  8.012567
31   0.495605  8.158561  0.497314  8.127774
32   0.496094  8.144751  0.501587  8.053704
33   0.500732  8.065342  0.503906  8.012186
34   0.503174  8.022267  0.499512  8.079670
35   0.494019  8.166802  0.498291  8.096613
36   0.505005  7.987248  0.501343  8.045193
37   0.487183  8.272493  0.501343  8.043381
38   0.500732  8.052466  0.494385  8.154074
39   0.493042  8.175119  0.493652  8.164724
40   0.507202  7.945859  0.500977  8.045770
41   0.500122  8.059182  0.494141  8.155260
42   0.506592  7.954299  0.501465  8.036687
43   0.503540  8.003038  0.500854  8.046141
44   0.510498  7.890559  0.504761  7.982902
45   0.498291  8.087078  0.511963  7.866621
46   0.499268  8.071173  0.506714  7.951089
47   0.504517  7.986455  0.502808  8.013957
48   0.497803  8.094594  0.504395  7.988317
49   0.498901  8.076835  0.493896  8.157486
50   0.502319  8.021712  0.507690  7.935129
51   0.500610  8.049238  0.505249  7.974464
52   0.495972  8.123992  0.496826  8.110215
53   0.507446  7.939036  0.502563  8.017735
54   0.489014  8.236131  0.500244  8.055116
55   0.495483  8.131849  0.504395  7.988218
56   0.503540  8.001990  0.512695  7.854424
57   0.498901  8.076756  0.505493  7.970509
58   0.499268  8.070853  0.492676  8.177100
59   0.496704  8.093143  0.496826  8.134166
60   0.499878  8.180837  0.505005  8.084748
61   0.499512  8.159650  0.493042  8.251755
62   0.495117  8.210014  0.503174  8.073646
63   0.498535  8.140929  0.497681  8.148291
64   0.504761  8.029936  0.505249  8.016923
65   0.499146  8.109527  0.500244  8.087481
66   0.496460  8.143659  0.499023  8.098759
67   0.507202  7.964628  0.494995  8.155580
68   0.494263  8.163831  0.492798  8.183823
69   0.511230  7.886793  0.493530  8.165858
70   0.504150  7.993592  0.498901  8.074361
71   0.495117  8.131919  0.497559  8.090263
72   0.489380  8.218044  0.493530  8.149301
73   0.500977  8.028130  0.508301  7.908932
74   0.502075  8.005857  0.489990  8.196218
75   0.501099  8.016920  0.500732  8.020575
76   0.499634  8.035996  0.505371  7.942454
77   0.505859  7.932676  0.507935  7.897616
78   0.501099  8.004694  0.494507  8.107894
79   0.504150  7.952333  0.501465  7.993339
80   0.491699  8.147280  0.495728  8.081323
81   0.498657  8.032938  0.498291  8.037106
82   0.504395  7.938186  0.496948  8.055290
83   0.490601  8.154930  0.504150  7.937365
84   0.499268  8.013710  0.494995  8.080331
85   0.497192  8.043861  0.507446  7.878956
86   0.505493  7.908713  0.498901  8.012430
87   0.500244  7.989705  0.496948  8.040943
88   0.501221  7.971579  0.500977  7.974235
89   0.498779  8.008087  0.500244  7.983572
90   0.500366  7.980527  0.496948  8.033936
91   0.499146  7.997890  0.498413  8.008571
92   0.502930  7.935636  0.510010  7.821856
93   0.496338  8.038978  0.503296  7.927235
94   0.490479  8.130826  0.485718  8.205999
95   0.501465  7.954294  0.494263  8.068480
96   0.505249  7.892760  0.509644  7.822154
97   0.489258  8.146663  0.502319  7.937966
98   0.495483  8.046537  0.498535  7.997495
99   0.494507  8.061376  0.507446  7.854770
100  0.499512  7.980989  0.500610  7.963214
101  0.489258  8.143978  0.499756  7.976406
102  0.493530  8.075481  0.497925  8.005259
103  0.503052  7.923386  0.500854  7.958290
104  0.497559  8.010730  0.492432  8.092370
105  0.505859  7.878221  0.500977  7.955992
106  0.505249  7.887820  0.507324  7.854684
107  0.500854  7.957784  0.497559  8.010290
108  0.505371  7.885709  0.503418  7.916819
109  0.507446  7.852577  0.497314  8.014083
110  0.505981  7.875896  0.508423  7.836961
111  0.499756  7.975123  0.505371  7.885594
112  0.487305  8.173609  0.495972  8.035431
113  0.494629  8.056834  0.508301  7.838868
114  0.510254  7.807728  0.495605  8.041257
115  0.502319  7.934221  0.500610  7.961465
116  0.498413  7.996494  0.496094  8.033468
117  0.503662  7.939143  0.505127  8.107388
118  0.503418  8.126220  0.499023  8.188617
119  0.494995  8.247745  0.500977  8.146107
120  0.497925  8.191019  0.493652  8.255940
121  0.507812  8.024420  0.503174  8.096149
122  0.504517  8.071976  0.513672  7.922076
123  0.496216  8.201500  0.504150  8.071829
124  0.501953  8.105778  0.507324  8.017861
125  0.490112  8.294180  0.510254  7.968521
126  0.496704  8.186078  0.516235  7.870496
127  0.499878  8.133490  0.491943  8.260765

2018-02-25 12:09:54.944028 Finish.
Total elapsed time: 15:43:37.94.
