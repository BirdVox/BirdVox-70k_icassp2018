2018-02-24 20:27:45.652542: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:45.652844: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:45.652864: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:45.652873: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:45.652881: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:47.104761 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit07, unit10, unit01.
Validation set: unit02, unit03.
Test set: unit05.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.502930  8.028754  0.505981  7.977924
1    0.505493  7.984416  0.500244  8.067736
2    0.489624  8.237814  0.496582  8.124632
3    0.499390  8.078485  0.503174  8.016646
4    0.504028  8.002131  0.506226  7.966010
5    0.495972  8.130660  0.506958  7.952986
6    0.505493  7.976066  0.507202  7.948014
7    0.502441  8.024294  0.501343  8.041567
8    0.496460  8.119880  0.502686  8.019164
9    0.492432  8.184105  0.494507  8.150339
10   0.503784  8.000523  0.498413  8.086826
11   0.500244  8.057074  0.503296  8.007659
12   0.496948  8.109773  0.502563  8.019078
13   0.496338  8.119258  0.500000  8.060078
14   0.504639  7.985178  0.495239  8.136554
15   0.498657  8.081357  0.501953  8.028135
16   0.496704  8.112656  0.495850  8.126351
17   0.490723  8.206134  0.500977  8.138528
18   0.504028  8.033541  0.497925  8.106175
19   0.501099  8.044072  0.508423  7.917983
20   0.497437  8.086505  0.496948  8.088433
21   0.497070  8.081860  0.495728  8.099055
22   0.499390  8.037219  0.505005  7.944519
23   0.488403  8.206538  0.493164  8.128186
24   0.503540  7.960705  0.492065  8.141719
25   0.493896  8.110900  0.502441  7.973153
26   0.498413  8.036071  0.491943  8.137989
27   0.493408  8.113572  0.501587  7.982174
28   0.507202  7.891760  0.502808  7.960964
29   0.507446  7.886236  0.490234  8.159883
30   0.491455  8.139723  0.502563  7.961942
31   0.505859  7.908743  0.510132  7.839980
32   0.503174  7.950273  0.508789  7.860117
33   0.511963  7.808889  0.490234  8.154655
34   0.500244  7.994436  0.503540  7.941241
35   0.502686  7.954205  0.496826  8.046946
36   0.499390  8.005396  0.513550  7.778953
37   0.502441  7.955340  0.494873  8.075276
38   0.496216  8.053137  0.505005  7.912272
39   0.502319  7.954332  0.508667  7.852369
40   0.505981  7.894412  0.494995  8.068780
41   0.505859  7.894797  0.501709  7.960176
42   0.509766  7.830952  0.498413  8.011152
43   0.493042  8.096006  0.496948  8.032955
44   0.484985  8.222914  0.493530  8.085933
45   0.500977  7.966490  0.503296  7.928789
46   0.504395  7.910579  0.509277  7.832049
47   0.491577  8.113582  0.500610  7.968931
48   0.492798  8.092880  0.501953  7.946336
49   0.497803  8.011958  0.498047  8.007537
50   0.496826  8.026511  0.493408  8.080531
51   0.505981  7.879657  0.499878  7.976551
52   0.490112  8.131870  0.501343  7.952478
53   0.503906  7.911299  0.497070  8.019984
54   0.499512  7.980803  0.498901  7.990289
55   0.501221  7.953103  0.498657  7.993773
56   0.501343  7.950791  0.500000  7.972042
57   0.498901  7.989426  0.502319  7.934815
58   0.497681  8.008668  0.501831  7.942410
59   0.496582  8.026019  0.495972  8.035683
60   0.497803  8.006440  0.505981  7.876006
61   0.496704  8.023873  0.512329  7.774742
62   0.496704  8.023818  0.499756  7.975146
63   0.512573  7.770791  0.511963  7.780509
64   0.495117  8.049061  0.501831  7.942018
65   0.501953  7.919403  0.498901  8.002118
66   0.499023  8.022032  0.508545  7.870406
67   0.511597  7.820957  0.504395  7.934998
68   0.500244  8.000432  0.513184  7.793423
69   0.499268  8.014586  0.500244  7.998331
70   0.494873  8.083297  0.493530  8.104045
71   0.498901  8.017778  0.496216  8.059953
72   0.489624  8.164420  0.492310  8.120984
73   0.487183  8.202111  0.493896  8.094467
74   0.505127  7.914829  0.508057  7.867524
75   0.509644  7.841636  0.499390  8.004517
76   0.492188  8.118753  0.496826  8.044216
77   0.497803  8.028068  0.501343  7.971048
78   0.498779  8.011336  0.502686  7.948478
79   0.493286  8.097746  0.493286  8.097159
80   0.506714  7.882504  0.499878  7.990895
81   0.506226  7.889108  0.496948  8.036414
82   0.499756  7.991059  0.496826  8.037163
83   0.492554  8.104677  0.497437  8.026228
84   0.504517  7.912753  0.506104  7.886847
85   0.499390  7.993282  0.502563  7.942080
86   0.504639  7.908403  0.492676  8.098525
87   0.504028  7.916955  0.504272  7.912480
88   0.501587  7.954728  0.502686  7.936650
89   0.503296  7.926375  0.492310  8.100984
90   0.500488  7.970080  0.498169  8.006545
91   0.501587  7.929432  0.498413  8.029531
92   0.505371  7.918119  0.509766  7.847552
93   0.500000  8.002762  0.501343  7.980885
94   0.500366  7.996009  0.508301  7.869077
95   0.494019  8.096361  0.500610  7.990871
96   0.508301  7.867894  0.493530  8.103008
97   0.494263  8.090992  0.494263  8.090661
98   0.498657  8.020294  0.502686  7.955772
99   0.494263  8.089773  0.498535  8.021384
100  0.495239  8.073669  0.508057  7.869075
101  0.499390  8.007003  0.493774  8.096280
102  0.504517  7.924788  0.504883  7.918714
103  0.496460  8.052760  0.502197  7.961056
104  0.515259  7.752587  0.491699  8.127940
105  0.494995  8.075149  0.488525  8.178040
106  0.494995  8.074638  0.503662  7.936200
107  0.495605  8.064370  0.507080  7.881158
108  0.504150  7.927577  0.492554  8.112162
109  0.512085  7.800486  0.498657  8.014247
110  0.501709  7.965279  0.495483  8.064208
111  0.498779  8.011336  0.503418  7.937051
112  0.497192  8.035964  0.494141  8.084272
113  0.500122  7.988568  0.503540  7.933726
114  0.496582  8.044300  0.500610  7.979722
115  0.498535  8.012449  0.500122  7.986788
116  0.499146  8.001997  0.500000  7.988011
117  0.505371  7.902021  0.500488  7.979500
118  0.494995  8.066712  0.495972  8.050777
119  0.511841  7.797422  0.501709  7.958582
120  0.499512  7.993248  0.502930  7.938390
121  0.500244  7.980840  0.501221  7.964903
122  0.494507  8.071571  0.503662  7.925244
123  0.507080  7.870383  0.495239  8.058781
124  0.497559  8.021432  0.493652  8.083330
125  0.505737  7.890290  0.496948  8.030027
126  0.499878  7.982941  0.494995  8.060400
127  0.506104  7.882923  0.495605  8.049901

2018-02-25 11:17:58.342281 Finish.
Total elapsed time: 14:51:11.34.
