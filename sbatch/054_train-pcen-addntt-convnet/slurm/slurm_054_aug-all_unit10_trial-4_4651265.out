2018-02-24 20:27:50.940906: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:50.941220: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:50.941241: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:50.941251: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:50.941260: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:50.248461 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit01, unit02, unit03.
Validation set: unit05, unit07.
Test set: unit10.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.907471  0.347686  0.935181  0.229363
1    0.911255  0.288432  0.909668  0.315661
2    0.934448  0.221685  0.955322  0.183183
3    0.918701  0.256256  0.925659  0.258576
4    0.926880  0.227553  0.953125  0.181095
5    0.604736  6.137573  0.483032  8.274257
6    0.496582  8.054500  0.501709  7.969375
7    0.497803  8.028868  0.498657  8.012678
8    0.498779  8.008578  0.497803  8.022144
9    0.494629  8.071046  0.496338  8.042218
10   0.508423  7.848207  0.506958  7.870300
11   0.499878  7.982097  0.497681  8.016120
12   0.503662  7.919900  0.503174  7.926878
13   0.502075  7.943703  0.500732  7.964464
14   0.499023  7.991157  0.497559  8.013994
15   0.499512  7.982415  0.494995  8.054008
16   0.500854  7.960244  0.499390  7.983269
17   0.505615  7.883739  0.498901  7.990513
18   0.500122  7.970832  0.493042  8.083499
19   0.499634  7.978236  0.494263  8.063703
20   0.496826  8.022698  0.505493  7.884400
21   0.500122  7.969922  0.494141  8.065183
22   0.508301  7.839355  0.501343  7.950207
23   0.498901  7.989067  0.494629  8.057124
24   0.511719  7.784625  0.494263  8.062873
25   0.490479  8.123168  0.498535  7.994695
26   0.502930  7.924610  0.495972  8.035515
27   0.505127  7.946533  0.504639  8.036126
28   0.507568  7.984977  0.508911  7.960310
29   0.509277  7.952076  0.503296  8.046306
30   0.501587  8.071931  0.514038  7.869397
31   0.495850  8.160876  0.497803  8.127757
32   0.493530  8.195095  0.493408  8.195567
33   0.510010  7.926573  0.492065  8.214415
34   0.495117  8.163910  0.493896  8.182285
35   0.496094  8.145630  0.495361  8.156209
36   0.501587  8.054694  0.507568  7.957126
37   0.509766  7.920603  0.490112  8.236283
38   0.503052  8.026679  0.498535  8.098448
39   0.511719  7.884973  0.496094  8.135852
40   0.501465  8.048365  0.501831  8.041562
41   0.493408  8.176475  0.496216  8.130390
42   0.496704  8.121742  0.505615  7.977350
43   0.496948  8.116338  0.487061  8.275020
44   0.490967  8.211423  0.507568  7.943221
45   0.506836  7.954463  0.503296  8.010977
46   0.502319  8.026224  0.500366  8.057230
47   0.495850  8.129604  0.504150  7.995404
48   0.499512  8.069810  0.502808  8.016343
49   0.495972  8.126224  0.503174  8.009855
50   0.505737  7.968290  0.503174  8.009377
51   0.509521  7.906866  0.502319  8.022766
52   0.494751  8.144598  0.503296  8.006725
53   0.496582  8.114820  0.494141  8.154060
54   0.500000  8.059528  0.505737  7.966970
55   0.500977  8.043638  0.501587  8.033740
56   0.504395  7.988439  0.498047  8.090708
57   0.494629  8.145766  0.498169  8.088677
58   0.498291  8.086687  0.505981  7.962712
59   0.504272  7.990242  0.503540  8.002034
60   0.498535  8.082693  0.497314  8.102360
61   0.500610  8.049231  0.501831  8.029550
62   0.490601  8.210560  0.495972  8.123985
63   0.494629  8.145626  0.507202  7.942968
64   0.501953  8.027571  0.497681  8.096433
65   0.494019  8.155459  0.492432  8.181036
66   0.497559  8.098399  0.497803  8.094464
67   0.496582  8.114139  0.501221  8.039373
68   0.493530  8.163327  0.503662  8.000022
69   0.499023  8.069895  0.491333  8.220944
70   0.503418  8.020471  0.499634  8.073954
71   0.493408  8.167452  0.503662  7.998558
72   0.499268  8.063875  0.498901  8.065213
73   0.513672  7.825754  0.498291  8.067162
74   0.505249  7.952838  0.494019  8.128620
75   0.497070  8.077022  0.494629  8.113103
76   0.499512  8.032667  0.508057  7.893929
77   0.509277  7.872157  0.496704  8.070359
78   0.495972  8.079959  0.501587  7.988417
79   0.503052  7.963189  0.499146  8.023638
80   0.490479  8.160120  0.507202  7.891860
81   0.510132  7.843636  0.494751  8.087369
82   0.508057  7.873892  0.495850  8.067192
83   0.503052  7.951180  0.495483  8.070689
84   0.499756  8.001538  0.501587  7.971352
85   0.500488  7.987977  0.506226  7.895662
86   0.513916  7.772307  0.496704  8.045992
87   0.498413  8.018122  0.492310  8.114836
88   0.499756  7.995611  0.495728  8.059350
89   0.505127  7.909084  0.504761  7.914531
90   0.500610  7.980362  0.494019  8.085135
91   0.505493  7.901930  0.501709  7.962002
92   0.498047  8.020160  0.501343  7.967402
93   0.498779  8.008080  0.506104  7.891131
94   0.500366  7.982429  0.504028  7.923881
95   0.498901  8.005460  0.508301  7.855455
96   0.499756  7.991527  0.499878  7.989426
97   0.504883  7.909480  0.499878  7.989109
98   0.498779  8.006459  0.507446  7.868117
99   0.503540  7.930217  0.489502  8.153836
100  0.507080  7.873409  0.496826  8.036685
101  0.507446  7.867170  0.493774  8.084921
102  0.500000  7.985449  0.501099  7.967706
103  0.493896  8.082287  0.504028  7.920516
104  0.505493  7.896909  0.499756  7.988113
105  0.497681  8.020925  0.497070  8.030377
106  0.496826  8.033983  0.500977  7.967522
107  0.508911  7.840725  0.494751  8.066165
108  0.491943  8.110611  0.500122  7.979904
109  0.509277  7.833623  0.500610  7.971467
110  0.502319  7.943890  0.491943  8.108972
111  0.492554  8.098905  0.509033  7.835842
112  0.498047  8.010652  0.501465  7.955821
113  0.498047  8.009975  0.498657  7.999905
114  0.503784  7.917837  0.497070  8.024540
115  0.506592  7.872419  0.507568  7.856526
116  0.503906  7.914594  0.491577  8.110836
117  0.490723  8.124156  0.503052  7.927301
118  0.501099  7.958150  0.487427  8.175827
119  0.507080  7.862235  0.506226  7.875589
120  0.508667  7.836413  0.504883  7.896491
121  0.510620  7.804788  0.499146  7.987489
122  0.494019  8.069007  0.503296  7.920892
123  0.500366  7.967400  0.499390  7.982777
124  0.495850  8.039036  0.497559  8.011619
125  0.505859  7.879128  0.515259  7.729128
126  0.497681  8.009229  0.502075  7.939040
127  0.495605  8.042067  0.504761  7.896001

2018-02-25 11:51:35.181320 Finish.
Total elapsed time: 15:24:45.18.
