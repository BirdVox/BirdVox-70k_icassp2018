2018-02-24 20:27:59.133195: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:59.133581: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:59.133603: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:59.133614: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:59.133624: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:50.248354 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.960083  0.149589  0.840942  0.421976
1    0.970215  0.116113  0.872314  0.375194
2    0.973022  0.102663  0.863770  0.338086
3    0.976685  0.089914  0.817871  0.510915
4    0.976562  0.096662  0.828125  0.528384
5    0.964844  0.146717  0.590698  1.177082
6    0.923462  0.250155  0.714600  0.980009
7    0.951782  0.178563  0.739136  1.149557
8    0.969360  0.129144  0.782104  0.920083
9    0.965576  0.140655  0.747192  1.087578
10   0.942871  0.229780  0.687744  1.137975
11   0.949219  0.206110  0.647705  1.614258
12   0.684448  4.831555  0.495605  8.083748
13   0.492554  8.127411  0.498535  8.027983
14   0.500000  8.001623  0.497803  8.033929
15   0.493774  8.095885  0.496338  8.052902
16   0.498657  8.014099  0.497192  8.035726
17   0.500854  7.975830  0.491699  8.120350
18   0.500122  7.984800  0.504395  7.915478
19   0.491211  8.124582  0.504761  7.907542
20   0.494019  8.152867  0.495728  8.168555
21   0.498047  8.128006  0.509766  7.936344
22   0.498169  8.121070  0.509033  7.943957
23   0.496582  8.142974  0.506592  7.980073
24   0.502197  8.049547  0.507446  7.963655
25   0.505493  7.993985  0.509521  7.927951
26   0.507080  7.966296  0.501587  8.053860
27   0.501831  8.049026  0.491577  8.213424
28   0.498169  8.106363  0.500732  8.064249
29   0.498657  8.096954  0.492920  8.188701
30   0.504028  8.008973  0.494141  8.167675
31   0.500854  8.058831  0.502930  8.024766
32   0.503662  8.012379  0.497314  8.114120
33   0.497437  8.111613  0.496094  8.132725
34   0.504150  8.002364  0.488892  8.247812
35   0.507935  7.940406  0.513306  7.853371
36   0.491333  8.207089  0.492065  8.194850
37   0.496216  8.127542  0.503906  8.003180
38   0.502197  8.030339  0.493164  8.175557
39   0.505249  7.980409  0.501953  8.033177
40   0.510986  7.887241  0.498535  8.087599
41   0.500122  8.061708  0.503052  8.014179
42   0.506104  7.964701  0.494141  8.157236
43   0.488770  8.243541  0.503540  8.005208
44   0.508057  7.932166  0.504639  7.987019
45   0.505859  7.967123  0.494141  8.155792
46   0.506592  7.954904  0.502197  8.025543
47   0.502197  8.025366  0.501343  8.038967
48   0.511108  7.881407  0.500977  8.044563
49   0.497070  8.107388  0.488525  8.244986
50   0.499146  8.073693  0.498291  8.087355
51   0.497192  8.104966  0.491333  8.199316
52   0.488037  8.252360  0.501343  8.037823
53   0.505371  7.972831  0.496338  8.118370
54   0.494873  8.141931  0.509888  7.899879
55   0.511719  7.870329  0.501953  8.027700
56   0.501343  8.037512  0.507202  7.943047
57   0.503906  7.996152  0.503418  8.004007
58   0.508057  7.929229  0.495361  8.133843
59   0.498047  8.090550  0.491089  8.202694
60   0.506592  7.952812  0.497803  8.094471
61   0.496216  8.120047  0.501831  8.029539
62   0.503174  8.007894  0.497192  8.104303
63   0.492432  8.181036  0.491699  8.192841
64   0.501587  8.033470  0.505493  7.970509
65   0.492676  8.177100  0.501831  8.029535
66   0.507202  7.942963  0.500244  8.055113
67   0.500122  8.057080  0.498535  8.082658
68   0.505371  7.972476  0.488892  8.238094
69   0.505127  7.971309  0.494141  8.142790
70   0.509399  7.889751  0.503540  7.976678
71   0.502197  7.995450  0.503296  7.976143
72   0.487793  8.222442  0.494873  8.108945
73   0.495605  8.096922  0.500854  8.012967
74   0.495361  8.100355  0.496826  8.076837
75   0.488403  8.210975  0.500610  8.016228
76   0.508667  7.887649  0.503052  7.977030
77   0.502075  7.992453  0.502441  7.986463
78   0.499634  8.031061  0.505127  7.943317
79   0.502686  7.982057  0.502563  7.983813
80   0.510620  7.855167  0.499878  8.026209
81   0.505737  7.932567  0.493164  8.132774
82   0.498657  8.044943  0.494995  8.103057
83   0.505493  7.935406  0.505371  7.937051
84   0.496216  8.082686  0.495850  8.088188
85   0.500366  8.015824  0.491577  8.155568
86   0.494385  8.110409  0.498291  8.047719
87   0.511597  7.835152  0.505493  7.931995
88   0.497803  8.054109  0.501221  7.999109
89   0.499512  8.025814  0.503174  7.966870
90   0.498535  8.040228  0.506226  7.917008
91   0.498291  8.042855  0.492432  8.135593
92   0.498901  8.031743  0.497192  8.058255
93   0.497070  8.059434  0.501099  7.994418
94   0.490967  8.155116  0.501709  7.983005
95   0.497314  8.052176  0.491943  8.136888
96   0.509155  7.861542  0.499146  8.020148
97   0.489014  8.180671  0.493652  8.105691
98   0.490356  8.157182  0.502563  7.961495
99   0.502563  7.960398  0.490234  8.155833
100  0.499878  8.000959  0.500244  7.993968
101  0.491943  8.125143  0.505737  7.904061
102  0.494629  8.079983  0.500488  7.985387
103  0.497437  8.032867  0.501343  7.969414
104  0.498779  8.009125  0.500000  7.988506
105  0.496826  8.037978  0.512329  7.789703
106  0.494873  8.066913  0.500366  7.978269
107  0.500732  7.971411  0.503418  7.927593
108  0.500732  7.969462  0.510376  7.814795
109  0.497314  8.022168  0.510864  7.805316
110  0.503784  7.917424  0.496460  8.033451
111  0.504761  7.900451  0.501831  7.946519
112  0.502441  7.936224  0.504272  7.906496
113  0.490234  8.129829  0.505615  7.884183
114  0.498413  7.998628  0.505615  7.883460
115  0.503174  7.922092  0.506104  7.875118
116  0.506958  7.861278  0.497070  8.018714
117  0.494629  8.057479  0.496704  8.024255
118  0.497192  8.016362  0.488159  8.160278
119  0.501831  7.942244  0.505371  7.885746
120  0.493774  8.070580  0.508667  7.833119
121  0.505371  7.885637  0.501831  7.942052
122  0.496094  8.033504  0.510742  7.799961
123  0.497437  8.012077  0.504150  7.905036
124  0.489136  8.144402  0.493164  8.080178
125  0.503540  7.914759  0.496460  8.027631
126  0.501709  7.943948  0.495605  8.041252
127  0.499756  7.975085  0.497803  8.006222

2018-02-25 11:20:44.223922 Finish.
Total elapsed time: 14:53:54.22.
