2018-02-24 20:27:45.865678: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:45.865934: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:45.865947: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:45.865952: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:45.865959: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:49.270484 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit02, unit03, unit05.
Validation set: unit07, unit10.
Test set: unit01.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.966309  0.135557  0.953613  0.188823
1    0.976807  0.096925  0.946655  0.203078
2    0.976196  0.093867  0.953979  0.163139
3    0.723389  4.359356  0.490845  8.238515
4    0.494385  8.177378  0.506470  7.979407
5    0.505737  7.988949  0.500488  8.071520
6    0.493896  8.176087  0.500977  8.060400
7    0.496582  8.129872  0.496582  8.128583
8    0.500610  8.062516  0.506104  7.972891
9    0.500854  8.056528  0.498169  8.098888
10   0.501221  8.048872  0.492920  8.181872
11   0.498901  8.084753  0.491333  8.206060
12   0.503052  8.016566  0.506226  7.964826
13   0.505249  7.980042  0.506958  7.951994
14   0.508789  7.922032  0.492920  8.177382
15   0.499146  8.076652  0.495850  8.129408
16   0.494751  8.146788  0.490601  8.213371
17   0.495850  8.128488  0.505493  7.972787
18   0.494385  8.151598  0.503662  8.001841
19   0.508057  7.930813  0.495483  8.133284
20   0.499512  8.068192  0.506470  7.955889
21   0.507812  7.934113  0.497192  8.105164
22   0.491211  8.201467  0.501465  8.036093
23   0.506470  7.955339  0.502930  8.012319
24   0.502686  8.016188  0.495361  8.134179
25   0.504883  7.980660  0.497070  8.106536
26   0.500122  8.057309  0.502930  8.012021
27   0.509521  7.905745  0.505737  7.966713
28   0.507568  7.937179  0.507446  7.939127
29   0.493652  8.161444  0.500244  8.055183
30   0.493652  8.161419  0.511719  7.870213
31   0.497925  8.078693  0.501099  7.977375
32   0.509033  7.847133  0.509644  7.835235
33   0.503662  7.929233  0.496094  8.048646
34   0.502686  7.942481  0.491943  8.112709
35   0.510986  7.808188  0.502441  7.943515
36   0.494019  8.076969  0.497559  8.019729
37   0.497681  8.017042  0.503784  7.919017
38   0.490845  8.124639  0.500000  7.978038
39   0.492432  8.098104  0.499268  7.988551
40   0.506714  7.869317  0.504150  7.909683
41   0.502075  7.942312  0.504028  7.910739
42   0.502563  7.933703  0.499878  7.976145
43   0.504761  7.897973  0.494385  8.063079
44   0.502197  7.938257  0.505981  7.877672
45   0.498535  7.996162  0.506592  7.867513
46   0.494751  8.056107  0.506592  7.867172
47   0.493164  8.081104  0.498901  7.989510
48   0.496094  8.034164  0.501221  7.952330
49   0.489868  8.133235  0.507812  7.847086
50   0.506836  7.862594  0.505859  7.878107
51   0.500977  7.955905  0.496948  8.020085
52   0.504883  7.893555  0.490601  8.121217
53   0.500610  7.961612  0.495117  8.049163
54   0.500488  7.963516  0.498291  7.998529
55   0.510864  7.798067  0.491455  8.107483
56   0.505005  7.891456  0.511353  7.790249
57   0.510010  7.811649  0.498291  7.998467
58   0.509888  7.813582  0.492065  8.097707
59   0.505493  7.883633  0.502563  7.930336
60   0.513672  7.753239  0.495605  8.041258
61   0.501465  7.947844  0.502197  7.936167
62   0.496216  8.031524  0.496826  8.021793
63   0.500244  7.996481  0.493408  8.231136
64   0.496826  8.165737  0.490479  8.259894
65   0.498047  8.132773  0.504395  8.026217
66   0.498779  8.113814  0.501953  8.060186
67   0.498657  8.111535  0.493042  8.200506
68   0.497192  8.132463  0.504395  8.015366
69   0.494629  8.171982  0.506226  7.984356
70   0.498291  8.111668  0.497559  8.122939
71   0.501587  8.057551  0.499023  8.098434
72   0.496704  8.135425  0.502563  8.040600
73   0.497070  8.128778  0.504517  8.008399
74   0.502441  8.041496  0.505127  7.997855
75   0.498047  8.111618  0.494629  8.166348
76   0.493408  8.185657  0.496338  8.138062
77   0.498779  8.098330  0.501587  8.052687
78   0.507690  7.953913  0.504395  8.006630
79   0.494995  8.157717  0.500854  8.062852
80   0.509033  7.930597  0.491333  8.215453
81   0.495483  8.148114  0.502686  8.031578
82   0.503296  8.021286  0.504028  8.009019
83   0.497559  8.112836  0.502808  8.027763
84   0.507446  7.952527  0.501953  8.040592
85   0.491211  8.213263  0.511108  7.892078
86   0.498535  8.094263  0.503052  8.020989
87   0.499634  8.075611  0.497314  8.112524
88   0.493286  8.176990  0.488892  8.247358
89   0.493286  8.176073  0.510986  7.890326
90   0.497925  8.100411  0.508179  7.934697
91   0.491699  8.199887  0.495483  8.138468
92   0.506592  7.959012  0.503906  8.001890
93   0.491211  8.206124  0.493286  8.172288
94   0.486328  8.284070  0.491089  8.206972
95   0.504272  7.994134  0.493286  8.170874
96   0.498779  8.082019  0.502563  8.020715
97   0.506348  7.959435  0.507446  7.941449
98   0.496582  8.116305  0.503784  7.999974
99   0.498901  8.078454  0.502075  8.027085
100  0.503784  7.999352  0.497559  8.099517
101  0.494019  8.156421  0.500000  8.059865
102  0.505615  7.962662  0.494629  8.135445
103  0.496704  8.100060  0.504272  7.977370
104  0.495361  8.117597  0.500610  8.032106
105  0.500000  8.040100  0.488159  8.227141
106  0.489014  8.211829  0.500610  8.025258
107  0.508301  7.900988  0.491211  8.171764
108  0.499023  8.045554  0.501831  7.999120
109  0.502197  7.991621  0.500854  8.011353
110  0.496338  8.081694  0.492798  8.136453
111  0.491821  8.150356  0.498535  8.041642
112  0.488892  8.193720  0.510742  7.843695
113  0.497192  8.058058  0.502808  7.966876
114  0.502930  7.963294  0.503662  7.949977
115  0.507080  7.893878  0.501221  7.985681
116  0.502808  7.958811  0.504150  7.935836
117  0.505737  7.909014  0.505981  7.903605
118  0.508301  7.865163  0.502808  7.951281
119  0.500366  7.988804  0.503296  7.940711
120  0.506836  7.882950  0.501343  7.969215
121  0.493286  8.096412  0.503906  7.925877
122  0.500000  7.986994  0.507812  7.861307
123  0.495728  8.052902  0.510010  7.824165
124  0.495361  8.056721  0.498291  8.009063
125  0.497314  8.023751  0.493286  8.087116
126  0.497192  8.024054  0.506714  7.871497
127  0.504395  7.907777  0.501709  7.949922

2018-02-25 11:25:38.691687 Finish.
Total elapsed time: 14:58:49.69.
