2018-02-24 20:27:36.444066: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:36.444364: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:36.444377: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:48.459799 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit10, unit01, unit02.
Validation set: unit03, unit05.
Test set: unit07.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.500366  7.977567  0.499512  7.990285
1    0.499634  7.987559  0.494995  8.060773
2    0.500244  7.976440  0.501343  7.958304
3    0.489136  8.152358  0.502075  7.945541
4    0.503540  7.921709  0.500366  7.971848
5    0.495605  8.047330  0.498535  8.000223
6    0.510254  7.813036  0.508179  7.845770
7    0.493652  8.077038  0.497437  8.016404
8    0.493164  8.084240  0.503906  7.912716
9    0.508179  7.844359  0.498413  7.999811
10   0.500366  7.968459  0.494141  8.067503
11   0.507324  7.857136  0.507446  7.855008
12   0.505615  7.884032  0.508301  7.841056
13   0.503174  7.922644  0.512695  7.770706
14   0.505005  7.893179  0.501587  7.947542
15   0.501221  7.953264  0.486450  8.188628
16   0.500610  7.962777  0.490356  8.126149
17   0.501709  7.945070  0.490112  8.129860
18   0.501465  7.948792  0.504272  7.903954
19   0.502319  8.051584  0.498169  8.140223
20   0.491821  8.234918  0.503418  8.042343
21   0.497803  8.129137  0.494507  8.179014
22   0.498535  8.111513  0.502441  8.046184
23   0.504517  8.010726  0.496094  8.144593
24   0.508545  7.942242  0.503418  8.023294
25   0.506836  7.966787  0.497314  8.118896
26   0.502197  8.038966  0.495605  8.144029
27   0.501099  8.054414  0.497803  8.106499
28   0.502808  8.024884  0.501343  8.047581
29   0.500610  8.058554  0.499756  8.071524
30   0.500000  8.066859  0.502441  8.026804
31   0.498657  8.087161  0.504150  7.998009
32   0.503540  8.007295  0.499634  8.069727
33   0.505005  7.982682  0.485229  8.300971
34   0.509888  7.903125  0.488159  8.252965
35   0.498169  8.091289  0.503418  8.006364
36   0.503906  7.998214  0.503906  7.997950
37   0.496338  8.119709  0.502563  8.019150
38   0.507935  7.932394  0.496826  8.111268
39   0.498535  8.083577  0.506348  7.957519
40   0.505127  7.977081  0.501709  8.032068
41   0.497803  8.094943  0.490479  8.212916
42   0.496826  8.110539  0.506104  7.960947
43   0.511963  7.866457  0.490356  8.214669
44   0.493896  8.157577  0.504150  7.992273
45   0.500732  8.047341  0.491699  8.192918
46   0.500244  8.012057  0.494751  8.073589
47   0.486328  8.203862  0.511963  7.792462
48   0.501221  7.962080  0.498291  8.007380
49   0.492310  8.101658  0.503540  7.921640
50   0.506714  7.870242  0.500732  7.964866
51   0.503296  7.923385  0.511841  7.786590
52   0.496582  8.029373  0.492554  8.093149
53   0.491699  8.106394  0.496948  8.022359
54   0.503174  7.922808  0.500122  7.971180
55   0.500610  7.963156  0.499146  7.986284
56   0.496094  8.034745  0.502197  7.937262
57   0.504028  7.907919  0.500854  7.958377
58   0.494629  8.057509  0.498901  7.989286
59   0.507812  7.847131  0.498291  7.998842
60   0.508545  7.835302  0.486816  8.181645
61   0.487183  8.175757  0.499390  7.981103
62   0.496704  8.023882  0.496582  8.025797
63   0.500366  7.965444  0.502930  7.924555
64   0.488647  8.152233  0.499756  7.975125
65   0.499023  7.986792  0.500610  7.961484
66   0.500854  7.957586  0.500732  7.959527
67   0.504639  7.897249  0.492676  8.087964
68   0.492432  8.091854  0.514893  7.733772
69   0.501343  7.949787  0.499512  7.978978
70   0.490356  8.124934  0.501343  7.949786
71   0.495361  8.045144  0.498291  7.998438
72   0.502563  8.063114  0.502197  8.179975
73   0.494873  8.296285  0.494995  8.292491
74   0.489258  8.383025  0.496094  8.270849
75   0.493408  8.312060  0.494263  8.296137
76   0.501099  8.183716  0.493164  8.309298
77   0.498535  8.220334  0.499878  8.196228
78   0.496094  8.254678  0.497803  8.224522
79   0.511597  7.999506  0.509033  8.038074
80   0.499634  8.186764  0.502441  8.138638
81   0.499512  8.182935  0.499390  8.181924
82   0.497070  8.216293  0.489136  8.341121
83   0.505249  8.078324  0.501221  8.140131
84   0.501343  8.135042  0.505859  8.059088
85   0.498901  8.168104  0.498291  8.174783
86   0.504028  8.079189  0.502197  8.105570
87   0.488281  8.326792  0.499512  8.142697
88   0.510376  7.964578  0.503296  8.075692
89   0.499390  8.135736  0.516113  7.863277
90   0.509155  7.972619  0.504883  8.038693
91   0.505615  8.024204  0.492920  8.226166
92   0.494873  8.192134  0.500244  8.103037
93   0.485840  8.332793  0.505737  8.009698
94   0.493774  8.200243  0.492554  8.217673
95   0.502075  8.062069  0.506958  7.981261
96   0.502808  8.046156  0.496460  8.146496
97   0.502075  8.054120  0.501221  8.066053
98   0.508789  7.942324  0.499390  8.092111
99   0.507202  7.964573  0.505005  7.998402
100  0.492676  8.195630  0.498657  8.097758
101  0.506226  7.974400  0.498657  8.095048
102  0.503784  8.011166  0.498535  8.094556
103  0.497437  8.111144  0.495483  8.141538
104  0.507202  7.951661  0.501709  8.039243
105  0.501465  8.042313  0.499023  8.080834
106  0.500977  8.048615  0.506592  7.957406
107  0.490845  8.210603  0.496704  8.115582
108  0.512085  7.867173  0.498413  8.087072
109  0.504517  7.988304  0.497681  8.098125
110  0.500244  8.056510  0.499878  8.062143
111  0.494507  8.148499  0.491089  8.203396
112  0.494873  8.142252  0.495361  8.134249
113  0.490601  8.210884  0.498657  8.080941
114  0.498779  8.078911  0.490479  8.212651
115  0.509399  7.907646  0.500366  8.053213
116  0.507080  7.944978  0.500854  8.045307
117  0.500122  8.057102  0.506104  7.960684
118  0.489502  8.228265  0.505371  7.972481
119  0.498047  8.090532  0.500488  8.051180
120  0.497192  8.104302  0.504639  7.984282
121  0.495728  8.127912  0.496826  8.110204
122  0.500977  8.043307  0.505371  7.972476
123  0.498535  8.082658  0.494751  8.143652
124  0.497681  8.054252  0.502441  7.997150
125  0.492798  8.150549  0.500122  8.033379
126  0.491333  8.173069  0.496826  8.085049
127  0.491699  8.166312  0.502075  8.000404

2018-02-25 11:21:13.123082 Finish.
Total elapsed time: 14:54:25.12.
