2018-02-24 20:26:37.436238: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:37.436503: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:37.436517: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:18.141001 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit10, unit01, unit02.
Validation set: unit03, unit05.
Test set: unit07.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.501099  7.960127  0.498901  7.993941
1    0.510132  7.813979  0.501221  7.955229
2    0.492798  8.088886  0.506958  7.862587
3    0.494019  8.118435  0.492310  8.189609
4    0.504639  7.987372  0.496826  8.111749
5    0.508179  7.906719  0.502319  7.978121
6    0.499268  8.013288  0.500732  7.980315
7    0.494751  8.070361  0.502441  7.943570
8    0.503906  7.917551  0.502075  7.944545
9    0.502930  7.929435  0.508911  7.832827
10   0.505005  7.894238  0.501709  7.946053
11   0.503296  7.920249  0.503296  7.919825
12   0.501099  7.954564  0.507812  7.847286
13   0.499878  7.973619  0.510010  7.811958
14   0.496948  8.020101  0.493164  8.080356
15   0.502319  7.934351  0.504028  7.907067
16   0.493774  8.070514  0.489014  8.146390
17   0.514526  7.739644  0.504883  7.893374
18   0.502930  7.924505  0.504517  7.899200
19   0.497559  8.010124  0.504517  7.899194
20   0.508545  7.834971  0.499390  7.980926
21   0.501709  8.062715  0.497437  8.182724
22   0.494751  8.215489  0.494995  8.202063
23   0.501343  8.092349  0.500000  8.105551
24   0.501709  8.070779  0.498047  8.121831
25   0.500610  8.074120  0.498657  8.098547
26   0.498901  8.088327  0.496826  8.115183
27   0.493042  8.169616  0.496216  8.113210
28   0.491211  8.187504  0.489990  8.201558
29   0.497192  8.081640  0.503296  7.979332
30   0.498413  8.052486  0.513062  7.814370
31   0.502319  7.981361  0.495117  8.092028
32   0.499268  8.022032  0.502686  7.963832
33   0.503540  7.946824  0.495850  8.066164
34   0.501587  7.971750  0.504028  7.930003
35   0.498047  8.022839  0.503784  7.928969
36   0.495605  8.057238  0.499023  8.000739
37   0.507812  7.858873  0.497437  8.022646
38   0.502197  7.945336  0.500122  7.977100
39   0.505615  7.888409  0.494629  8.062520
40   0.508667  7.837856  0.502441  7.936312
41   0.496582  8.029072  0.500488  7.966202
42   0.497803  8.008537  0.492432  8.093731
43   0.504639  7.898779  0.505493  7.884848
44   0.509033  7.828173  0.494507  8.059547
45   0.503662  7.913429  0.501831  7.942480
46   0.493408  8.076656  0.503174  7.920878
47   0.494751  8.055092  0.499268  7.983031
48   0.496704  8.023859  0.495239  8.047179
49   0.494141  8.064671  0.496338  8.029622
50   0.488770  8.150267  0.499756  7.975108
51   0.499756  7.975101  0.493042  8.082131
52   0.500122  7.969254  0.500000  7.971197
53   0.496338  8.029579  0.495483  8.043200
54   0.496460  8.027630  0.501831  7.942002
55   0.503418  7.916702  0.508911  7.829128
56   0.496948  8.019845  0.500488  7.963408
57   0.510620  7.801882  0.495850  8.037359
58   0.506226  8.037667  0.502686  8.320170
59   0.496216  8.422252  0.495728  8.428127
60   0.497803  8.393058  0.492310  8.480098
61   0.502319  8.317477  0.499878  8.355606
62   0.494629  8.439104  0.495728  8.420317
63   0.497559  8.389781  0.503296  8.296277
64   0.493530  8.452654  0.490601  8.498823
65   0.504028  8.281323  0.505737  8.252675
66   0.499023  8.359735  0.496094  8.405767
67   0.491333  8.481245  0.495483  8.413044
68   0.494751  8.423479  0.492920  8.451569
69   0.508301  8.202159  0.502319  8.297016
70   0.501465  8.309156  0.497559  8.370426
71   0.488770  8.510313  0.500122  8.325495
72   0.497681  8.362926  0.492920  8.437672
73   0.502930  8.274259  0.494507  8.407877
74   0.506348  8.214793  0.506226  8.214455
75   0.504517  8.239607  0.498779  8.329614
76   0.507202  8.191290  0.499023  8.320478
77   0.503784  8.241011  0.497070  8.346416
78   0.493286  8.404505  0.498413  8.318882
79   0.496338  8.349250  0.492554  8.407081
80   0.501221  8.264130  0.491943  8.410323
81   0.500610  8.267195  0.500366  8.267613
82   0.501099  8.252200  0.505615  8.175706
83   0.491333  8.402128  0.499634  8.264468
84   0.503174  8.203460  0.504395  8.179748
85   0.494751  8.331070  0.494141  8.336708
86   0.505859  8.143550  0.507935  8.105744
87   0.500000  8.229208  0.492065  8.352589
88   0.505737  8.127655  0.497192  8.260731
89   0.492432  8.332762  0.494141  8.300435
90   0.500366  8.195265  0.496826  8.247424
91   0.499390  8.201177  0.495361  8.261108
92   0.498901  8.199039  0.502197  8.140845
93   0.491699  8.304992  0.499634  8.171990
94   0.503784  8.100016  0.488037  8.348716
95   0.499512  8.158718  0.496216  8.206772
96   0.505859  8.046365  0.512329  7.937114
97   0.491943  8.260855  0.500977  8.110439
98   0.498413  8.147110  0.504761  8.040190
99   0.498413  8.138102  0.501343  8.086541
100  0.501343  8.082441  0.498413  8.125640
101  0.495972  8.161237  0.499390  8.102487
102  0.491699  8.223071  0.502808  8.040762
103  0.492554  8.203072  0.498291  8.107750
104  0.489136  8.252770  0.502197  8.039819
105  0.493896  8.171480  0.501709  8.043546
106  0.502563  8.028040  0.493530  8.172018
107  0.504272  7.997508  0.507080  7.950991
108  0.495483  8.136866  0.496460  8.120176
109  0.505127  7.979717  0.504639  7.986901
110  0.505371  7.974560  0.500366  8.054754
111  0.510132  7.896993  0.501831  8.030473
112  0.494873  8.142395  0.499512  8.067434
113  0.503418  8.004337  0.502441  8.019963
114  0.499878  8.061205  0.499390  8.069013
115  0.495728  8.128000  0.500122  8.057137
116  0.506958  7.946935  0.487915  8.253857
117  0.501221  8.039387  0.509399  7.907556
118  0.494263  8.151527  0.500854  8.045278
119  0.504150  7.992153  0.503296  8.005925
120  0.507446  7.939028  0.497070  8.106269
121  0.508545  7.921320  0.510132  7.895742
122  0.504028  7.994119  0.499268  8.070853
123  0.504517  7.986249  0.501587  8.033470
124  0.497559  8.091009  0.505249  7.974444
125  0.506348  7.956736  0.503418  8.003957
126  0.502686  8.045513  0.500732  8.154332
127  0.510254  8.001729  0.505981  8.068983

2018-02-25 12:12:30.269320 Finish.
Total elapsed time: 15:46:12.27.
