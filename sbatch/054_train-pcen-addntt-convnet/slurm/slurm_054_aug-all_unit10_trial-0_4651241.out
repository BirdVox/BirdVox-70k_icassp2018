2018-02-24 20:27:49.222644: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:49.222905: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:49.222926: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:49.222935: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:49.222944: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:47.668137 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit01, unit02, unit03.
Validation set: unit05, unit07.
Test set: unit10.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.958130  0.150635  0.962402  0.146439
1    0.955811  0.158461  0.949463  0.202695
2    0.958008  0.148438  0.971802  0.127097
3    0.966919  0.127826  0.968994  0.125807
4    0.961182  0.140474  0.943481  0.172223
5    0.963623  0.140642  0.971191  0.121986
6    0.944458  0.448925  0.571411  6.942700
7    0.534058  7.508237  0.496338  8.098118
8    0.515747  7.784976  0.493774  8.130994
9    0.497681  8.064948  0.506836  7.915503
10   0.507202  7.906669  0.505127  7.936916
11   0.495605  8.086197  0.506104  7.916427
12   0.506592  7.906475  0.496338  8.067858
13   0.500488  7.999788  0.490234  8.161417
14   0.498657  8.025446  0.495728  8.070511
15   0.509033  7.856871  0.497559  8.038329
16   0.498657  8.019446  0.495728  8.064818
17   0.491821  8.125852  0.506226  7.895000
18   0.500488  7.985334  0.505127  7.910275
19   0.496704  8.043519  0.498291  8.017205
20   0.501831  7.959816  0.496582  8.042565
21   0.499634  7.993035  0.493530  8.089479
22   0.497192  8.030286  0.501953  7.953594
23   0.494263  8.075449  0.501221  7.963787
24   0.499023  7.998124  0.511841  7.793106
25   0.494141  8.074650  0.500854  7.966988
26   0.494141  8.073435  0.505493  7.891871
27   0.491699  8.111239  0.503662  7.919993
28   0.493164  8.086863  0.494385  8.066919
29   0.501465  7.953597  0.491943  8.104954
30   0.494141  8.069519  0.497925  8.008797
31   0.493652  8.076547  0.507812  7.850449
32   0.495972  8.038898  0.499023  7.989935
33   0.511475  7.791151  0.509644  7.820070
34   0.492676  8.090331  0.495850  8.039497
35   0.514282  7.745426  0.500732  7.961240
36   0.492065  8.099234  0.502075  7.939484
37   0.498047  8.003555  0.491211  8.112393
38   0.496704  8.024695  0.513428  7.757963
39   0.492065  8.098428  0.503052  7.923185
40   0.505737  7.880290  0.499512  7.979464
41   0.502808  7.926855  0.495361  8.045506
42   0.505371  7.885877  0.502075  7.938375
43   0.489502  8.138783  0.503540  7.914947
44   0.484009  8.229152  0.502441  8.033761
45   0.501099  8.070928  0.504761  8.007312
46   0.494629  8.166786  0.496826  8.128006
47   0.506226  7.973893  0.496338  8.130910
48   0.497803  8.105414  0.492676  8.186338
49   0.495728  8.135765  0.502441  8.026290
50   0.505859  7.970180  0.506348  7.961383
51   0.492065  8.190840  0.505615  7.971767
52   0.500122  8.059767  0.488770  8.242261
53   0.498779  8.080538  0.498535  8.084128
54   0.504517  7.987449  0.509644  7.904573
55   0.495361  8.134590  0.506958  7.947511
56   0.505371  7.972965  0.499023  8.075169
57   0.507080  7.945231  0.503662  8.000252
58   0.501831  8.029714  0.509766  7.901780
59   0.507812  7.933229  0.501343  8.037481
60   0.508789  7.917443  0.487549  8.259779
61   0.512573  7.856422  0.505615  7.968564
62   0.505127  7.976428  0.487061  8.267619
63   0.503662  8.000030  0.502441  8.019703
64   0.501587  8.033474  0.497559  8.098401
65   0.501587  8.033472  0.506470  7.954769
66   0.499146  8.072821  0.500977  8.043308
67   0.491455  8.196776  0.509399  7.907547
68   0.503784  7.998054  0.497803  8.094464
69   0.500000  8.123032  0.500977  8.119316
70   0.492676  8.249806  0.489746  8.294669
71   0.502197  8.092462  0.510864  7.951411
72   0.499268  8.137167  0.508057  7.994382
73   0.501343  8.101535  0.499512  8.129990
74   0.507812  7.995153  0.511230  7.939005
75   0.506592  8.012712  0.497070  8.165103
76   0.495728  8.185656  0.499512  8.123554
77   0.503174  8.063406  0.498291  8.140966
78   0.501343  8.090625  0.496704  8.164220
79   0.497925  8.143366  0.499146  8.122495
80   0.505005  8.026855  0.502808  8.061059
81   0.500977  8.089365  0.495728  8.172751
82   0.513184  7.890184  0.506226  8.001119
83   0.494019  8.196676  0.505737  8.006590
84   0.504395  8.027057  0.496094  8.159674
85   0.503174  8.044411  0.491089  8.238056
86   0.503052  8.044132  0.500488  8.084352
87   0.496216  8.152159  0.500366  8.084215
88   0.500244  8.085180  0.502686  8.044837
89   0.502563  8.045860  0.499634  8.092148
90   0.493652  8.187672  0.501221  8.064812
91   0.497437  8.124978  0.496460  8.139904
92   0.499756  8.086008  0.496704  8.134437
93   0.504150  8.013696  0.497314  8.123169
94   0.503662  8.020181  0.502197  8.043125
95   0.507812  7.951982  0.496094  8.140237
96   0.498535  8.100282  0.508789  7.934411
97   0.496338  8.134523  0.502441  8.035573
98   0.497925  8.107818  0.500732  8.062012
99   0.500000  8.073281  0.500244  8.068812
100  0.501587  8.046649  0.498779  8.091383
101  0.506348  7.968891  0.496094  8.133660
102  0.496338  8.129236  0.498169  8.099234
103  0.501099  8.051539  0.496216  8.129768
104  0.497070  8.115539  0.493164  8.178046
105  0.496826  8.118582  0.501953  8.035511
106  0.494507  8.155114  0.507935  7.938271
107  0.500854  8.051994  0.496582  8.120468
108  0.512085  7.870220  0.489868  8.227948
109  0.498779  8.083973  0.500977  8.048218
110  0.496094  8.126601  0.505249  7.978723
111  0.498657  8.084678  0.496704  8.115874
112  0.488892  8.241531  0.507690  7.938270
113  0.505249  7.977382  0.503906  7.998792
114  0.494385  8.152046  0.509644  7.905896
115  0.500488  8.053271  0.501465  8.037347
116  0.494629  8.147362  0.499756  8.064564
117  0.493896  8.158860  0.495483  8.133142
118  0.484741  8.306160  0.502686  8.016810
119  0.497437  8.101307  0.502808  8.014632
120  0.495117  8.138495  0.519043  7.752770
121  0.500732  8.047825  0.503052  8.010369
122  0.484009  8.317242  0.507202  7.943350
123  0.499023  8.075124  0.503906  7.996375
124  0.499023  8.075036  0.506714  7.951043
125  0.504150  7.992330  0.502441  8.019846
126  0.503418  8.004082  0.501099  8.041443
127  0.497559  8.098484  0.509399  7.907616

2018-02-25 11:54:57.757241 Finish.
Total elapsed time: 15:28:10.76.
