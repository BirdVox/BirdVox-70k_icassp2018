2018-02-24 20:27:39.525009: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:39.525532: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:39.525544: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:39.525552: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:39.525559: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:49.841850 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit07, unit10, unit01.
Validation set: unit02, unit03.
Test set: unit05.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.925293  0.230218  0.922729  0.313666
1    0.808472  2.455192  0.500854  8.072677
2    0.501831  8.052299  0.491943  8.208159
3    0.496094  8.112194  0.494263  8.098241
4    0.503174  7.950921  0.502319  7.960758
5    0.501343  7.973673  0.495483  8.064683
6    0.499023  8.006254  0.495483  8.060835
7    0.504517  7.915233  0.501831  7.956550
8    0.499512  7.992224  0.494995  8.062999
9    0.492188  8.106682  0.493896  8.078415
10   0.501343  7.958806  0.493774  8.078612
11   0.498901  7.996127  0.507812  7.853351
12   0.499634  7.983113  0.494507  8.064256
13   0.498901  7.993675  0.503052  7.927013
14   0.501221  7.955771  0.495483  8.046826
15   0.496338  8.032845  0.499023  7.989691
16   0.495605  8.043884  0.494751  8.057227
17   0.498901  7.990815  0.488892  8.150164
18   0.497070  8.019576  0.507935  7.846188
19   0.497437  8.013387  0.493774  8.067368
20   0.498657  7.993640  0.509155  7.797267
21   0.496094  8.065776  0.494263  8.150385
22   0.507324  7.921415  0.507935  7.896359
23   0.500244  8.009603  0.499878  8.007672
24   0.507446  7.881530  0.503296  7.942941
25   0.496826  8.042506  0.496582  8.043225
26   0.502686  7.943449  0.491943  8.112485
27   0.489746  8.145751  0.494263  8.072148
28   0.501099  7.961881  0.502686  7.935412
29   0.489502  8.144641  0.502563  7.935542
30   0.504517  7.903697  0.497314  8.017870
31   0.494019  8.069885  0.510254  7.810571
32   0.498779  7.993106  0.495117  8.051125
33   0.499634  7.978822  0.503296  7.920166
34   0.508545  7.836262  0.497803  8.007314
35   0.494507  8.059692  0.489868  8.133492
36   0.500610  7.962114  0.502808  7.926972
37   0.510254  7.808171  0.500122  7.969616
38   0.500732  7.959821  0.500977  7.955870
39   0.490479  8.123188  0.498413  7.996652
40   0.506104  7.874016  0.507202  7.856474
41   0.494629  8.150450  0.512573  8.063315
42   0.500977  8.221781  0.507568  8.093699
43   0.499146  8.215061  0.501831  8.159241
44   0.507324  8.060816  0.496704  8.222893
45   0.505249  8.077432  0.505493  8.066211
46   0.495728  8.217224  0.502197  8.106861
47   0.492554  8.256882  0.503906  8.068718
48   0.497314  8.170312  0.505493  8.034018
49   0.502441  8.079172  0.498901  8.132346
50   0.500488  8.103245  0.498779  8.127392
51   0.504761  8.027890  0.505615  8.011129
52   0.504883  8.020207  0.506592  7.990024
53   0.500610  8.084021  0.507812  7.965600
54   0.497803  8.124799  0.496582  8.142402
55   0.498047  8.116892  0.497803  8.118987
56   0.507202  7.965800  0.501587  8.054674
57   0.502319  8.041372  0.500854  8.063534
58   0.497803  8.111398  0.499512  8.082570
59   0.496826  8.124685  0.500610  8.062560
60   0.498413  8.096944  0.501953  8.038889
61   0.494019  8.165873  0.495361  8.143357
62   0.503174  8.016642  0.492554  8.187055
63   0.502075  8.032896  0.499756  8.069615
64   0.506470  7.960802  0.496582  8.119598
65   0.506348  7.961679  0.507202  7.947412
66   0.494873  8.145692  0.505737  7.970159
67   0.507690  7.938304  0.499512  8.069772
68   0.502197  8.026172  0.509155  7.913725
69   0.492310  8.184985  0.497559  8.100134
70   0.507812  7.934649  0.499756  8.064307
71   0.502441  8.020850  0.495117  8.138742
72   0.504395  7.989074  0.509155  7.912214
73   0.504639  7.971978  0.500488  8.006032
74   0.501953  7.977988  0.505981  7.909639
75   0.489502  8.169292  0.504272  7.931120
76   0.498779  8.016639  0.500244  7.991453
77   0.495605  8.063959  0.501831  7.963396
78   0.494629  8.077139  0.510254  7.827042
79   0.494507  8.077237  0.506104  7.891556
80   0.512695  7.785760  0.497925  8.020559
81   0.503052  7.938209  0.502075  7.953184
82   0.507690  7.863116  0.501953  7.954049
83   0.502197  7.949660  0.505005  7.904415
84   0.501953  7.952614  0.492188  8.107856
85   0.501099  7.965374  0.510498  7.815116
86   0.502441  7.943173  0.500000  7.981717
87   0.499390  7.991091  0.487549  8.179511
88   0.500610  7.970947  0.497070  8.027057
89   0.489990  8.139619  0.495850  8.045899
90   0.505371  7.893809  0.500366  7.973307
91   0.505127  7.897128  0.497314  8.021398
92   0.502563  7.937444  0.502563  7.937174
93   0.492920  8.090651  0.489990  8.137094
94   0.487793  8.171865  0.496460  8.033435
95   0.500854  7.963123  0.492065  8.102990
96   0.501465  7.952894  0.492554  8.094713
97   0.509277  7.827859  0.497803  8.010553
98   0.494263  8.066759  0.489502  8.142428
99   0.500122  7.972898  0.492188  8.099176
100  0.509033  7.830409  0.504761  7.898318
101  0.502808  7.929263  0.494263  8.065301
102  0.496338  8.032041  0.513916  7.751632
103  0.495239  8.049225  0.492432  8.093830
104  0.495728  8.041143  0.498535  7.996245
105  0.488525  8.155698  0.506958  7.861717
106  0.502686  7.929720  0.495361  8.046379
107  0.499023  7.987900  0.495117  8.050082
108  0.500122  7.970208  0.494019  8.067432
109  0.496582  8.026491  0.489990  8.131509
110  0.497070  8.018572  0.505493  7.884230
111  0.505371  7.886121  0.504272  7.903582
112  0.505005  7.891857  0.499390  7.981330
113  0.499146  7.985180  0.495972  8.035738
114  0.496704  8.024025  0.497192  8.016206
115  0.496826  8.022013  0.498047  8.002523
116  0.499390  7.981091  0.495117  8.049180
117  0.494141  8.064728  0.504883  7.893452
118  0.499268  7.982956  0.501587  7.945965
119  0.495972  8.035473  0.497925  8.004324
120  0.491821  8.101619  0.487427  8.171670
121  0.497314  8.014031  0.499268  7.982888
122  0.503174  7.920608  0.497681  8.008179
123  0.506958  7.860273  0.504272  7.903085
124  0.515747  7.720151  0.500854  7.957573
125  0.500732  7.959518  0.495361  8.045145
126  0.504639  7.897242  0.511108  7.794099
127  0.511475  7.788260  0.494019  8.066551

2018-02-25 12:12:21.459911 Finish.
Total elapsed time: 15:45:32.46.
