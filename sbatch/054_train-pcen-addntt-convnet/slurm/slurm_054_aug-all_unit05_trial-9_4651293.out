2018-02-24 20:27:55.094413: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:55.094735: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:55.094754: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:55.094762: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:27:55.094770: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:27:08.405217 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit07, unit10, unit01.
Validation set: unit02, unit03.
Test set: unit05.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.933472  0.213161  0.945190  0.182187
1    0.934814  0.218449  0.948120  0.177234
2    0.936523  0.194305  0.946411  0.246748
3    0.907471  0.272906  0.941406  0.249344
4    0.725952  4.027794  0.502686  8.034905
5    0.506348  7.972516  0.500366  8.066379
6    0.496704  8.123693  0.500366  8.063169
7    0.498291  8.095449  0.503784  8.005847
8    0.496948  8.115157  0.498169  8.094675
9    0.503052  8.015296  0.498291  8.090899
10   0.510132  7.895737  0.495605  8.184119
11   0.510864  7.925960  0.504517  8.021838
12   0.496094  8.154603  0.504639  8.014445
13   0.494385  8.177961  0.493164  8.196057
14   0.492065  8.212459  0.501587  8.057770
15   0.500488  8.074410  0.511719  7.892377
16   0.496338  8.139370  0.503540  8.022400
17   0.503906  8.015685  0.491577  8.213617
18   0.510254  7.911849  0.504395  8.005573
19   0.506714  7.967515  0.498779  8.094742
20   0.502197  8.039023  0.500122  8.071851
21   0.509888  7.913857  0.502319  8.035259
22   0.502686  8.028796  0.489502  8.240734
23   0.499146  8.084764  0.499023  8.086201
24   0.500122  8.067982  0.508423  7.933681
25   0.511597  7.882035  0.507080  7.954348
26   0.495483  8.140796  0.491821  8.199358
27   0.501099  8.049378  0.504150  7.999747
28   0.497803  8.101635  0.508301  7.932006
29   0.502441  8.026047  0.501465  8.041392
30   0.510254  7.899353  0.508423  7.928495
31   0.503296  8.010782  0.514282  7.833359
32   0.502075  8.029789  0.495605  8.133752
33   0.505493  7.974085  0.495239  8.139069
34   0.486328  8.282432  0.505127  7.979170
35   0.502563  8.020248  0.493042  8.173484
36   0.499756  8.065056  0.493774  8.161261
37   0.496704  8.113855  0.501343  8.038910
38   0.497559  8.099744  0.502930  8.013020
39   0.504883  7.981404  0.499634  8.065880
40   0.494263  8.152340  0.502686  8.016473
41   0.510742  7.886524  0.491699  8.193374
42   0.497192  8.104761  0.495728  8.128303
43   0.494629  8.145953  0.506348  7.957016
44   0.498535  8.082895  0.504639  7.984478
45   0.501587  8.033633  0.496216  8.120175
46   0.496704  8.112281  0.503174  8.007980
47   0.496582  8.114211  0.503662  8.000079
48   0.497681  8.096477  0.492310  8.183038
49   0.505493  7.970536  0.502808  8.013816
50   0.499390  8.068902  0.493530  8.163340
51   0.498169  8.088570  0.495605  8.129886
52   0.496094  8.122014  0.500610  8.049214
53   0.502808  8.013797  0.498535  8.082660
54   0.500610  8.049211  0.500610  8.049211
55   0.492920  8.173166  0.498047  8.090529
56   0.498291  8.088374  0.502808  8.130870
57   0.493408  8.275414  0.492554  8.272027
58   0.499023  8.159696  0.504028  8.072320
59   0.496582  8.185348  0.502319  8.088674
60   0.497070  8.167858  0.503418  8.062347
61   0.497437  8.153734  0.497925  8.142059
62   0.504150  8.039117  0.499634  8.107468
63   0.502808  8.053349  0.496460  8.151043
64   0.495239  8.167102  0.510010  7.928230
65   0.495483  8.156505  0.497803  8.116222
66   0.497070  8.124667  0.504028  8.010509
67   0.505249  7.987891  0.496948  8.117068
68   0.505249  7.981651  0.501099  8.044736
69   0.490967  8.203255  0.497070  8.102947
70   0.499634  8.059153  0.501953  8.019258
71   0.496826  8.098154  0.500977  8.029156
72   0.493286  8.149012  0.501953  8.008103
73   0.509521  7.884795  0.499878  8.035900
74   0.488892  8.208499  0.502319  7.991896
75   0.495728  8.094543  0.504517  7.951999
76   0.499756  8.025564  0.504639  7.945406
77   0.493408  8.122225  0.498291  8.042181
78   0.499268  8.024505  0.498047  8.041881
79   0.509399  7.858902  0.507690  7.884178
80   0.494751  8.088587  0.498169  8.032245
81   0.496216  8.061622  0.501221  7.980097
82   0.502686  7.955100  0.498657  8.017704
83   0.505249  7.911087  0.498901  8.010783
84   0.500366  7.986018  0.507446  7.871761
85   0.500366  7.983337  0.498657  8.009313
86   0.506836  7.877742  0.495972  8.049789
87   0.497803  8.019526  0.501099  7.965937
88   0.498169  8.011680  0.506226  7.882302
89   0.503174  7.930095  0.494507  8.067435
90   0.511841  7.790331  0.490356  8.132110
91   0.515747  7.726657  0.506226  7.877813
92   0.501953  7.945349  0.497070  8.022639
93   0.500977  7.959870  0.500122  7.973020
94   0.492554  8.093259  0.513794  7.754240
95   0.500244  7.969906  0.509766  7.817780
96   0.503174  7.922581  0.501099  7.955394
97   0.500854  7.959053  0.489746  8.135931
98   0.503662  7.913892  0.494019  8.067463
99   0.498047  8.003100  0.500488  7.964047
100  0.506104  7.874420  0.496216  8.031956
101  0.491455  8.107776  0.505493  7.883905
102  0.498413  7.996723  0.496460  8.027811
103  0.499023  7.986906  0.500610  7.961573
104  0.497559  8.010201  0.499756  7.975150
105  0.504272  7.903129  0.497681  8.008205
106  0.508179  7.840831  0.502319  7.934236
107  0.499146  7.984829  0.498657  7.992609
108  0.507568  7.850542  0.502319  7.934221
109  0.503052  7.922543  0.508057  7.842752
110  0.496582  8.025684  0.496216  8.031522
111  0.495361  8.045144  0.502075  7.938109
112  0.491943  8.099634  0.502808  7.926432
113  0.504883  7.893349  0.505615  7.881672
114  0.499878  7.993359  0.509155  8.023372
115  0.503784  8.108383  0.496460  8.220850
116  0.503540  8.103595  0.500000  8.158186
117  0.503906  8.093602  0.498535  8.178785
118  0.494019  8.250524  0.510864  7.978046
119  0.492920  8.266482  0.502197  8.116207
120  0.493042  8.263115  0.502563  8.109013
121  0.497803  8.185174  0.495117  8.227895
122  0.493164  8.258837  0.502686  8.104837
123  0.497192  8.192854  0.501709  8.119531
124  0.492188  8.272475  0.498901  8.163728
125  0.496460  8.202537  0.493286  8.253139
126  0.508423  8.008592  0.498901  8.161472
127  0.505005  8.062484  0.503784  8.081527

2018-02-25 11:35:34.284663 Finish.
Total elapsed time: 15:08:26.28.
