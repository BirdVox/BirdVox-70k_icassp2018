2018-02-24 20:26:37.977990: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:37.978265: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:37.978278: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:17.661112 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit10, unit01, unit02.
Validation set: unit03, unit05.
Test set: unit07.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.506958  7.865385  0.501587  7.949841
1    0.483276  8.240909  0.496216  8.033896
2    0.500488  7.965261  0.496216  8.032928
3    0.500244  7.968389  0.488037  8.162728
4    0.499390  7.981550  0.500854  7.958036
5    0.498169  8.000738  0.501953  7.940314
6    0.493896  8.080141  0.514038  7.905123
7    0.505981  7.993082  0.500854  8.057315
8    0.498047  8.097959  0.501343  8.035097
9    0.496460  8.100106  0.499756  8.036732
10   0.501099  8.007431  0.503540  7.961554
11   0.505615  7.923063  0.498413  8.033005
12   0.497437  8.044637  0.502930  7.953460
13   0.500000  7.997188  0.503296  7.941891
14   0.500854  7.978499  0.504150  7.923800
15   0.502930  7.941426  0.507690  7.863811
16   0.509399  7.835088  0.504272  7.915435
17   0.490112  8.139977  0.499512  7.988992
18   0.495728  8.048331  0.489990  8.138861
19   0.506226  7.879213  0.501709  7.950443
20   0.499023  7.992580  0.489136  8.149572
21   0.494263  8.067276  0.508179  7.844892
22   0.491943  8.103261  0.498779  7.993844
23   0.492554  8.092718  0.499512  7.981437
24   0.498901  7.990862  0.497681  8.010038
25   0.501099  7.955304  0.499268  7.984268
26   0.501709  7.945154  0.489990  8.131802
27   0.494507  8.059648  0.496338  8.030319
28   0.499268  7.983500  0.504395  7.901660
29   0.500854  7.958013  0.503296  7.919014
30   0.496216  8.031825  0.503174  7.920842
31   0.492676  8.088162  0.503784  7.911028
32   0.508423  7.837045  0.507935  7.844801
33   0.487427  8.171724  0.494141  8.064670
34   0.494019  8.066602  0.505005  7.891442
35   0.508545  7.834996  0.501953  7.940077
36   0.505005  7.891419  0.501953  7.940067
37   0.497559  8.010123  0.503296  7.918654
38   0.505981  7.875838  0.493164  8.080176
39   0.497314  8.014009  0.493408  8.076283
40   0.506104  7.873889  0.498169  8.000384
41   0.504761  7.895295  0.505615  7.881672
42   0.498901  8.018893  0.506226  8.014135
43   0.509399  7.962576  0.495361  8.188256
44   0.504150  8.045967  0.490845  8.259779
45   0.499512  8.119402  0.499023  8.126568
46   0.499878  8.112058  0.495972  8.174257
47   0.497314  8.151821  0.500366  8.101815
48   0.501709  8.079324  0.485107  8.346036
49   0.495972  8.170023  0.499756  8.108104
50   0.498779  8.122893  0.505371  8.015671
51   0.504639  8.026478  0.499023  8.115965
52   0.503418  8.044096  0.495850  8.165026
53   0.504761  8.020323  0.504395  8.025136
54   0.500732  8.083065  0.499268  8.105562
55   0.512207  7.895887  0.502930  8.044292
56   0.492798  8.206473  0.495361  8.164019
57   0.493530  8.192407  0.494629  8.173567
58   0.500854  8.072105  0.502319  8.047373
59   0.493164  8.193838  0.497803  8.117969
60   0.498779  8.101152  0.504272  8.011538
61   0.503662  8.020329  0.499756  8.082249
62   0.498047  8.108786  0.496948  8.125491
63   0.497925  8.108785  0.492798  8.190464
64   0.509521  7.919993  0.503174  8.021396
65   0.490601  8.223186  0.506592  7.964583
66   0.511230  7.889005  0.505371  7.982648
67   0.500366  8.062564  0.498169  8.097241
68   0.505615  7.976529  0.498047  8.097839
69   0.505493  7.977190  0.505249  7.980513
70   0.507568  7.942565  0.505371  7.977434
71   0.501465  8.039897  0.497559  8.102377
72   0.494141  8.157035  0.501831  8.032664
73   0.515747  7.807994  0.497314  8.104741
74   0.505615  7.970639  0.491455  8.198583
75   0.510254  7.895329  0.494629  8.146939
76   0.492188  8.186090  0.498413  8.085560
77   0.497192  8.105082  0.505859  7.965246
78   0.501587  8.033995  0.503052  8.010282
79   0.500977  8.043648  0.499390  8.069153
80   0.499756  8.063194  0.499023  8.074949
81   0.515137  7.815197  0.498779  8.078816
82   0.504272  7.990254  0.499390  8.068936
83   0.500854  8.045312  0.495483  8.131873
84   0.505981  7.962657  0.502686  8.015774
85   0.507202  7.942971  0.493286  8.167268
86   0.500977  8.043311  0.498047  8.090531
87   0.496704  8.112173  0.500122  8.057081
88   0.500000  8.059048  0.494751  8.143652
89   0.495728  8.127912  0.497314  8.102334
90   0.509033  7.913450  0.494385  8.149555
91   0.505859  7.964606  0.493774  8.159392
92   0.502686  8.015762  0.498901  8.076756
93   0.509155  7.911482  0.502930  8.011827
94   0.497925  8.154661  0.494629  8.241531
95   0.502441  8.110644  0.499268  8.157881
96   0.496094  8.206348  0.505249  8.056429
97   0.502441  8.099877  0.500610  8.127773
98   0.493042  8.248482  0.485352  8.371290
99   0.500244  8.130337  0.497437  8.174770
100  0.497925  8.166234  0.497803  8.167590
101  0.503052  8.082471  0.494629  8.217749
102  0.494141  8.225188  0.498779  8.150005
103  0.499390  8.139774  0.495972  8.194475
104  0.498291  8.156707  0.503906  8.065807
105  0.509766  7.970971  0.508545  7.990236
106  0.496338  8.186565  0.496704  8.180225
107  0.501221  8.106970  0.500122  8.124204
108  0.499878  8.127642  0.498901  8.142870
109  0.493652  8.226936  0.496704  8.177190
110  0.507446  8.003466  0.499756  8.126821
111  0.496582  8.177353  0.497681  8.159002
112  0.502686  8.077667  0.505493  8.031726
113  0.487793  8.316312  0.497314  8.162118
114  0.499390  8.127920  0.501465  8.093704
115  0.503052  8.067342  0.494873  8.198367
116  0.494629  8.201484  0.500488  8.106206
117  0.505005  8.032558  0.490845  8.259928
118  0.509521  7.958019  0.502441  8.071245
119  0.491699  8.243488  0.497070  8.156004
120  0.504150  8.040969  0.510864  7.931822
121  0.499023  8.121737  0.500854  8.091277
122  0.485474  8.338239  0.506836  7.992959
123  0.493042  8.214331  0.496948  8.150400
124  0.497437  8.141562  0.498413  8.124843
125  0.509155  7.950726  0.499512  8.105178
126  0.502441  8.056979  0.501587  8.069765
127  0.499023  8.110104  0.512085  7.898591

2018-02-25 12:08:47.037158 Finish.
Total elapsed time: 15:42:30.04.
