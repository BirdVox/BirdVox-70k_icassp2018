2018-02-24 20:26:31.692298: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:31.692583: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:31.692596: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:16.829113 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit02, unit03, unit05.
Validation set: unit07, unit10.
Test set: unit01.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.491821  8.196354  0.492676  8.181465
1    0.504761  7.966849  0.504883  7.912145
2    0.488281  8.165420  0.496338  8.032650
3    0.493164  8.082205  0.497192  8.017311
4    0.493530  8.075363  0.497437  8.012832
5    0.497314  8.046388  0.503784  8.111988
6    0.488770  8.322806  0.497803  8.155939
7    0.499756  8.111662  0.484741  8.342993
8    0.503540  8.032505  0.512573  7.880491
9    0.502808  8.033245  0.494507  8.163014
10   0.499146  8.085291  0.503906  8.005986
11   0.501343  8.045404  0.500732  8.053587
12   0.499390  8.074003  0.502686  8.019810
13   0.497070  8.109525  0.497803  8.097030
14   0.495483  8.133902  0.507080  7.946541
15   0.496704  8.113453  0.495361  8.134811
16   0.503540  8.002777  0.504272  7.990790
17   0.508667  7.919826  0.495605  8.130240
18   0.497437  8.100644  0.510986  7.882177
19   0.498779  8.078881  0.505249  7.974560
20   0.497192  8.104388  0.498535  8.082720
21   0.502563  8.017775  0.493286  8.167294
22   0.502686  8.015785  0.504639  7.984297
23   0.506592  7.952811  0.502686  8.015769
24   0.494751  8.143657  0.500122  8.057083
25   0.506836  7.948867  0.495850  8.125945
26   0.498535  8.082659  0.502563  8.017730
27   0.503418  8.003957  0.495117  8.136513
28   0.499023  8.250165  0.499390  8.256011
29   0.494995  8.310193  0.504517  8.143124
30   0.500244  8.197270  0.496216  8.247918
31   0.496338  8.233373  0.504150  8.096514
32   0.500977  8.135602  0.498779  8.159355
33   0.497803  8.164343  0.495361  8.192893
34   0.494019  8.204566  0.496216  8.160001
35   0.506592  7.985656  0.500000  8.082013
36   0.506714  7.966836  0.507446  7.947215
37   0.499878  8.060506  0.493408  8.156483
38   0.493896  8.142098  0.497559  8.077316
39   0.500610  8.022813  0.502441  7.987971
40   0.490356  8.175509  0.501587  7.991540
41   0.501465  7.989053  0.494873  8.089897
42   0.490845  8.150334  0.497559  8.039692
43   0.489990  8.157163  0.501465  7.971208
44   0.505615  7.902395  0.492188  8.113967
45   0.503784  7.926923  0.493408  8.090307
46   0.498535  8.006825  0.503296  7.929295
47   0.506714  7.873416  0.512573  7.778713
48   0.490479  8.129870  0.509399  7.827221
49   0.501831  7.947042  0.506226  7.876212
50   0.493774  8.074079  0.498657  7.995654
51   0.497314  8.016587  0.505005  7.893552
52   0.494019  8.068352  0.489746  8.136150
53   0.511719  7.785601  0.499390  7.981930
54   0.497192  8.016781  0.503296  7.919316
55   0.497437  8.012604  0.494629  8.057254
56   0.499390  7.981271  0.501099  7.953951
57   0.500122  7.969463  0.487305  8.173754
58   0.493164  8.080304  0.495117  8.049135
59   0.497803  8.006298  0.505249  7.887567
60   0.508057  7.842793  0.506958  7.860296
61   0.494263  8.062681  0.494629  8.056837
62   0.502563  7.930336  0.506714  7.864165
63   0.494873  8.052934  0.501465  7.947843
64   0.500977  7.955626  0.504639  7.897243
65   0.494629  8.056821  0.492676  8.087959
66   0.477295  8.333166  0.495605  8.041252
67   0.491333  8.109365  0.496216  8.031521
68   0.499023  8.209517  0.493896  8.396109
69   0.509277  8.140507  0.498535  8.306506
70   0.502686  8.233260  0.500000  8.270371
71   0.497559  8.303881  0.490967  8.404339
72   0.496704  8.306263  0.492920  8.361669
73   0.501587  8.216530  0.500732  8.224862
74   0.509277  8.081829  0.510864  8.050949
75   0.499146  8.234664  0.495361  8.290497
76   0.498779  8.230381  0.492310  8.329647
77   0.507812  8.074898  0.492676  8.314018
78   0.502197  8.155843  0.497559  8.225923
79   0.498779  8.201716  0.499878  8.179503
80   0.496338  8.232216  0.486938  8.379402
81   0.495605  8.235553  0.502808  8.115350
82   0.503174  8.105493  0.495728  8.221595
83   0.497681  8.186358  0.504028  8.080325
84   0.503540  8.084633  0.506592  8.031916
85   0.496582  8.189875  0.502930  8.084216
86   0.499023  8.143969  0.501465  8.101438
87   0.512329  7.923274  0.497925  8.152416
88   0.494263  8.208530  0.507202  7.997079
89   0.507446  7.990359  0.504639  8.032846
90   0.495850  8.171843  0.493774  8.202642
91   0.501221  8.080069  0.503662  8.038183
92   0.499878  8.096740  0.493774  8.192701
93   0.501831  8.060530  0.506836  7.977573
94   0.499634  8.091481  0.496948  8.132622
95   0.497070  8.128630  0.494263  8.171898
96   0.508301  7.943776  0.499634  8.081663
97   0.500244  8.070155  0.498779  8.092149
98   0.502075  8.037554  0.503052  8.020399
99   0.508911  7.924690  0.503418  8.012022
100  0.497192  8.111305  0.505859  7.970607
101  0.495728  8.133048  0.492432  8.185363
102  0.496704  8.115818  0.506714  7.953850
103  0.494141  8.155988  0.509155  7.913508
104  0.499634  8.066597  0.496826  8.111509
105  0.507324  7.942034  0.497437  8.101169
106  0.494995  8.140343  0.506836  7.949336
107  0.487427  8.262062  0.500610  8.049471
108  0.499512  8.067111  0.496826  8.110340
109  0.501831  8.029633  0.500854  8.045342
110  0.504028  7.994165  0.503296  8.005954
111  0.493896  8.157445  0.494629  8.145632
112  0.497192  8.104309  0.503662  8.000026
113  0.501587  8.033473  0.495972  8.123979
114  0.500854  8.045276  0.493408  8.165296
115  0.498291  8.086594  0.496704  8.112171
116  0.497437  8.099041  0.495972  8.052824
117  0.500732  8.058719  0.500122  8.053001
118  0.500244  8.043124  0.503418  7.986487
119  0.487061  8.243178  0.493896  8.130577
120  0.502563  7.989467  0.497070  8.074315
121  0.497070  8.071984  0.496704  8.075630
122  0.503662  7.962799  0.493042  8.130311
123  0.496338  8.076199  0.504150  7.950168
124  0.510742  7.843786  0.502319  7.976844
125  0.505615  7.923230  0.504272  7.943624
126  0.498291  8.038092  0.497314  8.052815
127  0.504395  7.939194  0.505493  7.920965

2018-02-25 11:32:47.066307 Finish.
Total elapsed time: 15:06:31.07.
