2018-02-24 20:26:43.049232: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:43.049454: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-24 20:26:43.049467: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-24 20:26:18.001187 Start.
Training mixture of experts with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit07, unit10, unit01.
Validation set: unit02, unit03.
Test set: unit05.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense (Dense)               (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_experts (Dense)               (None, 4)             260         bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense[0][0]                 
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_experts[0][0]                 
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
mixture_of_experts (Flatten)     (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
adaptive_threshold (Dense)       (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
concatenate (Concatenate)        (None, 65)            0           mixture_of_experts[0][0]         
                                                                   adaptive_threshold[0][0]         
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             66          concatenate[0][0]                
====================================================================================================
Total params: 682,338
Trainable params: 682,336
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.502808  8.017774  0.501709  8.034489
1    0.498901  8.079053  0.499634  8.066665
2    0.501343  7.988501  0.505981  7.881010
3    0.494751  8.101833  0.503418  8.028956
4    0.502563  8.032362  0.503662  8.008490
5    0.506348  7.962533  0.505249  7.978296
6    0.489136  8.236930  0.497681  8.098358
7    0.498047  8.091937  0.500610  8.050201
8    0.499268  8.071576  0.491089  8.203184
9    0.499878  8.061380  0.507812  7.933376
10   0.504028  8.039574  0.498169  8.196389
11   0.511597  7.978223  0.504150  8.092807
12   0.497925  8.187959  0.493530  8.253885
13   0.504395  8.076573  0.494629  8.228112
14   0.498657  8.159763  0.497559  8.173112
15   0.500732  8.118366  0.495850  8.192024
16   0.504761  8.045793  0.500000  8.117485
17   0.494141  8.206714  0.506592  8.003993
18   0.497437  8.145757  0.495605  8.170721
19   0.494873  8.178203  0.501343  8.070836
20   0.494629  8.173689  0.498901  8.101369
21   0.506958  7.968773  0.496338  8.133909
22   0.505615  7.981898  0.489258  8.238555
23   0.499146  8.076879  0.503052  8.010558
24   0.496216  8.115585  0.492065  8.177801
25   0.492676  8.164225  0.496094  8.105899
26   0.496338  8.098291  0.503906  7.973937
27   0.497070  8.079352  0.495117  8.106952
28   0.503662  7.967330  0.505737  7.930886
29   0.509033  7.875134  0.496460  8.072416
30   0.506958  7.902051  0.488403  8.194904
31   0.508789  7.867124  0.501831  7.975324
32   0.500488  7.994181  0.498413  8.024774
33   0.492676  8.113929  0.500000  7.994917
34   0.495483  8.064856  0.500977  7.975282
35   0.498047  8.020166  0.505493  7.899699
36   0.502563  7.944822  0.500244  7.980280
37   0.489502  8.150181  0.497803  8.016555
38   0.501831  7.951193  0.491943  8.107744
39   0.509521  7.826562  0.494751  8.061148
40   0.494019  8.072053  0.496582  8.030463
41   0.504028  7.911133  0.501099  7.957263
42   0.492554  8.093002  0.500977  7.958270
43   0.494019  8.068819  0.495239  8.049010
44   0.504883  7.894981  0.505249  7.888878
45   0.495728  8.040458  0.499146  7.985771
46   0.492920  8.084863  0.498901  7.989360
47   0.500610  7.962001  0.498779  7.991090
48   0.503662  7.913166  0.501831  7.942286
49   0.496094  8.033697  0.507568  7.850716
50   0.506348  7.870140  0.497925  8.004388
51   0.497925  8.004364  0.494019  8.066618
52   0.491577  8.105525  0.495239  8.047129
53   0.496094  8.033497  0.486938  8.179446
54   0.494263  8.062675  0.503662  7.912821
55   0.499756  7.975093  0.490356  8.124940
56   0.501221  7.951736  0.507690  7.848591
57   0.506714  7.864159  0.496948  8.019846
58   0.512207  7.776584  0.511108  7.794099
59   0.499023  7.986761  0.496704  8.023737
60   0.499756  7.975085  0.500732  7.959516
61   0.501221  7.951731  0.505493  7.883618
62   0.497925  8.190367  0.502197  8.363524
63   0.503784  8.325424  0.496338  8.433948
64   0.490723  8.513961  0.499146  8.367782
65   0.496704  8.396972  0.491943  8.463520
66   0.508423  8.187899  0.493164  8.423815
67   0.506348  8.201491  0.492432  8.415949
68   0.505371  8.197767  0.494873  8.357354
69   0.498779  8.285008  0.501343  8.234319
70   0.512573  8.044189  0.503418  8.182665
71   0.494751  8.313543  0.497314  8.263446
72   0.504639  8.136910  0.495483  8.276043
73   0.494873  8.277765  0.496582  8.242174
74   0.496216  8.240370  0.504517  8.098959
75   0.497925  8.197954  0.512817  7.950767
76   0.501953  8.119123  0.496216  8.204964
77   0.497681  8.175133  0.492310  8.255623
78   0.497803  8.161429  0.492554  8.240529
79   0.492432  8.237425  0.486206  8.332857
80   0.505859  8.011600  0.493164  8.211900
81   0.503906  8.034849  0.486694  8.308521
82   0.511475  7.905750  0.485840  8.315722
83   0.503540  8.027577  0.502930  8.034701
84   0.498535  8.103141  0.503418  8.022171
85   0.495728  8.144141  0.500244  8.069466
86   0.501587  8.046191  0.502197  8.034816
87   0.510986  7.891822  0.502808  8.022398
88   0.498779  8.086253  0.500854  8.051798
89   0.497803  8.100128  0.511841  7.873059
90   0.494507  8.151772  0.502563  8.021284
91   0.499878  8.064042  0.491333  8.201283
92   0.497437  8.102504  0.512085  7.866030
93   0.499878  8.062484  0.503174  8.009087
94   0.495605  8.130857  0.494751  8.144434
95   0.497192  8.104930  0.509521  7.906072
96   0.503784  7.998443  0.499146  8.073118
97   0.501709  8.031732  0.501587  8.033641
98   0.494995  8.139847  0.503418  8.004051
99   0.502930  8.011896  0.493164  8.169279
100  0.501343  8.037440  0.499756  8.063007
101  0.501465  8.035454  0.509155  7.911493
102  0.495239  8.135789  0.489136  8.234164
103  0.501953  8.027570  0.500488  8.051179
104  0.498901  8.076757  0.499390  8.068886
105  0.504395  7.988217  0.497070  8.106269
106  0.492920  8.173165  0.503418  8.003957
107  0.498657  8.071340  0.502686  8.066775
108  0.497070  8.154865  0.482910  8.378925
109  0.496948  8.153464  0.502441  8.064208
110  0.502319  8.064445  0.498169  8.128861
111  0.499512  8.105655  0.501099  8.078508
112  0.488281  8.280941  0.503174  8.041559
113  0.505005  8.010348  0.496948  8.136714
114  0.506836  7.976941  0.497803  8.118755
115  0.496216  8.141795  0.505859  7.985737
116  0.496826  8.127371  0.495239  8.150236
117  0.506470  7.968705  0.501343  8.047895
118  0.503174  8.016109  0.508301  7.931724
119  0.487793  8.255978  0.508545  7.922404
120  0.498901  8.073376  0.505005  7.973257
121  0.494751  8.133897  0.491089  8.189409
122  0.491699  8.176804  0.502563  8.000695
123  0.506104  7.941363  0.501831  8.006557
124  0.505493  7.945284  0.500610  8.020222
125  0.489990  8.186672  0.502075  7.991143
126  0.494263  8.112891  0.493042  8.129552
127  0.503906  7.953632  0.491943  8.141644

2018-02-25 12:12:48.101171 Finish.
Total elapsed time: 15:46:30.10.
