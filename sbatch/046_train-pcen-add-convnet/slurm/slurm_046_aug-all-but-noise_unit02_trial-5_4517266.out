2018-02-12 20:13:49.114825: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:13:49.115151: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:13:49.115162: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 20:13:30.428691 Start.
Training convnet with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.500000  8.067897  0.490112  8.226075
1    0.498413  8.091308  0.506592  7.958589
2    0.504883  7.985395  0.493042  8.175564
3    0.499634  8.068750  0.500122  8.060356
4    0.490234  8.219289  0.499390  8.071320
5    0.491577  8.196906  0.494873  8.143472
6    0.507690  7.936622  0.501953  8.028860
7    0.490112  8.219516  0.506470  7.954718
8    0.496582  8.061480  0.498047  8.017780
9    0.499023  7.996020  0.491089  8.118495
10   0.503906  7.912326  0.499023  7.988858
11   0.539307  6.891449  0.721924  1.027547
12   0.959229  0.183705  0.783447  1.161268
13   0.969727  0.138277  0.728027  1.492170
14   0.972534  0.122663  0.791626  1.029531
15   0.977539  0.098076  0.807617  0.982800
16   0.979736  0.090834  0.844849  0.778947
17   0.977905  0.092958  0.826538  0.949910
18   0.980225  0.091438  0.760620  1.037859
19   0.980469  0.088556  0.812134  0.989295
20   0.982544  0.079814  0.828369  0.911173
21   0.982910  0.076078  0.798340  1.057875
22   0.978027  0.089678  0.814087  0.826863
23   0.983521  0.070652  0.813721  0.799889
24   0.979858  0.077871  0.779053  1.211709
25   0.982666  0.073135  0.842773  0.794994
26   0.984497  0.066499  0.831665  0.804494
27   0.982300  0.067915  0.826050  0.920264
28   0.981689  0.074934  0.787598  1.129295
29   0.982788  0.070471  0.828369  0.835721
30   0.983765  0.068702  0.795532  1.054162
31   0.981079  0.076259  0.841553  0.785338
32   0.983765  0.067264  0.829590  0.878406
33   0.982788  0.067438  0.819092  0.846452
34   0.983398  0.065696  0.820557  1.017190
35   0.984131  0.065733  0.798828  1.116514
36   0.984131  0.067072  0.815308  0.919420
37   0.985962  0.058330  0.774658  1.371849
38   0.984009  0.066306  0.834839  0.859025
39   0.985474  0.063926  0.812744  0.960277
40   0.985107  0.063934  0.824463  0.858558
41   0.983643  0.063702  0.812134  0.866624
42   0.983398  0.064202  0.823242  0.856605
43   0.984253  0.063516  0.820068  0.900673
44   0.984131  0.067302  0.857300  0.639731
45   0.984619  0.062390  0.834961  0.737822
46   0.984131  0.062003  0.852905  0.705348
47   0.986450  0.058843  0.810669  0.884465
48   0.984131  0.066345  0.865479  0.638588
49   0.985474  0.061345  0.793579  1.073633
50   0.983032  0.067758  0.818726  0.887271
51   0.985840  0.057892  0.835571  0.851020
52   0.984375  0.065694  0.832764  0.826451
53   0.987793  0.055969  0.823975  0.920960
54   0.984985  0.061332  0.848633  0.746823
55   0.987061  0.056115  0.797485  1.097634
56   0.983398  0.063014  0.814819  0.988404
57   0.985229  0.063364  0.801392  1.058088
58   0.985596  0.060657  0.833008  0.718141
59   0.987305  0.055365  0.782837  1.052021
60   0.985718  0.059647  0.823364  0.859570
61   0.985718  0.057768  0.833984  0.851007
62   0.987671  0.053601  0.837891  0.882113
63   0.987061  0.055644  0.815186  0.935720
64   0.986450  0.053114  0.826782  0.926092
65   0.984375  0.062570  0.814331  0.952803
66   0.987061  0.052517  0.821777  0.919907
67   0.986328  0.057662  0.833252  0.791551
68   0.987549  0.056740  0.855469  0.671815
69   0.983887  0.065299  0.801758  0.966166
70   0.987915  0.048239  0.841431  0.864036
71   0.986328  0.057159  0.834839  0.902499
72   0.986450  0.053103  0.810425  0.966898
73   0.985229  0.057990  0.846313  0.736292
74   0.985596  0.056955  0.843628  0.734116
75   0.984741  0.061626  0.813843  0.852215
76   0.986572  0.057518  0.823242  0.900767
77   0.986694  0.059950  0.814209  0.857857
78   0.986694  0.057904  0.817505  0.908940
79   0.987549  0.050025  0.835083  0.788926
80   0.986572  0.056292  0.802734  1.004352
81   0.987061  0.051362  0.861816  0.682823
82   0.985962  0.054906  0.830200  0.947560
83   0.986816  0.055254  0.843506  0.825826
84   0.989990  0.045102  0.811890  1.029331
85   0.986816  0.056880  0.850464  0.669977
86   0.985107  0.061057  0.862183  0.642447
87   0.986694  0.053618  0.812744  0.954287
88   0.988403  0.051007  0.833130  0.804212
89   0.987183  0.049747  0.854858  0.704996
90   0.990234  0.044293  0.809570  0.976739
91   0.987183  0.052337  0.845703  0.788081
92   0.988403  0.047734  0.841553  0.821273
93   0.987793  0.050661  0.833496  0.905438
94   0.987793  0.049782  0.837524  0.937512
95   0.987915  0.051428  0.831665  0.891597
96   0.989502  0.048769  0.803223  1.140334
97   0.986328  0.054048  0.838013  0.755540
98   0.986328  0.057337  0.832764  0.824353
99   0.986816  0.050673  0.828979  0.934816
100  0.987549  0.052335  0.826904  0.960934
101  0.987793  0.050146  0.842651  0.860345
102  0.987915  0.049853  0.847168  0.859924
103  0.987793  0.047761  0.849365  0.780959
104  0.985962  0.056193  0.791382  1.208475
105  0.986938  0.053044  0.811768  0.978436
106  0.988647  0.047577  0.811646  0.990881
107  0.987305  0.049061  0.827759  0.916269
108  0.984497  0.058216  0.836060  0.895876
109  0.986694  0.053247  0.821533  0.917613
110  0.988647  0.047967  0.835938  0.878104
111  0.987183  0.053278  0.834106  0.695242
112  0.987549  0.053421  0.812012  0.979756
113  0.986572  0.049997  0.804810  1.085195
114  0.988037  0.047048  0.818481  1.015235
115  0.989258  0.045053  0.850342  0.762039
116  0.986084  0.051043  0.854614  0.696450
117  0.986572  0.057184  0.834717  0.903280
118  0.988159  0.052600  0.830444  0.809903
119  0.986816  0.051485  0.804321  1.055296
120  0.990479  0.047005  0.824463  1.001491
121  0.988770  0.046835  0.807373  0.987020
122  0.989258  0.043612  0.831421  0.930676
123  0.987305  0.050271  0.819824  1.010330
124  0.987183  0.056795  0.848999  0.763398
125  0.986206  0.057471  0.847412  0.749364
126  0.989014  0.047000  0.833618  0.829787
127  0.988525  0.051170  0.822998  0.876303

2018-02-13 12:57:04.130800 Finish.
Total elapsed time: 16:43:34.13.
