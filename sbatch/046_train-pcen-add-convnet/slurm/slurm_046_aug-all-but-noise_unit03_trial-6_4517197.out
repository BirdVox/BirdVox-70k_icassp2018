2018-02-12 20:01:38.983149: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:01:38.983642: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:01:38.983657: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 20:01:19.080704 Start.
Training convnet with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.973877  0.118757  0.884644  0.453937
1    0.976196  0.097841  0.876587  0.515890
2    0.975952  0.108833  0.895630  0.414131
3    0.977417  0.103540  0.882690  0.444734
4    0.977539  0.098969  0.896118  0.484641
5    0.977295  0.097215  0.888428  0.519924
6    0.980713  0.081852  0.896118  0.472695
7    0.976440  0.102195  0.880127  0.450807
8    0.981689  0.085210  0.874634  0.547262
9    0.979004  0.094517  0.879517  0.500557
10   0.979614  0.085693  0.890747  0.473079
11   0.980835  0.085126  0.879639  0.547730
12   0.979370  0.086679  0.881714  0.565981
13   0.977905  0.095799  0.864746  0.532718
14   0.982788  0.082421  0.888550  0.500011
15   0.980103  0.083133  0.893799  0.412176
16   0.981689  0.078348  0.878784  0.587301
17   0.982788  0.078972  0.882446  0.620471
18   0.979736  0.087255  0.887939  0.451033
19   0.982788  0.071469  0.903809  0.464883
20   0.980957  0.079129  0.896240  0.435145
21   0.981079  0.082653  0.881592  0.679059
22   0.981812  0.084507  0.904053  0.409186
23   0.983887  0.071039  0.913940  0.369584
24   0.980591  0.080421  0.898438  0.386983
25   0.984985  0.068154  0.904175  0.422598
26   0.982666  0.078885  0.911987  0.385396
27   0.983398  0.074140  0.891724  0.417238
28   0.984009  0.073719  0.877930  0.535643
29   0.982056  0.076781  0.873535  0.548357
30   0.983276  0.071740  0.866333  0.626622
31   0.983765  0.068226  0.885498  0.535676
32   0.984497  0.069797  0.874268  0.606739
33   0.983521  0.073282  0.888428  0.496297
34   0.983765  0.071293  0.898315  0.433667
35   0.984253  0.066169  0.896729  0.481226
36   0.983276  0.075572  0.896484  0.430518
37   0.986694  0.063259  0.894653  0.475390
38   0.983887  0.067677  0.888916  0.553355
39   0.986572  0.060530  0.899048  0.491323
40   0.980225  0.084087  0.897705  0.430572
41   0.984741  0.059925  0.896973  0.436590
42   0.986816  0.061167  0.900024  0.462910
43   0.983154  0.078511  0.868408  0.512791
44   0.986450  0.058433  0.890015  0.515039
45   0.986450  0.060411  0.888672  0.500187
46   0.985962  0.060261  0.887573  0.508525
47   0.982056  0.076969  0.898071  0.436753
48   0.986206  0.060549  0.896851  0.469627
49   0.985718  0.065364  0.861084  0.535831
50   0.984619  0.063532  0.877441  0.548907
51   0.985229  0.069331  0.867798  0.658984
52   0.983765  0.072436  0.843506  0.716713
53   0.985352  0.064340  0.906494  0.407514
54   0.983765  0.065353  0.872803  0.469964
55   0.985962  0.061924  0.881836  0.523946
56   0.985352  0.067674  0.901367  0.431363
57   0.986328  0.064432  0.897949  0.484533
58   0.986206  0.058871  0.899292  0.423213
59   0.984985  0.057596  0.892334  0.481535
60   0.984741  0.064976  0.846680  0.689160
61   0.984253  0.064367  0.888916  0.507868
62   0.986938  0.055987  0.884155  0.497910
63   0.984863  0.062502  0.881470  0.436891
64   0.988770  0.049852  0.894897  0.510405
65   0.986572  0.055126  0.907227  0.396494
66   0.986084  0.058251  0.887329  0.493332
67   0.987305  0.056848  0.900146  0.507053
68   0.985840  0.058496  0.897949  0.438798
69   0.987061  0.061651  0.901978  0.405744
70   0.987671  0.058314  0.893311  0.456712
71   0.988525  0.051595  0.898315  0.495093
72   0.985840  0.064981  0.893311  0.499815
73   0.984863  0.062261  0.885376  0.477261
74   0.987427  0.056299  0.870117  0.598301
75   0.985596  0.063558  0.901855  0.432955
76   0.986938  0.058354  0.900635  0.464568
77   0.985718  0.059258  0.864746  0.660531
78   0.984985  0.068477  0.885742  0.420503
79   0.984253  0.058929  0.887207  0.569485
80   0.984863  0.062251  0.901123  0.468715
81   0.986938  0.055248  0.895142  0.486941
82   0.988892  0.049532  0.889404  0.530527
83   0.985474  0.060178  0.895386  0.448968
84   0.985840  0.055279  0.906372  0.404359
85   0.987305  0.059471  0.875122  0.600897
86   0.986572  0.056255  0.893555  0.474077
87   0.987305  0.055368  0.869995  0.636876
88   0.983887  0.060788  0.885254  0.500006
89   0.987183  0.053834  0.892456  0.482030
90   0.987549  0.052332  0.903320  0.478699
91   0.987793  0.057159  0.903442  0.428146
92   0.987915  0.052072  0.895874  0.541261
93   0.988037  0.057767  0.875610  0.526975
94   0.987793  0.050380  0.906250  0.412526
95   0.983032  0.066119  0.889526  0.528116
96   0.987671  0.053135  0.884766  0.579330
97   0.988647  0.049175  0.896484  0.504166
98   0.986694  0.056122  0.889038  0.480258
99   0.986328  0.058317  0.865723  0.563697
100  0.987183  0.057157  0.883423  0.493777
101  0.987793  0.050713  0.899658  0.468511
102  0.988037  0.053482  0.897583  0.479652
103  0.987915  0.052270  0.899536  0.484690
104  0.988281  0.054663  0.886353  0.565493
105  0.988525  0.055311  0.893921  0.519113
106  0.987427  0.057405  0.914307  0.386247
107  0.984985  0.062910  0.897217  0.535270
108  0.987427  0.051806  0.888916  0.580376
109  0.987915  0.054671  0.885742  0.580206
110  0.987427  0.059131  0.864502  0.609150
111  0.987915  0.054178  0.898804  0.409198
112  0.986572  0.056334  0.904053  0.459151
113  0.987549  0.050149  0.899048  0.478883
114  0.988525  0.050805  0.904785  0.431516
115  0.986816  0.059063  0.892578  0.429777
116  0.988403  0.052732  0.896729  0.459527
117  0.987671  0.054977  0.901367  0.448129
118  0.986938  0.054886  0.891846  0.492144
119  0.986206  0.055649  0.891602  0.540418
120  0.987183  0.055564  0.896729  0.464523
121  0.986450  0.052953  0.906372  0.411627
122  0.990479  0.049298  0.895142  0.505544
123  0.988037  0.050483  0.900269  0.437782
124  0.988403  0.055995  0.872192  0.515252
125  0.987427  0.050052  0.888428  0.516516
126  0.984497  0.060726  0.889038  0.424075
127  0.989624  0.049868  0.892212  0.532327

2018-02-13 12:13:45.882390 Finish.
Total elapsed time: 16:12:26.88.
