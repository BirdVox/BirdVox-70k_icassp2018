2018-02-12 20:13:51.670598: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:13:51.671071: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:13:51.671085: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 20:13:35.723676 Start.
Training convnet with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.977783  0.095255  0.842896  0.679314
1    0.976807  0.103403  0.819092  0.756353
2    0.974243  0.097278  0.876709  0.546527
3    0.979858  0.080085  0.872559  0.551933
4    0.979492  0.085444  0.874634  0.577823
5    0.978882  0.092095  0.871460  0.530715
6    0.981323  0.082044  0.859741  0.650086
7    0.983154  0.076388  0.845703  0.688839
8    0.979126  0.089889  0.864990  0.575242
9    0.982544  0.079593  0.856689  0.600851
10   0.982910  0.078000  0.870605  0.632847
11   0.983521  0.078222  0.860596  0.633499
12   0.982056  0.073826  0.859619  0.691729
13   0.984009  0.070719  0.848755  0.857520
14   0.981567  0.073855  0.869141  0.601299
15   0.984375  0.069780  0.855835  0.732479
16   0.986206  0.068785  0.845703  0.844036
17   0.986084  0.065573  0.846558  0.765429
18   0.985474  0.061889  0.850708  0.732204
19   0.984375  0.068528  0.870239  0.621089
20   0.987671  0.064129  0.868774  0.722286
21   0.986938  0.064010  0.880005  0.627922
22   0.986572  0.061084  0.865479  0.767036
23   0.986450  0.062164  0.869019  0.707678
24   0.985229  0.065262  0.863281  0.659509
25   0.982910  0.068400  0.867798  0.672678
26   0.986938  0.060294  0.867188  0.746728
27   0.987061  0.061278  0.854248  0.844462
28   0.987549  0.057933  0.881836  0.670097
29   0.985718  0.065976  0.872314  0.670084
30   0.986694  0.063744  0.864014  0.652247
31   0.986694  0.064137  0.872803  0.664042
32   0.989014  0.060317  0.872803  0.774166
33   0.988647  0.056820  0.842041  0.967232
34   0.987061  0.055005  0.856934  0.848572
35   0.986816  0.061080  0.876831  0.627286
36   0.984375  0.062131  0.867798  0.748812
37   0.985107  0.064104  0.855957  0.793998
38   0.986938  0.056076  0.850464  0.776850
39   0.986816  0.057186  0.866943  0.704198
40   0.988281  0.056209  0.838135  0.972955
41   0.987061  0.059811  0.826294  1.079015
42   0.987793  0.055357  0.873657  0.642186
43   0.988159  0.053377  0.870239  0.738508
44   0.987549  0.053921  0.872070  0.674129
45   0.987915  0.055500  0.878052  0.670368
46   0.987671  0.054162  0.841675  0.952528
47   0.985107  0.059983  0.839600  0.881291
48   0.989014  0.051796  0.868164  0.691353
49   0.990234  0.047689  0.856812  0.805080
50   0.988525  0.051757  0.852417  0.898754
51   0.987671  0.053163  0.865845  0.723193
52   0.989380  0.050855  0.845215  0.974898
53   0.988770  0.049494  0.867554  0.838499
54   0.988037  0.054164  0.853638  0.790145
55   0.989258  0.050151  0.862427  0.822051
56   0.987549  0.056408  0.842773  0.792702
57   0.987793  0.055713  0.857666  0.761511
58   0.988525  0.053248  0.864624  0.692718
59   0.989136  0.051289  0.860840  0.769249
60   0.989014  0.050057  0.850220  0.889346
61   0.988525  0.049477  0.843750  0.987295
62   0.989990  0.046098  0.846191  0.942941
63   0.989258  0.043026  0.860718  0.879321
64   0.988525  0.051597  0.834473  1.033272
65   0.987427  0.058020  0.836670  0.951159
66   0.990234  0.047165  0.855591  0.872291
67   0.989136  0.047157  0.847900  0.918063
68   0.989136  0.050990  0.847778  0.913233
69   0.991089  0.045406  0.851440  0.902588
70   0.989014  0.047786  0.866333  0.788791
71   0.989746  0.050274  0.852295  0.794884
72   0.989502  0.047868  0.875244  0.657545
73   0.988770  0.051698  0.847778  0.828889
74   0.989990  0.048043  0.845337  0.867659
75   0.988403  0.053464  0.849487  0.896366
76   0.988281  0.052145  0.873413  0.675880
77   0.990234  0.044479  0.855713  0.935774
78   0.990845  0.047864  0.841553  0.900453
79   0.988647  0.048964  0.874512  0.669921
80   0.992065  0.039756  0.862793  0.946953
81   0.990479  0.048024  0.869873  0.779420
82   0.988770  0.048293  0.854614  1.031046
83   0.988281  0.047902  0.864624  0.868456
84   0.991089  0.044262  0.859009  0.869136
85   0.990845  0.049753  0.856445  0.948085
86   0.992065  0.041253  0.868042  0.846816
87   0.991089  0.043226  0.865356  0.832000
88   0.988525  0.051325  0.829468  1.152205
89   0.988403  0.046323  0.848145  0.937639
90   0.989014  0.051822  0.844238  1.012450
91   0.988403  0.048345  0.854736  0.837474
92   0.988770  0.048430  0.831787  1.033265
93   0.992554  0.040366  0.860229  0.875770
94   0.989380  0.048721  0.835938  1.013424
95   0.990112  0.045203  0.815430  1.404224
96   0.989990  0.045784  0.850830  0.971835
97   0.990356  0.044986  0.851562  0.847577
98   0.992310  0.041189  0.866821  0.852956
99   0.990234  0.042694  0.828735  1.055951
100  0.990356  0.042973  0.848267  0.989370
101  0.991577  0.041115  0.840210  1.093577
102  0.991333  0.045567  0.852295  0.915173
103  0.989624  0.048187  0.859741  0.781939
104  0.989502  0.047267  0.871338  0.868252
105  0.990601  0.047057  0.866577  0.763881
106  0.990967  0.042196  0.865845  0.796446
107  0.991089  0.044156  0.863159  0.841953
108  0.991211  0.043162  0.847778  0.956724
109  0.992188  0.043395  0.850342  0.946618
110  0.990112  0.045383  0.818237  1.238534
111  0.992676  0.040801  0.858643  0.845322
112  0.990112  0.045687  0.848877  0.891670
113  0.992798  0.036570  0.865479  0.895938
114  0.992188  0.038170  0.853027  0.986157
115  0.991455  0.042864  0.827026  1.195764
116  0.991211  0.042850  0.837891  1.094001
117  0.991577  0.041255  0.825928  1.149692
118  0.992798  0.037948  0.862915  0.910242
119  0.989746  0.048724  0.864014  0.886068
120  0.992310  0.038734  0.880249  0.811327
121  0.990967  0.041650  0.842407  0.986872
122  0.992188  0.035933  0.836914  1.181203
123  0.990356  0.050604  0.852783  0.841304
124  0.989380  0.046736  0.855225  0.907308
125  0.991699  0.039713  0.828735  1.200236
126  0.992554  0.039815  0.828247  1.232708
127  0.993652  0.037127  0.831055  1.273894

2018-02-13 13:27:22.916700 Finish.
Total elapsed time: 17:13:47.92.
