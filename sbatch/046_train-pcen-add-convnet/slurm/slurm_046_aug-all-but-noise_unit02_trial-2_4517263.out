2018-02-12 20:13:26.873586: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:13:26.873858: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:13:26.873872: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 20:13:09.247109 Start.
Training convnet with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.978027  0.099349  0.858643  0.653450
1    0.979370  0.095273  0.836670  0.921155
2    0.981201  0.080613  0.857056  0.735070
3    0.978882  0.091898  0.851685  0.788713
4    0.980835  0.084783  0.849609  0.790657
5    0.981079  0.079694  0.850708  0.814254
6    0.983154  0.073782  0.858032  0.709999
7    0.984253  0.068992  0.842285  0.902925
8    0.983887  0.069209  0.856934  0.931399
9    0.981323  0.079182  0.838745  0.922430
10   0.979736  0.089949  0.841431  0.912010
11   0.987061  0.066028  0.849854  0.894993
12   0.982544  0.070369  0.837402  0.808144
13   0.982544  0.072912  0.828003  0.885687
14   0.983398  0.064961  0.858521  0.821813
15   0.986206  0.059624  0.845337  0.847728
16   0.985352  0.071143  0.865845  0.712448
17   0.986328  0.064672  0.855713  0.695632
18   0.985474  0.066337  0.847656  0.770618
19   0.984131  0.070185  0.852783  0.762023
20   0.986938  0.057707  0.852173  0.817453
21   0.984009  0.067171  0.859375  0.700932
22   0.984253  0.066283  0.823242  1.005277
23   0.988159  0.056925  0.813599  1.146224
24   0.985840  0.061499  0.867065  0.736301
25   0.987183  0.059452  0.848999  0.923873
26   0.986938  0.058309  0.833374  0.924947
27   0.984619  0.063469  0.823486  1.117221
28   0.985596  0.062713  0.823486  1.033656
29   0.988159  0.051325  0.844482  0.853106
30   0.984375  0.061664  0.869385  0.682715
31   0.987305  0.051640  0.815063  1.192736
32   0.984253  0.069406  0.828125  0.842211
33   0.987793  0.055784  0.848999  0.869640
34   0.984375  0.064099  0.862427  0.686682
35   0.986206  0.060524  0.837280  0.870274
36   0.988403  0.057593  0.852295  0.880311
37   0.987427  0.058317  0.855469  0.791358
38   0.987671  0.053481  0.864380  0.746545
39   0.987915  0.053622  0.844971  0.835395
40   0.986938  0.059607  0.884277  0.629520
41   0.986084  0.059266  0.838135  0.955723
42   0.987427  0.055676  0.847046  0.790046
43   0.989868  0.046504  0.857056  0.874941
44   0.987549  0.051541  0.866333  0.679643
45   0.987915  0.057750  0.862793  0.794664
46   0.988770  0.052549  0.837646  0.935173
47   0.988403  0.054383  0.848633  0.801483
48   0.987915  0.053727  0.828735  0.938562
49   0.990112  0.045054  0.859131  0.767436
50   0.987671  0.054035  0.859009  0.716836
51   0.987915  0.049913  0.873047  0.667214
52   0.989136  0.047498  0.874878  0.564572
53   0.985962  0.052647  0.842163  0.951067
54   0.987915  0.055264  0.853638  0.818746
55   0.986206  0.056152  0.866455  0.768575
56   0.988525  0.052479  0.865967  0.741798
57   0.987549  0.051089  0.862305  0.790187
58   0.989136  0.044923  0.851562  0.844529
59   0.988770  0.052244  0.856445  0.864082
60   0.988037  0.053146  0.853882  0.758154
61   0.989014  0.049615  0.851196  0.856940
62   0.989014  0.047935  0.857300  0.761620
63   0.989014  0.048761  0.859985  0.658412
64   0.987549  0.053510  0.838257  0.953875
65   0.989258  0.049613  0.850342  0.878905
66   0.989258  0.051036  0.850952  0.767001
67   0.988525  0.050541  0.869995  0.666561
68   0.987915  0.048937  0.881958  0.627376
69   0.988892  0.054244  0.872070  0.646057
70   0.988647  0.048204  0.864258  0.739266
71   0.989746  0.042011  0.867554  0.715378
72   0.987671  0.050586  0.880127  0.674981
73   0.989502  0.049245  0.840942  0.971535
74   0.988281  0.050881  0.845459  0.885376
75   0.988403  0.048844  0.858643  0.915723
76   0.987427  0.052589  0.851807  0.939808
77   0.989258  0.048489  0.881958  0.688701
78   0.987793  0.051757  0.843262  0.900911
79   0.988892  0.049023  0.872192  0.742943
80   0.990234  0.045735  0.864990  0.789236
81   0.988159  0.050332  0.859863  0.783130
82   0.990112  0.048273  0.857178  0.825501
83   0.989014  0.048697  0.854980  0.756178
84   0.987671  0.051730  0.866089  0.770674
85   0.988159  0.052153  0.860107  0.782955
86   0.990967  0.040152  0.835327  1.004393
87   0.989746  0.049907  0.866943  0.779021
88   0.987061  0.051519  0.855225  0.834420
89   0.988281  0.046450  0.841675  0.916634
90   0.987671  0.050307  0.845825  0.930334
91   0.989258  0.044410  0.860352  0.784110
92   0.990112  0.047483  0.843384  0.889852
93   0.988770  0.048809  0.852783  0.818040
94   0.989258  0.046916  0.840576  1.007903
95   0.990356  0.043484  0.851196  0.855801
96   0.991455  0.040389  0.846069  0.888632
97   0.987793  0.053015  0.868286  0.690881
98   0.988159  0.053269  0.863281  0.807862
99   0.990723  0.048068  0.875977  0.717155
100  0.990479  0.046113  0.868652  0.702763
101  0.991211  0.042620  0.856445  0.903152
102  0.988281  0.051352  0.855591  0.755370
103  0.987061  0.050948  0.860840  0.703384
104  0.988403  0.049106  0.870850  0.698727
105  0.989258  0.046416  0.841553  0.902926
106  0.991943  0.036918  0.822632  1.175427
107  0.989258  0.046801  0.840820  0.964117
108  0.991943  0.038153  0.862061  0.817000
109  0.987915  0.052143  0.851562  0.830388
110  0.987793  0.051784  0.863770  0.779353
111  0.989502  0.048224  0.873901  0.722397
112  0.989868  0.043199  0.870605  0.704403
113  0.989868  0.046290  0.857544  0.822591
114  0.990601  0.043472  0.839111  0.850794
115  0.988770  0.044252  0.885620  0.706638
116  0.989136  0.047017  0.858276  0.816328
117  0.990356  0.044138  0.873779  0.695725
118  0.989868  0.042743  0.874146  0.706498
119  0.989136  0.045809  0.852905  0.796134
120  0.990112  0.042729  0.833740  0.994415
121  0.990723  0.039566  0.858398  0.836808
122  0.989136  0.044045  0.858398  0.757656
123  0.988159  0.049549  0.862915  0.751581
124  0.991211  0.040868  0.869507  0.775291
125  0.988892  0.049748  0.851562  0.804326
126  0.991455  0.041378  0.858032  0.706741
127  0.989624  0.047665  0.871948  0.672512

2018-02-13 11:49:23.167308 Finish.
Total elapsed time: 15:36:14.17.
