2018-02-12 20:13:49.254656: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:13:49.254975: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:13:49.254989: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 20:13:28.586766 Start.
Training convnet with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.489258  8.242699  0.503784  8.007000
1    0.495483  8.139536  0.503784  8.004600
2    0.505371  7.978095  0.492310  8.187775
3    0.496582  8.118223  0.497192  8.107757
4    0.498169  8.091506  0.509644  7.906091
5    0.504150  7.994253  0.492310  8.184761
6    0.500000  8.060530  0.487549  8.260968
7    0.498657  8.081721  0.504761  7.983163
8    0.493774  8.160098  0.496826  8.110781
9    0.501709  8.031978  0.500610  8.049595
10   0.501099  8.014724  0.502197  7.968602
11   0.493652  8.092938  0.496582  8.038280
12   0.505371  7.894217  0.496582  8.031384
13   0.859375  1.556137  0.812256  1.077632
14   0.971069  0.130632  0.807861  1.334448
15   0.976807  0.095371  0.818481  1.150966
16   0.975830  0.104127  0.783813  1.289665
17   0.977783  0.085533  0.809082  1.078603
18   0.979980  0.089265  0.788574  1.317471
19   0.979980  0.073505  0.806519  1.022875
20   0.980591  0.069755  0.815674  1.093417
21   0.983765  0.060341  0.795776  1.043788
22   0.983276  0.063700  0.842651  0.819709
23   0.982178  0.071582  0.825562  0.834369
24   0.980347  0.072123  0.812500  0.920853
25   0.984131  0.060181  0.822510  0.971851
26   0.984497  0.064578  0.816406  0.993321
27   0.985962  0.062771  0.799194  1.181248
28   0.987061  0.056018  0.823486  0.983580
29   0.985229  0.057673  0.805298  1.086138
30   0.987427  0.052944  0.811401  1.106344
31   0.988159  0.051673  0.848145  0.747421
32   0.984497  0.064124  0.843994  0.754305
33   0.985962  0.056144  0.818604  1.046279
34   0.983765  0.066211  0.826782  0.827138
35   0.987793  0.053068  0.837036  0.944789
36   0.986450  0.061281  0.835571  0.880596
37   0.983276  0.063060  0.851074  0.787815
38   0.988037  0.048321  0.819580  0.910587
39   0.984497  0.059995  0.821289  0.917603
40   0.985229  0.058617  0.802368  1.038297
41   0.986816  0.054841  0.848389  0.814409
42   0.985229  0.061167  0.848633  0.766774
43   0.988892  0.043308  0.815186  1.063250
44   0.988281  0.049304  0.842407  0.827507
45   0.986084  0.056733  0.790161  1.048726
46   0.988525  0.044071  0.797974  1.037856
47   0.986816  0.053911  0.827637  0.945604
48   0.987915  0.046311  0.817749  1.008232
49   0.985107  0.059369  0.846436  0.709565
50   0.987671  0.053780  0.795898  1.068226
51   0.986938  0.053981  0.831543  0.889849
52   0.986694  0.056633  0.836060  0.736335
53   0.986938  0.050256  0.746094  1.553622
54   0.986816  0.051175  0.818726  0.980831
55   0.986206  0.051784  0.836060  0.868009
56   0.986450  0.058006  0.796753  1.165261
57   0.987427  0.057404  0.838623  0.716171
58   0.988281  0.047441  0.816162  0.973439
59   0.989868  0.041195  0.835693  0.952318
60   0.987305  0.052436  0.796753  1.089597
61   0.986694  0.050191  0.792847  1.046711
62   0.988770  0.045181  0.816040  0.990944
63   0.985474  0.060018  0.814697  0.974717
64   0.989380  0.045810  0.820923  0.960657
65   0.989014  0.050697  0.845215  0.856471
66   0.988037  0.048173  0.815308  0.967735
67   0.987061  0.049492  0.808105  0.983103
68   0.986694  0.054543  0.794189  1.026286
69   0.986084  0.052848  0.810181  0.996670
70   0.987671  0.049713  0.811890  0.873344
71   0.988037  0.046097  0.833862  0.847843
72   0.989380  0.044797  0.817871  0.956372
73   0.986328  0.049143  0.843384  0.802293
74   0.988403  0.049823  0.822144  0.980464
75   0.988647  0.047004  0.831787  0.841938
76   0.988159  0.045634  0.838135  0.809472
77   0.988525  0.042174  0.839600  0.751767
78   0.987915  0.052311  0.824097  0.860114
79   0.986206  0.053326  0.844849  0.668090
80   0.989258  0.045637  0.803467  1.046998
81   0.989624  0.049061  0.812622  0.964141
82   0.988647  0.046131  0.836060  0.816565
83   0.988647  0.048248  0.840820  0.754097
84   0.988159  0.053010  0.851074  0.750705
85   0.988281  0.048596  0.792725  1.097334
86   0.989502  0.045116  0.809326  0.981889
87   0.989868  0.041421  0.850342  0.739182
88   0.987427  0.048968  0.833496  0.873265
89   0.988281  0.044586  0.824219  0.887958
90   0.989380  0.044557  0.860962  0.674266
91   0.988647  0.045680  0.836792  0.827080
92   0.989624  0.040098  0.833862  0.922536
93   0.989136  0.048736  0.809570  0.965758
94   0.987793  0.047250  0.821045  0.962231
95   0.988037  0.046810  0.805908  1.145901
96   0.990479  0.040861  0.807495  1.184009
97   0.987671  0.051720  0.792114  1.078285
98   0.988403  0.046784  0.835815  0.849855
99   0.989868  0.040164  0.837158  0.827148
100  0.985229  0.055858  0.828247  0.857207
101  0.988281  0.048129  0.845581  0.770617
102  0.989502  0.044505  0.827515  0.939808
103  0.988159  0.051372  0.829590  0.916740
104  0.990479  0.040280  0.840698  0.901223
105  0.989502  0.041974  0.828857  0.934848
106  0.988281  0.046400  0.835449  0.794260
107  0.988403  0.049864  0.851685  0.750872
108  0.989502  0.044980  0.819458  0.930750
109  0.989746  0.043267  0.817505  0.955430
110  0.990967  0.039618  0.808960  1.153190
111  0.988892  0.045355  0.788086  1.190949
112  0.990845  0.041528  0.839966  0.839454
113  0.989868  0.039969  0.828735  0.947466
114  0.989624  0.047041  0.841553  0.806125
115  0.989258  0.046064  0.831787  0.886856
116  0.990112  0.039817  0.801147  1.159784
117  0.988159  0.045860  0.828979  0.885404
118  0.989136  0.043609  0.839722  0.737288
119  0.988770  0.043595  0.828491  0.856954
120  0.990723  0.038123  0.849121  0.808358
121  0.989990  0.044533  0.816528  0.886929
122  0.988770  0.048521  0.835205  0.864745
123  0.989014  0.043582  0.847412  0.781211
124  0.988770  0.043702  0.841431  0.804276
125  0.989746  0.043875  0.828735  0.914764
126  0.990601  0.040751  0.820923  0.924016
127  0.988770  0.041910  0.808472  1.082308

2018-02-13 12:56:23.142935 Finish.
Total elapsed time: 16:42:55.14.
