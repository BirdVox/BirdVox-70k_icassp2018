2018-02-12 20:01:24.358122: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:01:24.358387: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:01:24.358400: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 20:01:07.981825 Start.
Training convnet with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.977539  0.095288  0.794800  0.715521
1    0.979370  0.087411  0.806763  0.942078
2    0.974854  0.104324  0.816406  0.744435
3    0.978516  0.093129  0.814209  0.694508
4    0.980835  0.091386  0.812378  0.874093
5    0.979248  0.085938  0.815796  0.704099
6    0.982666  0.080328  0.794312  0.870871
7    0.978882  0.089586  0.839844  0.586736
8    0.981445  0.082753  0.807739  0.735742
9    0.980957  0.084175  0.821655  0.682497
10   0.983154  0.078284  0.786377  0.816341
11   0.981567  0.077839  0.807983  0.701357
12   0.981689  0.082222  0.800049  0.786463
13   0.983276  0.072521  0.819946  0.759350
14   0.981445  0.083345  0.828003  0.591505
15   0.984497  0.063460  0.826172  0.643206
16   0.982788  0.076628  0.817627  0.691461
17   0.983398  0.073973  0.820312  0.668243
18   0.983765  0.069522  0.827759  0.687287
19   0.984131  0.067559  0.825562  0.711609
20   0.982910  0.068509  0.829712  0.604368
21   0.983887  0.065346  0.811890  0.709355
22   0.981934  0.075114  0.839722  0.556771
23   0.982788  0.076925  0.828735  0.783653
24   0.982788  0.073772  0.810547  0.748522
25   0.982056  0.080827  0.795898  0.703009
26   0.984375  0.066565  0.828247  0.787670
27   0.985107  0.076006  0.817627  0.685664
28   0.984009  0.069525  0.826904  0.650807
29   0.984253  0.070543  0.827881  0.664860
30   0.984009  0.069725  0.817383  0.834983
31   0.983398  0.072383  0.812256  0.778208
32   0.986328  0.065565  0.784180  0.970907
33   0.984009  0.075231  0.813232  0.712181
34   0.986328  0.063325  0.828125  0.788341
35   0.985718  0.075838  0.823730  0.682959
36   0.987183  0.060715  0.794678  0.904764
37   0.986450  0.059246  0.832031  0.776146
38   0.984863  0.066045  0.815552  0.784148
39   0.984863  0.072731  0.838013  0.678189
40   0.984863  0.063552  0.836548  0.704089
41   0.984619  0.068518  0.804443  0.778015
42   0.987915  0.060074  0.852051  0.607418
43   0.987427  0.056816  0.787720  0.922392
44   0.989014  0.052262  0.834839  0.686491
45   0.985962  0.063635  0.813965  0.818702
46   0.984863  0.066438  0.815186  0.787685
47   0.986084  0.064629  0.828857  0.599802
48   0.985718  0.061165  0.828491  0.660308
49   0.984253  0.063672  0.823486  0.623235
50   0.987671  0.058122  0.812744  0.811739
51   0.990112  0.050314  0.819580  0.743282
52   0.988281  0.056525  0.813232  0.886287
53   0.987305  0.057654  0.817261  0.764329
54   0.985229  0.060046  0.793213  1.020573
55   0.985352  0.066567  0.818970  0.734702
56   0.985474  0.060617  0.833008  0.755314
57   0.984497  0.062982  0.858398  0.636327
58   0.986572  0.055898  0.827148  0.720306
59   0.986450  0.059204  0.812378  0.722921
60   0.988037  0.052474  0.802856  0.898209
61   0.984497  0.065226  0.824097  0.627779
62   0.986816  0.055077  0.828125  0.658845
63   0.985352  0.062250  0.840698  0.657445
64   0.988403  0.053099  0.804077  0.853934
65   0.987061  0.064578  0.804565  0.734985
66   0.987183  0.051528  0.830566  0.708413
67   0.986084  0.060912  0.807617  0.794525
68   0.984741  0.062096  0.808350  0.723553
69   0.987793  0.051060  0.837280  0.696102
70   0.985840  0.058519  0.819092  0.686174
71   0.990356  0.042695  0.817017  0.815565
72   0.988037  0.054579  0.810669  0.817828
73   0.986450  0.060460  0.780884  0.758311
74   0.985718  0.061682  0.833862  0.697552
75   0.986206  0.061510  0.819580  0.747410
76   0.989258  0.052489  0.837891  0.833144
77   0.986084  0.056216  0.823486  0.787523
78   0.987671  0.055300  0.822754  0.907895
79   0.986328  0.058875  0.811035  0.783072
80   0.989136  0.049359  0.841431  0.772091
81   0.985229  0.064233  0.809570  0.870681
82   0.989136  0.050631  0.825439  0.819407
83   0.986572  0.058437  0.834473  0.599287
84   0.986694  0.056946  0.821289  0.789321
85   0.987549  0.053841  0.808350  0.778935
86   0.985596  0.058309  0.829346  0.836927
87   0.986572  0.060203  0.798706  0.823917
88   0.987793  0.059552  0.792603  0.903430
89   0.987061  0.055746  0.820312  0.831987
90   0.988403  0.055173  0.836182  0.646109
91   0.987793  0.054089  0.796021  0.829763
92   0.987915  0.049566  0.797363  0.909946
93   0.989014  0.050448  0.833252  0.758932
94   0.987915  0.052507  0.799438  0.888485
95   0.988159  0.054972  0.806152  0.742873
96   0.989868  0.048376  0.800171  0.943970
97   0.987305  0.057544  0.814941  0.835468
98   0.985474  0.059387  0.809082  0.783477
99   0.987915  0.049334  0.795532  0.991336
100  0.986938  0.056718  0.811523  0.753699
101  0.987427  0.051228  0.829102  0.709376
102  0.988403  0.047321  0.777222  1.100714
103  0.988037  0.059283  0.790405  0.660968
104  0.989136  0.051219  0.817871  0.796853
105  0.986938  0.054750  0.814209  0.789928
106  0.987793  0.054693  0.814697  0.829886
107  0.988403  0.049128  0.830933  0.744060
108  0.988159  0.052840  0.825073  0.974507
109  0.989014  0.053696  0.833252  0.726987
110  0.990234  0.047748  0.813110  0.808438
111  0.988892  0.052319  0.796509  0.889918
112  0.987061  0.050256  0.819946  0.828160
113  0.988647  0.052754  0.827515  0.804316
114  0.990234  0.048930  0.826782  0.796499
115  0.988525  0.046580  0.835449  0.674087
116  0.990479  0.049294  0.796997  0.877001
117  0.988281  0.046516  0.820923  0.831808
118  0.988159  0.050649  0.811523  0.759248
119  0.988647  0.049448  0.850342  0.624861
120  0.989380  0.051796  0.820679  0.803100
121  0.989136  0.048345  0.829224  0.842796
122  0.989380  0.048561  0.810669  0.914351
123  0.989746  0.049826  0.816528  0.755190
124  0.988525  0.049639  0.812378  0.853277
125  0.987183  0.049545  0.807129  0.850307
126  0.991455  0.042337  0.833618  0.750015
127  0.990112  0.046431  0.837402  0.693426

2018-02-13 12:10:16.234927 Finish.
Total elapsed time: 16:09:09.23.
