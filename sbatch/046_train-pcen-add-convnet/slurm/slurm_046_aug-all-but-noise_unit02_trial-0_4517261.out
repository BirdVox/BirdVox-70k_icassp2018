2018-02-12 20:13:38.597763: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:13:38.598994: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:13:38.599008: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 20:13:03.299040 Start.
Training convnet with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.503662  7.917555  0.495728  8.043011
1    0.501709  7.946906  0.501709  7.946262
2    0.492676  8.089806  0.493896  8.069941
3    0.504761  7.896445  0.493896  8.069392
4    0.505737  7.880436  0.492310  8.094345
5    0.496460  8.028062  0.510132  7.809999
6    0.497437  8.012320  0.492798  8.086041
7    0.491455  8.226419  0.500977  8.091079
8    0.715332  3.514403  0.841309  0.689769
9    0.965820  0.146109  0.853149  0.619639
10   0.967285  0.122085  0.876343  0.459548
11   0.974243  0.098431  0.861328  0.610925
12   0.975098  0.094457  0.797852  0.948937
13   0.978882  0.081118  0.870972  0.551984
14   0.978027  0.079158  0.875122  0.537793
15   0.980591  0.078757  0.831665  0.869496
16   0.979736  0.074854  0.820923  0.868910
17   0.979370  0.081270  0.834595  0.728047
18   0.980957  0.068965  0.855713  0.679145
19   0.983032  0.068800  0.869629  0.643598
20   0.979858  0.079909  0.854126  0.705975
21   0.984863  0.058004  0.841431  0.847572
22   0.978638  0.073039  0.858887  0.684224
23   0.984131  0.062121  0.866211  0.661893
24   0.980591  0.069478  0.869629  0.677622
25   0.983643  0.060829  0.846802  0.785706
26   0.981689  0.071571  0.862671  0.627941
27   0.984619  0.063052  0.877075  0.607012
28   0.982056  0.066340  0.884888  0.529808
29   0.983643  0.063498  0.891724  0.464059
30   0.982422  0.070350  0.881592  0.556578
31   0.982544  0.062461  0.878052  0.594576
32   0.984985  0.059298  0.870361  0.680699
33   0.983276  0.070060  0.860474  0.636516
34   0.982300  0.058367  0.889404  0.497727
35   0.984253  0.060103  0.873657  0.601847
36   0.985718  0.059035  0.862427  0.629823
37   0.986694  0.056743  0.864990  0.668508
38   0.984741  0.058777  0.859009  0.700521
39   0.983887  0.061893  0.860718  0.622693
40   0.985107  0.057593  0.879150  0.569102
41   0.982910  0.062961  0.879150  0.576337
42   0.987061  0.054616  0.860718  0.714775
43   0.985962  0.057138  0.876221  0.587751
44   0.984131  0.059795  0.885620  0.507000
45   0.985962  0.055234  0.865845  0.682782
46   0.984741  0.059096  0.885376  0.565198
47   0.983398  0.061066  0.887329  0.563941
48   0.983887  0.062591  0.869507  0.693507
49   0.985352  0.057866  0.859497  0.679866
50   0.984741  0.058559  0.892212  0.577786
51   0.985718  0.054396  0.879761  0.558258
52   0.988281  0.051534  0.862183  0.790824
53   0.984863  0.061688  0.869873  0.750619
54   0.986938  0.053126  0.889893  0.512061
55   0.987305  0.050237  0.880127  0.645435
56   0.984863  0.056576  0.902588  0.401623
57   0.985352  0.057692  0.885620  0.516212
58   0.985962  0.052368  0.871826  0.618187
59   0.987183  0.052134  0.883667  0.638125
60   0.984863  0.058278  0.870850  0.617256
61   0.987305  0.054652  0.870605  0.652588
62   0.986938  0.056269  0.880615  0.640174
63   0.987183  0.049855  0.877686  0.541409
64   0.989624  0.045659  0.889282  0.575831
65   0.988403  0.049484  0.857300  0.796003
66   0.986328  0.055276  0.884399  0.505078
67   0.982666  0.070103  0.869751  0.635237
68   0.985474  0.058959  0.878540  0.568657
69   0.987183  0.051019  0.885620  0.540930
70   0.986328  0.051260  0.857544  0.831509
71   0.989746  0.045563  0.880859  0.524371
72   0.985107  0.054465  0.869751  0.682004
73   0.986816  0.053360  0.866821  0.689539
74   0.985229  0.058572  0.891235  0.482599
75   0.987915  0.052968  0.886475  0.607456
76   0.987427  0.051951  0.881470  0.613872
77   0.985840  0.057645  0.867310  0.642599
78   0.988037  0.047665  0.875732  0.594269
79   0.987793  0.051063  0.873413  0.652586
80   0.988647  0.048784  0.871948  0.696096
81   0.985962  0.053204  0.875122  0.622778
82   0.984985  0.058160  0.881226  0.576997
83   0.987061  0.050059  0.877075  0.514258
84   0.990479  0.044065  0.880859  0.589544
85   0.989502  0.050005  0.876343  0.587145
86   0.988159  0.048477  0.858276  0.832795
87   0.986938  0.053972  0.869751  0.621692
88   0.987427  0.049755  0.869385  0.668304
89   0.985352  0.058039  0.880981  0.601544
90   0.988403  0.052003  0.881958  0.574931
91   0.989380  0.044673  0.871826  0.758357
92   0.988159  0.047556  0.857422  0.828974
93   0.986816  0.055141  0.859131  0.737073
94   0.986206  0.052780  0.871704  0.649441
95   0.987183  0.050535  0.881836  0.592974
96   0.987183  0.050573  0.883911  0.559649
97   0.989014  0.043405  0.868652  0.704310
98   0.989380  0.050216  0.844116  0.845516
99   0.987061  0.052735  0.868408  0.587351
100  0.988525  0.052106  0.868652  0.714156
101  0.985840  0.056657  0.874390  0.713470
102  0.984985  0.057432  0.875366  0.606245
103  0.986816  0.048912  0.888916  0.515843
104  0.986328  0.052726  0.883545  0.568564
105  0.987549  0.053068  0.880249  0.568327
106  0.986084  0.050866  0.896606  0.483607
107  0.986328  0.050607  0.875610  0.611802
108  0.986450  0.053713  0.893188  0.497415
109  0.987549  0.051208  0.885986  0.639135
110  0.988892  0.051163  0.882080  0.567514
111  0.987671  0.053866  0.868530  0.668963
112  0.989258  0.049974  0.858765  0.791365
113  0.988159  0.050761  0.880127  0.635569
114  0.985718  0.056139  0.877563  0.591748
115  0.987915  0.052956  0.881836  0.527064
116  0.986572  0.055989  0.867310  0.632202
117  0.988892  0.050948  0.869873  0.715547
118  0.987183  0.052349  0.888672  0.570614
119  0.991211  0.039630  0.875977  0.691910
120  0.989868  0.043738  0.867065  0.754673
121  0.986450  0.053406  0.872803  0.690607
122  0.989502  0.050343  0.864868  0.734197
123  0.988647  0.050005  0.876953  0.586440
124  0.988525  0.051161  0.885864  0.565285
125  0.987671  0.053075  0.880005  0.631446
126  0.987549  0.050117  0.854248  0.816632
127  0.989990  0.042456  0.864746  0.724989

2018-02-13 11:37:01.120027 Finish.
Total elapsed time: 15:23:58.12.
