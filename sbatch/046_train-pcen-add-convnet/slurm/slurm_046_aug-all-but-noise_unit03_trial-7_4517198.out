2018-02-12 20:01:36.517640: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:01:36.517971: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:01:36.517984: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 20:01:19.459062 Start.
Training convnet with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.490967  8.211319  0.493896  8.162797
1    0.494751  8.148053  0.502319  8.025213
2    0.491333  8.201651  0.497803  8.096807
3    0.489624  8.228206  0.490601  8.212091
4    0.504150  7.993410  0.498901  8.077764
5    0.493042  8.172017  0.506836  7.949519
6    0.503296  8.006452  0.501343  8.037823
7    0.740356  3.595104  0.840576  0.642113
8    0.966553  0.164553  0.833618  0.725147
9    0.973633  0.119984  0.847046  0.554279
10   0.973389  0.117214  0.835815  0.642674
11   0.980225  0.087500  0.850708  0.569640
12   0.978149  0.095093  0.839478  0.616553
13   0.977051  0.103788  0.839722  0.565743
14   0.980591  0.080971  0.850952  0.634666
15   0.981323  0.079522  0.863403  0.477436
16   0.976685  0.090761  0.840088  0.577445
17   0.983521  0.076087  0.858032  0.479474
18   0.978638  0.086656  0.860596  0.606123
19   0.980591  0.080347  0.833862  0.643380
20   0.982544  0.072528  0.852905  0.626042
21   0.984497  0.077996  0.827881  0.600460
22   0.982300  0.068264  0.833862  0.834399
23   0.981689  0.079604  0.828857  0.693537
24   0.980835  0.087244  0.849243  0.624388
25   0.980469  0.082171  0.844727  0.576419
26   0.984375  0.080206  0.844238  0.648452
27   0.982422  0.070950  0.806152  0.791972
28   0.980713  0.076211  0.857910  0.566016
29   0.981445  0.086246  0.813599  0.732978
30   0.984741  0.067719  0.844116  0.516476
31   0.982178  0.077989  0.802734  0.749171
32   0.984375  0.071006  0.847534  0.559588
33   0.982544  0.068511  0.831665  0.716403
34   0.982178  0.075593  0.854736  0.537573
35   0.984741  0.072301  0.853149  0.525345
36   0.979736  0.079310  0.832520  0.571220
37   0.984009  0.067363  0.859009  0.481179
38   0.983765  0.064669  0.841309  0.570984
39   0.982666  0.074293  0.820435  0.637559
40   0.985840  0.066744  0.867676  0.468479
41   0.983643  0.062935  0.850464  0.772664
42   0.982788  0.075123  0.832397  0.616840
43   0.980957  0.077658  0.864014  0.610273
44   0.984985  0.062053  0.852051  0.587954
45   0.983276  0.073236  0.845703  0.609474
46   0.982788  0.071182  0.815186  0.617645
47   0.983154  0.068282  0.836670  0.571622
48   0.984985  0.065147  0.853394  0.525788
49   0.984863  0.068486  0.844116  0.674995
50   0.985107  0.062333  0.855591  0.578447
51   0.987793  0.054967  0.845581  0.645843
52   0.987427  0.057259  0.808716  0.893710
53   0.983521  0.067491  0.850220  0.648592
54   0.984009  0.064007  0.844482  0.588839
55   0.984619  0.063339  0.846191  0.563561
56   0.982788  0.067843  0.870483  0.429463
57   0.984375  0.064345  0.857056  0.561801
58   0.984375  0.066493  0.797852  0.719207
59   0.983643  0.071593  0.831177  0.552475
60   0.984863  0.063990  0.838989  0.627908
61   0.985474  0.057961  0.855225  0.710009
62   0.981689  0.069794  0.844238  0.620016
63   0.984253  0.063767  0.882690  0.469041
64   0.988281  0.051347  0.832520  0.829328
65   0.987671  0.060717  0.854248  0.580628
66   0.985229  0.060584  0.791992  0.784950
67   0.986694  0.062068  0.832642  0.622162
68   0.985596  0.061669  0.843994  0.644246
69   0.984741  0.059648  0.826538  0.773192
70   0.986206  0.063672  0.824219  0.741743
71   0.985840  0.058278  0.842163  0.729394
72   0.983032  0.069538  0.861206  0.567046
73   0.986084  0.063167  0.839600  0.644510
74   0.986694  0.056517  0.852783  0.686503
75   0.986328  0.065373  0.850830  0.596095
76   0.985107  0.057159  0.851807  0.609722
77   0.985962  0.062515  0.848999  0.593954
78   0.986206  0.062815  0.842529  0.684296
79   0.985718  0.059808  0.860962  0.573238
80   0.984985  0.066616  0.848633  0.565579
81   0.984985  0.063002  0.854370  0.566993
82   0.986328  0.060226  0.819946  0.866181
83   0.986572  0.054864  0.861328  0.560359
84   0.984497  0.062825  0.832642  0.659817
85   0.986084  0.064131  0.812988  0.810384
86   0.984131  0.066077  0.825684  0.572590
87   0.985596  0.061483  0.817017  0.687953
88   0.986328  0.054627  0.850342  0.681849
89   0.988037  0.051446  0.840576  0.775039
90   0.987061  0.058847  0.859619  0.515634
91   0.989014  0.047893  0.856201  0.604590
92   0.987183  0.061196  0.845459  0.629037
93   0.985962  0.054849  0.862671  0.619790
94   0.985474  0.063907  0.833740  0.646569
95   0.986694  0.061248  0.836060  0.719471
96   0.986450  0.056156  0.837646  0.683834
97   0.987183  0.054271  0.852661  0.651377
98   0.985840  0.058830  0.842651  0.687974
99   0.987305  0.052618  0.861206  0.742598
100  0.989990  0.048574  0.828979  0.787550
101  0.985229  0.060854  0.846313  0.663219
102  0.988037  0.051545  0.858521  0.639438
103  0.985840  0.062806  0.855225  0.564098
104  0.985107  0.063340  0.841797  0.605291
105  0.986938  0.060060  0.843262  0.775819
106  0.986938  0.057405  0.847168  0.638222
107  0.986572  0.061222  0.849121  0.686389
108  0.986938  0.054191  0.811890  0.789129
109  0.984497  0.062183  0.811890  0.829875
110  0.986450  0.065464  0.843262  0.603790
111  0.988403  0.055750  0.827637  0.740248
112  0.986328  0.058759  0.845337  0.674241
113  0.987793  0.057444  0.827637  0.694951
114  0.986572  0.058427  0.825928  0.704055
115  0.988281  0.052208  0.845581  0.624903
116  0.988525  0.052702  0.849487  0.626458
117  0.986084  0.056411  0.848389  0.680320
118  0.987427  0.055673  0.851685  0.663444
119  0.986328  0.062783  0.836548  0.656055
120  0.983643  0.065333  0.811401  0.751217
121  0.984863  0.060034  0.838745  0.610392
122  0.989624  0.050971  0.820801  0.769047
123  0.985596  0.066046  0.834595  0.704554
124  0.984741  0.062598  0.830200  0.649164
125  0.985962  0.055128  0.841064  0.669847
126  0.986694  0.054554  0.811401  0.829471
127  0.985840  0.051840  0.837769  0.767939

2018-02-13 11:13:11.951943 Finish.
Total elapsed time: 15:11:52.95.
