2018-02-12 20:13:56.809746: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:13:56.810122: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:13:56.810136: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 20:13:30.562208 Start.
Training convnet with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.977661  0.082509  0.814697  0.832890
1    0.978149  0.081475  0.833740  0.680934
2    0.979248  0.084869  0.820557  0.850983
3    0.979614  0.079183  0.840820  0.613143
4    0.983154  0.065563  0.850586  0.746325
5    0.981445  0.078286  0.848999  0.600127
6    0.985840  0.058435  0.803589  0.889458
7    0.984253  0.062729  0.809814  0.990519
8    0.981812  0.075420  0.839355  0.641355
9    0.981201  0.080398  0.860718  0.571837
10   0.986450  0.063537  0.843140  0.704228
11   0.984375  0.060681  0.855957  0.815818
12   0.984253  0.065921  0.842041  0.848086
13   0.984253  0.066475  0.826172  0.808335
14   0.981445  0.070038  0.850342  0.664482
15   0.985840  0.063188  0.856445  0.581093
16   0.986694  0.059652  0.853882  0.655211
17   0.983887  0.068491  0.854004  0.692999
18   0.986206  0.061253  0.814819  0.910321
19   0.982666  0.066812  0.841797  0.656160
20   0.986938  0.057923  0.839233  0.843608
21   0.985962  0.058969  0.844116  0.721503
22   0.984619  0.059224  0.819336  0.846604
23   0.983887  0.065339  0.859497  0.645252
24   0.988525  0.054242  0.815063  0.908842
25   0.986328  0.054205  0.832275  0.698469
26   0.985596  0.065164  0.816895  0.850872
27   0.985962  0.057263  0.830933  0.829806
28   0.987061  0.055171  0.848633  0.692631
29   0.985962  0.061915  0.828857  0.810762
30   0.987427  0.051058  0.815674  0.978818
31   0.987183  0.053341  0.848999  0.755313
32   0.985474  0.065540  0.872681  0.560404
33   0.988647  0.054251  0.850098  0.747016
34   0.986328  0.056877  0.845459  0.711289
35   0.987183  0.056055  0.848877  0.757630
36   0.985962  0.060217  0.824951  0.813022
37   0.988770  0.051201  0.837036  0.750341
38   0.988159  0.052493  0.840698  0.812749
39   0.987915  0.050823  0.840332  0.756142
40   0.989380  0.048816  0.852051  0.834629
41   0.989136  0.048578  0.824951  0.899610
42   0.987061  0.050128  0.815308  0.955406
43   0.987061  0.055060  0.814575  0.953953
44   0.987427  0.051029  0.824463  0.874763
45   0.985962  0.058480  0.846436  0.792098
46   0.988525  0.048639  0.832886  0.825943
47   0.988281  0.050308  0.852173  0.688511
48   0.988647  0.048635  0.809570  1.206748
49   0.988770  0.050209  0.813965  1.177146
50   0.989868  0.047872  0.844849  0.722345
51   0.987427  0.054166  0.829102  0.817419
52   0.986084  0.059107  0.836304  0.777424
53   0.987061  0.057993  0.834351  0.869951
54   0.987915  0.049885  0.855103  0.639436
55   0.987793  0.054587  0.816528  1.005677
56   0.989014  0.046474  0.856934  0.701923
57   0.986938  0.051862  0.825073  0.910824
58   0.990601  0.046136  0.839966  0.717648
59   0.987671  0.049505  0.819458  0.974197
60   0.988037  0.047428  0.845703  0.712054
61   0.988281  0.051141  0.849609  0.731169
62   0.989258  0.041837  0.843750  0.875728
63   0.988281  0.053038  0.854004  0.656233
64   0.986450  0.053759  0.829956  0.791533
65   0.985474  0.053867  0.826660  0.852783
66   0.985596  0.057833  0.839966  0.767953
67   0.988647  0.051574  0.817993  0.933678
68   0.991089  0.045514  0.835938  0.759431
69   0.986938  0.054187  0.832397  0.756636
70   0.990601  0.043410  0.842285  0.734804
71   0.989014  0.045370  0.832153  0.887632
72   0.987793  0.051737  0.814453  0.947078
73   0.987671  0.055357  0.853882  0.633128
74   0.989136  0.052111  0.828369  0.792774
75   0.986938  0.049187  0.803345  1.118757
76   0.989014  0.046848  0.833740  0.917193
77   0.988403  0.052513  0.839966  0.770809
78   0.989868  0.049801  0.825073  0.971193
79   0.988281  0.046432  0.825439  0.872804
80   0.989014  0.051574  0.838257  0.810446
81   0.989014  0.051740  0.857178  0.653940
82   0.989990  0.051440  0.850708  0.699958
83   0.989990  0.049329  0.830200  0.810903
84   0.988037  0.049721  0.821777  0.997594
85   0.988525  0.050007  0.816528  0.884019
86   0.989258  0.048303  0.822021  0.978288
87   0.990356  0.046523  0.840820  0.819208
88   0.989380  0.049602  0.807983  1.066551
89   0.990601  0.046802  0.808716  1.040971
90   0.989990  0.045627  0.839355  0.864842
91   0.990356  0.045770  0.843018  0.773654
92   0.988281  0.055035  0.849243  0.659712
93   0.988159  0.047477  0.846069  0.787661
94   0.990723  0.044207  0.854370  0.736523
95   0.989624  0.045691  0.839355  0.785792
96   0.991089  0.046260  0.839722  0.862361
97   0.991577  0.042739  0.834595  0.932354
98   0.988281  0.045905  0.838501  0.834214
99   0.989502  0.047468  0.862549  0.705963
100  0.989746  0.047466  0.850708  0.759689
101  0.991089  0.040335  0.849731  0.896719
102  0.989380  0.044936  0.864258  0.759400
103  0.990112  0.041231  0.852051  0.848845
104  0.990112  0.044415  0.846436  0.875216
105  0.989136  0.048431  0.833496  0.912087
106  0.988525  0.049397  0.847900  0.762343
107  0.989014  0.046365  0.845215  0.866440
108  0.993164  0.036739  0.866943  0.757001
109  0.989624  0.047721  0.828003  0.940943
110  0.991699  0.040547  0.858032  0.788067
111  0.989868  0.047659  0.836060  0.924021
112  0.989868  0.044538  0.851440  0.749642
113  0.990234  0.045318  0.833374  0.891246
114  0.987915  0.049380  0.851807  0.723189
115  0.991943  0.043954  0.830078  0.917860
116  0.992432  0.039449  0.837524  0.912465
117  0.990356  0.039751  0.839233  0.878316
118  0.992188  0.038898  0.823486  1.021082
119  0.990234  0.041389  0.834229  1.001984
120  0.988770  0.049560  0.864380  0.723337
121  0.991577  0.038019  0.857300  0.860263
122  0.990845  0.038164  0.847412  0.938283
123  0.990967  0.041256  0.846924  0.846625
124  0.991211  0.038426  0.807007  1.260272
125  0.990479  0.044199  0.855347  0.721302
126  0.989624  0.042822  0.853882  0.805958
127  0.988770  0.048702  0.851807  0.760301

2018-02-13 13:03:23.768538 Finish.
Total elapsed time: 16:49:53.77.
