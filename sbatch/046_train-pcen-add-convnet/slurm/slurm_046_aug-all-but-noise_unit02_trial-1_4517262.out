2018-02-12 20:13:43.203421: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:13:43.203734: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:13:43.203746: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 20:13:04.953265 Start.
Training convnet with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.981567  0.084176  0.869995  0.657978
1    0.979248  0.087956  0.875000  0.622652
2    0.983643  0.081527  0.879761  0.673739
3    0.980225  0.086146  0.903198  0.511194
4    0.980957  0.080313  0.882446  0.604989
5    0.982788  0.075406  0.855225  0.767338
6    0.982178  0.082012  0.885376  0.562485
7    0.982300  0.075933  0.878540  0.669348
8    0.986694  0.062694  0.879150  0.568092
9    0.984131  0.071235  0.882812  0.569600
10   0.985718  0.070171  0.885498  0.649614
11   0.982422  0.077394  0.880493  0.609242
12   0.985229  0.067437  0.907104  0.450481
13   0.985596  0.062794  0.906128  0.503818
14   0.988281  0.050011  0.900513  0.590557
15   0.984741  0.068612  0.886841  0.725597
16   0.985840  0.059724  0.879639  0.691904
17   0.982422  0.075334  0.884644  0.575888
18   0.986206  0.065145  0.892578  0.600892
19   0.986084  0.065193  0.900757  0.504473
20   0.986816  0.062318  0.879150  0.709213
21   0.983154  0.068956  0.881470  0.618919
22   0.985596  0.067420  0.888062  0.562033
23   0.988159  0.053750  0.897705  0.520602
24   0.985229  0.062425  0.902954  0.492210
25   0.985229  0.058878  0.885864  0.576682
26   0.986694  0.055660  0.892456  0.585078
27   0.987427  0.060751  0.893799  0.571546
28   0.987671  0.053884  0.887451  0.550397
29   0.988525  0.048440  0.897583  0.456546
30   0.987061  0.055171  0.894043  0.565324
31   0.986328  0.056020  0.890259  0.550890
32   0.986572  0.059179  0.883789  0.584397
33   0.985352  0.063550  0.891235  0.547112
34   0.988281  0.047356  0.899536  0.600222
35   0.986084  0.058322  0.902222  0.503433
36   0.987061  0.057446  0.895752  0.550911
37   0.985596  0.060024  0.895630  0.511114
38   0.989502  0.045173  0.899170  0.558845
39   0.988647  0.051544  0.890015  0.554655
40   0.989746  0.045934  0.880615  0.759460
41   0.989624  0.046801  0.893677  0.580698
42   0.990601  0.047813  0.901733  0.503153
43   0.987183  0.056973  0.881226  0.641796
44   0.991211  0.045569  0.887573  0.576084
45   0.987549  0.055895  0.869141  0.681146
46   0.990479  0.043033  0.889526  0.585570
47   0.986694  0.057054  0.884155  0.657556
48   0.990356  0.044164  0.895508  0.559009
49   0.988281  0.048999  0.880615  0.642901
50   0.989502  0.045392  0.906616  0.527718
51   0.987915  0.051429  0.893066  0.583884
52   0.988525  0.055827  0.886597  0.710855
53   0.987427  0.056154  0.904053  0.485116
54   0.990601  0.045675  0.889160  0.613857
55   0.989258  0.049042  0.892212  0.601436
56   0.990112  0.044226  0.885132  0.599035
57   0.987061  0.051381  0.888794  0.594203
58   0.987793  0.048309  0.898071  0.550452
59   0.989746  0.043508  0.887329  0.615977
60   0.989502  0.048119  0.900146  0.447629
61   0.989868  0.047447  0.893311  0.539875
62   0.987549  0.048759  0.897949  0.541732
63   0.990479  0.043103  0.902832  0.527163
64   0.989258  0.045187  0.891479  0.604009
65   0.987183  0.051260  0.899292  0.531251
66   0.988403  0.049084  0.890503  0.590756
67   0.990845  0.038940  0.901733  0.593971
68   0.989258  0.045437  0.904663  0.523395
69   0.990356  0.045865  0.878296  0.799377
70   0.990723  0.039401  0.893555  0.574924
71   0.988281  0.045978  0.863037  0.823081
72   0.989746  0.046404  0.887207  0.662383
73   0.989380  0.043908  0.898926  0.600792
74   0.988281  0.045710  0.882446  0.636044
75   0.990723  0.043470  0.887573  0.665872
76   0.988281  0.048660  0.894043  0.617490
77   0.988159  0.048424  0.883545  0.639911
78   0.989624  0.046773  0.895386  0.608668
79   0.989136  0.049226  0.900757  0.511579
80   0.988892  0.047476  0.903931  0.483979
81   0.990601  0.039144  0.878784  0.705086
82   0.991333  0.041024  0.908936  0.496123
83   0.989136  0.046915  0.902710  0.539530
84   0.989624  0.046657  0.901978  0.477269
85   0.990967  0.041152  0.894165  0.571696
86   0.989624  0.047775  0.898193  0.500342
87   0.988037  0.045671  0.891602  0.580894
88   0.992310  0.038268  0.894897  0.558042
89   0.991455  0.039203  0.904419  0.584754
90   0.989136  0.048753  0.884521  0.667945
91   0.990967  0.040964  0.878784  0.777680
92   0.989502  0.046867  0.883667  0.661184
93   0.990967  0.042294  0.899048  0.625645
94   0.988525  0.050550  0.870117  0.884532
95   0.991943  0.038437  0.902100  0.577802
96   0.989258  0.045603  0.893188  0.624706
97   0.991089  0.039260  0.885010  0.745081
98   0.989746  0.043294  0.894531  0.608086
99   0.990723  0.042468  0.898071  0.555697
100  0.991333  0.040031  0.905884  0.472306
101  0.988159  0.043662  0.898071  0.568410
102  0.991211  0.039707  0.904541  0.539143
103  0.991455  0.037269  0.906738  0.526169
104  0.989624  0.048036  0.905151  0.506136
105  0.990601  0.039861  0.921387  0.418717
106  0.991211  0.036321  0.895630  0.577504
107  0.989624  0.041132  0.902344  0.532001
108  0.989990  0.043298  0.902832  0.510860
109  0.991455  0.037176  0.898682  0.563816
110  0.990845  0.041254  0.889160  0.683971
111  0.991943  0.034282  0.891479  0.656516
112  0.989746  0.042395  0.896606  0.624931
113  0.992432  0.037902  0.899902  0.597835
114  0.991089  0.037759  0.882446  0.728079
115  0.989990  0.043265  0.899048  0.559308
116  0.990967  0.036350  0.892822  0.668067
117  0.989624  0.047716  0.879639  0.713249
118  0.990967  0.042859  0.904175  0.530418
119  0.990967  0.037694  0.908081  0.520892
120  0.991455  0.039169  0.904907  0.507665
121  0.991455  0.039166  0.888062  0.697870
122  0.990845  0.036391  0.883423  0.701671
123  0.990112  0.044297  0.885498  0.574970
124  0.991943  0.037789  0.893799  0.629525
125  0.989258  0.042127  0.907471  0.493973
126  0.992432  0.037428  0.895630  0.568268
127  0.989990  0.042698  0.889893  0.624402

2018-02-13 11:37:18.836658 Finish.
Total elapsed time: 15:24:14.84.
