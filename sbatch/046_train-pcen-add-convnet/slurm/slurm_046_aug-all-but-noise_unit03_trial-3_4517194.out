2018-02-12 20:01:26.206649: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:01:26.207012: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:01:26.207026: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 20:01:11.364404 Start.
Training convnet with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.499268  7.985270  0.500122  7.970989
1    0.502441  7.933576  0.499756  7.976030
2    0.733765  3.626374  0.825317  0.686869
3    0.968628  0.176378  0.813354  0.761574
4    0.974976  0.128596  0.822998  0.669014
5    0.973511  0.123100  0.848389  0.528123
6    0.974854  0.107755  0.819824  0.655648
7    0.975464  0.102059  0.842773  0.597034
8    0.973633  0.108762  0.816040  0.710529
9    0.977661  0.101155  0.829712  0.705639
10   0.976562  0.101361  0.821899  0.639668
11   0.980469  0.085086  0.827637  0.731180
12   0.978516  0.095455  0.851318  0.575932
13   0.980103  0.075216  0.839722  0.696676
14   0.981201  0.086206  0.810913  0.846800
15   0.977295  0.088613  0.830811  0.765278
16   0.977295  0.089811  0.842041  0.601849
17   0.980347  0.079505  0.807251  0.743608
18   0.976318  0.092279  0.814087  0.661181
19   0.981079  0.081770  0.821899  0.622089
20   0.984985  0.076093  0.836426  0.685545
21   0.980957  0.083770  0.857300  0.555149
22   0.980957  0.083402  0.857544  0.562276
23   0.983276  0.077014  0.844482  0.675560
24   0.981689  0.076187  0.838745  0.710131
25   0.980469  0.083821  0.826416  0.716024
26   0.982178  0.079012  0.823853  0.732271
27   0.983521  0.072846  0.852295  0.571911
28   0.981689  0.078963  0.836792  0.653484
29   0.982666  0.072828  0.855713  0.537546
30   0.984253  0.069678  0.835449  0.644849
31   0.979980  0.077400  0.841064  0.601045
32   0.981812  0.078103  0.844116  0.567031
33   0.982300  0.072349  0.855469  0.523773
34   0.986450  0.063972  0.853149  0.599872
35   0.981567  0.074589  0.843628  0.638798
36   0.984253  0.068485  0.854858  0.611799
37   0.982788  0.068589  0.845093  0.604559
38   0.985229  0.066350  0.855957  0.645958
39   0.983398  0.069839  0.849854  0.563488
40   0.984375  0.058965  0.865356  0.607227
41   0.984253  0.069815  0.868164  0.570519
42   0.982666  0.071416  0.866333  0.547516
43   0.980957  0.076246  0.842041  0.602006
44   0.985840  0.064749  0.863647  0.579085
45   0.982300  0.065848  0.857056  0.618494
46   0.984375  0.067294  0.826416  0.725318
47   0.984497  0.070072  0.855957  0.652088
48   0.985718  0.064711  0.824463  0.645428
49   0.982788  0.066131  0.853638  0.586044
50   0.986206  0.059824  0.859009  0.591529
51   0.985352  0.063639  0.862427  0.510236
52   0.983032  0.067565  0.862183  0.631049
53   0.985229  0.066418  0.837036  0.603024
54   0.981567  0.074875  0.854736  0.556134
55   0.984985  0.064740  0.858521  0.640504
56   0.985107  0.066504  0.850708  0.656001
57   0.984741  0.066594  0.864990  0.555583
58   0.985107  0.070134  0.848267  0.653707
59   0.984985  0.065467  0.839966  0.681147
60   0.982666  0.075540  0.846558  0.691260
61   0.983154  0.067056  0.843506  0.607043
62   0.984497  0.062665  0.844849  0.671785
63   0.987549  0.056918  0.862671  0.574911
64   0.985840  0.062673  0.842529  0.613379
65   0.985596  0.062836  0.822998  0.674703
66   0.986450  0.061067  0.857544  0.651060
67   0.986084  0.062616  0.853271  0.609986
68   0.986206  0.059425  0.875610  0.562015
69   0.985718  0.065031  0.868896  0.589533
70   0.985229  0.062455  0.850708  0.647619
71   0.985718  0.061577  0.848633  0.827730
72   0.985107  0.068075  0.844604  0.568552
73   0.985229  0.062495  0.861572  0.597478
74   0.982544  0.071848  0.847534  0.613192
75   0.984619  0.065464  0.870483  0.543673
76   0.989868  0.051434  0.864380  0.661556
77   0.985962  0.059045  0.859009  0.636113
78   0.983276  0.065206  0.865967  0.589311
79   0.984863  0.067063  0.834473  0.678271
80   0.987915  0.057538  0.860718  0.624797
81   0.984375  0.064461  0.833618  0.617505
82   0.984009  0.062534  0.852173  0.573999
83   0.984863  0.063900  0.864014  0.570032
84   0.987183  0.055646  0.849365  0.774201
85   0.985840  0.067393  0.867554  0.546965
86   0.987549  0.062893  0.849731  0.557689
87   0.986450  0.057228  0.858765  0.676188
88   0.984863  0.064928  0.844727  0.821557
89   0.983521  0.066158  0.872925  0.509803
90   0.984741  0.065679  0.845215  0.636108
91   0.984985  0.060279  0.851318  0.666054
92   0.985718  0.059138  0.859375  0.631088
93   0.984985  0.068663  0.826660  0.611713
94   0.987183  0.056453  0.830322  0.766683
95   0.984375  0.064514  0.842285  0.599173
96   0.988159  0.053393  0.855347  0.663997
97   0.986084  0.061403  0.828613  0.861851
98   0.987305  0.058553  0.865845  0.640781
99   0.987671  0.053170  0.852173  0.667145
100  0.984741  0.066185  0.851318  0.587642
101  0.985352  0.057896  0.854980  0.577750
102  0.985840  0.056902  0.845093  0.704574
103  0.986938  0.061119  0.860107  0.677439
104  0.987061  0.058129  0.862305  0.596494
105  0.985596  0.060000  0.845947  0.601830
106  0.985107  0.065296  0.875244  0.478283
107  0.987183  0.057294  0.864380  0.532017
108  0.985474  0.061462  0.854248  0.632922
109  0.986694  0.054933  0.853638  0.605492
110  0.986938  0.057784  0.879639  0.531742
111  0.987305  0.052605  0.857666  0.620948
112  0.985229  0.066225  0.860596  0.523145
113  0.986572  0.059396  0.876099  0.512698
114  0.985229  0.064347  0.847290  0.592137
115  0.987183  0.053765  0.873779  0.621046
116  0.986938  0.065209  0.869995  0.494331
117  0.986572  0.062167  0.859619  0.528001
118  0.986938  0.060886  0.867554  0.599039
119  0.985229  0.061278  0.852173  0.585951
120  0.987793  0.052418  0.865967  0.546050
121  0.987183  0.058729  0.856201  0.646552
122  0.987671  0.060594  0.859009  0.622751
123  0.987549  0.056466  0.865112  0.600825
124  0.986572  0.053981  0.864624  0.698640
125  0.986938  0.056489  0.847656  0.612588
126  0.988525  0.053131  0.856201  0.796415
127  0.988159  0.056776  0.869385  0.655570

2018-02-13 12:11:37.678981 Finish.
Total elapsed time: 16:10:26.68.
