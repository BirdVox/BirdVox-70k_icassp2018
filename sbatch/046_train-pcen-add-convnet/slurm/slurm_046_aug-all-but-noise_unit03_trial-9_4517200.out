2018-02-12 20:01:38.268872: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:01:38.269280: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:01:38.269292: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 20:01:23.142657 Start.
Training convnet with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.503296  7.929638  0.500977  7.964853
1    0.496582  8.033517  0.502441  7.938845
2    0.495972  8.040982  0.499023  7.991420
3    0.491577  8.109404  0.506348  7.873269
4    0.495850  8.040108  0.498047  8.004605
5    0.505127  7.891356  0.488037  8.163471
6    0.500488  7.964703  0.502686  7.929435
7    0.501465  7.948710  0.509644  7.818155
8    0.494629  8.057396  0.483276  8.238268
9    0.743164  3.458843  0.823853  0.850237
10   0.965942  0.191681  0.820312  0.818600
11   0.968994  0.148388  0.818115  0.813128
12   0.971802  0.129656  0.860229  0.638241
13   0.977173  0.106498  0.843872  0.740713
14   0.976929  0.103983  0.868896  0.630230
15   0.978149  0.101518  0.813843  0.949392
16   0.979858  0.090066  0.847046  0.666317
17   0.979126  0.088153  0.842529  0.765221
18   0.977295  0.092518  0.865112  0.626625
19   0.981201  0.077403  0.858643  0.673302
20   0.982056  0.087283  0.843628  0.828807
21   0.981812  0.083815  0.856689  0.689989
22   0.981934  0.075934  0.853516  0.733272
23   0.984131  0.073561  0.854858  0.732912
24   0.984497  0.076125  0.871460  0.614705
25   0.984009  0.076309  0.856812  0.739328
26   0.984375  0.073974  0.843872  0.818256
27   0.982666  0.074731  0.850464  0.770792
28   0.984131  0.074969  0.868774  0.563841
29   0.988403  0.059841  0.855103  0.809510
30   0.984131  0.074534  0.853271  0.730872
31   0.982666  0.073326  0.837769  0.719265
32   0.985718  0.061653  0.854370  0.731886
33   0.983521  0.074080  0.841309  0.675156
34   0.985718  0.062680  0.865234  0.676988
35   0.984131  0.068222  0.850586  0.696930
36   0.983276  0.078766  0.850830  0.640127
37   0.983765  0.077512  0.858398  0.614777
38   0.982544  0.077283  0.849609  0.650741
39   0.987427  0.057361  0.854126  0.757013
40   0.986206  0.066322  0.862427  0.661821
41   0.984863  0.070565  0.868652  0.553644
42   0.986450  0.063337  0.866333  0.694892
43   0.983887  0.073371  0.867554  0.651265
44   0.984985  0.069148  0.837158  0.777412
45   0.986206  0.062493  0.857056  0.671350
46   0.986084  0.064442  0.854614  0.698573
47   0.981689  0.070374  0.868042  0.648580
48   0.986084  0.062603  0.848755  0.754464
49   0.984741  0.065226  0.866333  0.659106
50   0.983276  0.067663  0.874878  0.613477
51   0.985596  0.060219  0.875610  0.659676
52   0.985718  0.065119  0.850220  0.770013
53   0.986084  0.063824  0.848389  0.809832
54   0.985474  0.065351  0.867065  0.600720
55   0.985840  0.064742  0.871216  0.702791
56   0.987915  0.057603  0.867188  0.719048
57   0.986328  0.061999  0.873169  0.620641
58   0.988892  0.050508  0.869507  0.635067
59   0.987427  0.058030  0.857300  0.722346
60   0.984619  0.067471  0.852173  0.737013
61   0.987061  0.056868  0.865234  0.688744
62   0.988525  0.059711  0.843994  0.877255
63   0.986572  0.057036  0.864136  0.652190
64   0.985596  0.065702  0.865112  0.561622
65   0.987915  0.057512  0.851685  0.740180
66   0.987549  0.055572  0.875000  0.608117
67   0.985962  0.060141  0.856934  0.749245
68   0.984985  0.062997  0.862549  0.682637
69   0.986938  0.055959  0.852417  0.772014
70   0.986206  0.057397  0.859863  0.681806
71   0.987305  0.055302  0.865723  0.613374
72   0.986694  0.057881  0.871704  0.626698
73   0.986938  0.063146  0.861206  0.590852
74   0.987183  0.057544  0.861572  0.606140
75   0.985596  0.059505  0.865723  0.603821
76   0.987427  0.055480  0.861084  0.681484
77   0.988403  0.054095  0.865112  0.638331
78   0.989380  0.049360  0.854248  0.666989
79   0.987427  0.063403  0.833008  0.733226
80   0.987183  0.056151  0.867798  0.619357
81   0.988892  0.054877  0.858643  0.726476
82   0.986206  0.054804  0.869019  0.658255
83   0.988403  0.052065  0.869995  0.677437
84   0.988647  0.056472  0.871460  0.599218
85   0.987427  0.056621  0.852295  0.655638
86   0.988647  0.050948  0.874512  0.656803
87   0.985840  0.060150  0.857422  0.697194
88   0.987671  0.053996  0.855469  0.752547
89   0.987427  0.058123  0.870239  0.611739
90   0.988525  0.050599  0.875854  0.672698
91   0.989258  0.048745  0.868408  0.764557
92   0.987305  0.057564  0.869873  0.623389
93   0.989868  0.045382  0.879028  0.712765
94   0.986206  0.056908  0.879272  0.550315
95   0.987793  0.054707  0.862671  0.639545
96   0.985718  0.061229  0.866577  0.604952
97   0.986938  0.054947  0.859375  0.780268
98   0.987671  0.055271  0.864502  0.728172
99   0.987671  0.057301  0.834961  0.687702
100  0.986084  0.060711  0.877930  0.560911
101  0.988525  0.055921  0.844238  0.866222
102  0.989136  0.053178  0.874146  0.671457
103  0.989624  0.054155  0.864136  0.703252
104  0.988281  0.054522  0.872314  0.709479
105  0.985474  0.063251  0.856201  0.687604
106  0.987183  0.053864  0.858398  0.709681
107  0.988159  0.048965  0.860718  0.643670
108  0.987183  0.054783  0.880249  0.565374
109  0.988770  0.048295  0.884277  0.598602
110  0.988159  0.052654  0.882690  0.597782
111  0.987671  0.055601  0.888306  0.560251
112  0.988037  0.057097  0.854126  0.739281
113  0.987427  0.058213  0.877563  0.576107
114  0.988525  0.051345  0.878540  0.592696
115  0.986938  0.057908  0.874268  0.689990
116  0.988525  0.051038  0.875244  0.658129
117  0.988159  0.052801  0.873291  0.593285
118  0.988403  0.052034  0.863892  0.666490
119  0.986450  0.050994  0.860107  0.687002
120  0.988525  0.054082  0.865967  0.625734
121  0.987915  0.058004  0.884033  0.592447
122  0.988159  0.047746  0.840454  0.855207
123  0.988647  0.046740  0.858521  0.715511
124  0.989258  0.048903  0.861450  0.710592
125  0.988892  0.052422  0.867310  0.694608
126  0.985596  0.056188  0.859253  0.648703
127  0.986694  0.060653  0.874878  0.559190

2018-02-13 11:15:09.215603 Finish.
Total elapsed time: 15:13:46.22.
