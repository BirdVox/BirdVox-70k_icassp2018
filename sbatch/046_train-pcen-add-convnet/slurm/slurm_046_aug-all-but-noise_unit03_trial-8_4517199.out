2018-02-12 20:01:35.585672: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:01:35.586003: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:01:35.586016: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 20:01:21.454728 Start.
Training convnet with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.974243  0.114688  0.855957  0.498736
1    0.974487  0.106965  0.868652  0.541105
2    0.977783  0.097339  0.865845  0.495636
3    0.970459  0.114380  0.878052  0.475342
4    0.975708  0.099268  0.863159  0.517835
5    0.977783  0.096463  0.874878  0.471808
6    0.977783  0.102308  0.874756  0.491027
7    0.977051  0.102145  0.865112  0.520003
8    0.975708  0.104943  0.838013  0.668417
9    0.979248  0.092161  0.873169  0.544977
10   0.982178  0.081147  0.873535  0.530989
11   0.979980  0.089598  0.874268  0.507915
12   0.982300  0.080502  0.868896  0.551330
13   0.981689  0.083673  0.869141  0.562359
14   0.979248  0.083660  0.879150  0.541496
15   0.977539  0.099733  0.845215  0.510834
16   0.979248  0.091096  0.854004  0.529647
17   0.982056  0.085139  0.847778  0.539422
18   0.982300  0.079237  0.853638  0.557723
19   0.979858  0.080963  0.876221  0.467267
20   0.982422  0.079273  0.876831  0.522987
21   0.983521  0.078770  0.883911  0.455378
22   0.981323  0.085907  0.861084  0.542553
23   0.979980  0.082957  0.878540  0.449257
24   0.982056  0.082615  0.867065  0.527017
25   0.981689  0.085955  0.874878  0.530180
26   0.984253  0.073235  0.835693  0.667644
27   0.981934  0.075223  0.839722  0.707102
28   0.983398  0.075519  0.861450  0.513105
29   0.983154  0.073417  0.880859  0.491588
30   0.985229  0.067287  0.858398  0.507575
31   0.984131  0.070084  0.869507  0.567813
32   0.984741  0.071012  0.871582  0.562048
33   0.981689  0.071755  0.875244  0.531298
34   0.984009  0.075838  0.882935  0.405609
35   0.983032  0.071203  0.857666  0.536083
36   0.982544  0.072242  0.877686  0.512487
37   0.983154  0.066759  0.877686  0.467268
38   0.985718  0.058138  0.858765  0.594502
39   0.982910  0.070736  0.840576  0.558123
40   0.982788  0.074699  0.862549  0.507550
41   0.983032  0.071414  0.871582  0.434364
42   0.983276  0.063013  0.875122  0.563600
43   0.986084  0.064362  0.879028  0.482707
44   0.985474  0.063723  0.876831  0.482629
45   0.982788  0.065055  0.866211  0.497581
46   0.982788  0.070594  0.863403  0.531275
47   0.983276  0.064300  0.864380  0.680231
48   0.985718  0.062041  0.880005  0.501474
49   0.983154  0.067137  0.864868  0.628584
50   0.984741  0.067202  0.882935  0.462505
51   0.984497  0.070188  0.879272  0.437571
52   0.986450  0.055102  0.851685  0.615568
53   0.983887  0.066867  0.887939  0.495580
54   0.983887  0.067932  0.872314  0.502303
55   0.984985  0.061498  0.859619  0.624541
56   0.982422  0.070306  0.869995  0.493736
57   0.985474  0.062464  0.888184  0.504194
58   0.983032  0.070960  0.865967  0.582733
59   0.984131  0.063148  0.871216  0.581726
60   0.985840  0.065512  0.877441  0.518147
61   0.985474  0.063981  0.864014  0.575293
62   0.985352  0.060564  0.877563  0.527492
63   0.985718  0.058750  0.873657  0.551823
64   0.985840  0.064645  0.866943  0.570076
65   0.987183  0.056828  0.861816  0.591647
66   0.986328  0.058919  0.858398  0.574906
67   0.984863  0.060432  0.871338  0.579276
68   0.982666  0.067451  0.890259  0.428344
69   0.984863  0.062090  0.852905  0.628482
70   0.986450  0.054315  0.881958  0.477559
71   0.987793  0.052535  0.873901  0.587180
72   0.983765  0.064460  0.880127  0.460296
73   0.985229  0.058166  0.877686  0.625616
74   0.983765  0.066292  0.874878  0.582836
75   0.985962  0.055810  0.884155  0.470798
76   0.985596  0.059686  0.884521  0.521963
77   0.987427  0.051653  0.878418  0.555224
78   0.984863  0.067377  0.869263  0.539276
79   0.986084  0.060940  0.883179  0.489900
80   0.986938  0.056436  0.884277  0.515366
81   0.987427  0.054566  0.867188  0.597442
82   0.983765  0.067515  0.897095  0.443876
83   0.984253  0.066619  0.865845  0.559284
84   0.986572  0.063395  0.889404  0.405599
85   0.986450  0.059963  0.881470  0.427960
86   0.985718  0.056664  0.879272  0.539206
87   0.985474  0.060202  0.875977  0.543671
88   0.986084  0.061382  0.865723  0.475978
89   0.986816  0.056160  0.873413  0.605464
90   0.986694  0.058994  0.848267  0.563251
91   0.987427  0.056998  0.871948  0.521513
92   0.989624  0.048947  0.865601  0.652559
93   0.986938  0.056912  0.869141  0.593645
94   0.988037  0.051175  0.858765  0.710921
95   0.988525  0.053976  0.877319  0.592265
96   0.985596  0.062396  0.875244  0.545223
97   0.985962  0.059481  0.895630  0.515654
98   0.986572  0.052021  0.895264  0.401220
99   0.986816  0.055209  0.881714  0.528108
100  0.984253  0.061285  0.876587  0.571394
101  0.985962  0.057954  0.890137  0.519517
102  0.987061  0.058215  0.878662  0.563154
103  0.987915  0.054948  0.869019  0.575045
104  0.985962  0.058160  0.887207  0.418782
105  0.985107  0.057474  0.886108  0.482390
106  0.987793  0.052845  0.875000  0.531732
107  0.986206  0.055950  0.858032  0.593192
108  0.988159  0.053432  0.851807  0.606876
109  0.986450  0.056080  0.872925  0.494911
110  0.988403  0.056825  0.883789  0.528923
111  0.987061  0.050608  0.882202  0.638107
112  0.985596  0.051655  0.880127  0.627929
113  0.987915  0.051541  0.864380  0.659754
114  0.986938  0.055253  0.891846  0.501924
115  0.986816  0.055855  0.855225  0.601651
116  0.986084  0.060661  0.883545  0.491957
117  0.986206  0.057721  0.869263  0.541601
118  0.988281  0.049949  0.871338  0.559777
119  0.987427  0.055864  0.859009  0.574648
120  0.987183  0.052871  0.882935  0.573201
121  0.988403  0.050629  0.880859  0.574783
122  0.988159  0.056838  0.851807  0.644296
123  0.987915  0.057520  0.878540  0.501272
124  0.989258  0.045676  0.885376  0.589923
125  0.988647  0.049883  0.872925  0.648411
126  0.989258  0.047723  0.867065  0.684237
127  0.988647  0.055518  0.875366  0.503390

2018-02-13 11:15:10.967871 Finish.
Total elapsed time: 15:13:49.97.
