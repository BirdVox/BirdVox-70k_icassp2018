2018-02-12 20:14:00.828506: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:14:00.828824: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:14:00.828837: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 20:13:36.230352 Start.
Training convnet with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.497559  8.021872  0.494873  8.062897
1    0.500366  7.973907  0.501831  7.949276
2    0.496948  8.026095  0.500366  7.970674
3    0.488525  8.158693  0.498535  7.998429
4    0.496094  8.036794  0.504028  7.909791
5    0.501099  7.956085  0.490234  8.128911
6    0.499390  7.982649  0.500854  7.959018
7    0.499512  7.980201  0.504028  7.907992
8    0.496704  8.024594  0.496460  8.028339
9    0.497314  8.014598  0.501709  7.944214
10   0.505493  7.884020  0.504517  7.895984
11   0.506714  7.865935  0.497559  8.056195
12   0.495850  8.071703  0.501953  7.965937
13   0.506836  7.883298  0.504150  7.922283
14   0.497681  8.022908  0.496460  8.040248
15   0.505371  7.896658  0.501343  7.959545
16   0.499634  7.985764  0.497314  8.021816
17   0.501221  7.958793  0.507812  7.853015
18   0.499756  7.980876  0.509277  7.828537
19   0.506714  7.868935  0.494873  8.057260
20   0.491333  8.113305  0.500000  7.974759
21   0.500488  7.966643  0.502686  7.931296
22   0.506958  7.862900  0.502686  7.930743
23   0.499390  7.983047  0.504028  7.908865
24   0.491577  8.126119  0.501221  8.058384
25   0.495972  8.138993  0.498169  8.100711
26   0.492310  8.193252  0.504150  8.000746
27   0.500244  8.062439  0.502441  8.025887
28   0.496338  8.123363  0.496582  8.118613
29   0.501221  8.043192  0.497681  8.099656
30   0.508545  7.924066  0.504272  7.992496
31   0.502197  8.025595  0.487427  8.263351
32   0.501953  8.028961  0.504761  7.983479
33   0.489624  8.227273  0.503418  8.004778
34   0.501343  8.038098  0.501709  8.032080
35   0.501587  8.033957  0.506836  7.949272
36   0.492432  8.181379  0.506470  7.955055
37   0.497925  8.092739  0.492676  8.174718
38   0.759521  3.144952  0.758667  0.965857
39   0.961548  0.148335  0.817627  0.894095
40   0.968506  0.120565  0.845215  0.723597
41   0.969849  0.117401  0.820801  0.900838
42   0.973145  0.103926  0.809937  0.907731
43   0.976196  0.098361  0.857422  0.688543
44   0.976929  0.092610  0.824585  0.693997
45   0.979248  0.083632  0.795410  0.970691
46   0.976807  0.079156  0.824097  0.957794
47   0.982056  0.074837  0.825195  1.012278
48   0.980835  0.081276  0.842407  0.741742
49   0.975464  0.087737  0.839966  0.689780
50   0.978516  0.084041  0.848877  0.724989
51   0.980591  0.077035  0.851929  0.678020
52   0.977661  0.088926  0.846802  0.702631
53   0.979858  0.074810  0.832153  0.816888
54   0.981567  0.068191  0.854492  0.694960
55   0.981201  0.078744  0.863403  0.637116
56   0.979858  0.079030  0.869385  0.556008
57   0.981445  0.071428  0.852417  0.665064
58   0.986084  0.057743  0.853760  0.780753
59   0.978516  0.075378  0.861328  0.672377
60   0.984741  0.062755  0.864746  0.664391
61   0.981079  0.073370  0.841797  0.755044
62   0.983032  0.065292  0.883911  0.555685
63   0.982178  0.064525  0.872070  0.561554
64   0.982178  0.068746  0.873291  0.645583
65   0.986206  0.059507  0.854248  0.739023
66   0.985718  0.058029  0.859863  0.666098
67   0.982422  0.063171  0.856934  0.679456
68   0.983643  0.066099  0.866211  0.625123
69   0.983154  0.065624  0.860718  0.659851
70   0.983765  0.066439  0.855347  0.716268
71   0.985474  0.058094  0.862915  0.630126
72   0.984131  0.057849  0.843994  0.776852
73   0.986328  0.056223  0.869751  0.660521
74   0.982544  0.064918  0.876221  0.682853
75   0.983887  0.061141  0.856323  0.725278
76   0.983398  0.064876  0.870972  0.620387
77   0.986572  0.056284  0.885132  0.607312
78   0.981812  0.071698  0.866943  0.636888
79   0.984741  0.064101  0.878296  0.535518
80   0.983276  0.064371  0.870850  0.673245
81   0.985352  0.059098  0.856201  0.773050
82   0.987305  0.054138  0.877563  0.629750
83   0.985962  0.058679  0.863037  0.767513
84   0.985352  0.060224  0.859741  0.700791
85   0.985962  0.057208  0.879272  0.589959
86   0.985596  0.055635  0.860229  0.722655
87   0.984009  0.064893  0.853027  0.775536
88   0.987427  0.055907  0.878540  0.628842
89   0.984253  0.059074  0.859741  0.690066
90   0.985718  0.057545  0.853516  0.797429
91   0.985474  0.057511  0.860840  0.685562
92   0.986938  0.053654  0.855225  0.768974
93   0.984741  0.060378  0.861938  0.714225
94   0.985474  0.059361  0.866089  0.696294
95   0.986450  0.053603  0.876953  0.651326
96   0.986572  0.059776  0.875977  0.614829
97   0.984131  0.059018  0.848877  0.810405
98   0.988037  0.046417  0.867188  0.777663
99   0.988403  0.048138  0.850830  0.747556
100  0.988037  0.055778  0.857056  0.870887
101  0.987549  0.048835  0.877075  0.686017
102  0.985718  0.053121  0.882568  0.616919
103  0.986572  0.051900  0.846802  0.830150
104  0.984375  0.059974  0.871826  0.608821
105  0.987305  0.049693  0.877930  0.617987
106  0.986450  0.053622  0.874634  0.646353
107  0.988037  0.047798  0.862305  0.715464
108  0.985718  0.053873  0.877563  0.655404
109  0.987427  0.045072  0.880249  0.640474
110  0.986816  0.051315  0.863403  0.719468
111  0.988159  0.053891  0.867310  0.751915
112  0.984863  0.060780  0.861450  0.761581
113  0.986206  0.051230  0.884766  0.643961
114  0.987671  0.053247  0.866455  0.656074
115  0.984863  0.057795  0.879761  0.606663
116  0.986450  0.055128  0.837646  0.858600
117  0.987793  0.048658  0.825439  0.941002
118  0.986572  0.058389  0.867065  0.655375
119  0.988281  0.048943  0.864380  0.757313
120  0.986816  0.053672  0.853394  0.828931
121  0.984985  0.066105  0.885010  0.554123
122  0.984497  0.055068  0.876465  0.613205
123  0.987793  0.048916  0.863403  0.734195
124  0.986084  0.055589  0.877319  0.629754
125  0.989136  0.052921  0.864258  0.682471
126  0.987915  0.050069  0.879639  0.566985
127  0.985229  0.053961  0.865356  0.646920

2018-02-13 13:28:42.185663 Finish.
Total elapsed time: 17:15:06.19.
