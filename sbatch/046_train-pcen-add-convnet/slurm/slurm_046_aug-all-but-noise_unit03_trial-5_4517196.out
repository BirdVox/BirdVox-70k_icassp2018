2018-02-12 20:01:39.056904: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:01:39.057229: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:01:39.057243: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 20:01:16.221318 Start.
Training convnet with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.500732  7.966907  0.500366  7.971382
1    0.496216  8.036520  0.491455  8.111510
2    0.497437  8.015458  0.501099  7.956461
3    0.509033  7.880074  0.501221  8.082826
4    0.504150  8.014908  0.505493  7.981827
5    0.723999  3.482466  0.837891  0.580403
6    0.959473  0.162153  0.855103  0.531194
7    0.969604  0.121091  0.842529  0.581808
8    0.973633  0.112613  0.851685  0.498670
9    0.976074  0.104790  0.852173  0.557107
10   0.976929  0.096579  0.853027  0.561131
11   0.977173  0.085274  0.868530  0.561932
12   0.976440  0.096644  0.855835  0.523192
13   0.980713  0.087057  0.861206  0.556347
14   0.976929  0.091235  0.839966  0.612373
15   0.980347  0.084493  0.870605  0.436133
16   0.981689  0.083655  0.869751  0.459127
17   0.981567  0.082307  0.847778  0.516252
18   0.980103  0.078463  0.863403  0.497858
19   0.980103  0.076901  0.855957  0.546572
20   0.981079  0.083690  0.826416  0.631414
21   0.979614  0.088597  0.867432  0.523193
22   0.981201  0.077476  0.867920  0.489751
23   0.980469  0.089577  0.866699  0.446346
24   0.980591  0.082533  0.865723  0.469346
25   0.982300  0.065818  0.862793  0.591851
26   0.980713  0.079178  0.846558  0.566575
27   0.982788  0.075469  0.852661  0.551216
28   0.984375  0.065217  0.861206  0.637167
29   0.983521  0.071204  0.853027  0.579518
30   0.983154  0.065680  0.857788  0.467569
31   0.980591  0.075606  0.852661  0.543729
32   0.981445  0.082691  0.880981  0.422593
33   0.983154  0.063431  0.857788  0.570334
34   0.981934  0.071859  0.859253  0.518569
35   0.983276  0.066891  0.862305  0.577078
36   0.979492  0.081016  0.850342  0.607942
37   0.984253  0.063402  0.837524  0.612204
38   0.983765  0.066841  0.870483  0.491907
39   0.984497  0.069695  0.871338  0.463122
40   0.983765  0.071626  0.876343  0.430956
41   0.978271  0.080508  0.865234  0.445434
42   0.983276  0.065784  0.868896  0.465739
43   0.985352  0.057235  0.859863  0.493759
44   0.983643  0.066522  0.872070  0.443718
45   0.981567  0.071875  0.872192  0.412688
46   0.982666  0.068174  0.852173  0.491389
47   0.984985  0.061965  0.868774  0.462172
48   0.985474  0.060952  0.863037  0.491697
49   0.983887  0.068375  0.855713  0.497119
50   0.983154  0.067267  0.859619  0.555266
51   0.985718  0.062557  0.865967  0.601871
52   0.984253  0.063668  0.860840  0.583868
53   0.985107  0.067305  0.840698  0.554247
54   0.984619  0.060747  0.859497  0.539068
55   0.985474  0.060969  0.856812  0.566245
56   0.983643  0.065566  0.858032  0.517555
57   0.985352  0.063324  0.875000  0.416598
58   0.984009  0.060609  0.821533  0.729114
59   0.986816  0.058530  0.875000  0.473855
60   0.984253  0.064701  0.864014  0.553424
61   0.984131  0.063284  0.863159  0.531752
62   0.984375  0.065349  0.853760  0.481364
63   0.985718  0.055832  0.854248  0.526716
64   0.983276  0.065655  0.842407  0.630714
65   0.985840  0.056632  0.858398  0.612366
66   0.986328  0.055611  0.872437  0.572085
67   0.984375  0.065652  0.862427  0.548824
68   0.985107  0.061094  0.883789  0.468126
69   0.987061  0.060660  0.869751  0.528610
70   0.985107  0.061062  0.876831  0.527274
71   0.985474  0.056228  0.865845  0.597503
72   0.984741  0.062487  0.865967  0.527525
73   0.989380  0.053267  0.862183  0.532303
74   0.982300  0.067197  0.877197  0.415433
75   0.984375  0.068079  0.865234  0.471866
76   0.987671  0.049507  0.875977  0.483454
77   0.985474  0.059125  0.867676  0.502651
78   0.987671  0.053736  0.878296  0.568794
79   0.986206  0.061704  0.853882  0.536571
80   0.984985  0.062625  0.838379  0.619657
81   0.985352  0.060443  0.860596  0.573794
82   0.985229  0.063877  0.860352  0.579611
83   0.987549  0.058451  0.856201  0.602368
84   0.986694  0.057898  0.864502  0.577117
85   0.985962  0.061978  0.848389  0.567759
86   0.987671  0.053126  0.869751  0.561122
87   0.987305  0.056991  0.862427  0.589797
88   0.984863  0.060934  0.842529  0.614984
89   0.986938  0.061937  0.860596  0.540147
90   0.986328  0.057052  0.846313  0.616430
91   0.984863  0.067148  0.852783  0.635105
92   0.985840  0.057854  0.872314  0.446947
93   0.987915  0.052004  0.855103  0.627042
94   0.987183  0.057972  0.861328  0.553941
95   0.982910  0.066340  0.848267  0.610728
96   0.985596  0.060999  0.860596  0.523756
97   0.984985  0.062709  0.879883  0.525939
98   0.985474  0.057266  0.870605  0.482016
99   0.983521  0.065007  0.855957  0.578027
100  0.987183  0.056695  0.849731  0.620389
101  0.987183  0.054632  0.858887  0.586942
102  0.986206  0.055283  0.865234  0.485474
103  0.986572  0.051279  0.859009  0.568077
104  0.987671  0.059285  0.867188  0.492982
105  0.984253  0.061985  0.840698  0.746500
106  0.985229  0.057584  0.858765  0.615469
107  0.985229  0.059862  0.874512  0.518622
108  0.986694  0.053469  0.880005  0.525860
109  0.988525  0.055920  0.847900  0.654301
110  0.988647  0.051767  0.887939  0.548044
111  0.987671  0.051973  0.874756  0.500436
112  0.986328  0.056958  0.847534  0.645351
113  0.987061  0.060701  0.863647  0.547174
114  0.986572  0.062181  0.859741  0.526028
115  0.991821  0.045182  0.884277  0.524095
116  0.987183  0.053140  0.868042  0.504899
117  0.987671  0.052049  0.864746  0.557434
118  0.985718  0.057966  0.850098  0.620667
119  0.986206  0.064075  0.807129  0.840003
120  0.985718  0.057512  0.860840  0.510022
121  0.987915  0.051099  0.850952  0.712628
122  0.988159  0.049277  0.863403  0.631364
123  0.985718  0.055683  0.873901  0.548506
124  0.986572  0.057116  0.852173  0.599407
125  0.986450  0.060980  0.880249  0.506603
126  0.988159  0.055365  0.874268  0.490174
127  0.986694  0.051652  0.863647  0.492143

2018-02-13 12:12:58.530197 Finish.
Total elapsed time: 16:11:42.53.
