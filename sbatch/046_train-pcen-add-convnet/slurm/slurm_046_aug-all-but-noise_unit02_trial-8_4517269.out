2018-02-12 20:14:01.556707: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:14:01.557184: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:14:01.557203: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 20:13:39.091064 Start.
Training convnet with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.492798  8.088320  0.495361  8.046878
1    0.494507  8.060104  0.498779  7.991656
2    0.506958  7.898035  0.501221  8.095014
3    0.489746  8.251654  0.506226  7.972143
4    0.500244  8.063534  0.510742  7.890454
5    0.911743  0.385073  0.763672  1.093520
6    0.963623  0.147459  0.824951  0.802294
7    0.965088  0.128704  0.862305  0.661741
8    0.970459  0.113600  0.861816  0.670527
9    0.975586  0.098464  0.851807  0.723333
10   0.976562  0.094488  0.865845  0.663917
11   0.977661  0.090037  0.850830  0.783833
12   0.978149  0.088579  0.856812  0.652263
13   0.978882  0.081529  0.849976  0.724295
14   0.979980  0.082024  0.894165  0.547322
15   0.980835  0.076131  0.882812  0.605662
16   0.982056  0.072899  0.858154  0.723419
17   0.982910  0.072472  0.866943  0.669226
18   0.981323  0.067769  0.879272  0.623609
19   0.980225  0.080602  0.886230  0.588557
20   0.985229  0.065411  0.893799  0.565903
21   0.983643  0.069604  0.858398  0.719818
22   0.983032  0.071691  0.835938  0.810187
23   0.982422  0.071324  0.880737  0.603820
24   0.982666  0.070803  0.858398  0.665387
25   0.985596  0.060607  0.846069  0.893085
26   0.985718  0.056539  0.858032  0.843336
27   0.984253  0.064214  0.870850  0.649181
28   0.983887  0.064586  0.866455  0.736007
29   0.986816  0.058675  0.878540  0.666243
30   0.984741  0.061036  0.863159  0.756486
31   0.985962  0.062316  0.870117  0.688108
32   0.984741  0.059573  0.884399  0.576219
33   0.983765  0.068035  0.897217  0.529374
34   0.985474  0.058954  0.875610  0.649555
35   0.986206  0.061081  0.888672  0.556387
36   0.987305  0.053367  0.827515  1.018128
37   0.984497  0.064062  0.872803  0.630526
38   0.985718  0.062420  0.901855  0.517084
39   0.986206  0.056304  0.893799  0.534653
40   0.983643  0.060269  0.888184  0.551954
41   0.986328  0.055766  0.886230  0.589347
42   0.985962  0.059553  0.877075  0.609780
43   0.986572  0.057004  0.880005  0.579268
44   0.984619  0.058218  0.875244  0.703206
45   0.987427  0.049015  0.884399  0.613071
46   0.986328  0.052682  0.855835  0.834168
47   0.986816  0.055652  0.877319  0.722726
48   0.987427  0.054872  0.853516  0.867957
49   0.986450  0.057560  0.877441  0.583142
50   0.986572  0.055756  0.883545  0.646536
51   0.986572  0.057279  0.880005  0.642075
52   0.988159  0.050100  0.843140  0.914282
53   0.984985  0.063192  0.877319  0.616226
54   0.985107  0.059917  0.879395  0.634296
55   0.988281  0.051226  0.866699  0.701628
56   0.986328  0.059483  0.880005  0.598219
57   0.986938  0.052418  0.874756  0.675169
58   0.988281  0.050161  0.886597  0.634139
59   0.984375  0.060757  0.891724  0.605526
60   0.984009  0.067408  0.855713  0.735724
61   0.990601  0.045669  0.880615  0.616109
62   0.988403  0.049896  0.886230  0.608897
63   0.985840  0.051406  0.880493  0.628876
64   0.988037  0.051370  0.876221  0.665831
65   0.986938  0.051128  0.888062  0.556014
66   0.986328  0.058468  0.880981  0.588936
67   0.986084  0.054091  0.891357  0.547196
68   0.987061  0.050587  0.869385  0.680658
69   0.986328  0.057776  0.878296  0.651639
70   0.990112  0.045176  0.869019  0.759771
71   0.988647  0.050237  0.859985  0.834460
72   0.987915  0.053973  0.886597  0.634283
73   0.987427  0.053036  0.875732  0.674432
74   0.989014  0.052763  0.878174  0.536546
75   0.987549  0.053458  0.864502  0.684350
76   0.987793  0.052566  0.865967  0.631032
77   0.986938  0.055236  0.875244  0.596251
78   0.988037  0.051100  0.862427  0.750312
79   0.983276  0.062387  0.888428  0.568176
80   0.989502  0.047974  0.876465  0.635642
81   0.986450  0.056320  0.865967  0.729707
82   0.990356  0.045092  0.875610  0.677177
83   0.989746  0.048045  0.884155  0.623994
84   0.989990  0.044055  0.875122  0.718316
85   0.987793  0.053004  0.876465  0.669286
86   0.988403  0.051765  0.875732  0.649181
87   0.985352  0.061496  0.896729  0.492140
88   0.989136  0.047080  0.874756  0.695890
89   0.988770  0.049755  0.886353  0.635394
90   0.986816  0.055041  0.862671  0.805953
91   0.988770  0.049240  0.866943  0.774951
92   0.988892  0.048945  0.878540  0.662235
93   0.988770  0.048342  0.878052  0.620225
94   0.986694  0.051801  0.884644  0.593651
95   0.987061  0.051802  0.893921  0.555105
96   0.991943  0.038434  0.877930  0.700459
97   0.991333  0.042033  0.887573  0.665848
98   0.985962  0.053651  0.873291  0.630263
99   0.986816  0.057122  0.873413  0.686281
100  0.989990  0.044063  0.885986  0.665083
101  0.989990  0.045366  0.869995  0.755717
102  0.986572  0.053378  0.885498  0.587231
103  0.986084  0.054970  0.880981  0.638540
104  0.987671  0.050056  0.896729  0.532073
105  0.989624  0.048860  0.900757  0.476728
106  0.987793  0.054145  0.895264  0.534958
107  0.989136  0.047888  0.892578  0.587805
108  0.987061  0.051528  0.886719  0.544833
109  0.989990  0.045292  0.899048  0.518039
110  0.988892  0.047464  0.888550  0.590708
111  0.988403  0.048504  0.887207  0.558612
112  0.986694  0.049243  0.889038  0.587514
113  0.986938  0.049291  0.879761  0.633495
114  0.987915  0.048769  0.905029  0.484292
115  0.991943  0.040509  0.894897  0.528655
116  0.989502  0.046061  0.895142  0.553995
117  0.988770  0.045218  0.894653  0.551658
118  0.987549  0.051273  0.871704  0.649030
119  0.989380  0.045043  0.879639  0.682590
120  0.988525  0.046160  0.870239  0.763293
121  0.988647  0.051306  0.894775  0.548286
122  0.990234  0.043749  0.880981  0.664875
123  0.986938  0.055435  0.892578  0.512860
124  0.986572  0.051634  0.895996  0.538899
125  0.988647  0.046562  0.892212  0.507206
126  0.988281  0.042187  0.896118  0.496950
127  0.987915  0.050997  0.874634  0.590119

2018-02-13 13:29:36.830124 Finish.
Total elapsed time: 17:15:57.83.
