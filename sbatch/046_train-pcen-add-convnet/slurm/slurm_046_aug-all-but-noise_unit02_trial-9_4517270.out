2018-02-12 20:13:57.702935: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:13:57.703431: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:13:57.703444: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 20:13:38.266688 Start.
Training convnet with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.493896  8.159599  0.504395  7.989769
1    0.497803  8.095614  0.494385  8.150376
2    0.498779  8.079331  0.498413  8.085058
3    0.500977  8.043626  0.495361  8.134039
4    0.501587  8.033635  0.490479  8.212632
5    0.507935  7.931242  0.497925  8.092554
6    0.505127  7.964268  0.498047  8.052146
7    0.618652  5.555876  0.844727  0.667162
8    0.965210  0.154382  0.832397  0.934480
9    0.974365  0.120622  0.822510  1.002031
10   0.975586  0.105241  0.817505  1.020511
11   0.976685  0.097806  0.853516  0.721054
12   0.977173  0.096619  0.816772  1.079136
13   0.979004  0.086379  0.801758  1.133986
14   0.981079  0.074509  0.825195  1.084951
15   0.980713  0.082071  0.809814  1.190403
16   0.981201  0.072455  0.835205  0.953696
17   0.981689  0.070315  0.851074  0.836625
18   0.979614  0.076168  0.835327  0.924226
19   0.983276  0.064932  0.826538  0.987005
20   0.981812  0.078901  0.833740  1.064308
21   0.983276  0.067131  0.864868  0.640649
22   0.982544  0.074451  0.838501  0.864595
23   0.982788  0.071664  0.820190  1.062748
24   0.982788  0.066169  0.840210  0.843809
25   0.983154  0.067824  0.860474  0.804068
26   0.982788  0.067594  0.871948  0.570110
27   0.982422  0.067975  0.859375  0.667383
28   0.984131  0.061596  0.854004  0.824438
29   0.983643  0.063844  0.860596  0.757731
30   0.986694  0.057628  0.833618  0.970065
31   0.983643  0.066446  0.843628  0.903268
32   0.984375  0.061278  0.869995  0.698407
33   0.986206  0.060413  0.847412  0.795206
34   0.986938  0.063532  0.866333  0.782172
35   0.985840  0.060463  0.842163  0.909801
36   0.985229  0.063402  0.843994  0.857711
37   0.984863  0.062606  0.853882  0.774029
38   0.983276  0.064916  0.868408  0.710763
39   0.984741  0.063402  0.847046  0.845243
40   0.986084  0.059752  0.876099  0.605504
41   0.986206  0.058667  0.882690  0.558536
42   0.986450  0.058523  0.857422  0.708829
43   0.985229  0.060983  0.875122  0.512868
44   0.985840  0.053013  0.868408  0.726021
45   0.986206  0.055445  0.857910  0.795226
46   0.985229  0.060759  0.872681  0.688849
47   0.984131  0.062525  0.885620  0.600850
48   0.986206  0.059598  0.829590  0.939750
49   0.984619  0.061959  0.855713  0.812958
50   0.988281  0.052206  0.859009  0.734940
51   0.986572  0.058853  0.884033  0.572542
52   0.986572  0.057633  0.881470  0.595665
53   0.985596  0.058054  0.862183  0.723746
54   0.986450  0.060747  0.852905  0.743360
55   0.987427  0.056701  0.847656  0.821269
56   0.986450  0.057510  0.880127  0.595719
57   0.986572  0.051537  0.832031  1.119472
58   0.985840  0.058350  0.873901  0.614123
59   0.986694  0.056358  0.846436  0.846463
60   0.987915  0.051594  0.857056  0.850101
61   0.986938  0.056973  0.863159  0.711686
62   0.985474  0.062654  0.859131  0.788670
63   0.986572  0.056820  0.869263  0.607607
64   0.984253  0.057498  0.859863  0.715060
65   0.986084  0.058001  0.849854  0.685407
66   0.985718  0.059481  0.852783  0.768251
67   0.988159  0.052522  0.837036  0.908322
68   0.990234  0.046301  0.837158  0.980606
69   0.985474  0.056190  0.884155  0.642031
70   0.986938  0.054419  0.873047  0.686826
71   0.986938  0.050768  0.894653  0.497913
72   0.987915  0.047205  0.833130  1.031711
73   0.984863  0.059006  0.856934  0.881645
74   0.985352  0.060307  0.848877  0.845801
75   0.987061  0.057541  0.847900  0.867330
76   0.986084  0.060814  0.871704  0.629319
77   0.985474  0.057233  0.860962  0.732541
78   0.990479  0.046293  0.828613  0.879769
79   0.986572  0.054737  0.832397  0.898490
80   0.989868  0.049703  0.864746  0.755701
81   0.988647  0.047979  0.862671  0.713227
82   0.989258  0.050406  0.872437  0.629814
83   0.986938  0.054396  0.836548  0.850029
84   0.986084  0.055214  0.875854  0.621262
85   0.988037  0.052178  0.869873  0.700328
86   0.986328  0.052391  0.879639  0.668193
87   0.986450  0.056036  0.870728  0.656163
88   0.987793  0.052486  0.871460  0.646881
89   0.988159  0.049809  0.864014  0.740552
90   0.989380  0.044968  0.879883  0.629248
91   0.990356  0.042455  0.840454  0.992543
92   0.988770  0.049494  0.862061  0.689696
93   0.989746  0.046092  0.860596  0.701092
94   0.986328  0.056684  0.875488  0.605335
95   0.989868  0.044666  0.830444  1.022459
96   0.986694  0.050948  0.850098  0.886543
97   0.987671  0.050489  0.873535  0.627034
98   0.988403  0.048563  0.854858  0.761309
99   0.988159  0.052565  0.860474  0.749236
100  0.988037  0.049111  0.861450  0.732013
101  0.986816  0.052998  0.847534  0.771297
102  0.988770  0.047397  0.859619  0.676003
103  0.989746  0.045010  0.827148  1.072207
104  0.987183  0.053488  0.829102  0.986187
105  0.988770  0.051909  0.856445  0.763142
106  0.988281  0.053715  0.870728  0.646234
107  0.986938  0.053237  0.861816  0.746380
108  0.989868  0.046709  0.847534  0.863342
109  0.989258  0.047600  0.844360  0.852033
110  0.988892  0.048395  0.846558  0.736025
111  0.989990  0.046560  0.864380  0.718563
112  0.988525  0.047685  0.873779  0.694257
113  0.987061  0.053799  0.863647  0.636208
114  0.989502  0.046099  0.871826  0.684047
115  0.989136  0.043862  0.871338  0.683107
116  0.988770  0.047332  0.841431  0.877709
117  0.988525  0.046953  0.849854  0.882096
118  0.987549  0.046730  0.869019  0.683569
119  0.987915  0.049116  0.868042  0.679313
120  0.986816  0.050886  0.843994  0.892139
121  0.987793  0.050336  0.847168  0.853564
122  0.988525  0.050477  0.846558  0.823061
123  0.987671  0.053940  0.890503  0.478543
124  0.989624  0.046104  0.887695  0.529247
125  0.987549  0.049468  0.861084  0.787243
126  0.988647  0.050890  0.865112  0.727446
127  0.989380  0.045473  0.863770  0.760100

2018-02-13 11:23:26.561135 Finish.
Total elapsed time: 15:09:48.56.
