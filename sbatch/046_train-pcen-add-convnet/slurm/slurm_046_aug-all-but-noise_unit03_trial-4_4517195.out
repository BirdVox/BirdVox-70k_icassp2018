2018-02-12 20:01:24.795917: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:01:24.796262: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:01:24.796275: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 20:01:12.963133 Start.
Training convnet with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.979370  0.094364  0.874390  0.541133
1    0.981812  0.092359  0.867920  0.575730
2    0.979248  0.097268  0.858887  0.527365
3    0.978882  0.101859  0.862549  0.478181
4    0.977905  0.101545  0.856689  0.570868
5    0.979248  0.093605  0.878540  0.443055
6    0.982788  0.078788  0.879150  0.591989
7    0.983765  0.074581  0.872437  0.563797
8    0.981079  0.080525  0.873291  0.526701
9    0.981445  0.086778  0.834595  0.747242
10   0.982910  0.074881  0.873291  0.605291
11   0.981201  0.082622  0.871216  0.559517
12   0.985107  0.070395  0.879761  0.499151
13   0.983398  0.070996  0.891724  0.531256
14   0.985352  0.075105  0.894409  0.489556
15   0.982178  0.083259  0.890015  0.441261
16   0.984741  0.066392  0.891479  0.507101
17   0.985107  0.062777  0.891113  0.478926
18   0.982422  0.075736  0.863770  0.575236
19   0.984985  0.071785  0.898682  0.397051
20   0.980835  0.081330  0.868042  0.467367
21   0.988892  0.059619  0.892212  0.453402
22   0.988281  0.059556  0.880371  0.517201
23   0.985107  0.067892  0.867310  0.685601
24   0.986450  0.067347  0.882202  0.534507
25   0.984619  0.065461  0.877197  0.483906
26   0.985352  0.063071  0.864136  0.686833
27   0.986938  0.065862  0.876953  0.655480
28   0.986694  0.065960  0.889526  0.510872
29   0.986694  0.058538  0.878052  0.527652
30   0.987061  0.059522  0.868896  0.557276
31   0.987305  0.066131  0.867676  0.662720
32   0.985718  0.069377  0.861206  0.743214
33   0.987061  0.066242  0.872559  0.642947
34   0.985229  0.062131  0.881470  0.545235
35   0.988159  0.057655  0.879883  0.574182
36   0.986450  0.064962  0.857910  0.550149
37   0.986694  0.059771  0.882446  0.557731
38   0.987671  0.062649  0.883423  0.474722
39   0.987549  0.063680  0.885010  0.512731
40   0.988159  0.058872  0.866455  0.581704
41   0.986084  0.061733  0.891846  0.444958
42   0.989014  0.060554  0.885864  0.461314
43   0.988770  0.052714  0.886353  0.507737
44   0.987793  0.063499  0.885742  0.483651
45   0.987183  0.059500  0.886597  0.535832
46   0.986938  0.056967  0.871338  0.576276
47   0.987915  0.061998  0.860718  0.661614
48   0.988281  0.056001  0.876221  0.635336
49   0.987671  0.056856  0.880127  0.485010
50   0.987427  0.061039  0.887085  0.431276
51   0.988281  0.054366  0.881470  0.513811
52   0.987793  0.057672  0.889160  0.527783
53   0.986328  0.061974  0.882080  0.494791
54   0.989258  0.050268  0.884521  0.481272
55   0.986938  0.063904  0.889893  0.541444
56   0.988647  0.053644  0.891724  0.487759
57   0.986938  0.060096  0.882568  0.522033
58   0.987305  0.059015  0.891113  0.486229
59   0.987305  0.057546  0.887329  0.477711
60   0.989136  0.054120  0.872314  0.578064
61   0.988159  0.055146  0.897583  0.512886
62   0.988647  0.055974  0.876343  0.515402
63   0.987183  0.059210  0.873535  0.581843
64   0.987427  0.056523  0.874512  0.670134
65   0.985962  0.064339  0.879517  0.559690
66   0.986938  0.062966  0.865845  0.577996
67   0.989868  0.050099  0.886719  0.534811
68   0.989136  0.058378  0.880249  0.471581
69   0.990356  0.044382  0.886108  0.482984
70   0.987427  0.054203  0.886841  0.513716
71   0.987305  0.055884  0.891602  0.489270
72   0.987183  0.063024  0.879761  0.566243
73   0.987549  0.056984  0.895874  0.492629
74   0.989258  0.049003  0.881714  0.473801
75   0.989624  0.045982  0.872314  0.599174
76   0.987427  0.050595  0.882568  0.533095
77   0.989258  0.057857  0.874756  0.555834
78   0.987305  0.053869  0.879395  0.503401
79   0.987915  0.057326  0.873657  0.535979
80   0.988403  0.053033  0.874023  0.467556
81   0.988403  0.051776  0.873291  0.625776
82   0.989624  0.053305  0.872925  0.552682
83   0.989746  0.052984  0.881470  0.478758
84   0.988770  0.051678  0.883545  0.554004
85   0.988770  0.049880  0.877563  0.574810
86   0.986938  0.056572  0.889771  0.495358
87   0.990356  0.046652  0.888306  0.560819
88   0.989746  0.050982  0.860352  0.569996
89   0.987793  0.056555  0.884155  0.494998
90   0.987793  0.053519  0.885132  0.539038
91   0.990601  0.046435  0.880127  0.586019
92   0.989258  0.047302  0.896973  0.449669
93   0.990845  0.047796  0.870239  0.565405
94   0.989868  0.050505  0.879395  0.501677
95   0.989624  0.052150  0.889526  0.509981
96   0.988159  0.054048  0.878296  0.640964
97   0.988647  0.048636  0.884888  0.488429
98   0.991211  0.043998  0.882202  0.592343
99   0.988525  0.050860  0.874268  0.494306
100  0.987305  0.050560  0.873169  0.737582
101  0.988770  0.050097  0.882690  0.548362
102  0.989014  0.048583  0.875488  0.540563
103  0.991455  0.043568  0.869751  0.637859
104  0.989624  0.044580  0.875854  0.616839
105  0.990723  0.044068  0.869995  0.583799
106  0.988770  0.051743  0.879150  0.540412
107  0.989746  0.055121  0.865967  0.540691
108  0.989868  0.048622  0.885742  0.499850
109  0.987793  0.052523  0.874512  0.553195
110  0.990967  0.041781  0.885254  0.528572
111  0.991211  0.044239  0.877808  0.604389
112  0.989380  0.050766  0.868286  0.653989
113  0.992065  0.042574  0.894409  0.544550
114  0.989868  0.048659  0.881104  0.517261
115  0.986816  0.056781  0.879272  0.472759
116  0.988892  0.048610  0.876099  0.562228
117  0.989258  0.048629  0.890625  0.480490
118  0.989746  0.047281  0.890259  0.520541
119  0.991333  0.045314  0.867676  0.667906
120  0.990112  0.046337  0.884888  0.466775
121  0.989868  0.048563  0.876099  0.514860
122  0.989502  0.048581  0.884399  0.455959
123  0.989868  0.048522  0.884766  0.518739
124  0.990234  0.047577  0.872681  0.640745
125  0.986206  0.054850  0.881348  0.575139
126  0.989746  0.047143  0.886353  0.493243
127  0.991699  0.043180  0.870728  0.613456

2018-02-13 12:13:32.458833 Finish.
Total elapsed time: 16:12:20.46.
