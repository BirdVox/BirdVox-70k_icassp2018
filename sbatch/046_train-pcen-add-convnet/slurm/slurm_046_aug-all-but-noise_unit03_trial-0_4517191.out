2018-02-12 20:01:19.618679: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:01:19.619019: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 20:01:19.619032: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 20:01:05.293967 Start.
Training convnet with adaptive threshold on BirdVox-70k with PCEN input. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.499023  7.992920  0.492676  8.092798
1    0.508179  7.844685  0.494751  8.057925
2    0.499023  7.989203  0.506348  7.871909
3    0.500854  7.959094  0.494995  8.052170
4    0.498657  7.993540  0.492188  8.096469
5    0.506714  7.864728  0.496094  8.033904
6    0.493530  8.074675  0.499268  7.983126
7    0.664429  4.677895  0.781616  0.811314
8    0.960815  0.180047  0.781860  0.858558
9    0.972290  0.132233  0.786255  0.896940
10   0.973877  0.116387  0.793579  0.891869
11   0.975830  0.102862  0.799927  0.802165
12   0.975586  0.100863  0.786011  0.824566
13   0.974487  0.105615  0.820068  0.677679
14   0.977173  0.095718  0.807983  0.653531
15   0.979248  0.086095  0.811157  0.776401
16   0.981323  0.079064  0.827515  0.868220
17   0.980347  0.076976  0.828735  0.803906
18   0.981201  0.082201  0.791260  0.767997
19   0.981079  0.077240  0.811401  0.738861
20   0.983887  0.071634  0.811768  0.768796
21   0.981445  0.078260  0.805420  0.803384
22   0.984375  0.068516  0.781250  0.828888
23   0.980713  0.078010  0.805664  0.722188
24   0.981812  0.071307  0.828369  0.636538
25   0.982788  0.069415  0.822144  0.717159
26   0.981812  0.071459  0.833740  0.650430
27   0.982422  0.068407  0.835327  0.650557
28   0.979248  0.084721  0.820801  0.707500
29   0.983032  0.067084  0.799194  0.801697
30   0.984253  0.066087  0.821289  0.775203
31   0.984619  0.067716  0.807739  0.833187
32   0.981567  0.078214  0.780273  0.917623
33   0.981567  0.078015  0.826416  0.618442
34   0.984497  0.064484  0.806274  0.728537
35   0.983765  0.063864  0.811157  0.905290
36   0.980957  0.073374  0.847168  0.644676
37   0.983276  0.071417  0.835693  0.590054
38   0.984497  0.066703  0.834595  0.586866
39   0.985962  0.059160  0.820435  0.755809
40   0.983521  0.070174  0.825684  0.722005
41   0.986938  0.057218  0.839478  0.689879
42   0.983154  0.074926  0.803833  0.737134
43   0.982056  0.071506  0.811890  0.754512
44   0.985962  0.062076  0.827515  0.793944
45   0.984985  0.064597  0.831421  0.774859
46   0.984985  0.061894  0.789307  1.069887
47   0.984985  0.066002  0.829712  0.726006
48   0.984497  0.056496  0.834351  0.710467
49   0.983643  0.068052  0.833252  0.634680
50   0.984375  0.061223  0.809326  0.925671
51   0.986328  0.056904  0.803955  0.906763
52   0.987183  0.057771  0.812134  0.855389
53   0.985718  0.060805  0.794556  0.891967
54   0.986084  0.053695  0.813721  0.827708
55   0.986816  0.059996  0.827271  0.751505
56   0.985474  0.060024  0.806030  0.827766
57   0.983887  0.065385  0.850342  0.670904
58   0.987671  0.057049  0.816772  0.873488
59   0.986572  0.058245  0.828247  0.885983
60   0.982422  0.073010  0.827271  0.811883
61   0.986328  0.061566  0.837524  0.697467
62   0.984619  0.070243  0.829102  0.722434
63   0.986206  0.060398  0.824707  0.734953
64   0.985474  0.058704  0.819580  0.724943
65   0.986694  0.057880  0.802124  0.876596
66   0.985596  0.060528  0.792725  1.028329
67   0.987061  0.057370  0.821777  0.920147
68   0.985474  0.064217  0.801758  0.771434
69   0.988037  0.052670  0.826904  0.831095
70   0.987671  0.057552  0.786133  0.834436
71   0.986572  0.060968  0.806763  0.872938
72   0.985962  0.058668  0.819824  0.734455
73   0.987183  0.056586  0.796265  0.881013
74   0.985474  0.061059  0.824097  0.683861
75   0.986938  0.056871  0.790283  0.870319
76   0.987061  0.053598  0.796753  0.821652
77   0.986816  0.054994  0.795044  0.849645
78   0.986206  0.060527  0.827393  0.722357
79   0.987061  0.058909  0.835083  0.902802
80   0.988037  0.053652  0.812988  0.806862
81   0.986450  0.062086  0.826904  0.681752
82   0.986450  0.061916  0.841064  0.679075
83   0.984253  0.061485  0.838013  0.673450
84   0.989014  0.056765  0.831543  0.623571
85   0.989258  0.051911  0.826416  0.786648
86   0.984863  0.063234  0.815918  0.837454
87   0.985352  0.062276  0.809204  0.806917
88   0.986694  0.055748  0.798462  0.800688
89   0.985229  0.059473  0.812012  0.875733
90   0.983765  0.066298  0.770874  1.009737
91   0.987549  0.051321  0.827393  0.737775
92   0.988770  0.052262  0.822754  0.832996
93   0.987549  0.054223  0.806274  0.717926
94   0.986572  0.056598  0.821899  0.685771
95   0.989746  0.051044  0.823853  0.810417
96   0.986938  0.056923  0.800537  0.923812
97   0.988525  0.046313  0.814209  0.913835
98   0.986206  0.061101  0.812256  0.732274
99   0.986084  0.058204  0.806274  0.877705
100  0.988525  0.055607  0.820679  0.827179
101  0.985840  0.056518  0.814209  0.879787
102  0.986572  0.055939  0.828125  0.748968
103  0.987793  0.055307  0.813721  0.869072
104  0.988037  0.052985  0.830811  0.834159
105  0.987183  0.055119  0.822144  0.814584
106  0.987915  0.052799  0.840820  0.679565
107  0.986450  0.059178  0.808838  0.939218
108  0.987427  0.058664  0.830811  0.732181
109  0.989502  0.047650  0.802246  0.836687
110  0.990112  0.046968  0.824585  0.803114
111  0.986572  0.055722  0.811646  0.800200
112  0.986450  0.057659  0.818726  0.848031
113  0.985229  0.060260  0.803467  0.757486
114  0.985596  0.058439  0.795898  0.917836
115  0.988159  0.051760  0.831055  0.770960
116  0.987793  0.051786  0.814087  0.781571
117  0.990234  0.046247  0.828857  0.814700
118  0.988770  0.053964  0.815796  0.831739
119  0.987671  0.056483  0.839600  0.701068
120  0.987793  0.054654  0.821289  0.811633
121  0.989380  0.048274  0.812134  1.007578
122  0.985962  0.060644  0.839233  0.649246
123  0.986328  0.055315  0.801514  0.818094
124  0.990723  0.042617  0.829834  0.812684
125  0.988403  0.052177  0.819214  0.806890
126  0.987183  0.051806  0.831787  0.733156
127  0.989014  0.052285  0.807495  0.828775

2018-02-13 12:12:58.045928 Finish.
Total elapsed time: 16:11:53.05.
