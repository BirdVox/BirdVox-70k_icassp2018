2018-02-12 19:59:02.547462: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 19:59:02.547694: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 19:59:02.547707: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 19:58:47.765483 Start.
Training NTT-like convnet on BirdVox-70k. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 16)            1168        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 4)             68          bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_dense2[0][0]                  
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
flatten (Flatten)                (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             65          flatten[0][0]                    
====================================================================================================
Total params: 678,577
Trainable params: 678,575
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.965210  0.123556  0.764038  0.822527
1    0.971191  0.106688  0.759155  0.897382
2    0.971069  0.106444  0.790771  1.081510
3    0.976562  0.085743  0.773804  0.875606
4    0.977051  0.090950  0.782104  0.665533
5    0.974487  0.096655  0.760620  1.060036
6    0.976440  0.099873  0.767944  1.453921
7    0.975220  0.096256  0.751709  1.050255
8    0.980469  0.083967  0.780640  1.060422
9    0.977051  0.091062  0.772095  1.473430
10   0.976440  0.094260  0.738647  1.801078
11   0.977539  0.097298  0.795776  1.114152
12   0.978149  0.092144  0.786255  1.374876
13   0.980957  0.082170  0.791504  1.432728
14   0.979736  0.085748  0.789551  1.367197
15   0.978149  0.085765  0.794678  0.952141
16   0.975586  0.092076  0.786499  1.632212
17   0.980225  0.075782  0.793945  1.409310
18   0.981812  0.080460  0.805908  0.737479
19   0.978882  0.084319  0.803955  0.815922
20   0.982300  0.073171  0.786499  1.361356
21   0.979126  0.082825  0.795166  1.048426
22   0.758179  3.629866  0.492676  8.124072
23   0.497314  8.047482  0.501709  7.975358
24   0.506958  7.889794  0.497803  8.034263
25   0.500122  7.995817  0.499146  8.010182
26   0.494507  8.082902  0.505737  7.902890
27   0.512817  7.788888  0.507080  7.879665
28   0.495728  8.059480  0.511597  7.805768
29   0.492432  8.110344  0.505859  7.895675
30   0.502197  7.953121  0.498413  8.010816
31   0.500488  7.978951  0.498657  8.007117
32   0.492554  8.104135  0.514648  7.751565
33   0.501587  7.958925  0.515015  7.743078
34   0.499268  7.994739  0.518311  7.690619
35   0.504761  7.906099  0.519165  7.672165
36   0.495483  8.053025  0.515259  7.736083
37   0.507324  7.863362  0.527954  7.534166
38   0.502441  7.940309  0.532715  7.455524
39   0.496948  8.027063  0.538208  7.369277
40   0.495728  8.057260  0.640869  5.759045
41   0.622192  6.079435  0.513062  7.867982
42   0.563110  7.001028  0.494385  8.077618
43   0.489868  8.148615  0.501343  7.964744
44   0.503296  7.932793  0.504150  7.918397
45   0.506714  7.876838  0.500000  7.983229
46   0.496094  8.045007  0.499878  7.984008
47   0.503784  7.921655  0.503418  7.926632
48   0.498535  8.003899  0.500977  7.964473
49   0.498901  7.997115  0.496826  8.029770
50   0.494995  8.058565  0.492798  8.093209
51   0.502197  7.943024  0.504517  7.905679
52   0.498657  7.999202  0.500610  8.056505
53   0.498047  8.114431  0.496338  8.142082
54   0.495605  8.153179  0.502686  8.038376
55   0.504395  8.010186  0.498169  8.109895
56   0.503906  8.016810  0.503296  8.026040
57   0.499390  8.088413  0.504028  8.013059
58   0.496826  8.128570  0.492188  8.202763
59   0.495361  8.151047  0.500610  8.065881
60   0.504395  8.004337  0.494507  8.163156
61   0.499634  8.079979  0.507812  7.947612
62   0.495483  8.145804  0.507446  7.952454
63   0.503784  8.010962  0.488892  8.250483
64   0.500854  8.057158  0.508911  7.926796
65   0.497681  8.107319  0.492676  8.187499
66   0.497070  8.116194  0.502075  8.035054
67   0.507202  7.951963  0.497192  8.112851
68   0.503784  8.006172  0.492920  8.180855
69   0.507568  7.944341  0.503662  8.006899
70   0.501343  8.043899  0.498169  8.094677
71   0.493286  8.173021  0.503540  8.007397
72   0.503906  8.001165  0.498413  8.089380
73   0.497559  8.102851  0.501099  8.045496
74   0.509033  7.917331  0.498169  8.092173
75   0.499756  8.066346  0.500122  8.060201
76   0.512329  7.863223  0.498779  8.081402
77   0.498901  8.079234  0.506226  7.960988
78   0.494019  8.157564  0.499512  8.068852
79   0.497314  8.104111  0.494263  8.153147
80   0.496948  8.109724  0.506226  7.960057
81   0.504761  7.983547  0.483276  8.329718
82   0.501099  8.042352  0.511108  7.880914
83   0.496094  8.122831  0.499878  8.061751
84   0.492310  8.183662  0.497437  8.100952
85   0.499146  8.073342  0.500732  8.047702
86   0.503906  7.996492  0.505493  7.970864
87   0.501831  8.029846  0.504150  7.992422
88   0.495117  8.137984  0.504639  7.984483
89   0.498535  8.082832  0.500610  8.049358
90   0.495605  8.130005  0.507080  7.945037
91   0.504639  7.984371  0.506836  7.948940
92   0.493164  8.169292  0.487061  8.267658
93   0.499756  8.063025  0.501221  8.039406
94   0.492188  8.184998  0.494385  8.149577
95   0.493286  8.167280  0.515869  7.803281
96   0.498657  8.080702  0.513184  7.846561
97   0.509277  7.909521  0.491699  8.192846
98   0.506592  7.952804  0.494019  8.155460
99   0.506714  7.950835  0.498413  8.084627
100  0.501343  8.033692  0.499268  8.044446
101  0.497070  8.091333  0.500854  8.024903
102  0.503052  7.984567  0.499756  8.032248
103  0.498291  8.051496  0.505249  7.936715
104  0.487061  8.223318  0.502930  7.967129
105  0.504272  7.942883  0.503052  7.959628
106  0.503296  7.953298  0.502930  7.956796
107  0.504272  7.933275  0.503052  7.950702
108  0.491577  8.131792  0.493042  8.106664
109  0.495972  8.058349  0.497437  8.033444
110  0.495972  8.055391  0.504272  7.921702
111  0.505981  7.893231  0.496704  8.039953
112  0.507080  7.873469  0.509033  7.841306
113  0.496948  8.033046  0.502563  7.942639
114  0.500244  7.978818  0.504150  7.915779
115  0.517090  7.708810  0.498901  7.998124
116  0.488281  8.166850  0.506104  7.882163
117  0.504028  7.914751  0.503174  7.927900
118  0.496338  8.036461  0.498291  8.004924
119  0.500732  7.965647  0.502563  7.936117
120  0.503052  7.928033  0.502319  7.939424
121  0.508423  7.841865  0.504761  7.900005
122  0.499023  7.991253  0.503418  7.920985
123  0.498047  8.006425  0.500366  7.969267
124  0.498169  8.004131  0.499878  7.976725
125  0.492676  8.091395  0.505615  7.884964
126  0.499023  7.989916  0.505737  7.882747
127  0.494507  8.061660  0.490479  8.125755

2018-02-13 11:15:04.844655 Finish.
Total elapsed time: 15:16:17.84.
