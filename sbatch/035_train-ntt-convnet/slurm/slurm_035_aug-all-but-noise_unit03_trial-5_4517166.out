2018-02-12 19:59:09.448510: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 19:59:09.448807: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 19:59:09.448821: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 19:58:54.722956 Start.
Training NTT-like convnet on BirdVox-70k. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 16)            1168        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 4)             68          bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_dense2[0][0]                  
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
flatten (Flatten)                (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             65          flatten[0][0]                    
====================================================================================================
Total params: 678,577
Trainable params: 678,575
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.498047  0.712282  0.497314  0.707066
1    0.504150  0.719803  0.496338  0.706901
2    0.495605  0.704216  0.490967  0.702254
3    0.499634  2.994737  0.503784  7.923318
4    0.501831  7.952699  0.507202  7.865513
5    0.493774  8.078344  0.503906  7.915691
6    0.510376  7.811633  0.510864  7.803012
7    0.496704  8.028075  0.494629  8.060533
8    0.504028  7.910173  0.496948  8.022579
9    0.507202  7.858727  0.504517  7.901193
10   0.499634  7.978755  0.500488  7.964876
11   0.498047  8.003590  0.506226  7.873013
12   0.504028  7.907890  0.514893  7.734550
13   0.502197  7.936833  0.505127  7.890026
14   0.500732  7.960005  0.500366  7.965770
15   0.497192  8.016310  0.510986  7.796349
16   0.491943  8.099897  0.499268  7.983093
17   0.500854  8.011361  0.503418  8.030362
18   0.503296  8.028898  0.504639  8.004145
19   0.493652  8.178678  0.501831  8.044509
20   0.508667  7.932375  0.502686  8.026981
21   0.504395  7.997929  0.499634  8.073270
22   0.502319  8.028824  0.496948  8.114325
23   0.488037  8.257069  0.496460  8.120494
24   0.499146  8.076538  0.505005  7.981484
25   0.510620  7.890480  0.503784  8.000209
26   0.498413  8.086419  0.502930  8.013292
27   0.497925  8.093702  0.504028  7.995093
28   0.489624  8.227082  0.507812  7.933757
29   0.498169  8.089070  0.503662  8.000422
30   0.511963  7.866548  0.494385  8.149803
31   0.499390  8.069081  0.496948  8.108386
32   0.504517  7.986365  0.497314  8.102422
33   0.489746  8.224389  0.499023  8.074839
34   0.498047  8.090567  0.498413  8.084654
35   0.495117  8.137770  0.510376  7.891822
36   0.503174  8.007903  0.509521  7.905587
37   0.506714  7.950839  0.505859  7.964610
38   0.500122  8.057083  0.504395  7.988218
39   0.497437  8.090324  0.502808  7.975517
40   0.503906  7.955473  0.513794  7.795258
41   0.502441  7.974042  0.501709  7.983622
42   0.504639  7.935026  0.501099  7.989636
43   0.500732  7.993796  0.499756  8.007732
44   0.508423  7.868043  0.514526  7.769257
45   0.496948  8.048107  0.492676  8.114859
46   0.502197  7.961783  0.502197  7.960523
47   0.508057  7.865920  0.490723  8.141091
48   0.500488  7.984291  0.500122  7.989031
49   0.497070  8.036641  0.497437  8.029774
50   0.494995  8.067719  0.506958  7.876038
51   0.491699  8.118385  0.505005  7.905361
52   0.502441  7.945376  0.492798  8.098278
53   0.494507  8.070242  0.504272  7.913777
54   0.496094  8.043436  0.503662  7.922063
55   0.500122  7.977832  0.498413  8.004424
56   0.505859  7.885105  0.503662  7.919543
57   0.507935  7.850883  0.494263  8.068314
58   0.504517  7.904354  0.491943  8.104330
59   0.498291  8.002702  0.507202  7.860222
60   0.499756  7.978558  0.501343  7.952898
61   0.498901  7.991497  0.499756  7.977564
62   0.490356  8.127139  0.501343  7.951729
63   0.498169  8.002097  0.502930  7.925982
64   0.500488  7.964715  0.494873  8.054057
65   0.497070  8.018875  0.502563  7.931158
66   0.492065  8.098401  0.511108  7.794699
67   0.490845  8.117657  0.494507  8.059188
68   0.500732  7.959868  0.493774  8.070731
69   0.503784  7.911100  0.486694  8.183506
70   0.499756  7.975238  0.496338  8.029696
71   0.498413  7.996588  0.491577  8.105547
72   0.489624  8.136668  0.509155  7.825279
73   0.500366  7.977542  0.486938  8.330095
74   0.496826  8.172066  0.506836  8.004836
75   0.501343  8.088339  0.499268  8.117100
76   0.501831  8.071725  0.502319  8.060006
77   0.509888  7.934611  0.510010  7.929391
78   0.494385  8.178341  0.498413  8.110650
79   0.496460  8.139682  0.507446  7.960275
80   0.493286  8.186462  0.500122  8.074341
81   0.492310  8.198576  0.500732  8.061226
82   0.499634  8.077564  0.506958  7.958228
83   0.498047  8.100759  0.499268  8.080058
84   0.509766  7.909976  0.493530  8.170843
85   0.493896  8.164247  0.494873  8.147859
86   0.500977  8.048927  0.498291  8.091694
87   0.498047  8.095180  0.503540  8.006218
88   0.502563  8.021588  0.494385  8.153062
89   0.504517  7.989446  0.498657  8.083592
90   0.504395  7.990853  0.509644  7.905996
91   0.502930  8.013984  0.498779  8.080662
92   0.502319  8.023409  0.498779  8.080281
93   0.493774  8.160783  0.502563  8.018962
94   0.499512  8.068010  0.494873  8.142643
95   0.504395  7.989058  0.504517  7.986980
96   0.495239  8.136418  0.499878  8.061563
97   0.498657  8.081162  0.491211  8.201112
98   0.510986  7.882311  0.496948  8.108524
99   0.502808  8.014037  0.504517  7.986451
100  0.500244  8.055046  0.494263  8.158022
101  0.500366  8.057484  0.501831  8.030999
102  0.511475  7.874353  0.499146  8.068061
103  0.509277  7.903834  0.494385  8.138584
104  0.497559  8.085409  0.499268  8.055596
105  0.496216  8.101747  0.506470  7.935772
106  0.493774  8.135713  0.501709  8.006759
107  0.502319  7.994616  0.496338  8.087557
108  0.494019  8.122165  0.496216  8.084766
109  0.501953  7.990989  0.495117  8.097664
110  0.495361  8.091534  0.501587  7.990058
111  0.503906  7.950939  0.507080  7.898216
112  0.496216  8.069389  0.497925  8.040140
113  0.493408  8.110247  0.500366  7.997454
114  0.488647  8.182526  0.495361  8.073775
115  0.504395  7.928166  0.504761  7.920770
116  0.502441  7.956305  0.494873  8.075563
117  0.504272  7.924429  0.499390  8.001029
118  0.496826  8.040761  0.497925  8.022149
119  0.487549  8.186567  0.508423  7.852822
120  0.493530  8.089368  0.500000  7.985379
121  0.506348  7.883412  0.498047  8.015001
122  0.496338  8.041565  0.500977  7.966953
123  0.489990  8.141493  0.498901  7.998836
124  0.494995  8.060562  0.490967  8.124246
125  0.499146  7.993356  0.508545  7.843014
126  0.486816  8.188953  0.495361  8.052269
127  0.515015  7.738515  0.497070  8.024162

2018-02-13 11:19:13.961572 Finish.
Total elapsed time: 15:20:19.96.
