2018-02-12 19:59:19.217668: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 19:59:19.217883: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 19:59:19.217895: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 19:59:05.453312 Start.
Training NTT-like convnet on BirdVox-70k. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 16)            1168        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 4)             68          bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_dense2[0][0]                  
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
flatten (Flatten)                (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             65          flatten[0][0]                    
====================================================================================================
Total params: 678,577
Trainable params: 678,575
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.968384  0.115757  0.746094  1.378380
1    0.978027  0.094741  0.759277  1.007866
2    0.973022  0.104380  0.764404  0.888861
3    0.970947  0.098416  0.774658  1.212282
4    0.978882  0.087932  0.770142  1.165483
5    0.977661  0.089339  0.749634  1.688119
6    0.977905  0.088159  0.755493  1.702749
7    0.978760  0.086250  0.794800  0.889317
8    0.973511  0.124442  0.789673  1.028807
9    0.980713  0.083830  0.795776  1.002256
10   0.979004  0.080924  0.792725  1.323229
11   0.978027  0.091140  0.793213  1.147294
12   0.978882  0.085312  0.790283  0.936721
13   0.981445  0.078773  0.777832  1.346657
14   0.979004  0.082053  0.793823  0.980099
15   0.978882  0.081738  0.799194  1.076690
16   0.979980  0.074862  0.831055  0.551407
17   0.979004  0.085677  0.820801  0.843149
18   0.978027  0.084532  0.818359  1.113941
19   0.976562  0.095018  0.846680  0.401090
20   0.981689  0.079671  0.797607  1.244325
21   0.980591  0.075472  0.825439  0.565497
22   0.978394  0.088107  0.796387  1.134625
23   0.979614  0.081041  0.804443  0.765612
24   0.981445  0.073360  0.810547  0.993812
25   0.982910  0.074804  0.815918  0.956183
26   0.982910  0.078879  0.821411  0.889626
27   0.982544  0.077741  0.824341  0.881791
28   0.986206  0.065721  0.844482  0.707475
29   0.979980  0.079857  0.807373  1.581930
30   0.981689  0.081741  0.807373  1.107482
31   0.983154  0.076989  0.816650  0.493552
32   0.982666  0.074358  0.818115  1.291809
33   0.981323  0.071734  0.813843  1.015911
34   0.982178  0.075550  0.786865  1.147605
35   0.980957  0.079821  0.833984  0.475936
36   0.979736  0.093146  0.806396  1.543279
37   0.983032  0.072577  0.809082  1.307727
38   0.983276  0.081741  0.792358  1.205751
39   0.982788  0.086133  0.814087  1.094503
40   0.979126  0.090253  0.816650  0.904759
41   0.973755  0.111809  0.832520  1.323903
42   0.976562  0.093328  0.827637  1.206198
43   0.976929  0.107805  0.810791  2.058815
44   0.979980  0.088547  0.827026  2.217450
45   0.982300  0.083029  0.843750  0.729525
46   0.986816  0.069560  0.824097  1.573578
47   0.970459  0.131816  0.817505  1.071916
48   0.979004  0.124046  0.780762  2.893253
49   0.982056  0.103354  0.798340  0.989923
50   0.978394  0.108120  0.800049  1.598585
51   0.976196  0.115279  0.800171  1.099172
52   0.982788  0.104277  0.796021  1.215583
53   0.983643  0.082447  0.801025  2.373204
54   0.848755  2.208642  0.507080  7.875448
55   0.505615  7.898060  0.499878  7.988629
56   0.505127  7.904356  0.500488  7.978239
57   0.495728  8.053469  0.500610  7.974898
58   0.494751  8.067631  0.502686  7.940469
59   0.492432  8.103315  0.496094  8.044317
60   0.497803  8.016492  0.502808  7.936134
61   0.506836  7.871376  0.500610  7.970101
62   0.492188  8.103886  0.503052  7.930199
63   0.497803  8.013426  0.504639  7.903999
64   0.500977  7.961965  0.506348  7.875930
65   0.497559  8.015670  0.495361  8.050330
66   0.489136  8.149237  0.500610  7.965971
67   0.494385  8.064913  0.509033  7.831081
68   0.502441  7.935894  0.506592  7.869458
69   0.499634  7.980139  0.497070  8.020769
70   0.495239  8.049742  0.504150  7.907466
71   0.499023  7.989010  0.501221  7.953794
72   0.504395  7.903028  0.496216  8.033253
73   0.501953  7.941640  0.499634  7.978474
74   0.510376  7.807092  0.506104  7.875083
75   0.495483  8.044284  0.500732  7.960497
76   0.506592  7.866992  0.504395  7.901932
77   0.496948  8.020565  0.502686  7.929024
78   0.500732  7.960095  0.497559  8.010631
79   0.502686  7.928841  0.497559  8.010525
80   0.502808  7.926798  0.498901  7.989030
81   0.497559  8.010401  0.496338  8.029827
82   0.503052  7.922763  0.494507  8.058961
83   0.501831  7.942172  0.499268  7.983018
84   0.504639  7.897371  0.496094  8.033581
85   0.498413  7.996590  0.504517  7.899272
86   0.501099  7.953758  0.506958  7.860916
87   0.497559  8.010322  0.502563  7.930378
88   0.499756  7.975127  0.492798  8.086046
89   0.501343  7.949815  0.499878  7.973163
90   0.498169  8.010440  0.492310  8.188164
91   0.503174  8.012961  0.507935  7.935805
92   0.509155  7.915857  0.493042  8.175335
93   0.497192  8.108245  0.504150  7.995913
94   0.499146  8.076419  0.499756  8.066422
95   0.491577  8.198099  0.490112  8.221563
96   0.495117  8.140754  0.507690  7.937959
97   0.497681  8.099166  0.494873  8.144288
98   0.499146  8.075298  0.505249  7.976795
99   0.505249  7.976676  0.499878  8.063129
100  0.491577  8.196807  0.499390  8.070772
101  0.503540  8.003768  0.492676  8.178773
102  0.496094  8.123581  0.491943  8.190378
103  0.494019  8.156836  0.496094  8.123296
104  0.494873  8.142885  0.497314  8.103449
105  0.508789  7.918420  0.495117  8.138707
106  0.500732  8.048128  0.497192  8.105116
107  0.501709  8.032252  0.494141  8.154176
108  0.496094  8.122638  0.496948  8.108809
109  0.504761  7.982836  0.495117  8.138222
110  0.503052  8.010287  0.503174  8.008277
111  0.495361  8.134161  0.498657  8.081001
112  0.490967  8.204924  0.504761  7.982560
113  0.501953  7.954840  0.493042  8.084819
114  0.495239  8.049528  0.497437  8.014263
115  0.493530  8.076343  0.500610  7.963288
116  0.507568  7.852204  0.496216  8.033043
117  0.500977  7.957016  0.503052  7.923809
118  0.502319  7.935377  0.506592  7.867161
119  0.505859  7.878746  0.505005  7.892282
120  0.496948  8.020646  0.498901  7.989435
121  0.510132  7.810329  0.494019  8.067151
122  0.494019  8.067095  0.498291  7.998929
123  0.502197  7.936607  0.504150  7.905425
124  0.495972  8.035774  0.496460  8.027952
125  0.500244  7.967590  0.509766  7.815764
126  0.497437  8.012292  0.497314  8.014211
127  0.495483  8.043380  0.491211  8.111472

2018-02-13 12:43:17.503320 Finish.
Total elapsed time: 16:44:12.50.
