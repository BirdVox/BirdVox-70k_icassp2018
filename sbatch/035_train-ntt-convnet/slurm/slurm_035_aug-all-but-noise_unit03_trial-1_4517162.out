2018-02-12 19:58:59.303891: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 19:58:59.304197: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 19:58:59.304210: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 19:58:45.063788 Start.
Training NTT-like convnet on BirdVox-70k. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 16)            1168        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 4)             68          bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_dense2[0][0]                  
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
flatten (Flatten)                (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             65          flatten[0][0]                    
====================================================================================================
Total params: 678,577
Trainable params: 678,575
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.971191  0.106587  0.753662  1.774587
1    0.975342  0.086999  0.768188  1.437089
2    0.975464  0.091440  0.780029  1.463755
3    0.975220  0.090785  0.764648  1.724826
4    0.972778  0.106453  0.775391  1.252032
5    0.979248  0.080498  0.787109  1.292997
6    0.977905  0.080185  0.817017  0.916086
7    0.977051  0.089681  0.775757  1.588409
8    0.977539  0.087234  0.785645  1.456502
9    0.978638  0.088198  0.817383  0.808072
10   0.976685  0.087553  0.800171  0.967445
11   0.973633  0.103161  0.822144  0.675891
12   0.977783  0.079674  0.788452  1.570531
13   0.977173  0.089529  0.820435  0.744820
14   0.977173  0.082098  0.815796  1.005815
15   0.981201  0.077968  0.832031  0.800090
16   0.979248  0.079554  0.824707  1.048585
17   0.976562  0.089379  0.826172  1.071114
18   0.951416  0.203675  0.798462  1.066574
19   0.976807  0.111329  0.770752  1.425156
20   0.981079  0.089899  0.837036  0.823381
21   0.981934  0.071336  0.815796  1.128079
22   0.977905  0.086228  0.789917  1.846869
23   0.977295  0.097866  0.842407  0.801182
24   0.978516  0.091642  0.826782  0.899063
25   0.981323  0.080842  0.814941  0.987320
26   0.982788  0.077996  0.822876  1.123542
27   0.980469  0.085681  0.851929  0.631302
28   0.981812  0.082889  0.838745  0.668109
29   0.981445  0.077690  0.869263  0.660187
30   0.982178  0.075460  0.828979  0.799513
31   0.981201  0.080348  0.802490  1.301425
32   0.979736  0.080752  0.842651  0.748528
33   0.981689  0.074217  0.841919  0.673851
34   0.980957  0.083541  0.810547  1.112115
35   0.980103  0.080905  0.828857  0.944687
36   0.980591  0.080421  0.812134  0.948430
37   0.983887  0.072390  0.848389  0.577555
38   0.979736  0.083504  0.846924  0.997185
39   0.975220  0.104056  0.830200  1.250018
40   0.982300  0.082561  0.817139  0.846856
41   0.982056  0.081474  0.847168  0.733154
42   0.983643  0.080032  0.847778  0.886937
43   0.980225  0.086600  0.808960  1.102364
44   0.983398  0.077156  0.805908  1.759833
45   0.982910  0.073409  0.827881  1.468131
46   0.979858  0.089682  0.822266  1.155242
47   0.978882  0.088212  0.833862  0.993458
48   0.979614  0.075857  0.794556  1.487531
49   0.982788  0.084511  0.832275  1.318416
50   0.980347  0.096112  0.870728  1.025064
51   0.970337  0.103356  0.844360  0.886824
52   0.984375  0.080506  0.828125  1.151449
53   0.981079  0.086698  0.833618  1.262447
54   0.982178  0.087623  0.830811  1.228172
55   0.984863  0.078200  0.827148  1.172184
56   0.979614  0.077788  0.815674  1.934925
57   0.983521  0.087040  0.871704  0.630314
58   0.978271  0.110145  0.830933  0.989553
59   0.975830  0.133496  0.791382  2.375704
60   0.979492  0.088390  0.795166  1.664028
61   0.979248  0.109815  0.830444  1.533584
62   0.978394  0.109653  0.839233  1.206548
63   0.979858  0.104606  0.830444  0.575354
64   0.978394  0.094950  0.848755  1.073748
65   0.980957  0.109988  0.810303  1.333579
66   0.981689  0.106623  0.835205  0.991148
67   0.980103  0.109067  0.829346  1.824614
68   0.983521  0.108108  0.758179  3.131500
69   0.976929  0.124701  0.823486  1.459748
70   0.981079  0.128129  0.850098  1.595438
71   0.978149  0.124725  0.842407  1.343977
72   0.978516  0.123527  0.828857  1.055429
73   0.981934  0.103666  0.833008  1.680432
74   0.943237  0.691713  0.729248  4.388148
75   0.886230  1.831872  0.791260  3.374985
76   0.798096  3.278498  0.735229  4.305788
77   0.636475  5.841175  0.502930  7.963660
78   0.503662  7.951523  0.491699  8.141741
79   0.500854  7.995344  0.503052  7.959834
80   0.494995  8.087827  0.497070  8.054294
81   0.512573  7.806699  0.500366  8.000866
82   0.505127  7.924574  0.504028  7.941606
83   0.496704  8.057936  0.504028  7.940732
84   0.500732  7.992905  0.503418  7.949588
85   0.502441  7.964720  0.499023  8.018769
86   0.497070  8.049489  0.497803  8.037347
87   0.500488  7.994091  0.497070  8.048134
88   0.498169  8.030193  0.490601  8.150377
89   0.500366  7.994259  0.499634  8.005457
90   0.501709  7.971937  0.506348  7.897501
91   0.507324  7.881489  0.492920  8.110637
92   0.490234  8.152980  0.497681  8.033792
93   0.498291  8.023584  0.508423  7.861575
94   0.504761  7.919495  0.506470  7.891738
95   0.497559  8.033312  0.507812  7.869343
96   0.501343  7.971989  0.505981  7.897534
97   0.500366  7.986552  0.501221  7.972420
98   0.507568  7.870715  0.500122  7.988913
99   0.499512  7.998152  0.502563  7.948961
100  0.501465  7.965982  0.497925  8.021876
101  0.494141  8.081688  0.488281  8.174578
102  0.500854  7.973613  0.495117  8.064557
103  0.506226  7.886946  0.497559  8.024599
104  0.488037  8.175880  0.506348  7.883450
105  0.501587  7.958839  0.503540  7.927191
106  0.510254  7.819655  0.495850  8.048791
107  0.497803  8.017162  0.500000  7.981640
108  0.494629  8.066810  0.514893  7.743258
109  0.499634  7.986054  0.497925  8.012835
110  0.501221  7.959843  0.493164  8.087839
111  0.513184  7.768251  0.497803  8.013033
112  0.504395  7.907537  0.502197  7.942164
113  0.508301  7.844476  0.503662  7.918050
114  0.504272  7.907962  0.500732  7.964048
115  0.498901  7.992910  0.508423  7.840793
116  0.480957  8.278362  0.499878  7.976425
117  0.497437  8.015076  0.506592  7.868855
118  0.492310  8.096307  0.500610  7.963739
119  0.503662  7.914875  0.499634  7.978892
120  0.500854  7.959247  0.502563  7.931826
121  0.497803  8.007567  0.491821  8.102777
122  0.499756  7.976149  0.502197  7.937102
123  0.506226  7.872772  0.499146  7.985543
124  0.499268  7.983508  0.500366  7.965909
125  0.503174  7.921078  0.502930  7.924903
126  0.489868  8.133078  0.495850  8.037667
127  0.504883  7.893612  0.506348  7.870218

2018-02-13 11:15:23.232904 Finish.
Total elapsed time: 15:16:38.23.
