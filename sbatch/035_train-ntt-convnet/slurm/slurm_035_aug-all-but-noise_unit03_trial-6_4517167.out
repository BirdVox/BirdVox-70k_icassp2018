2018-02-12 19:59:13.801345: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 19:59:13.801676: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 19:59:13.801688: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 19:58:58.765332 Start.
Training NTT-like convnet on BirdVox-70k. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 16)            1168        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 4)             68          bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_dense2[0][0]                  
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
flatten (Flatten)                (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             65          flatten[0][0]                    
====================================================================================================
Total params: 678,577
Trainable params: 678,575
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.973267  0.099909  0.700439  1.957730
1    0.966919  0.119156  0.715454  1.493217
2    0.975586  0.092967  0.731079  1.411456
3    0.975464  0.089265  0.701538  2.041367
4    0.975342  0.095987  0.712036  2.086000
5    0.976196  0.089837  0.729858  0.936857
6    0.978516  0.082415  0.733521  1.592301
7    0.977417  0.087818  0.745972  1.283731
8    0.980713  0.083321  0.720825  0.997628
9    0.977905  0.091431  0.761963  1.123150
10   0.982910  0.069595  0.723511  0.776488
11   0.972168  0.109987  0.744263  1.223049
12   0.981812  0.080535  0.724243  1.848273
13   0.969360  0.115155  0.727539  2.921577
14   0.980225  0.090180  0.707031  1.427590
15   0.980347  0.082288  0.750732  1.394688
16   0.979736  0.083736  0.779541  0.883435
17   0.980713  0.084818  0.739868  1.832865
18   0.935669  0.163713  0.615356  0.987634
19   0.897217  1.247667  0.638916  5.826002
20   0.695801  4.893994  0.555298  7.163363
21   0.781982  3.528655  0.525513  7.647849
22   0.698242  4.862078  0.501465  8.001192
23   0.496948  8.073589  0.496948  8.072694
24   0.498901  8.040673  0.509033  7.878281
25   0.501221  8.002007  0.499634  8.026491
26   0.494995  8.099658  0.502563  7.978218
27   0.511841  7.829556  0.494385  8.107088
28   0.508179  7.886436  0.497437  8.056947
29   0.498047  8.046482  0.500488  8.006820
30   0.491699  8.146207  0.511719  7.826309
31   0.503540  7.955962  0.499146  8.025279
32   0.492432  8.131572  0.504639  7.936212
33   0.496094  8.071686  0.497681  8.045625
34   0.499512  8.015669  0.501831  7.977918
35   0.506958  7.895404  0.502930  7.958836
36   0.503662  7.946367  0.504150  7.937779
37   0.504883  7.925296  0.504028  7.938102
38   0.506226  7.902253  0.490356  8.154416
39   0.507202  7.885027  0.492432  8.119666
40   0.502930  7.951466  0.499756  8.001220
41   0.508301  7.864154  0.507568  7.874984
42   0.503662  7.936420  0.503540  7.937522
43   0.503540  7.936688  0.503784  7.931959
44   0.502075  7.958380  0.508057  7.862196
45   0.505981  7.894470  0.501953  7.957882
46   0.498047  8.019368  0.506592  7.882354
47   0.507202  7.871859  0.499390  7.995647
48   0.496826  8.035779  0.494995  8.064239
49   0.500977  7.968177  0.498657  8.004456
50   0.497925  8.015466  0.498657  8.003130
51   0.504150  7.914928  0.497192  8.025236
52   0.502808  7.935130  0.499634  7.985152
53   0.501465  7.955419  0.501465  7.954887
54   0.508423  7.843461  0.495361  8.051207
55   0.501587  7.951503  0.492554  8.095073
56   0.499878  7.977900  0.500977  7.959989
57   0.502808  7.930435  0.494019  8.070202
58   0.501587  7.949224  0.499878  7.976161
59   0.499390  7.983666  0.497681  8.010643
60   0.498291  8.000673  0.501465  7.949844
61   0.505127  7.891257  0.494385  8.062318
62   0.509888  7.814992  0.499268  7.984138
63   0.499878  7.974264  0.514404  7.742544
64   0.497437  8.012933  0.498901  7.989469
65   0.507324  7.855094  0.492920  8.084643
66   0.497559  8.010615  0.497314  8.014436
67   0.509399  7.821713  0.505127  7.889771
68   0.498291  7.998705  0.504150  7.905250
69   0.504272  7.903269  0.496460  8.027787
70   0.496460  8.027760  0.492676  8.088065
71   0.495605  8.041340  0.491699  8.103598
72   0.500244  7.967358  0.508667  7.833066
73   0.489868  8.132755  0.501343  7.949814
74   0.500366  7.965377  0.496338  8.029592
75   0.500244  7.967313  0.503296  7.918658
76   0.498901  8.047360  0.495483  8.153948
77   0.502075  8.045962  0.504272  8.009041
78   0.495361  8.151472  0.504028  8.010665
79   0.489868  8.237946  0.485352  8.309843
80   0.498169  8.102457  0.497437  8.113505
81   0.502319  8.034126  0.505005  7.990192
82   0.489014  8.247357  0.499146  8.083490
83   0.493286  8.177425  0.508667  7.929026
84   0.500732  8.056473  0.500244  8.063914
85   0.505127  7.984821  0.500366  8.061176
86   0.494263  8.159205  0.504639  7.991627
87   0.509644  7.910645  0.500122  8.063809
88   0.492065  8.193384  0.495483  8.138017
89   0.503906  8.001997  0.503784  8.003711
90   0.492310  8.188421  0.508545  7.926502
91   0.492432  8.185995  0.494507  8.152327
92   0.505981  7.967169  0.499023  8.079113
93   0.503540  8.006117  0.496216  8.123976
94   0.501831  8.033284  0.491089  8.206245
95   0.495361  8.137207  0.497314  8.105555
96   0.501953  8.030626  0.502319  8.024563
97   0.496826  8.112950  0.496460  8.118702
98   0.500488  8.053632  0.509399  7.909862
99   0.494751  8.145836  0.494751  8.145708
100  0.511475  7.876035  0.507568  7.938879
101  0.490601  8.212257  0.498291  8.088195
102  0.505615  7.970043  0.503052  8.011265
103  0.502930  8.013143  0.501587  8.034699
104  0.497559  8.099548  0.498535  8.083730
105  0.499023  8.075788  0.502563  8.018660
106  0.499146  8.073688  0.500122  8.057886
107  0.507202  7.943713  0.493530  8.164023
108  0.498047  8.091174  0.491333  8.199341
109  0.506348  7.957290  0.493896  8.157936
110  0.501099  8.041812  0.499756  8.063417
111  0.510742  7.886304  0.501587  8.033836
112  0.489624  8.226624  0.495483  8.132153
113  0.489014  8.236405  0.498169  8.088814
114  0.507080  7.945160  0.498535  8.082865
115  0.500000  8.059234  0.493408  8.165461
116  0.502563  8.017879  0.497314  8.102465
117  0.493652  8.161477  0.499268  8.070956
118  0.509888  7.899767  0.498169  8.088639
119  0.496948  8.108304  0.495972  8.124035
120  0.496094  8.122059  0.504028  7.994162
121  0.499756  8.063019  0.497803  8.094494
122  0.500488  8.051203  0.499390  8.068906
123  0.493530  8.163345  0.501587  8.033484
124  0.496338  8.118085  0.507080  7.944939
125  0.498169  8.088568  0.500732  8.047248
126  0.501587  8.012997  0.493408  8.084342
127  0.505615  7.889238  0.494019  8.073673

2018-02-13 11:34:54.784874 Finish.
Total elapsed time: 15:35:56.78.
