2018-02-12 19:58:57.587368: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 19:58:57.587693: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 19:58:57.587705: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 19:58:42.134118 Start.
Training NTT-like convnet on BirdVox-70k. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 16)            1168        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 4)             68          bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_dense2[0][0]                  
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
flatten (Flatten)                (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             65          flatten[0][0]                    
====================================================================================================
Total params: 678,577
Trainable params: 678,575
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.966187  0.122879  0.721558  1.010044
1    0.972778  0.102711  0.742920  1.321479
2    0.972290  0.102253  0.710693  2.280300
3    0.970581  0.102584  0.754639  1.136743
4    0.973389  0.099167  0.753174  1.439111
5    0.972656  0.099227  0.719482  2.209101
6    0.974121  0.095678  0.732422  1.116719
7    0.972534  0.099094  0.763184  1.332104
8    0.977173  0.087593  0.767822  1.214434
9    0.974243  0.096663  0.768555  1.268775
10   0.976685  0.090074  0.777588  1.036730
11   0.977661  0.092597  0.765381  0.681437
12   0.965820  0.136537  0.654297  4.145789
13   0.942017  0.330004  0.689819  1.394720
14   0.945679  0.217577  0.699463  1.644697
15   0.970703  0.112899  0.697998  1.840270
16   0.970825  0.108497  0.737061  1.397466
17   0.979614  0.083243  0.738159  1.496653
18   0.977905  0.096383  0.741577  1.415866
19   0.975586  0.091541  0.745361  1.394151
20   0.978760  0.086541  0.742065  2.498646
21   0.981812  0.083439  0.768311  1.063708
22   0.978271  0.083867  0.759888  1.256963
23   0.978882  0.083168  0.732544  1.228791
24   0.978149  0.089236  0.735840  1.853320
25   0.978027  0.089727  0.786743  0.846084
26   0.979126  0.086011  0.739868  1.799986
27   0.977295  0.091428  0.780273  0.751535
28   0.982056  0.080867  0.736938  1.482247
29   0.978271  0.086395  0.742554  1.134110
30   0.972290  0.098897  0.733643  2.168007
31   0.978027  0.081054  0.739014  1.032980
32   0.982788  0.075248  0.755005  0.659660
33   0.982056  0.064903  0.776733  0.768791
34   0.906494  0.405647  0.668579  0.667610
35   0.745483  0.528111  0.610107  0.637630
36   0.781006  0.943657  0.503296  0.767499
37   0.520386  0.731278  0.489258  0.738055
38   0.505249  0.724049  0.501709  0.724158
39   0.509521  0.716880  0.492554  0.721689
40   0.500122  0.715926  0.500000  0.718704
41   0.498413  7.698580  0.493530  8.103148
42   0.500122  7.997342  0.497437  8.039469
43   0.503174  7.947369  0.494385  8.086864
44   0.503784  7.936421  0.502075  7.963077
45   0.508545  7.859368  0.500977  7.979463
46   0.493530  8.097630  0.495483  8.065951
47   0.494995  8.073211  0.500732  7.981222
48   0.500000  7.992392  0.499023  8.007456
49   0.500366  7.985560  0.509644  7.837171
50   0.501343  7.969033  0.486938  8.198202
51   0.487427  8.189963  0.500488  7.981278
52   0.501221  7.969164  0.500610  7.978459
53   0.496216  8.048096  0.498169  8.016538
54   0.494629  8.072569  0.491455  8.122763
55   0.502686  7.943331  0.490356  8.139496
56   0.505249  7.901696  0.506592  7.879913
57   0.503052  7.935985  0.486816  8.194452
58   0.495972  8.048143  0.500854  7.969947
59   0.491089  8.125293  0.493530  8.086031
60   0.498413  8.007856  0.499023  7.997794
61   0.505371  7.896275  0.494629  8.067208
62   0.506714  7.874231  0.500366  7.975113
63   0.508301  7.848310  0.493286  8.087372
64   0.497925  8.013121  0.496582  8.034227
65   0.492310  8.102046  0.497314  8.021962
66   0.495972  8.043081  0.499146  7.992193
67   0.500000  7.978287  0.507446  7.859292
68   0.499756  7.981618  0.498047  8.008585
69   0.495361  8.051126  0.497681  8.013877
70   0.500610  7.966904  0.503052  7.927714
71   0.499023  7.991674  0.499512  7.983629
72   0.493652  8.076787  0.501099  7.957822
73   0.496704  8.027636  0.496216  8.035176
74   0.504883  7.967576  0.495972  8.166074
75   0.510010  7.937657  0.504639  8.022422
76   0.496704  8.148910  0.503052  8.045301
77   0.494873  8.176002  0.500488  8.084424
78   0.491699  8.225118  0.506348  7.988075
79   0.496094  8.152483  0.504761  8.011942
80   0.503662  8.028860  0.502319  8.049728
81   0.495239  8.163116  0.502075  8.052215
82   0.508179  7.953158  0.501831  8.054799
83   0.496948  8.132863  0.499268  8.094849
84   0.494629  8.169015  0.505493  7.993308
85   0.493408  8.187525  0.493652  8.183025
86   0.494263  8.172643  0.505249  7.995025
87   0.505493  7.990569  0.495972  8.143519
88   0.499512  8.085959  0.494507  8.166129
89   0.497803  8.112521  0.496948  8.125811
90   0.485962  8.302419  0.496094  8.138644
91   0.505615  7.984719  0.498047  8.106250
92   0.496948  8.123513  0.496216  8.134873
93   0.506470  7.969166  0.504272  8.004149
94   0.508423  7.936831  0.508057  7.942312
95   0.491699  8.205552  0.494263  8.163823
96   0.498169  8.100463  0.499634  8.076455
97   0.494385  8.160673  0.502319  8.032397
98   0.495728  8.138270  0.497925  8.102482
99   0.499878  8.070640  0.502686  8.025027
100  0.496948  8.117154  0.508911  7.923989
101  0.494507  8.155825  0.500488  8.059084
102  0.505615  7.976128  0.496338  8.125344
103  0.497559  8.105363  0.497070  8.112930
104  0.500244  8.061484  0.494629  8.151703
105  0.510132  7.901550  0.498901  8.082292
106  0.501831  8.034810  0.494507  8.152605
107  0.494873  8.146457  0.499512  8.071449
108  0.492432  8.185336  0.504517  7.990323
109  0.493896  8.161284  0.499146  8.076468
110  0.494995  8.143165  0.495972  8.127228
111  0.505859  7.967672  0.501221  8.042256
112  0.502441  8.022410  0.497925  8.095041
113  0.496826  8.112593  0.499146  8.075056
114  0.500610  8.051302  0.507080  7.946882
115  0.499390  8.070707  0.497437  8.102061
116  0.504761  7.983890  0.490234  8.217913
117  0.491455  8.198132  0.500488  8.052431
118  0.496460  8.117265  0.505737  7.967640
119  0.495361  8.134796  0.493774  8.160293
120  0.497070  8.107095  0.492432  8.181790
121  0.499756  8.063672  0.497559  8.099025
122  0.494873  8.142254  0.489014  8.236642
123  0.498901  8.077222  0.499756  8.063402
124  0.495239  8.136160  0.507812  7.933464
125  0.503174  8.008195  0.516357  7.795666
126  0.499878  8.061256  0.504761  7.982550
127  0.492920  8.173353  0.491333  8.198907

2018-02-13 11:14:58.766473 Finish.
Total elapsed time: 15:16:16.77.
