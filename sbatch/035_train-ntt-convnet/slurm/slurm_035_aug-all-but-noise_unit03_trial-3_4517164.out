2018-02-12 19:59:02.450482: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 19:59:02.450730: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 19:59:02.450742: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 19:58:48.192824 Start.
Training NTT-like convnet on BirdVox-70k. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 16)            1168        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 4)             68          bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_dense2[0][0]                  
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
flatten (Flatten)                (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             65          flatten[0][0]                    
====================================================================================================
Total params: 678,577
Trainable params: 678,575
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.968262  0.111040  0.706177  1.336761
1    0.971680  0.103067  0.652466  2.677214
2    0.972900  0.103754  0.735107  0.869405
3    0.973633  0.097493  0.683472  1.247075
4    0.818115  2.546599  0.509888  7.943656
5    0.730591  4.311620  0.505981  7.925749
6    0.497192  8.064112  0.503906  7.954914
7    0.498291  8.042805  0.494629  8.099739
8    0.572998  6.810677  0.534302  5.207240
9    0.687622  1.352412  0.506958  1.025513
10   0.672363  1.467883  0.485718  0.810670
11   0.552490  6.582380  0.507812  7.999977
12   0.500000  8.124727  0.501343  8.101943
13   0.502808  8.077254  0.497314  8.164725
14   0.500244  8.116472  0.491333  8.259072
15   0.494751  8.202973  0.493042  8.229507
16   0.499023  8.132099  0.499512  8.123224
17   0.500122  8.112388  0.505615  8.022841
18   0.497681  8.149726  0.504883  8.032624
19   0.505859  8.015866  0.494751  8.193882
20   0.504639  8.033479  0.502441  8.067848
21   0.497681  8.143532  0.503906  8.042124
22   0.507812  7.978095  0.516602  7.835350
23   0.493652  8.204163  0.496094  8.163714
24   0.504272  8.030790  0.505493  8.010002
25   0.494507  8.185970  0.505249  8.011704
26   0.502441  8.055838  0.498535  8.117670
27   0.500610  8.083100  0.496826  8.142964
28   0.507812  7.964766  0.494751  8.174167
29   0.492432  8.210440  0.504639  8.012571
30   0.495117  8.164945  0.490845  8.232712
31   0.496094  8.147035  0.503418  8.027910
32   0.492798  8.198043  0.500366  8.075015
33   0.495728  8.148773  0.501099  8.061199
34   0.490234  8.235343  0.509399  7.925480
35   0.494995  8.156729  0.500488  8.067279
36   0.491821  8.206105  0.502808  8.028167
37   0.504395  8.001773  0.503662  8.012775
38   0.501343  8.049398  0.484863  8.314268
39   0.494995  8.150259  0.498535  8.092511
40   0.498291  8.095800  0.507202  7.951537
41   0.493042  8.179182  0.505005  7.985788
42   0.495239  8.142657  0.495972  8.130331
43   0.503540  8.007864  0.500732  8.052651
44   0.505249  7.979424  0.506104  7.965236
45   0.500610  8.053396  0.502441  8.023517
46   0.490356  8.217971  0.490601  8.213717
47   0.490601  8.213428  0.499268  8.073455
48   0.496704  8.114525  0.505005  7.980494
49   0.503174  8.009795  0.501587  8.035171
50   0.498535  8.084181  0.498169  8.089913
51   0.500000  8.060251  0.500000  8.060109
52   0.503174  8.008830  0.489624  8.227111
53   0.497559  8.040391  0.491943  8.120059
54   0.501587  7.964311  0.498657  8.009348
55   0.507202  7.871880  0.503296  7.933036
56   0.493896  8.081966  0.499268  7.995482
57   0.495972  8.047292  0.502319  7.945400
58   0.484497  8.228917  0.504395  7.911120
59   0.490479  8.132451  0.506714  7.873119
60   0.506592  7.874611  0.507812  7.854713
61   0.497681  8.015840  0.489990  8.138059
62   0.504883  7.900283  0.497681  8.014761
63   0.495728  8.045584  0.500488  7.969381
64   0.494263  8.068349  0.498657  7.998016
65   0.503784  7.916025  0.495605  8.046166
66   0.500000  7.975876  0.505127  7.893916
67   0.494629  8.061070  0.504761  7.899340
68   0.511719  7.788222  0.492920  8.087733
69   0.501465  7.951333  0.502441  7.935594
70   0.502319  7.937381  0.492676  8.090966
71   0.499023  7.989624  0.503296  7.921369
72   0.501343  7.952374  0.501221  7.954191
73   0.503906  7.911256  0.502197  7.938384
74   0.506958  7.862376  0.500366  7.967358
75   0.502808  7.928336  0.503540  7.916562
76   0.500244  7.969016  0.505005  7.893030
77   0.503296  7.920193  0.494629  8.058285
78   0.497681  8.009557  0.508301  7.840174
79   0.498657  7.993846  0.502808  7.927612
80   0.500732  7.960633  0.496826  8.022847
81   0.502441  7.933270  0.496826  8.022734
82   0.494141  8.065496  0.499756  7.975924
83   0.495972  8.036205  0.496826  8.022535
84   0.499756  7.975785  0.498047  8.002987
85   0.498413  7.997108  0.497925  8.004853
86   0.497070  8.018438  0.498047  8.002833
87   0.505493  7.884088  0.497070  8.018335
88   0.507812  7.847048  0.509888  7.813934
89   0.505127  7.889804  0.490356  8.125254
90   0.500977  7.955918  0.501709  7.944217
91   0.502930  7.924733  0.502075  7.938334
92   0.497803  8.006427  0.495972  8.035598
93   0.507812  7.846810  0.485596  8.200981
94   0.514404  7.741688  0.506104  7.874008
95   0.498169  8.000490  0.503418  7.916796
96   0.497437  8.012143  0.494629  8.056893
97   0.496704  8.023800  0.497192  8.016007
98   0.501465  7.947886  0.498047  8.002370
99   0.504761  7.895329  0.503174  7.920623
100  0.501953  7.940079  0.495850  8.037380
101  0.493164  8.080190  0.502808  7.926446
102  0.496826  8.119447  0.486694  8.295532
103  0.495117  8.158422  0.504517  8.005735
104  0.508911  7.933945  0.507080  7.962567
105  0.502441  8.036560  0.504517  8.002376
106  0.506714  7.966300  0.503418  8.018787
107  0.500732  8.061493  0.509521  7.919268
108  0.502197  8.036801  0.494507  8.160250
109  0.503052  8.022052  0.505859  7.976339
110  0.489746  8.235624  0.502441  8.030578
111  0.501953  8.038052  0.496826  8.120300
112  0.500488  8.060906  0.499146  8.082187
113  0.503052  8.018883  0.496460  8.124792
114  0.498047  8.098892  0.492310  8.191048
115  0.504883  7.988088  0.495483  8.139288
116  0.494629  8.152773  0.503784  8.004923
117  0.506592  7.959396  0.506470  7.961093
118  0.503052  8.015924  0.500000  8.064855
119  0.495728  8.133471  0.503296  8.011238
120  0.501953  8.032645  0.499878  8.065860
121  0.503296  8.010545  0.496338  8.122473
122  0.494751  8.147839  0.504761  7.986291
123  0.492920  8.176943  0.497070  8.109849
124  0.501343  8.040797  0.506958  7.950105
125  0.503174  8.010923  0.508911  7.918276
126  0.498169  8.091256  0.503662  8.002556
127  0.505371  7.974860  0.493896  8.159660

2018-02-13 11:13:03.850603 Finish.
Total elapsed time: 15:14:15.85.
