2018-02-12 19:59:07.268730: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 19:59:07.269037: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 19:59:07.269050: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 19:58:51.182276 Start.
Training NTT-like convnet on BirdVox-70k. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 16)            1168        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 4)             68          bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_dense2[0][0]                  
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
flatten (Flatten)                (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             65          flatten[0][0]                    
====================================================================================================
Total params: 678,577
Trainable params: 678,575
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.969116  0.106521  0.724121  1.735820
1    0.973389  0.098669  0.742554  1.077290
2    0.972290  0.100469  0.762817  1.142097
3    0.975098  0.088118  0.727295  1.942242
4    0.976074  0.086283  0.735474  1.363879
5    0.977783  0.082986  0.733521  1.279616
6    0.976562  0.089117  0.713623  1.468289
7    0.980835  0.069724  0.773315  1.358083
8    0.975098  0.088984  0.754761  1.153357
9    0.977539  0.087055  0.749878  0.646203
10   0.981689  0.071538  0.747192  1.287695
11   0.981934  0.073333  0.765137  1.413744
12   0.981812  0.067022  0.746948  1.471946
13   0.977905  0.083757  0.754883  1.716578
14   0.978516  0.076511  0.732178  2.116965
15   0.981201  0.080182  0.765381  1.154332
16   0.984253  0.069583  0.772949  1.468492
17   0.981567  0.072035  0.757690  1.137343
18   0.982544  0.069806  0.768555  1.213309
19   0.984497  0.064696  0.756714  1.180981
20   0.980469  0.072416  0.756226  1.330440
21   0.979126  0.081297  0.799561  0.850196
22   0.982788  0.068936  0.791382  1.281520
23   0.981445  0.076196  0.794678  0.943731
24   0.981445  0.073897  0.762695  1.782415
25   0.982422  0.072203  0.798584  1.057941
26   0.983398  0.071809  0.771118  1.536763
27   0.983032  0.062556  0.812012  0.806577
28   0.980469  0.082182  0.773193  1.603518
29   0.981934  0.076228  0.768921  1.787298
30   0.979492  0.086467  0.850952  0.533742
31   0.982422  0.077073  0.742920  1.550137
32   0.980835  0.080058  0.749512  1.724591
33   0.984131  0.071607  0.813477  1.004582
34   0.984497  0.068718  0.786377  0.896623
35   0.984131  0.074828  0.745361  1.575659
36   0.982910  0.073172  0.803589  1.074731
37   0.982422  0.078743  0.811768  0.854599
38   0.978760  0.090740  0.777344  1.958406
39   0.984375  0.073386  0.798462  1.221284
40   0.984131  0.067854  0.763184  1.310243
41   0.985962  0.061147  0.763794  1.848578
42   0.976074  0.104027  0.804688  0.720210
43   0.982666  0.077008  0.748291  1.789575
44   0.984985  0.068535  0.807251  1.355949
45   0.986328  0.067024  0.803345  1.055304
46   0.981689  0.090930  0.820801  0.807171
47   0.984009  0.073763  0.773560  1.935107
48   0.979004  0.085925  0.830078  0.993297
49   0.984497  0.056805  0.806152  1.351656
50   0.984985  0.065259  0.803833  1.172849
51   0.982422  0.074832  0.811279  0.865647
52   0.982300  0.080154  0.778564  2.106378
53   0.983887  0.073862  0.795166  1.709872
54   0.984741  0.073612  0.801758  1.024027
55   0.985107  0.068565  0.789673  0.964273
56   0.973755  0.119164  0.783203  0.872467
57   0.981567  0.091431  0.788086  2.431052
58   0.986694  0.071923  0.796387  0.771444
59   0.875122  0.440915  0.502075  0.693479
60   0.745728  1.091652  0.541504  0.869286
61   0.526733  6.999722  0.499268  8.011493
62   0.497314  8.041445  0.503906  7.935250
63   0.497192  8.041312  0.499146  8.009239
64   0.498413  8.020059  0.506592  7.888837
65   0.501343  7.971741  0.499634  7.998223
66   0.495483  8.063670  0.500122  7.989009
67   0.500732  7.978605  0.498779  8.009077
68   0.503540  7.932545  0.503662  7.929971
69   0.498169  8.016945  0.509033  7.843148
70   0.499878  7.988535  0.497559  8.024947
71   0.494385  8.075004  0.500977  7.969378
72   0.496338  8.042815  0.494629  8.069550
73   0.516724  7.716818  0.497803  8.017977
74   0.502930  7.935776  0.486816  8.192199
75   0.494019  8.076938  0.492188  8.105692
76   0.497437  8.021590  0.503052  7.931655
77   0.507202  7.865091  0.492920  8.092390
78   0.493652  8.080338  0.503418  7.924278
79   0.500488  7.970629  0.505005  7.898273
80   0.497925  8.010811  0.503174  7.926799
81   0.491089  8.119147  0.499878  7.978717
82   0.512085  7.783812  0.492920  8.088203
83   0.505249  7.892226  0.492188  8.100186
84   0.490845  8.121335  0.502319  7.938171
85   0.521973  7.623448  0.499512  8.008114
86   0.501099  7.981621  0.506592  7.893047
87   0.498047  8.028485  0.495728  8.064727
88   0.503662  7.937589  0.505249  7.911676
89   0.493164  8.103783  0.504028  7.930041
90   0.493530  8.096905  0.497559  8.032196
91   0.496704  8.045360  0.502808  7.947606
92   0.508789  7.851821  0.494507  8.079093
93   0.494873  8.072854  0.504639  7.916770
94   0.498901  8.007856  0.496460  8.046400
95   0.499023  8.005168  0.499878  7.991184
96   0.490234  8.144574  0.502930  7.941832
97   0.493286  8.095234  0.499146  8.001483
98   0.503174  7.936932  0.504028  7.922980
99   0.500000  7.986878  0.504517  7.914550
100  0.504028  7.922018  0.507080  7.873048
101  0.500366  7.979771  0.498535  8.008649
102  0.495972  8.049209  0.505371  7.899050
103  0.504150  7.918206  0.499023  7.999635
104  0.500000  7.983764  0.494629  8.069088
105  0.501221  7.963699  0.499756  7.986751
106  0.499268  7.994238  0.505127  7.900527
107  0.504639  7.908017  0.498047  8.012810
108  0.494019  8.076741  0.493042  8.092018
109  0.491577  8.115083  0.507080  7.867642
110  0.496338  8.038616  0.508057  7.851508
111  0.497192  8.024433  0.493896  8.076700
112  0.491821  8.109513  0.501831  7.949663
113  0.495361  8.052542  0.495361  8.052279
114  0.507446  7.859360  0.503540  7.921380
115  0.495728  8.045682  0.502075  7.944238
116  0.507812  7.852534  0.500488  7.969062
117  0.500000  7.976618  0.492676  8.093156
118  0.502563  7.935303  0.498413  8.001253
119  0.497803  8.010775  0.504395  7.905480
120  0.499390  7.985071  0.496704  8.027689
121  0.502441  7.936035  0.498901  7.992285
122  0.508057  7.846151  0.503540  7.917981
123  0.495850  8.040418  0.504517  7.902080
124  0.502441  7.935007  0.495483  8.045779
125  0.500610  7.963896  0.505859  7.880070
126  0.498901  7.990860  0.495850  8.039377
127  0.492432  8.093740  0.501221  7.953497

2018-02-13 11:13:07.085941 Finish.
Total elapsed time: 15:14:16.09.
