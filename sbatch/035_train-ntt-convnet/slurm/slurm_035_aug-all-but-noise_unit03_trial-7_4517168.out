2018-02-12 19:59:15.723437: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 19:59:15.723765: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 19:59:15.723777: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 19:58:58.984388 Start.
Training NTT-like convnet on BirdVox-70k. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 16)            1168        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 4)             68          bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_dense2[0][0]                  
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
flatten (Flatten)                (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             65          flatten[0][0]                    
====================================================================================================
Total params: 678,577
Trainable params: 678,575
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.968384  0.102972  0.656372  1.359357
1    0.974365  0.095085  0.639160  2.869649
2    0.977783  0.084315  0.691772  1.234567
3    0.972412  0.097409  0.656006  1.444756
4    0.974243  0.091971  0.667236  1.752840
5    0.978882  0.081654  0.697021  0.973443
6    0.976318  0.087665  0.659180  1.393291
7    0.979980  0.076427  0.738770  0.932975
8    0.976196  0.095004  0.673584  1.204960
9    0.980225  0.080030  0.709229  0.924479
10   0.978516  0.078433  0.699707  1.314944
11   0.976196  0.086270  0.714355  1.241697
12   0.981689  0.076693  0.753906  0.765642
13   0.980225  0.074932  0.688354  1.155086
14   0.977051  0.078722  0.665161  2.301541
15   0.977783  0.084042  0.670044  2.321418
16   0.977295  0.078926  0.663818  3.076598
17   0.974976  0.091647  0.738037  0.893250
18   0.979980  0.083512  0.712524  1.396679
19   0.983521  0.072056  0.694946  1.570209
20   0.981079  0.074700  0.683472  1.584733
21   0.980957  0.069466  0.688843  1.868640
22   0.983765  0.072559  0.679688  2.075949
23   0.982178  0.064035  0.712524  2.051489
24   0.970337  0.121567  0.724121  0.878248
25   0.980835  0.089520  0.709229  1.644619
26   0.980347  0.086699  0.691406  1.202509
27   0.983154  0.072680  0.704346  1.816656
28   0.983643  0.073590  0.712646  1.315103
29   0.980713  0.068771  0.687134  1.156321
30   0.983398  0.072502  0.739136  1.196707
31   0.984009  0.075943  0.721802  1.637226
32   0.975830  0.101771  0.690918  1.592060
33   0.983765  0.074567  0.697632  2.313210
34   0.985229  0.065548  0.806030  0.875324
35   0.984863  0.074972  0.721802  1.408921
36   0.978882  0.081393  0.706299  1.500916
37   0.976562  0.095064  0.763306  0.825105
38   0.978027  0.080479  0.718628  1.220044
39   0.974365  0.118296  0.742310  1.260622
40   0.971558  0.129052  0.700684  3.255802
41   0.980103  0.089574  0.733887  2.343442
42   0.979004  0.106046  0.695190  1.985186
43   0.981689  0.089427  0.738525  1.770088
44   0.981689  0.091005  0.710449  1.843202
45   0.982910  0.084145  0.725220  2.941375
46   0.978882  0.100247  0.723633  2.972760
47   0.982666  0.080663  0.740967  1.788466
48   0.982422  0.089611  0.744995  1.845195
49   0.983765  0.080131  0.734131  1.326760
50   0.972412  0.102917  0.742676  1.206716
51   0.980713  0.094147  0.742065  2.212362
52   0.980957  0.090392  0.713867  2.510141
53   0.982788  0.082758  0.730347  1.828098
54   0.973389  0.114053  0.663086  3.124930
55   0.970215  0.121652  0.702515  3.771818
56   0.975220  0.117947  0.704590  4.105101
57   0.977539  0.121850  0.792847  0.751804
58   0.963867  0.179533  0.709595  3.952325
59   0.975098  0.137808  0.697144  3.442153
60   0.977905  0.125323  0.740234  3.382304
61   0.977783  0.115280  0.755615  2.517737
62   0.983032  0.113766  0.657471  3.077418
63   0.977417  0.110332  0.707153  1.842852
64   0.979126  0.104318  0.764771  1.122721
65   0.974365  0.114403  0.695801  2.733808
66   0.974609  0.126053  0.741089  2.565685
67   0.979248  0.104373  0.737549  2.693020
68   0.981934  0.098572  0.729736  2.311780
69   0.971924  0.311781  0.657959  5.121875
70   0.546753  7.253185  0.505127  7.919362
71   0.501465  7.976661  0.504272  7.931111
72   0.504395  7.928315  0.504761  7.921717
73   0.494507  8.082773  0.497437  8.037372
74   0.513672  7.777670  0.497314  8.037917
75   0.496826  8.044959  0.499023  8.009350
76   0.500122  7.991096  0.493774  8.091856
77   0.500122  7.989871  0.489746  8.154886
78   0.492920  8.103476  0.505005  7.910372
79   0.501099  7.971958  0.501465  7.965745
80   0.489258  8.159718  0.485962  8.209891
81   0.500610  7.977607  0.499756  7.989188
82   0.494751  8.070010  0.491333  8.124360
83   0.498047  8.017613  0.504150  7.919977
84   0.501587  7.960299  0.497070  8.031770
85   0.502686  7.941743  0.484131  8.237049
86   0.498169  8.012769  0.490723  8.131007
87   0.503662  7.924266  0.499756  7.986089
88   0.494751  8.065444  0.499512  7.989116
89   0.505249  7.897236  0.488403  8.165386
90   0.486328  8.198075  0.514526  7.748136
91   0.498535  8.002698  0.491455  8.115199
92   0.500244  7.974722  0.491455  8.114487
93   0.506348  7.876724  0.502930  7.930879
94   0.498535  8.000617  0.504150  7.910779
95   0.502808  7.931883  0.499878  7.978290
96   0.495728  8.044172  0.492310  8.098381
97   0.494873  8.057247  0.493530  8.078391
98   0.490112  8.132633  0.498413  8.000054
99   0.492310  8.097129  0.497437  8.015167
100  0.501953  7.942949  0.502197  7.938849
101  0.501465  7.950331  0.503540  7.917058
102  0.501221  7.953857  0.496338  8.031528
103  0.503784  7.912658  0.502075  7.939748
104  0.495361  8.046641  0.500854  7.958930
105  0.500854  7.958804  0.497437  8.013173
106  0.502563  7.931328  0.502930  7.925384
107  0.503052  7.923344  0.493286  8.078941
108  0.495239  8.047723  0.511108  7.794655
109  0.501953  7.940545  0.498535  7.994973
110  0.506226  7.872314  0.502319  7.934538
111  0.494019  8.066829  0.505981  7.876071
112  0.507935  7.844899  0.494385  8.060883
113  0.484619  8.216544  0.501099  7.953798
114  0.492432  8.091951  0.510620  7.801965
115  0.496338  8.029643  0.487671  8.167803
116  0.501709  8.014975  0.495117  8.147197
117  0.500366  8.062072  0.489746  8.232799
118  0.501831  8.037653  0.513550  7.848434
119  0.489136  8.241649  0.508545  7.928528
120  0.495972  8.130927  0.504761  7.989013
121  0.503418  8.010422  0.501587  8.039705
122  0.505249  7.980461  0.494263  8.157325
123  0.501465  8.041035  0.497437  8.105761
124  0.496460  8.121307  0.493408  8.170303
125  0.508057  7.934012  0.510864  7.888575
126  0.498657  8.085151  0.498657  8.084975
127  0.485718  8.293365  0.507568  7.941007

2018-02-13 11:37:17.096454 Finish.
Total elapsed time: 15:38:19.10.
