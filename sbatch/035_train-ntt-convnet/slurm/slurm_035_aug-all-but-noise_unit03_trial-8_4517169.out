2018-02-12 19:59:17.852912: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 19:59:17.853265: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 19:59:17.853278: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 19:59:03.527338 Start.
Training NTT-like convnet on BirdVox-70k. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 16)            1168        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 4)             68          bg_dense1[0][0]                  
____________________________________________________________________________________________________
spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_reshape (Reshape)             (None, 1, 4)          0           bg_dense2[0][0]                  
____________________________________________________________________________________________________
multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               
                                                                   bg_reshape[0][0]                 
____________________________________________________________________________________________________
flatten (Flatten)                (None, 64)            0           multiply[0][0]                   
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             65          flatten[0][0]                    
====================================================================================================
Total params: 678,577
Trainable params: 678,575
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.965820  0.116799  0.733765  0.722944
1    0.968994  0.108834  0.699341  2.201134
2    0.973145  0.098632  0.744873  0.847794
3    0.977905  0.090461  0.765137  1.312565
4    0.975464  0.096001  0.754639  1.296392
5    0.975220  0.101774  0.724731  1.724311
6    0.976562  0.088384  0.766235  1.213025
7    0.979004  0.089347  0.778442  0.931549
8    0.980347  0.081021  0.763550  1.367571
9    0.979004  0.077300  0.751953  1.296613
10   0.973877  0.094404  0.745117  1.557529
11   0.979004  0.086770  0.795776  0.957197
12   0.981323  0.079033  0.807983  0.976164
13   0.977539  0.093079  0.770142  1.137179
14   0.982666  0.088016  0.754028  1.972539
15   0.981079  0.080943  0.777710  1.072387
16   0.980225  0.084232  0.763794  1.213728
17   0.981934  0.075851  0.763794  1.258924
18   0.975830  0.086838  0.762085  1.735882
19   0.980591  0.088287  0.788208  1.278199
20   0.981079  0.081495  0.784790  1.462536
21   0.978760  0.094953  0.772217  0.699337
22   0.975952  0.098799  0.774536  1.257437
23   0.982666  0.080584  0.806030  1.164107
24   0.983276  0.069371  0.794067  1.289708
25   0.975830  0.106948  0.770264  2.337042
26   0.978027  0.099004  0.752930  1.703015
27   0.977539  0.096735  0.770264  1.586838
28   0.980347  0.085629  0.755127  2.499687
29   0.980591  0.083640  0.790527  0.934858
30   0.981445  0.079792  0.791260  1.523243
31   0.979858  0.091802  0.774536  2.242049
32   0.957886  0.129682  0.796143  2.457383
33   0.981201  0.091391  0.781128  1.620874
34   0.980103  0.090614  0.798828  1.236073
35   0.980835  0.080675  0.768066  2.038186
36   0.982300  0.082320  0.784790  1.706741
37   0.982910  0.080724  0.808960  1.144985
38   0.986694  0.070496  0.789429  1.506472
39   0.983887  0.077547  0.794312  1.634042
40   0.980469  0.085795  0.802734  1.579952
41   0.980713  0.083717  0.758911  2.127822
42   0.982300  0.071366  0.824829  0.918222
43   0.809814  0.445051  0.553711  1.121438
44   0.534180  6.404165  0.507935  7.966511
45   0.503662  8.031744  0.495117  8.166640
46   0.504883  8.007268  0.498535  8.107833
47   0.500000  8.082810  0.501953  8.050019
48   0.499634  8.086274  0.505981  7.982893
49   0.498413  8.103928  0.500366  8.071533
50   0.502075  8.043158  0.492310  8.199757
51   0.499268  8.086868  0.504761  7.997610
52   0.507690  7.949721  0.496338  8.132050
53   0.499146  8.086187  0.490479  8.225292
54   0.498169  8.100771  0.496948  8.119897
55   0.504150  8.003294  0.508301  7.935890
56   0.500854  8.055430  0.498657  8.090374
57   0.503540  8.011226  0.496216  8.128838
58   0.493530  8.171709  0.503906  8.004058
59   0.498169  8.096154  0.489990  8.228828
60   0.499146  8.080351  0.499390  8.075420
61   0.490723  8.214748  0.503662  8.005843
62   0.511719  7.875669  0.495117  8.142944
63   0.505493  7.975412  0.506592  7.957418
64   0.505981  7.966986  0.499390  8.072970
65   0.492310  8.118926  0.502808  7.956675
66   0.495972  8.063544  0.504150  7.931285
67   0.496826  8.046523  0.493774  8.093748
68   0.490967  8.137261  0.501343  7.970654
69   0.507935  7.864495  0.504150  7.923792
70   0.500732  7.977338  0.496338  8.046482
71   0.507202  7.872434  0.500610  7.976700
72   0.515259  7.742404  0.492432  8.105577
73   0.508179  7.853837  0.496094  8.045821
74   0.509766  7.827226  0.501953  7.951157
75   0.506104  7.884418  0.498535  8.004505
76   0.495361  8.054577  0.498291  8.007355
77   0.509766  7.879201  0.492676  8.220836
78   0.498169  8.130020  0.499023  8.114128
79   0.500854  8.082734  0.502808  8.049436
80   0.487427  8.295671  0.502441  8.052027
81   0.501709  8.062302  0.501343  8.066705
82   0.497070  8.134156  0.494873  8.168183
83   0.506958  7.972086  0.501221  8.063271
84   0.495728  8.150592  0.506226  7.980185
85   0.491943  8.209254  0.484497  8.328159
86   0.501953  8.045749  0.510010  7.914858
87   0.503296  8.022099  0.500977  8.058526
88   0.499023  8.089108  0.501221  8.052812
89   0.494751  8.156266  0.496460  8.127913
90   0.504639  7.995334  0.494385  8.159870
91   0.500854  8.054905  0.506836  7.957827
92   0.489624  8.234630  0.501465  8.043176
93   0.511841  7.875379  0.509644  7.910254
94   0.506226  7.964849  0.500732  8.052908
95   0.487915  8.259060  0.498779  8.083524
96   0.498901  8.081170  0.499512  8.070959
97   0.495117  8.141453  0.500854  8.048653
98   0.500854  8.048361  0.505249  7.977248
99   0.495239  8.138335  0.500854  8.047587
100  0.499390  8.070982  0.497559  8.100290
101  0.494629  8.147328  0.494141  8.155025
102  0.494141  8.154871  0.495972  8.125213
103  0.496094  8.123117  0.500854  8.046261
104  0.495239  8.136662  0.500977  8.044087
105  0.501465  8.036130  0.489868  8.222964
106  0.501709  8.032041  0.503418  8.004429
107  0.503784  7.998468  0.499390  8.069246
108  0.492676  8.177414  0.492676  8.177371
109  0.496582  8.114373  0.498047  8.090729
110  0.510132  7.895914  0.499268  8.070999
111  0.501953  8.027691  0.501343  8.037508
112  0.490112  8.218505  0.495972  8.124048
113  0.496338  8.118133  0.503906  7.996135
114  0.503540  8.002029  0.503906  7.996118
115  0.491089  8.202704  0.501221  8.039392
116  0.496826  8.110220  0.505737  7.966586
117  0.499023  8.074797  0.500122  8.057087
118  0.497925  8.092501  0.490112  8.218422
119  0.494507  8.129099  0.502319  7.969949
120  0.500854  7.991469  0.501465  7.980158
121  0.498169  8.031444  0.499634  8.006921
122  0.505371  7.914442  0.509399  7.849258
123  0.489258  8.169501  0.500366  7.991576
124  0.499634  8.002493  0.506348  7.894721
125  0.504028  7.931013  0.485229  8.230044
126  0.497559  8.032863  0.513794  7.773419
127  0.517700  7.710563  0.500244  7.988282

2018-02-13 12:33:50.119461 Finish.
Total elapsed time: 16:34:47.12.
