2017-12-13 17:29:28.136033: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:28.136280: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:28.136292: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 17:29:23.278706 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit02, unit03, unit05.
Validation set: unit07, unit10.
Test set: unit01.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.974487  0.101727  0.790039  0.490815
1   0.979736  0.096445  0.705566  0.612338
2   0.980225  0.084892  0.848511  0.403853
3   0.980225  0.082766  0.872559  0.343302
4   0.981689  0.083605  0.923096  0.247328
5   0.983276  0.076651  0.737305  0.544905
6   0.983521  0.078141  0.750488  0.605304
7   0.979614  0.078631  0.749268  0.623822
8   0.982300  0.075342  0.867554  0.324171
9   0.982788  0.073455  0.882568  0.306949
10  0.982910  0.073464  0.789673  0.494231
11  0.983154  0.070496  0.778076  0.564136
12  0.985962  0.062443  0.913208  0.267130
13  0.985107  0.066050  0.917236  0.238166
14  0.986084  0.070125  0.855469  0.351286
15  0.986816  0.061917  0.813354  0.446684
16  0.987671  0.058813  0.774292  0.579201
17  0.984497  0.068996  0.866821  0.331913
18  0.985596  0.061402  0.800781  0.477044
19  0.986938  0.060088  0.840210  0.410600
20  0.984253  0.074279  0.661865  0.789139
21  0.989624  0.056718  0.769165  0.559589
22  0.986938  0.058235  0.791992  0.486719
23  0.988525  0.054385  0.882202  0.306579
24  0.987915  0.055658  0.875488  0.342118
25  0.987549  0.057432  0.931519  0.225431
26  0.984863  0.064771  0.913330  0.259951
27  0.988037  0.058090  0.833374  0.448028
28  0.986938  0.061594  0.798218  0.500236
29  0.989380  0.050297  0.914795  0.244928
30  0.988525  0.052377  0.897339  0.286779
31  0.986938  0.058768  0.733643  0.533512
32  0.990234  0.047462  0.865479  0.333446
33  0.987549  0.051241  0.783203  0.506476
34  0.988525  0.053741  0.884644  0.315487
35  0.989502  0.054490  0.772949  0.515679
36  0.989136  0.054342  0.922852  0.218050
37  0.987671  0.056530  0.862793  0.351187
38  0.989136  0.052976  0.809082  0.468959
39  0.990112  0.050473  0.854980  0.324781
40  0.989868  0.049156  0.915039  0.259238
41  0.989380  0.046764  0.895264  0.307016
42  0.987427  0.059682  0.697754  0.613434
43  0.989014  0.048883  0.856934  0.361837
44  0.988037  0.055063  0.943726  0.191772
45  0.989868  0.048040  0.953491  0.178595
46  0.990601  0.045042  0.929565  0.231968
47  0.989746  0.045395  0.858521  0.338257
48  0.990112  0.044862  0.908447  0.282616
49  0.989014  0.047485  0.807251  0.445682
50  0.990845  0.043244  0.842163  0.444223
51  0.987915  0.052640  0.867310  0.379004
52  0.989380  0.048008  0.830322  0.406294
53  0.989624  0.048059  0.807129  0.421231
54  0.990234  0.043212  0.826050  0.454849
55  0.990112  0.048885  0.867554  0.332927
56  0.990356  0.043949  0.918091  0.219010
57  0.989380  0.048211  0.958008  0.171499
58  0.988647  0.051774  0.845581  0.359174
59  0.988159  0.051758  0.954102  0.184728
60  0.989136  0.049838  0.746094  0.539601
61  0.986206  0.058803  0.881836  0.302169
62  0.991089  0.044373  0.907959  0.265917
63  0.989990  0.044204  0.846680  0.350332

2017-12-14 01:21:08.931378 Finish.
Total elapsed time: 07:51:45.93.
