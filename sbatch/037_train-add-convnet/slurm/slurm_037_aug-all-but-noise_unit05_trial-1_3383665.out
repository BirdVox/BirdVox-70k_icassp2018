2017-12-13 08:29:56.849401: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 08:29:56.849649: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 08:29:56.849661: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 08:29:54.787261 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit07, unit10, unit01.
Validation set: unit02, unit03.
Test set: unit05.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.946289  0.193139  0.899414  0.378738
1   0.949341  0.178299  0.925415  0.260316
2   0.947876  0.176008  0.924561  0.278313
3   0.951050  0.170397  0.928101  0.246097
4   0.956055  0.159923  0.933105  0.251621
5   0.951538  0.161659  0.917969  0.319791
6   0.954102  0.156858  0.919189  0.297808
7   0.959229  0.142315  0.912598  0.339787
8   0.955444  0.151122  0.904663  0.516394
9   0.954102  0.152135  0.942017  0.223962
10  0.960449  0.136808  0.912842  0.421161
11  0.960815  0.137818  0.942871  0.244866
12  0.963989  0.133795  0.934692  0.286161
13  0.966675  0.134725  0.927979  0.316864
14  0.959351  0.137837  0.906982  0.472637
15  0.963745  0.124893  0.933960  0.299797
16  0.963379  0.128267  0.933105  0.305950
17  0.968384  0.123354  0.923706  0.336085
18  0.963867  0.131920  0.943726  0.218026
19  0.969849  0.120886  0.953125  0.220107
20  0.968262  0.119088  0.945923  0.235671
21  0.966309  0.125550  0.938599  0.248261
22  0.966309  0.125961  0.913330  0.405360
23  0.963867  0.137762  0.955200  0.178336
24  0.968018  0.116963  0.933716  0.289291
25  0.968872  0.120048  0.929077  0.271189
26  0.968872  0.114849  0.953491  0.190981
27  0.965820  0.118055  0.924438  0.344544
28  0.968262  0.123227  0.925781  0.250327
29  0.970459  0.114350  0.943726  0.238775
30  0.969727  0.111299  0.951660  0.182952
31  0.969727  0.111928  0.906128  0.490202
32  0.967651  0.112715  0.929810  0.358170
33  0.969727  0.115457  0.945679  0.238130
34  0.969604  0.113820  0.954834  0.185330
35  0.973999  0.106502  0.954590  0.191471
36  0.971313  0.110472  0.951050  0.211175
37  0.973267  0.109553  0.959351  0.161412
38  0.971313  0.108093  0.959839  0.170280
39  0.973877  0.106968  0.956055  0.193061
40  0.975342  0.095920  0.920532  0.457902
41  0.970825  0.105057  0.944458  0.261575
42  0.971191  0.102303  0.957764  0.179642
43  0.975220  0.101546  0.953125  0.188942
44  0.970703  0.108885  0.949585  0.231951
45  0.970825  0.107596  0.934326  0.306313
46  0.976929  0.097871  0.939209  0.250110
47  0.972290  0.101759  0.966431  0.135097
48  0.973877  0.107328  0.943115  0.267648
49  0.974731  0.099428  0.950562  0.237783
50  0.971802  0.105717  0.952637  0.186520
51  0.972778  0.100712  0.950928  0.208725
52  0.973755  0.094036  0.968018  0.135536
53  0.973145  0.101780  0.953735  0.246704
54  0.971313  0.106158  0.962280  0.146248
55  0.975830  0.098310  0.945557  0.272575
56  0.971680  0.106328  0.938599  0.258703
57  0.975342  0.100436  0.963501  0.141377
58  0.973267  0.098567  0.948242  0.278942
59  0.975586  0.100694  0.954590  0.201806
60  0.975586  0.093323  0.957153  0.187252
61  0.976440  0.090229  0.945679  0.282872
62  0.973633  0.100146  0.952393  0.225738
63  0.974731  0.099433  0.964844  0.159768

2017-12-13 16:20:51.072764 Finish.
Total elapsed time: 07:50:57.07.
