2017-12-13 17:29:38.538776: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:38.539103: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:38.539115: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 17:29:35.007883 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.492920  8.173165  0.498779  8.078723
1   0.498535  8.082658  0.499390  8.068885
2   0.504883  7.980346  0.503906  7.996086
3   0.494995  8.139717  0.500977  8.043307
4   0.497070  8.106269  0.504761  7.982314
5   0.501221  8.039372  0.499146  8.072821
6   0.506592  7.952801  0.504639  7.984281
7   0.506592  7.952801  0.500854  8.045275
8   0.502441  8.019697  0.500122  8.057080
9   0.501587  8.033470  0.502319  8.021664
10  0.501953  8.027567  0.501099  8.041340
11  0.497070  8.106269  0.498901  8.076756
12  0.493652  8.161360  0.485718  8.289250
13  0.503174  8.007892  0.494507  8.147587
14  0.494019  8.155457  0.508789  7.917385
15  0.508545  7.921320  0.494995  8.139717
16  0.501709  8.031502  0.494141  8.153490
17  0.496094  8.122009  0.496704  8.112171
18  0.491089  8.202678  0.492676  8.177100
19  0.503418  8.003957  0.501587  8.033470
20  0.498291  8.086593  0.505371  7.972476
21  0.500610  8.049210  0.495483  8.131847
22  0.503052  8.009859  0.504150  7.992151
23  0.500610  8.049210  0.494629  8.145620
24  0.496094  8.122009  0.496460  8.116106
25  0.500000  8.059048  0.504028  7.994119
26  0.501587  8.033470  0.499023  8.074788
27  0.495483  8.131847  0.490479  8.212516
28  0.504028  7.994119  0.501221  8.039372
29  0.501343  8.037405  0.507690  7.935093
30  0.495361  8.133814  0.500977  8.043307
31  0.503784  7.998054  0.511353  7.876066
32  0.488037  8.251867  0.494629  8.145620
33  0.503418  8.003957  0.499634  8.064950
34  0.497070  8.106269  0.502808  8.013794
35  0.500122  8.057080  0.503784  7.998054
36  0.505127  7.976411  0.500122  8.057080
37  0.496094  8.122009  0.500244  8.055113
38  0.491943  8.188905  0.496460  8.116106
39  0.500854  8.045275  0.506104  7.960671
40  0.500000  8.059048  0.493896  8.157425
41  0.494019  8.155457  0.503662  8.000022
42  0.493408  8.165295  0.507080  7.944930
43  0.497803  8.094464  0.504150  7.992151
44  0.502441  8.019697  0.496582  8.114139
45  0.502563  8.017729  0.505005  7.978379
46  0.495850  8.125944  0.503174  8.007892
47  0.496826  8.110204  0.506470  7.954768
48  0.499268  8.070853  0.498047  8.090528
49  0.499634  8.064950  0.502563  8.017729
50  0.510010  7.897709  0.501587  8.033470
51  0.507812  7.933125  0.503052  8.009859
52  0.504028  7.994119  0.498291  8.086593
53  0.502930  8.011827  0.505615  7.968541
54  0.502808  8.013794  0.510376  7.891807
55  0.506714  7.950833  0.490479  8.212516
56  0.511108  7.880002  0.503174  8.007892
57  0.497192  8.104301  0.503174  8.007892
58  0.501831  8.029535  0.503662  8.000022
59  0.501221  8.039372  0.499146  8.072821
60  0.495239  8.135782  0.500244  8.055113
61  0.498169  8.088561  0.493896  8.157425
62  0.510742  7.885904  0.493164  8.169230
63  0.490479  8.212516  0.500488  8.051178

2017-12-14 01:49:29.800841 Finish.
Total elapsed time: 08:19:54.80.
