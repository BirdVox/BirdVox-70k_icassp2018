2018-02-12 19:59:51.064923: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 19:59:51.065072: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 19:59:51.065084: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 19:59:35.354554 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.969727  0.106524  0.699707  2.004778
1    0.972412  0.105047  0.760376  0.825579
2    0.973145  0.102485  0.720947  1.253959
3    0.971191  0.100347  0.747681  0.979834
4    0.974976  0.105518  0.755249  0.702145
5    0.978271  0.088773  0.704590  0.813544
6    0.973267  0.101456  0.754150  0.693754
7    0.975586  0.092185  0.794556  0.672012
8    0.975586  0.096955  0.755005  0.547814
9    0.977783  0.084665  0.814209  0.562340
10   0.979614  0.084832  0.736206  1.176827
11   0.977905  0.078647  0.779663  0.670601
12   0.979492  0.077947  0.753784  0.695380
13   0.978638  0.084719  0.772217  0.506096
14   0.981812  0.076189  0.742432  0.998658
15   0.982422  0.071411  0.739380  1.151472
16   0.982056  0.072560  0.753906  1.031343
17   0.977905  0.081355  0.747803  0.952085
18   0.979614  0.076883  0.799316  0.551406
19   0.982544  0.073174  0.774536  0.768362
20   0.982056  0.072670  0.781372  0.721374
21   0.981689  0.073595  0.773926  0.783724
22   0.981934  0.075394  0.757568  0.747130
23   0.982544  0.069658  0.775879  0.742685
24   0.982422  0.076144  0.778198  0.761745
25   0.982788  0.074222  0.783447  0.856925
26   0.984131  0.070325  0.789673  0.853194
27   0.987305  0.057543  0.735352  1.114774
28   0.982056  0.069594  0.761841  0.944091
29   0.980103  0.080465  0.745972  0.717755
30   0.980713  0.075914  0.748413  0.993943
31   0.982544  0.067754  0.723145  0.928668
32   0.983643  0.070346  0.791748  0.949255
33   0.982422  0.068439  0.782471  0.898542
34   0.980957  0.073646  0.769897  0.943199
35   0.985596  0.060376  0.747437  1.585342
36   0.983521  0.062244  0.761841  1.018366
37   0.982422  0.066370  0.769531  0.920702
38   0.983887  0.067936  0.797852  0.586089
39   0.983765  0.071665  0.820068  0.612242
40   0.982544  0.068445  0.781006  1.069471
41   0.981689  0.069842  0.770264  0.894752
42   0.984863  0.061814  0.799316  0.792484
43   0.984497  0.062563  0.784668  0.967539
44   0.985840  0.060900  0.782104  1.177043
45   0.985718  0.056923  0.807373  0.594419
46   0.987183  0.056157  0.793457  0.582307
47   0.987061  0.054266  0.791504  0.835996
48   0.985718  0.057734  0.793701  0.642439
49   0.984863  0.061478  0.780762  0.849496
50   0.984497  0.060362  0.774170  0.847800
51   0.985229  0.061961  0.734009  1.129621
52   0.987793  0.055266  0.806152  0.535389
53   0.986816  0.056856  0.805298  0.697186
54   0.981567  0.071104  0.743896  1.144518
55   0.985474  0.060751  0.738403  1.381774
56   0.980835  0.070992  0.794800  0.728490
57   0.985352  0.056452  0.786133  0.665471
58   0.986206  0.064304  0.785400  0.785349
59   0.983521  0.064927  0.807861  0.614607
60   0.984863  0.062530  0.794556  0.728467
61   0.984253  0.062442  0.762207  1.147321
62   0.984253  0.064507  0.749878  0.986023
63   0.986694  0.055578  0.769287  0.817945
64   0.987549  0.059632  0.815552  0.673772
65   0.983887  0.060254  0.815063  0.630239
66   0.987305  0.053889  0.790283  0.796591
67   0.985474  0.060030  0.806885  0.705634
68   0.987793  0.052021  0.816650  0.699233
69   0.985840  0.057230  0.775879  1.057947
70   0.986938  0.055511  0.816772  0.503047
71   0.986206  0.058429  0.796997  0.887830
72   0.984619  0.064886  0.770386  1.040523
73   0.984863  0.064972  0.817505  0.780872
74   0.985474  0.057490  0.824219  0.647449
75   0.986206  0.057479  0.792358  0.859957
76   0.984375  0.058379  0.762451  1.240609
77   0.986450  0.055406  0.775879  1.190388
78   0.984253  0.060944  0.803467  0.746144
79   0.985107  0.061612  0.819458  0.612868
80   0.985596  0.057621  0.783813  1.121989
81   0.985474  0.058467  0.811401  0.689013
82   0.985962  0.063116  0.809570  0.755338
83   0.984253  0.066040  0.776245  0.926659
84   0.988403  0.048403  0.786133  1.072732
85   0.986572  0.058076  0.820679  0.736038
86   0.987671  0.056384  0.803589  0.928912
87   0.985962  0.054311  0.815430  0.683513
88   0.989258  0.044952  0.802734  1.014677
89   0.983643  0.058008  0.781128  1.004170
90   0.988403  0.049921  0.772705  1.055699
91   0.987427  0.054210  0.795532  0.821564
92   0.986328  0.052454  0.749390  1.423949
93   0.985352  0.058380  0.795044  0.821731
94   0.987793  0.051340  0.776001  1.068030
95   0.987427  0.051727  0.796509  0.926268
96   0.987671  0.053261  0.776123  1.203442
97   0.987183  0.051941  0.786865  0.724387
98   0.988525  0.046978  0.791870  1.125549
99   0.988037  0.047991  0.825562  0.632103
100  0.987061  0.051453  0.820801  0.637081
101  0.987549  0.053763  0.819336  0.649610
102  0.987671  0.057337  0.804932  0.811281
103  0.990112  0.047263  0.760864  1.218791
104  0.987061  0.054341  0.789307  1.235865
105  0.984619  0.058756  0.769287  1.250982
106  0.984863  0.056689  0.851562  0.430271
107  0.987671  0.052718  0.800293  0.869467
108  0.985840  0.059024  0.781372  0.921607
109  0.985840  0.060002  0.808472  0.739226
110  0.988159  0.048287  0.803955  0.839269
111  0.985474  0.061162  0.805176  0.833094
112  0.988403  0.044911  0.798462  0.746816
113  0.986694  0.052884  0.811157  0.710903
114  0.987305  0.055371  0.786255  0.825499
115  0.990845  0.043383  0.808350  0.849199
116  0.989136  0.048031  0.827271  0.528711
117  0.987549  0.055004  0.842896  0.470223
118  0.986572  0.054804  0.841431  0.578365
119  0.987549  0.048786  0.802979  0.827832
120  0.986816  0.056551  0.795288  0.722058
121  0.987183  0.052836  0.813965  0.739814
122  0.988037  0.054221  0.804077  0.933812
123  0.988525  0.047527  0.830688  0.605017
124  0.989380  0.046017  0.833374  0.466647
125  0.990845  0.040676  0.826416  0.721727
126  0.987427  0.053881  0.801758  0.734369
127  0.987305  0.053390  0.810303  0.498650

2018-02-13 11:09:29.710455 Finish.
Total elapsed time: 15:09:54.71.
