2018-02-12 19:59:41.417558: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 19:59:41.417952: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 19:59:41.417965: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 19:59:28.762573 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.977295  0.098645  0.690430  1.281316
1    0.977173  0.098767  0.720825  1.023636
2    0.977417  0.092632  0.702026  1.380606
3    0.981812  0.077782  0.735107  1.387373
4    0.981812  0.076617  0.706787  1.572911
5    0.979004  0.086711  0.723022  1.335510
6    0.977661  0.087552  0.760376  0.978273
7    0.978516  0.083331  0.800781  0.613242
8    0.980835  0.075902  0.724365  1.314822
9    0.980957  0.081486  0.773315  0.631299
10   0.986084  0.062135  0.742065  1.203356
11   0.980103  0.081386  0.756592  0.716546
12   0.983643  0.072876  0.734985  1.216183
13   0.981445  0.076355  0.751465  1.076425
14   0.981445  0.076197  0.775757  0.875848
15   0.982788  0.071459  0.763062  0.923962
16   0.981567  0.072041  0.742920  1.111835
17   0.981323  0.073731  0.762451  1.114814
18   0.984131  0.067668  0.762085  1.047418
19   0.983765  0.068993  0.752563  1.275658
20   0.984497  0.068075  0.777710  0.919047
21   0.988037  0.061875  0.730713  1.144503
22   0.984131  0.069037  0.746338  0.928147
23   0.985229  0.062971  0.776245  0.784839
24   0.986206  0.059940  0.764526  0.993259
25   0.982300  0.072885  0.816284  0.532298
26   0.987061  0.058329  0.756958  0.980835
27   0.982910  0.074437  0.788696  0.741266
28   0.987549  0.055791  0.761719  1.121957
29   0.985718  0.064179  0.810059  0.699922
30   0.986084  0.065035  0.765381  1.001117
31   0.987061  0.055861  0.780884  0.914303
32   0.986694  0.054603  0.828979  0.535987
33   0.985840  0.064287  0.773682  0.860363
34   0.984863  0.059963  0.785400  0.878460
35   0.982178  0.071499  0.750122  0.852507
36   0.987671  0.055603  0.812134  0.638902
37   0.988037  0.057479  0.763794  0.915959
38   0.985840  0.061983  0.777466  0.700268
39   0.988159  0.053717  0.771729  0.930122
40   0.985718  0.060103  0.743896  1.122808
41   0.986694  0.057292  0.772339  1.045628
42   0.986572  0.056153  0.760864  0.900967
43   0.989014  0.051872  0.780884  0.749166
44   0.986206  0.056701  0.761353  0.957398
45   0.986084  0.057318  0.760742  1.320339
46   0.988037  0.057631  0.759033  0.840614
47   0.988525  0.051388  0.764893  1.089914
48   0.985962  0.058814  0.772339  0.708728
49   0.987183  0.060494  0.791260  0.642314
50   0.988159  0.055512  0.802246  0.689770
51   0.986206  0.058727  0.794434  0.899479
52   0.986694  0.059826  0.796021  0.775000
53   0.985840  0.059293  0.803101  0.684497
54   0.988647  0.050149  0.790283  0.945057
55   0.987183  0.052447  0.803833  0.795461
56   0.986328  0.058383  0.771240  1.259897
57   0.988892  0.051430  0.798462  0.740893
58   0.991333  0.051220  0.802002  0.934113
59   0.988525  0.052900  0.757080  1.548359
60   0.987061  0.049942  0.778931  0.881514
61   0.990601  0.052131  0.759277  1.579823
62   0.989380  0.046631  0.776123  0.963050
63   0.987549  0.055977  0.812134  0.695199
64   0.986694  0.056950  0.827759  0.604023
65   0.987793  0.050675  0.826172  0.545529
66   0.990234  0.047643  0.751343  1.183126
67   0.987061  0.055852  0.765015  1.226677
68   0.987549  0.061036  0.760254  1.197431
69   0.986816  0.059886  0.789185  0.935843
70   0.988159  0.053316  0.757324  1.321402
71   0.985474  0.057653  0.755981  1.282573
72   0.991089  0.042632  0.749756  1.592569
73   0.989502  0.044132  0.799438  0.898352
74   0.990845  0.042308  0.799805  0.792554
75   0.989502  0.045244  0.770020  1.272266
76   0.988647  0.048805  0.798096  0.810359
77   0.989014  0.051323  0.764648  1.087608
78   0.989624  0.053679  0.784058  0.873102
79   0.991211  0.043548  0.845581  0.483756
80   0.990723  0.049306  0.785156  0.862968
81   0.989380  0.047333  0.783447  0.891794
82   0.986938  0.057338  0.771484  1.168304
83   0.991089  0.048370  0.762329  1.486294
84   0.990601  0.047268  0.772827  1.133166
85   0.989014  0.050669  0.795532  0.738745
86   0.990112  0.046684  0.762939  1.239788
87   0.989258  0.046820  0.840210  0.566750
88   0.988037  0.049880  0.762085  1.215297
89   0.988159  0.050931  0.813965  0.564796
90   0.989624  0.049240  0.795044  0.752828
91   0.990112  0.045761  0.838989  0.604548
92   0.988770  0.049934  0.791260  0.780333
93   0.989136  0.047364  0.779053  1.081765
94   0.989990  0.045939  0.800781  0.988054
95   0.989258  0.045031  0.800415  0.886662
96   0.990967  0.043535  0.777466  1.260445
97   0.990112  0.053470  0.812622  0.721354
98   0.990479  0.046608  0.824829  0.552940
99   0.989380  0.047839  0.816895  0.857843
100  0.989624  0.047425  0.802612  0.848737
101  0.990234  0.042926  0.796143  0.922467
102  0.989868  0.048589  0.808472  0.967629
103  0.989380  0.045406  0.764404  1.331354
104  0.990479  0.044573  0.808594  0.909154
105  0.988525  0.051212  0.801758  0.866663
106  0.989624  0.047133  0.812622  0.865240
107  0.990845  0.042697  0.778076  1.208074
108  0.990479  0.043905  0.790283  1.025047
109  0.989746  0.044298  0.775269  1.027936
110  0.988281  0.048302  0.803711  0.970408
111  0.990723  0.038540  0.828979  0.963936
112  0.988647  0.044659  0.813721  0.911186
113  0.990234  0.044541  0.774414  1.266347
114  0.990479  0.044505  0.787964  1.522202
115  0.990234  0.045124  0.799316  1.011194
116  0.990112  0.040370  0.781006  1.349620
117  0.992676  0.039720  0.770142  1.228513
118  0.992554  0.041611  0.781860  1.201555
119  0.991821  0.041297  0.803589  0.880511
120  0.990723  0.045328  0.771118  1.408715
121  0.990356  0.044306  0.805908  0.874786
122  0.989136  0.050302  0.776123  1.318852
123  0.991577  0.041098  0.806519  0.967640
124  0.992188  0.037418  0.783813  1.268227
125  0.992065  0.043748  0.791992  1.233468
126  0.992310  0.037645  0.782715  1.395509
127  0.990356  0.046533  0.824097  0.665667

2018-02-13 11:31:13.664684 Finish.
Total elapsed time: 15:31:45.66.
