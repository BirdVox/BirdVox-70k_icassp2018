2017-12-13 08:29:56.439310: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 08:29:56.439555: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 08:29:56.439567: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 08:29:54.217414 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit10, unit01, unit02.
Validation set: unit03, unit05.
Test set: unit07.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.950562  0.170797  0.800171  0.501477
1   0.955566  0.156260  0.859253  0.391397
2   0.955322  0.159645  0.858032  0.371780
3   0.961060  0.145691  0.945068  0.189706
4   0.962769  0.145298  0.953369  0.150938
5   0.963135  0.136687  0.866089  0.377587
6   0.961426  0.132214  0.944336  0.180264
7   0.965332  0.133981  0.925659  0.210592
8   0.964844  0.128996  0.967041  0.123700
9   0.963623  0.126721  0.962280  0.128727
10  0.970703  0.117692  0.942627  0.170951
11  0.966675  0.120394  0.963257  0.134647
12  0.965088  0.136404  0.935669  0.181217
13  0.968384  0.117000  0.955933  0.146011
14  0.970093  0.109172  0.962891  0.124309
15  0.970703  0.119123  0.972168  0.110351
16  0.970703  0.122902  0.946045  0.167316
17  0.970459  0.116593  0.941772  0.171552
18  0.967529  0.120136  0.965088  0.121050
19  0.973022  0.110931  0.956665  0.138065
20  0.972778  0.105827  0.961670  0.129049
21  0.971313  0.108187  0.949463  0.153863
22  0.969482  0.115805  0.967285  0.116628
23  0.972412  0.107481  0.965576  0.114412
24  0.971191  0.111913  0.949829  0.153977
25  0.974121  0.103281  0.965576  0.120576
26  0.974121  0.100915  0.922363  0.225226
27  0.975952  0.101363  0.949341  0.160355
28  0.971802  0.111251  0.967041  0.120569
29  0.973755  0.099734  0.957397  0.130492
30  0.972534  0.102682  0.965576  0.117189
31  0.973389  0.103957  0.974243  0.097320
32  0.975098  0.101081  0.963379  0.122259
33  0.973999  0.099958  0.967529  0.126678
34  0.976929  0.092458  0.951416  0.145023
35  0.973999  0.098906  0.923340  0.209521
36  0.975220  0.096985  0.945190  0.167223
37  0.976685  0.095539  0.979004  0.085999
38  0.972534  0.106850  0.960327  0.133836
39  0.977539  0.092860  0.958130  0.137598
40  0.974487  0.098516  0.953125  0.143832
41  0.975220  0.098173  0.965454  0.119910
42  0.975098  0.095258  0.943970  0.164238
43  0.975830  0.094163  0.969727  0.101886
44  0.974731  0.096043  0.944946  0.181645
45  0.977051  0.097003  0.962891  0.122581
46  0.974243  0.096523  0.963623  0.125295
47  0.973755  0.092328  0.976196  0.092356
48  0.976685  0.088218  0.959961  0.124022
49  0.977173  0.094139  0.956665  0.141751
50  0.973999  0.097447  0.977783  0.101284
51  0.980225  0.085172  0.970093  0.106752
52  0.976074  0.094279  0.938599  0.174807
53  0.975464  0.094222  0.962524  0.128854
54  0.977295  0.095807  0.943237  0.179314
55  0.975342  0.092089  0.937866  0.177169
56  0.979126  0.084842  0.950317  0.147315
57  0.978638  0.085580  0.970337  0.104432
58  0.975708  0.091583  0.966064  0.114140
59  0.973999  0.096203  0.972778  0.113796
60  0.979248  0.083381  0.969482  0.110015
61  0.976318  0.088950  0.951172  0.158128
62  0.976685  0.096821  0.905273  0.274556
63  0.979980  0.087575  0.942749  0.166877

2017-12-13 16:15:42.488715 Finish.
Total elapsed time: 07:45:48.49.
