2018-01-19 14:09:06.138926: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-19 14:09:06.139289: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-19 14:09:06.139309: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-01-19 14:09:06.139319: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-19 14:09:06.139328: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-01-19 14:08:00.371079 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit02, unit03, unit05.
Validation set: unit07, unit10.
Test set: unit01.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.493530  8.163327  0.501465  8.035437
1   0.504150  7.992151  0.502930  8.011827
2   0.503418  8.003957  0.501221  8.039372
3   0.504272  7.990184  0.510010  7.897709
4   0.495605  8.129879  0.498901  8.076756
5   0.499512  8.066918  0.503296  8.005924
6   0.504150  7.992151  0.507446  7.939028
7   0.502563  8.017729  0.488281  8.247932
8   0.498047  8.090528  0.495972  8.123977
9   0.507935  7.931158  0.489990  8.220386
10  0.501709  8.031502  0.503662  8.000022
11  0.502075  8.025600  0.494751  8.143652
12  0.495239  8.135782  0.497925  8.092496
13  0.499023  8.074788  0.498779  8.078723
14  0.497803  8.094463  0.498169  8.088561
15  0.497925  8.092496  0.495239  8.135782
16  0.488159  8.249899  0.490479  8.212516
17  0.503784  7.998054  0.499634  8.064950
18  0.498047  8.090528  0.500000  8.059048
19  0.491943  8.188905  0.500610  8.049210
20  0.496826  8.110204  0.504028  7.994119
21  0.510620  7.887872  0.504150  7.992151
22  0.500977  8.043307  0.504639  7.984281
23  0.502441  8.019697  0.502563  8.017729
24  0.491577  8.194808  0.507324  7.940995
25  0.502686  8.015762  0.509399  7.907547
26  0.511963  7.866229  0.498047  8.090528
27  0.497192  8.104301  0.502563  8.017729
28  0.500732  8.047243  0.499390  8.068885
29  0.499634  8.064950  0.504883  7.980346
30  0.493530  8.163327  0.500244  8.055113
31  0.505249  7.974444  0.500977  8.043307
32  0.498413  8.084626  0.495972  8.123977
33  0.504150  7.992151  0.500000  8.059048
34  0.495728  8.127912  0.497925  8.092496
35  0.507324  7.940995  0.511719  7.870164
36  0.500000  8.059048  0.496094  8.122009
37  0.510010  7.897709  0.492310  8.183003
38  0.497070  8.106269  0.500000  8.059048
39  0.487305  8.263672  0.493042  8.171198
40  0.497559  8.098399  0.498047  8.090528
41  0.494873  8.141685  0.505249  7.974444
42  0.499146  8.072821  0.503174  8.007892
43  0.504272  7.990184  0.500732  8.047243
44  0.509521  7.905580  0.497925  8.092496
45  0.490967  8.204646  0.495850  8.125944
46  0.508179  7.927223  0.497314  8.102334
47  0.502441  8.019697  0.502197  8.023632
48  0.509155  7.911482  0.505737  7.966573
49  0.498169  8.088561  0.495728  8.127912
50  0.506592  7.952801  0.492065  8.186938
51  0.507568  7.937060  0.491333  8.198743
52  0.512573  7.856391  0.489624  8.226289
53  0.502563  8.017729  0.497559  8.098399
54  0.498535  8.082658  0.509644  7.903612
55  0.501831  8.029535  0.497803  8.094464
56  0.498169  8.088561  0.499146  8.072821
57  0.502075  8.025600  0.506958  7.946898
58  0.494385  8.149555  0.504395  7.988216
59  0.509033  7.913450  0.492920  8.173165
60  0.497314  8.102334  0.492310  8.183003
61  0.497559  8.098399  0.501099  8.041340
62  0.492798  8.175133  0.495483  8.131847
63  0.505371  7.972476  0.503418  8.003957

2018-01-19 22:33:41.126334 Finish.
Total elapsed time: 08:25:41.13.
