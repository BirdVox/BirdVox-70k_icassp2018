2017-12-13 17:29:28.023342: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:28.023614: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:28.023626: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 17:29:21.682258 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit01, unit02, unit03.
Validation set: unit05, unit07.
Test set: unit10.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.947388  0.162724  0.920166  0.241858
1   0.946289  0.169491  0.883545  0.301757
2   0.950928  0.148508  0.868286  0.296344
3   0.956177  0.134823  0.923584  0.230703
4   0.952881  0.148435  0.937866  0.202973
5   0.952148  0.142664  0.902832  0.238537
6   0.953613  0.146908  0.869995  0.290026
7   0.956177  0.135564  0.880371  0.287778
8   0.958374  0.134212  0.932617  0.197731
9   0.953735  0.146259  0.934082  0.202285
10  0.961792  0.130294  0.921875  0.235008
11  0.963379  0.119943  0.914429  0.218474
12  0.964966  0.121486  0.923706  0.219137
13  0.968872  0.109134  0.910889  0.244924
14  0.962402  0.126563  0.940552  0.185488
15  0.966187  0.118736  0.811401  0.448605
16  0.965454  0.120940  0.936157  0.187748
17  0.964478  0.119414  0.895386  0.254595
18  0.968506  0.116743  0.860840  0.336671
19  0.964844  0.118806  0.918701  0.212981
20  0.966675  0.113881  0.900513  0.248365
21  0.966553  0.118744  0.934814  0.199737
22  0.967773  0.115639  0.900391  0.253047
23  0.968628  0.109747  0.861084  0.333545
24  0.969238  0.108321  0.935181  0.186677
25  0.968384  0.108987  0.933594  0.183961
26  0.969971  0.100696  0.898926  0.267253
27  0.966431  0.115707  0.929565  0.202246
28  0.971436  0.102961  0.914307  0.220671
29  0.972778  0.100683  0.942383  0.172991
30  0.969727  0.106507  0.945435  0.196127
31  0.973267  0.096300  0.891968  0.269794
32  0.971680  0.103561  0.929932  0.203524
33  0.968140  0.107663  0.940063  0.196210
34  0.971680  0.099719  0.940430  0.191278
35  0.969971  0.109252  0.904175  0.239950
36  0.970215  0.104518  0.934814  0.201238
37  0.973755  0.096511  0.943481  0.176780
38  0.972534  0.098539  0.913208  0.230707
39  0.972656  0.097828  0.926270  0.200244
40  0.971436  0.103766  0.923584  0.210427
41  0.973633  0.098016  0.941162  0.190385
42  0.972290  0.101518  0.917236  0.211554
43  0.973755  0.096587  0.936157  0.188219
44  0.975220  0.093342  0.922363  0.215819
45  0.973389  0.098829  0.949341  0.174560
46  0.978516  0.093005  0.924194  0.209389
47  0.973511  0.096636  0.942627  0.187716
48  0.975586  0.087511  0.919556  0.210555
49  0.975220  0.093789  0.920410  0.233476
50  0.973999  0.098257  0.922119  0.203664
51  0.971924  0.099905  0.941284  0.183814
52  0.972534  0.096291  0.925049  0.208162
53  0.974243  0.096282  0.933228  0.189795
54  0.975098  0.098110  0.918091  0.234792
55  0.974731  0.100381  0.902832  0.265527
56  0.973511  0.095091  0.938721  0.183516
57  0.974609  0.102852  0.944824  0.197936
58  0.975220  0.092283  0.885132  0.276451
59  0.974365  0.100045  0.945435  0.182583
60  0.973267  0.098343  0.903198  0.245784
61  0.977539  0.090670  0.935303  0.194289
62  0.975830  0.090529  0.913330  0.225987
63  0.972900  0.096022  0.889282  0.271163

2017-12-14 01:21:53.370622 Finish.
Total elapsed time: 07:52:32.37.
