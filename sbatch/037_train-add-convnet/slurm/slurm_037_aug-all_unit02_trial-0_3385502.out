2017-12-13 17:29:29.033333: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:29.033659: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:29.033670: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 17:29:22.609892 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.969727  0.117339  0.832642  0.545684
1   0.971802  0.101035  0.828247  0.558752
2   0.974365  0.090995  0.789185  0.663140
3   0.975708  0.091329  0.843750  0.591009
4   0.973022  0.093367  0.865356  0.426719
5   0.974243  0.092320  0.809814  0.585869
6   0.976685  0.088273  0.762573  0.823392
7   0.973511  0.093520  0.761353  0.939831
8   0.975586  0.081522  0.794067  0.610201
9   0.977295  0.081098  0.809814  0.695996
10  0.977783  0.080146  0.811768  0.706480
11  0.978394  0.078893  0.850708  0.515104
12  0.977417  0.080734  0.810791  0.667481
13  0.977417  0.080216  0.832764  0.566169
14  0.978638  0.080686  0.852905  0.505204
15  0.978149  0.082873  0.745483  0.867043
16  0.979004  0.079122  0.838867  0.552636
17  0.976807  0.081075  0.841797  0.568098
18  0.982788  0.069027  0.803467  0.650316
19  0.975708  0.089887  0.835205  0.509728
20  0.980103  0.074898  0.814209  0.587144
21  0.982544  0.065698  0.788940  0.725549
22  0.982056  0.072164  0.765015  0.623571
23  0.982666  0.069467  0.779785  0.654028
24  0.980835  0.072455  0.755493  0.791459
25  0.981201  0.075665  0.788574  0.734783
26  0.980225  0.072728  0.847412  0.470057
27  0.980835  0.071232  0.812622  0.481661
28  0.983765  0.064356  0.716431  1.054772
29  0.981201  0.075793  0.859131  0.463734
30  0.981079  0.068054  0.837646  0.564209
31  0.980957  0.071545  0.827637  0.543405
32  0.983154  0.063294  0.838379  0.629131
33  0.983765  0.067316  0.837402  0.557348
34  0.983032  0.064302  0.828003  0.614966
35  0.981689  0.067642  0.803711  0.704229
36  0.984741  0.059748  0.836182  0.591442
37  0.984253  0.061746  0.760010  0.771697
38  0.983887  0.065639  0.682007  1.184238
39  0.981812  0.068188  0.816284  0.638468
40  0.983154  0.066805  0.812988  0.681866
41  0.986206  0.055565  0.840088  0.548406
42  0.983521  0.063689  0.808838  0.701808
43  0.979858  0.072642  0.771240  0.907568
44  0.984619  0.058798  0.808472  0.654574
45  0.986328  0.054013  0.767700  0.923358
46  0.983643  0.064950  0.809204  0.689732
47  0.983398  0.062269  0.838867  0.569366
48  0.983398  0.064302  0.823853  0.613920
49  0.982910  0.070391  0.868408  0.429445
50  0.984131  0.060979  0.809082  0.661505
51  0.983521  0.060194  0.829102  0.645222
52  0.983887  0.060440  0.831787  0.582679
53  0.985840  0.059291  0.812866  0.742512
54  0.985352  0.060041  0.769043  0.798370
55  0.984009  0.058197  0.808228  0.735825
56  0.981689  0.063611  0.789429  0.773688
57  0.986328  0.052165  0.787476  0.890257
58  0.984375  0.059693  0.781982  0.820474
59  0.984253  0.060295  0.833496  0.590424
60  0.985474  0.056891  0.807617  0.682940
61  0.983032  0.062635  0.852661  0.539387
62  0.985840  0.054983  0.770630  0.878629
63  0.984375  0.061073  0.753540  0.883329

2017-12-14 01:25:43.647250 Finish.
Total elapsed time: 07:56:21.65.
