2017-12-13 17:29:27.608530: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:27.608849: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:27.608862: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 17:29:22.609826 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.511597  7.786314  0.502808  7.926432
1   0.500122  7.969246  0.488892  8.148287
2   0.502197  7.936163  0.497437  8.012060
3   0.503662  7.912810  0.499512  7.978977
4   0.510742  7.799936  0.508423  7.836912
5   0.495728  8.039306  0.503418  7.916702
6   0.503174  7.920594  0.498535  7.994545
7   0.495117  8.049036  0.502197  7.936163
8   0.497803  8.006222  0.496216  8.031521
9   0.502319  7.934217  0.501099  7.953678
10  0.499268  7.982869  0.496338  8.029575
11  0.491333  8.109365  0.494873  8.052928
12  0.501465  7.947839  0.503052  7.922540
13  0.507812  7.846642  0.517578  7.690955
14  0.501831  7.942001  0.492554  8.089904
15  0.500610  7.961462  0.502441  7.932271
16  0.495850  8.037359  0.486572  8.185262
17  0.498291  7.998438  0.500977  7.955624
18  0.494507  8.058766  0.497437  8.012060
19  0.504028  7.906971  0.500000  7.971192
20  0.496460  8.027629  0.502930  7.924486
21  0.502075  7.938109  0.493408  8.076281
22  0.498169  8.000384  0.494507  8.058766
23  0.500977  7.955624  0.500732  7.959516
24  0.500732  7.959516  0.499878  7.973138
25  0.494995  8.050982  0.502686  7.928378
26  0.503052  7.922540  0.493774  8.070443
27  0.504761  7.895295  0.489502  8.138556
28  0.496094  8.033467  0.492554  8.089904
29  0.501587  7.945893  0.504150  7.905025
30  0.491211  8.111311  0.495117  8.049036
31  0.489136  8.144395  0.503296  7.918648
32  0.501709  7.943947  0.504395  7.901133
33  0.497925  8.004276  0.493164  8.080173
34  0.497192  8.015952  0.497681  8.008168
35  0.503784  7.910864  0.496704  8.023737
36  0.495117  8.049036  0.501465  7.947839
37  0.500122  7.969246  0.505493  7.883618
38  0.491699  8.103527  0.498291  7.998438
39  0.495605  8.041252  0.493408  8.076281
40  0.493774  8.070443  0.499390  7.980923
41  0.504883  7.893349  0.490234  8.126880
42  0.506104  7.873888  0.512939  7.764907
43  0.492188  8.095742  0.495605  8.041252
44  0.497437  8.012060  0.500366  7.965354
45  0.505981  7.875834  0.490967  8.115203
46  0.505493  7.883618  0.502563  7.930324
47  0.492310  8.093796  0.503540  7.914756
48  0.499146  7.984815  0.498291  7.998438
49  0.493286  8.078227  0.503540  7.914756
50  0.503662  7.912810  0.498779  7.990653
51  0.490723  8.119095  0.499512  7.978977
52  0.494873  8.052928  0.498901  7.988707
53  0.504883  7.893349  0.493774  8.070443
54  0.499512  7.978977  0.499756  7.975085
55  0.505737  7.879726  0.502075  7.938109
56  0.504883  7.893349  0.499023  7.986761
57  0.502441  7.932271  0.513672  7.753230
58  0.502319  7.934217  0.509521  7.819397
59  0.505493  7.883618  0.498047  8.002330
60  0.491699  8.103527  0.499146  7.984815
61  0.501465  7.947839  0.512451  7.772691
62  0.502319  7.934217  0.496826  8.021791
63  0.498535  7.994545  0.489990  8.130772

2017-12-14 01:24:47.052158 Finish.
Total elapsed time: 07:55:25.05.
