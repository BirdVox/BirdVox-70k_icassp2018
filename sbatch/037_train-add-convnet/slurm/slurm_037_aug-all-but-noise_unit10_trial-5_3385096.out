2017-12-13 12:37:35.241782: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:35.242018: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:35.242028: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:35.242033: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:35.242037: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 12:37:33.299138 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit01, unit02, unit03.
Validation set: unit05, unit07.
Test set: unit10.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.946533  0.169633  0.865845  0.311982
1   0.938843  0.175943  0.878174  0.282850
2   0.949829  0.164346  0.853760  0.332832
3   0.957886  0.142806  0.943726  0.171352
4   0.954712  0.143803  0.925903  0.232818
5   0.952148  0.144748  0.914551  0.228231
6   0.955688  0.134520  0.926880  0.208123
7   0.961304  0.124318  0.940674  0.183317
8   0.961426  0.132142  0.897583  0.232519
9   0.959595  0.130744  0.764160  0.466917
10  0.956177  0.133504  0.832764  0.374516
11  0.962646  0.124105  0.911133  0.199512
12  0.967285  0.109480  0.925171  0.214477
13  0.965820  0.115817  0.921997  0.189959
14  0.967285  0.111343  0.940308  0.163994
15  0.962402  0.115233  0.922729  0.199231
16  0.964844  0.116496  0.869263  0.277677
17  0.964355  0.113538  0.911255  0.216967
18  0.968384  0.111152  0.941162  0.161877
19  0.963989  0.113159  0.894409  0.249588
20  0.966675  0.111025  0.894897  0.265837
21  0.968384  0.104966  0.933594  0.188091
22  0.968628  0.107692  0.950439  0.158005
23  0.969482  0.107741  0.940308  0.172400
24  0.963623  0.114544  0.911987  0.234811
25  0.964233  0.114890  0.934448  0.183411
26  0.972168  0.102682  0.929688  0.189157
27  0.970337  0.100128  0.887451  0.289381
28  0.967773  0.106668  0.949219  0.157308
29  0.969727  0.106019  0.914307  0.230924
30  0.966431  0.105284  0.953247  0.151864
31  0.970459  0.101097  0.939453  0.164989
32  0.969238  0.102934  0.944946  0.171154
33  0.970825  0.103522  0.946045  0.157746
34  0.971924  0.099485  0.945923  0.163499
35  0.971558  0.099404  0.911499  0.232340
36  0.972778  0.093970  0.957886  0.142407
37  0.975586  0.096269  0.944458  0.169624
38  0.970703  0.105387  0.941528  0.175858
39  0.969360  0.103114  0.959229  0.137020
40  0.970947  0.105497  0.878296  0.316490
41  0.972412  0.100093  0.912842  0.230912
42  0.970825  0.097854  0.939453  0.180504
43  0.972412  0.095763  0.935059  0.196645
44  0.974487  0.092891  0.927734  0.197443
45  0.975586  0.093589  0.940918  0.174718
46  0.971802  0.096896  0.922729  0.221260
47  0.976074  0.085054  0.934570  0.190644
48  0.973633  0.096622  0.944946  0.170359
49  0.972900  0.096586  0.954590  0.157721
50  0.974243  0.092526  0.947266  0.171626
51  0.976562  0.089089  0.945679  0.167898
52  0.976929  0.087280  0.938477  0.174333
53  0.975708  0.093630  0.951904  0.154841
54  0.976196  0.090602  0.945557  0.158860
55  0.975464  0.093149  0.890991  0.279792
56  0.973877  0.090514  0.950806  0.153070
57  0.973511  0.089576  0.932251  0.184687
58  0.977417  0.090269  0.929077  0.197761
59  0.971924  0.093022  0.959595  0.137868
60  0.974854  0.089223  0.950928  0.165995
61  0.978516  0.082284  0.941772  0.174765
62  0.975952  0.087446  0.950684  0.153860
63  0.975586  0.092981  0.940918  0.177257

2017-12-13 20:38:46.042307 Finish.
Total elapsed time: 08:01:13.04.
