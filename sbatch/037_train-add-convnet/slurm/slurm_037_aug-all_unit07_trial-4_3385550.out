2017-12-13 17:29:26.937655: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:26.937961: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:26.937974: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 17:29:22.892861 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit10, unit01, unit02.
Validation set: unit03, unit05.
Test set: unit07.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.948608  0.167010  0.958252  0.162790
1   0.946411  0.168259  0.954346  0.170566
2   0.952881  0.158735  0.957397  0.178744
3   0.958374  0.145823  0.953613  0.152513
4   0.951782  0.153801  0.894897  0.248990
5   0.956177  0.142995  0.955444  0.155549
6   0.958008  0.145647  0.972778  0.114079
7   0.962280  0.137827  0.972046  0.118045
8   0.960571  0.132012  0.972656  0.116948
9   0.957886  0.141744  0.966797  0.130239
10  0.960815  0.137862  0.938477  0.181795
11  0.962769  0.137183  0.968994  0.124067
12  0.961914  0.131904  0.959106  0.133331
13  0.964233  0.136713  0.948242  0.162119
14  0.964355  0.128123  0.952637  0.165089
15  0.961914  0.128996  0.968384  0.137011
16  0.963135  0.123201  0.956299  0.148715
17  0.968872  0.110605  0.923462  0.197887
18  0.966064  0.123898  0.956665  0.157120
19  0.968262  0.117163  0.964600  0.132560
20  0.968018  0.120975  0.943604  0.168012
21  0.967407  0.116660  0.966309  0.137259
22  0.968628  0.110394  0.942261  0.178172
23  0.966553  0.118576  0.971924  0.109787
24  0.968872  0.112635  0.932739  0.192163
25  0.969727  0.108659  0.931030  0.186857
26  0.967651  0.122157  0.957764  0.146136
27  0.966919  0.114938  0.970093  0.121605
28  0.970703  0.110914  0.957031  0.145040
29  0.968628  0.115120  0.966675  0.129882
30  0.969116  0.114686  0.969238  0.116104
31  0.971436  0.109154  0.955688  0.143072
32  0.971436  0.108322  0.943237  0.175964
33  0.972290  0.109505  0.926147  0.206973
34  0.972778  0.103921  0.973267  0.106868
35  0.972900  0.103077  0.963013  0.128294
36  0.975342  0.091314  0.945557  0.162874
37  0.970215  0.109856  0.934937  0.192280
38  0.969482  0.107736  0.959961  0.142306
39  0.969849  0.111392  0.968506  0.118296
40  0.974243  0.096931  0.967041  0.128090
41  0.969604  0.109145  0.950439  0.156689
42  0.970215  0.110616  0.959595  0.138997
43  0.973267  0.103150  0.920776  0.212354
44  0.972778  0.101665  0.949951  0.154488
45  0.972290  0.101719  0.970093  0.112731
46  0.973267  0.104287  0.962280  0.137293
47  0.971802  0.102887  0.956421  0.137488
48  0.972656  0.103896  0.921631  0.209712
49  0.974609  0.102287  0.944702  0.167701
50  0.972412  0.103466  0.954468  0.150790
51  0.972778  0.099673  0.963135  0.122597
52  0.973145  0.101714  0.958984  0.145804
53  0.974365  0.100839  0.967651  0.126482
54  0.974243  0.097649  0.971191  0.112296
55  0.973145  0.100418  0.958984  0.131930
56  0.975342  0.097772  0.968872  0.112335
57  0.973877  0.100866  0.937744  0.179596
58  0.973022  0.097025  0.954956  0.144223
59  0.976685  0.097038  0.919189  0.218919
60  0.974731  0.096147  0.934204  0.187835
61  0.976196  0.091240  0.919800  0.225479
62  0.979248  0.084041  0.937744  0.176011
63  0.975342  0.090712  0.917969  0.214152

2017-12-14 01:23:06.938400 Finish.
Total elapsed time: 07:53:44.94.
