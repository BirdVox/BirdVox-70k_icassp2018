2017-12-13 17:29:26.610335: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:26.610611: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:26.610624: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 17:29:21.922355 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit10, unit01, unit02.
Validation set: unit03, unit05.
Test set: unit07.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.949341  0.164135  0.958984  0.151201
1   0.949585  0.162743  0.928955  0.201286
2   0.953979  0.150206  0.965698  0.133320
3   0.956055  0.145718  0.957275  0.145815
4   0.958984  0.137157  0.964844  0.132998
5   0.958862  0.137595  0.946289  0.166995
6   0.959351  0.134451  0.966431  0.127205
7   0.957764  0.142233  0.947266  0.161703
8   0.960449  0.130075  0.906494  0.239007
9   0.961182  0.133011  0.970093  0.122361
10  0.964844  0.128814  0.963379  0.132413
11  0.966797  0.115369  0.942261  0.172022
12  0.966797  0.120898  0.947876  0.155604
13  0.960083  0.133741  0.947266  0.165389
14  0.965698  0.121153  0.913696  0.222490
15  0.963135  0.126615  0.918457  0.209856
16  0.966919  0.122755  0.951294  0.155808
17  0.966553  0.132424  0.935059  0.176430
18  0.969849  0.108423  0.969116  0.120254
19  0.969727  0.112886  0.955933  0.145687
20  0.967041  0.114219  0.954834  0.147822
21  0.967651  0.117580  0.949829  0.166651
22  0.973877  0.106009  0.954712  0.138737
23  0.970459  0.109911  0.962524  0.135131
24  0.965820  0.119677  0.937500  0.165752
25  0.973633  0.103386  0.973633  0.111382
26  0.972046  0.107762  0.950317  0.151442
27  0.966553  0.111974  0.973877  0.106127
28  0.969116  0.115753  0.926514  0.190706
29  0.973145  0.103200  0.966187  0.118623
30  0.969238  0.113389  0.966187  0.117325
31  0.969482  0.105309  0.954834  0.145973
32  0.971069  0.106328  0.946045  0.149854
33  0.972656  0.106494  0.969604  0.117067
34  0.974365  0.097622  0.955811  0.143683
35  0.971191  0.105187  0.944824  0.166391
36  0.972900  0.100898  0.934814  0.177935
37  0.968262  0.119063  0.950195  0.161948
38  0.972534  0.104304  0.937622  0.170626
39  0.969360  0.105896  0.961426  0.136677
40  0.973755  0.103233  0.971313  0.110869
41  0.972412  0.104615  0.957764  0.142698
42  0.973022  0.103810  0.965576  0.131406
43  0.972534  0.103938  0.967163  0.115595
44  0.971680  0.105271  0.942139  0.160203
45  0.970215  0.106488  0.933350  0.191035
46  0.974609  0.100710  0.962646  0.128895
47  0.977417  0.086927  0.932617  0.187347
48  0.974243  0.099842  0.967896  0.117230
49  0.974243  0.099593  0.941162  0.163767
50  0.973755  0.105552  0.949585  0.147922
51  0.971680  0.101687  0.961670  0.132534
52  0.975098  0.093125  0.935913  0.177764
53  0.973145  0.095043  0.966675  0.132020
54  0.973633  0.101585  0.949097  0.158598
55  0.974243  0.096242  0.959473  0.128653
56  0.977295  0.090604  0.950195  0.148765
57  0.976196  0.090870  0.970459  0.114370
58  0.974854  0.098295  0.961548  0.130230
59  0.971802  0.100239  0.947998  0.158321
60  0.976074  0.091582  0.944092  0.155959
61  0.972656  0.098189  0.926636  0.202884
62  0.978516  0.088668  0.924316  0.207605
63  0.975342  0.093156  0.965698  0.122258

2017-12-14 01:00:03.244159 Finish.
Total elapsed time: 07:30:42.24.
