2017-12-13 17:29:27.539005: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:27.540473: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:27.540488: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 17:29:22.229246 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit07, unit10, unit01.
Validation set: unit02, unit03.
Test set: unit05.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.495483  8.043198  0.494263  8.062659
1   0.503296  7.918648  0.492310  8.093796
2   0.488647  8.152179  0.498779  7.990653
3   0.515015  7.731823  0.495728  8.039306
4   0.505981  7.875834  0.497070  8.017899
5   0.492554  8.089904  0.483643  8.231969
6   0.504028  7.906971  0.503540  7.914756
7   0.500244  7.967300  0.494751  8.054874
8   0.499390  7.980923  0.513062  7.762961
9   0.505371  7.885564  0.501709  7.943947
10  0.496460  8.027629  0.505127  7.889457
11  0.496338  8.029575  0.502441  7.932271
12  0.494751  8.054874  0.500977  7.955624
13  0.502930  7.924486  0.494629  8.056820
14  0.496338  8.029575  0.499023  7.986761
15  0.489502  8.138556  0.503540  7.914756
16  0.496216  8.031521  0.489258  8.142448
17  0.498779  7.990653  0.497681  8.008168
18  0.497314  8.014006  0.500977  7.955624
19  0.492676  8.087958  0.496582  8.025683
20  0.501587  7.945893  0.493042  8.082120
21  0.498535  7.994545  0.502197  7.936163
22  0.503052  7.922540  0.505981  7.875834
23  0.495605  8.041252  0.491577  8.105473
24  0.496338  8.029575  0.495239  8.047090
25  0.491577  8.105473  0.511475  7.788260
26  0.494751  8.054874  0.499023  7.986761
27  0.496948  8.019845  0.502319  7.934217
28  0.498535  7.994545  0.486816  8.181370
29  0.488525  8.154125  0.498779  7.990653
30  0.504517  7.899187  0.493774  8.070443
31  0.490356  8.124934  0.505005  7.891403
32  0.506470  7.868050  0.506470  7.868049
33  0.506836  7.862211  0.496460  8.027629
34  0.499634  7.977031  0.503296  7.918648
35  0.495850  8.037359  0.497314  8.014006
36  0.498535  7.994545  0.496094  8.033467
37  0.496460  8.027629  0.500488  7.963408
38  0.501953  7.940055  0.499023  7.986761
39  0.494385  8.060713  0.500610  7.961462
40  0.488159  8.159963  0.502808  7.926432
41  0.493652  8.072389  0.501099  7.953678
42  0.503784  7.910864  0.497192  8.015952
43  0.506836  7.862211  0.504395  7.901133
44  0.500977  7.955624  0.502319  7.934217
45  0.493286  8.078227  0.504395  7.901133
46  0.504761  7.895295  0.495483  8.043198
47  0.495850  8.037359  0.497192  8.015952
48  0.508667  7.833020  0.498535  7.994545
49  0.497192  8.015952  0.505249  7.887510
50  0.493164  8.080173  0.491455  8.107419
51  0.494629  8.056820  0.505249  7.887510
52  0.501831  7.942001  0.502197  7.936163
53  0.499756  7.975085  0.504639  7.897241
54  0.505615  7.881672  0.494995  8.050982
55  0.500854  7.957570  0.504883  7.893349
56  0.496704  8.023737  0.485474  8.202777
57  0.500732  7.959516  0.495239  8.047090
58  0.497681  8.008168  0.506836  7.862211
59  0.505615  7.881672  0.500366  7.965354
60  0.504272  7.903079  0.506714  7.864157
61  0.502563  7.930324  0.508179  7.840804
62  0.503418  7.916702  0.487183  8.175532
63  0.491821  8.101580  0.504639  7.897241

2017-12-14 01:26:45.149574 Finish.
Total elapsed time: 07:57:23.15.
