2017-12-13 12:38:35.393594: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:38:35.393790: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:38:35.393801: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:38:35.393807: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:38:35.393812: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 12:38:33.047174 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.500977  7.955624  0.502197  7.936163
1   0.505005  7.891403  0.489502  8.138556
2   0.502075  7.938109  0.501831  7.942001
3   0.492798  8.086012  0.501709  7.943947
4   0.496826  8.021791  0.499512  7.978977
5   0.504272  7.903079  0.498535  7.994545
6   0.497559  8.010114  0.497681  8.008168
7   0.505005  7.891403  0.507446  7.852481
8   0.506958  7.860265  0.510864  7.797990
9   0.500732  7.959516  0.497803  8.006222
10  0.495605  8.041252  0.498169  8.000384
11  0.495361  8.045144  0.502808  7.926432
12  0.504395  7.901133  0.501831  7.942001
13  0.497925  8.004276  0.500610  7.961462
14  0.497925  8.004276  0.488525  8.154125
15  0.493896  8.068497  0.498779  7.990653
16  0.495117  8.049036  0.500610  7.961462
17  0.499268  7.982869  0.500610  7.961462
18  0.492188  8.095742  0.487793  8.165802
19  0.498779  7.990653  0.493042  8.082120
20  0.492676  8.087958  0.496704  8.023737
21  0.494507  8.058766  0.497192  8.015952
22  0.492432  8.091850  0.510132  7.809667
23  0.504517  7.899187  0.494385  8.060713
24  0.506226  7.871942  0.510498  7.803828
25  0.507812  7.846642  0.501465  7.947839
26  0.500000  7.971192  0.500366  7.965354
27  0.505493  7.883618  0.492798  8.086012
28  0.503784  7.910864  0.500244  7.967300
29  0.504761  7.895295  0.506714  7.864157
30  0.501099  7.953678  0.501465  7.947839
31  0.500366  7.965354  0.492920  8.084066
32  0.513672  7.753230  0.494141  8.064605
33  0.492432  8.091850  0.499268  7.982869
34  0.493530  8.074335  0.493408  8.076281
35  0.495972  8.035413  0.508545  7.834966
36  0.502197  7.936163  0.505371  7.885564
37  0.500977  7.955624  0.493896  8.068497
38  0.507568  7.850535  0.498291  7.998438
39  0.508179  7.840804  0.496338  8.029575
40  0.507690  7.848589  0.496826  8.021791
41  0.502319  7.934217  0.501343  7.949785
42  0.496582  8.025683  0.499146  7.984815
43  0.505127  7.889457  0.508911  7.829128
44  0.496948  8.019845  0.500610  7.961462
45  0.503540  7.914756  0.500000  7.971192
46  0.498535  7.994545  0.508057  7.842750
47  0.506714  7.864157  0.507080  7.858319
48  0.496582  8.025683  0.508911  7.829128
49  0.501587  7.945893  0.504883  7.893349
50  0.495850  8.037359  0.498291  7.998438
51  0.504761  7.895295  0.496216  8.031521
52  0.497192  8.015952  0.496948  8.019845
53  0.491089  8.113257  0.497314  8.014006
54  0.507935  7.844696  0.494629  8.056820
55  0.498657  7.992599  0.505859  7.877780
56  0.491577  8.105473  0.512207  7.776583
57  0.498169  8.000384  0.502930  7.924486
58  0.502441  7.932271  0.493896  8.068497
59  0.503296  7.918648  0.493164  8.080173
60  0.502319  7.934217  0.494141  8.064605
61  0.500244  7.967300  0.499390  7.980923
62  0.506470  7.868049  0.507690  7.848589
63  0.499756  7.975085  0.488037  8.161909

2017-12-13 20:42:19.798290 Finish.
Total elapsed time: 08:03:46.80.
