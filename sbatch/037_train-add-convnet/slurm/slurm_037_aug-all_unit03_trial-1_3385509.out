2017-12-13 17:29:27.679663: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:27.679927: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:27.679939: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 17:29:22.229869 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.969849  0.124930  0.784058  0.655238
1   0.969727  0.115516  0.832275  0.525368
2   0.971924  0.117937  0.806030  0.573720
3   0.974121  0.104740  0.834351  0.503938
4   0.974243  0.106576  0.832520  0.535343
5   0.975464  0.102104  0.807251  0.676823
6   0.970703  0.110180  0.769287  0.636211
7   0.974487  0.101468  0.823364  0.572193
8   0.977173  0.095266  0.804077  0.665732
9   0.975830  0.106763  0.798706  0.636665
10  0.975220  0.100465  0.830566  0.528625
11  0.978149  0.091770  0.824585  0.618209
12  0.977905  0.092176  0.819214  0.593653
13  0.977539  0.095512  0.831421  0.555336
14  0.979736  0.082825  0.840332  0.538164
15  0.981567  0.084946  0.811890  0.603462
16  0.978882  0.087484  0.841309  0.560148
17  0.977905  0.097586  0.860474  0.466425
18  0.978882  0.087313  0.838013  0.520868
19  0.981934  0.081136  0.810181  0.666745
20  0.979614  0.084255  0.822388  0.564342
21  0.982422  0.075841  0.804810  0.687516
22  0.980347  0.075767  0.838013  0.469951
23  0.979004  0.088730  0.815186  0.560192
24  0.978271  0.089258  0.814331  0.692092
25  0.980347  0.086338  0.819946  0.540717
26  0.978516  0.087789  0.840820  0.567189
27  0.981079  0.076211  0.862915  0.461975
28  0.979492  0.083960  0.844238  0.559591
29  0.976318  0.091535  0.806274  0.611153
30  0.981445  0.081721  0.845825  0.500154
31  0.978394  0.086116  0.830933  0.542396
32  0.979614  0.085442  0.814331  0.711026
33  0.977905  0.082018  0.836670  0.449408
34  0.982178  0.074065  0.845581  0.564434
35  0.981934  0.076895  0.828247  0.489236
36  0.982666  0.072177  0.841309  0.498421
37  0.982056  0.070524  0.836426  0.693354
38  0.980835  0.078773  0.840942  0.424043
39  0.981323  0.076933  0.832275  0.473130
40  0.984131  0.064481  0.815430  0.819938
41  0.984009  0.071256  0.808716  0.802131
42  0.982422  0.074525  0.812256  0.586932
43  0.980103  0.079132  0.819824  0.643662
44  0.984253  0.070351  0.810425  0.700656
45  0.981201  0.074078  0.806274  0.706811
46  0.980591  0.075487  0.802002  0.719838
47  0.982910  0.067727  0.836548  0.586683
48  0.984375  0.068592  0.837891  0.561913
49  0.980713  0.073414  0.812134  0.712598
50  0.982300  0.072242  0.796265  0.666410
51  0.981323  0.074407  0.820435  0.673972
52  0.984009  0.068548  0.851196  0.439524
53  0.982544  0.068445  0.863525  0.412329
54  0.980469  0.073172  0.855347  0.507732
55  0.982422  0.073644  0.854614  0.490492
56  0.984741  0.059246  0.866821  0.430099
57  0.983887  0.064097  0.852051  0.500429
58  0.983521  0.064164  0.803711  0.602333
59  0.982300  0.069026  0.821533  0.677982
60  0.981689  0.071714  0.836426  0.511423
61  0.985229  0.062434  0.809814  0.671235
62  0.983276  0.068973  0.822144  0.601315
63  0.982910  0.067070  0.832886  0.674233

2017-12-14 01:21:38.504761 Finish.
Total elapsed time: 07:52:16.50.
