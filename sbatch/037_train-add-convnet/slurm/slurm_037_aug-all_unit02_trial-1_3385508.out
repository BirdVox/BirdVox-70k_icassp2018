2017-12-13 17:29:28.485029: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:28.485282: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:28.485294: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 17:29:23.307635 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.972290  0.098250  0.746460  0.749977
1   0.973389  0.101442  0.765259  0.926518
2   0.974854  0.093553  0.767578  0.906696
3   0.975342  0.091867  0.763916  0.921066
4   0.974731  0.099498  0.809204  0.682851
5   0.977661  0.085057  0.785522  0.806130
6   0.975708  0.085833  0.753784  1.029340
7   0.974609  0.089620  0.777466  0.694926
8   0.974487  0.087726  0.805298  0.669362
9   0.976196  0.087892  0.744873  1.053108
10  0.981079  0.069594  0.814941  0.772667
11  0.980835  0.070174  0.795410  0.952408
12  0.978638  0.078771  0.808228  0.791822
13  0.975220  0.084463  0.812622  0.808000
14  0.980957  0.072482  0.775024  0.990938
15  0.978271  0.084525  0.834961  0.724242
16  0.979736  0.081708  0.830933  0.681784
17  0.979614  0.082379  0.808472  0.607173
18  0.980469  0.076914  0.836304  0.607792
19  0.982422  0.069738  0.795654  0.715092
20  0.982300  0.073702  0.784790  0.847942
21  0.979980  0.073338  0.776978  0.837788
22  0.981812  0.070893  0.837402  0.552754
23  0.981201  0.070797  0.775146  1.038691
24  0.978149  0.077457  0.815552  0.611092
25  0.984009  0.067970  0.830322  0.664619
26  0.982666  0.066995  0.809814  0.745897
27  0.982056  0.072042  0.792725  0.862634
28  0.980469  0.070882  0.812012  0.777797
29  0.983032  0.064902  0.774048  0.931919
30  0.983276  0.068532  0.870117  0.509373
31  0.980469  0.073591  0.816284  0.627745
32  0.981323  0.072967  0.807251  0.776133
33  0.982178  0.071693  0.770630  1.027050
34  0.982178  0.071425  0.837524  0.640916
35  0.983398  0.066573  0.826416  0.612795
36  0.981445  0.068661  0.831177  0.616763
37  0.983032  0.064981  0.769653  0.985297
38  0.984253  0.065096  0.758057  1.026632
39  0.986084  0.056437  0.741211  1.200221
40  0.984131  0.063432  0.783813  0.954531
41  0.983032  0.063083  0.711792  1.423025
42  0.982178  0.068984  0.841919  0.508462
43  0.986084  0.058339  0.796265  0.809839
44  0.985352  0.058988  0.805908  0.771182
45  0.984253  0.060699  0.782959  0.881695
46  0.982422  0.066756  0.802368  0.746465
47  0.982666  0.065497  0.804565  0.654083
48  0.984863  0.058169  0.822266  0.652004
49  0.987549  0.052260  0.764282  0.997874
50  0.983887  0.060061  0.771973  1.074546
51  0.984375  0.063117  0.818115  0.706541
52  0.984863  0.057937  0.823486  0.733492
53  0.982422  0.063504  0.799805  0.887336
54  0.983643  0.060824  0.768555  1.130615
55  0.982666  0.065020  0.794800  0.779169
56  0.985596  0.063501  0.810547  0.868270
57  0.984985  0.059859  0.810547  0.804288
58  0.984619  0.059595  0.822754  0.723460
59  0.985107  0.057043  0.794678  0.966111
60  0.986328  0.059533  0.839600  0.613770
61  0.985718  0.062120  0.785889  0.886229
62  0.983765  0.060905  0.810791  0.852835
63  0.985229  0.060816  0.801514  0.872537

2017-12-14 01:20:58.180126 Finish.
Total elapsed time: 07:51:35.18.
