2018-02-12 19:59:47.082822: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 19:59:47.083167: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 19:59:47.083180: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 19:59:32.311279 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.496338  8.118074  0.500122  8.057080
1    0.499634  8.064950  0.499390  8.068886
2    0.498169  8.088561  0.499756  8.062983
3    0.510620  7.887872  0.501343  8.037405
4    0.493652  8.161360  0.510254  7.893774
5    0.494995  8.139717  0.494995  8.139717
6    0.492554  8.179068  0.507690  7.935093
7    0.500732  8.047243  0.484619  8.306958
8    0.503052  8.009859  0.490479  8.212516
9    0.514404  7.826878  0.500244  8.055113
10   0.499023  8.074788  0.505859  7.964606
11   0.508545  7.921320  0.501709  8.031502
12   0.496826  8.110204  0.504639  7.984281
13   0.502808  8.013794  0.498657  8.080691
14   0.503296  8.005924  0.499512  8.066918
15   0.502930  8.011827  0.486450  8.277445
16   0.499146  8.072821  0.499023  8.074788
17   0.496826  8.110204  0.494751  8.143652
18   0.494751  8.143652  0.499390  8.068885
19   0.499023  8.074788  0.499756  8.062983
20   0.496094  8.122009  0.506348  7.956736
21   0.494873  8.141684  0.493530  8.163327
22   0.501465  8.035437  0.494019  8.155457
23   0.499878  8.061015  0.512573  7.856391
24   0.494995  8.139717  0.505859  7.964606
25   0.507446  7.939028  0.495850  8.125944
26   0.504150  7.992151  0.496338  8.118074
27   0.496826  8.110204  0.499634  8.064950
28   0.508667  7.919352  0.503906  7.996086
29   0.501831  8.029535  0.494873  8.141684
30   0.508057  7.929190  0.502563  8.017729
31   0.505371  7.972476  0.494873  8.141685
32   0.493530  8.163327  0.491577  8.194808
33   0.494141  8.153490  0.499756  8.062983
34   0.501709  8.031502  0.509888  7.899677
35   0.497925  8.092496  0.505371  7.972476
36   0.498413  8.084626  0.501709  8.031502
37   0.499756  8.062983  0.505249  7.974444
38   0.491089  8.202678  0.497925  8.092496
39   0.498047  8.090528  0.497559  8.098399
40   0.508423  7.923287  0.498901  8.076756
41   0.505005  7.978379  0.492676  8.177100
42   0.494019  8.155457  0.499756  8.062983
43   0.496948  8.108236  0.497437  8.100366
44   0.497437  8.100366  0.504639  7.984281
45   0.500122  8.057080  0.499023  8.074788
46   0.495605  8.129879  0.503052  8.009859
47   0.503052  8.009859  0.503052  8.009859
48   0.507568  7.937060  0.504028  7.994119
49   0.498657  8.080691  0.495972  8.123977
50   0.499268  8.070853  0.510742  7.885904
51   0.499878  8.061015  0.495117  8.137749
52   0.491821  8.190873  0.498169  8.088561
53   0.502686  8.015762  0.499146  8.072821
54   0.500000  8.059048  0.494507  8.147587
55   0.511963  7.866229  0.500732  8.047243
56   0.497681  8.096431  0.500732  8.047243
57   0.495972  8.123977  0.494385  8.149555
58   0.496826  8.110204  0.492798  8.175133
59   0.501343  8.037405  0.499023  8.074788
60   0.495483  8.131847  0.498413  8.084626
61   0.497314  8.102334  0.497314  8.102334
62   0.500977  8.043307  0.499268  8.070853
63   0.509644  7.903612  0.503784  7.998054
64   0.498779  8.078723  0.489014  8.236126
65   0.506958  7.946898  0.496948  8.108236
66   0.488647  8.242029  0.502075  8.025600
67   0.499512  8.066918  0.498779  8.078723
68   0.497925  8.092496  0.501709  8.031502
69   0.493286  8.167263  0.501953  8.027567
70   0.498901  8.076756  0.503418  8.003957
71   0.508301  7.925255  0.502319  8.021665
72   0.493408  8.165295  0.486816  8.271542
73   0.490967  8.204646  0.499390  8.068886
74   0.495972  8.123977  0.508911  7.915417
75   0.498047  8.090528  0.497925  8.092496
76   0.496826  8.110204  0.497070  8.106269
77   0.498535  8.082658  0.513306  7.844586
78   0.494995  8.139717  0.506470  7.954768
79   0.494995  8.139717  0.495117  8.137749
80   0.494263  8.151522  0.502197  8.023632
81   0.507080  7.944930  0.505737  7.966573
82   0.499634  8.064950  0.494385  8.149555
83   0.508057  7.929190  0.503418  8.003957
84   0.512817  7.852456  0.510498  7.889839
85   0.496582  8.114139  0.508789  7.917385
86   0.502441  8.019697  0.509888  7.899677
87   0.506714  7.950833  0.496094  8.122009
88   0.508057  7.929190  0.502075  8.025600
89   0.502930  8.011827  0.504883  7.980346
90   0.506470  7.954768  0.506714  7.950833
91   0.503784  7.998054  0.491821  8.190873
92   0.506348  7.956736  0.503418  8.003957
93   0.501221  8.039372  0.496582  8.114139
94   0.502563  8.017729  0.502686  8.015762
95   0.508179  7.927223  0.499146  8.072821
96   0.503662  8.000022  0.499634  8.064950
97   0.495850  8.125944  0.505371  7.972476
98   0.512451  7.858359  0.503174  8.007892
99   0.503052  8.009859  0.491821  8.190873
100  0.492798  8.175133  0.506836  7.948865
101  0.507812  7.933125  0.495728  8.127912
102  0.499878  8.061015  0.498535  8.082658
103  0.497803  8.094463  0.508667  7.919352
104  0.496460  8.116106  0.497925  8.092496
105  0.499390  8.068885  0.501465  8.035437
106  0.513306  7.844586  0.505493  7.970508
107  0.504028  7.994119  0.505859  7.964606
108  0.500488  8.051178  0.505859  7.964606
109  0.501221  8.039372  0.498779  8.078723
110  0.499146  8.072821  0.492432  8.181035
111  0.503906  7.996086  0.501221  8.039372
112  0.502319  8.021665  0.499023  8.074788
113  0.499756  8.062983  0.493530  8.163327
114  0.501831  8.029535  0.498291  8.086593
115  0.498779  8.078723  0.500610  8.049210
116  0.508423  7.923287  0.511963  7.866229
117  0.505859  7.964606  0.493774  8.159392
118  0.503662  8.000022  0.500488  8.051178
119  0.510498  7.889839  0.496948  8.108236
120  0.502441  8.019697  0.505615  7.968541
121  0.499878  8.061015  0.497925  8.092496
122  0.498657  8.080691  0.497681  8.096431
123  0.505737  7.966573  0.498535  8.082658
124  0.495483  8.131847  0.500854  8.045275
125  0.498657  8.080691  0.512573  7.856391
126  0.505859  7.964606  0.494507  8.147587
127  0.496338  8.118074  0.501221  8.039372

2018-02-13 11:31:53.597267 Finish.
Total elapsed time: 15:32:21.60.
