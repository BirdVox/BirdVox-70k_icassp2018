2017-12-13 12:37:36.220389: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:36.220640: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:36.220651: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:36.220656: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:36.220660: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 12:37:34.278429 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.502808  7.926432  0.497437  8.012060
1   0.496094  8.033467  0.499512  7.978977
2   0.499756  7.975085  0.508179  7.840804
3   0.502563  7.930324  0.493042  8.082120
4   0.507568  7.850535  0.501465  7.947839
5   0.491821  8.101580  0.499512  7.978977
6   0.503052  7.922540  0.499512  7.978977
7   0.504028  7.906971  0.498169  8.000384
8   0.511230  7.792152  0.503174  7.920594
9   0.495239  8.047090  0.490356  8.124934
10  0.506592  7.866103  0.498901  7.988707
11  0.502197  7.936163  0.506348  7.869996
12  0.501709  7.943947  0.506836  7.862211
13  0.504883  7.893349  0.488770  8.150233
14  0.499268  7.982869  0.502563  7.930324
15  0.492432  8.091850  0.496948  8.019845
16  0.500610  7.961462  0.494507  8.058766
17  0.494751  8.054874  0.494751  8.054874
18  0.497803  8.006222  0.499512  7.978977
19  0.498657  7.992599  0.506104  7.873888
20  0.490601  8.121041  0.505005  7.891403
21  0.494507  8.058766  0.501587  7.945893
22  0.494141  8.064605  0.493530  8.074335
23  0.490845  8.117149  0.492554  8.089904
24  0.506470  7.868050  0.502686  7.928378
25  0.498169  8.000384  0.496582  8.025683
26  0.498779  7.990653  0.495361  8.045144
27  0.497803  8.006222  0.496704  8.023737
28  0.496094  8.033467  0.508179  7.840804
29  0.500366  7.965354  0.509766  7.815505
30  0.503418  7.916702  0.509277  7.823289
31  0.488525  8.154125  0.494141  8.064605
32  0.486450  8.187209  0.491699  8.103527
33  0.496216  8.031521  0.501587  7.945893
34  0.500122  7.969246  0.496582  8.025683
35  0.505493  7.883618  0.487915  8.163855
36  0.500854  7.957570  0.500854  7.957570
37  0.507568  7.850535  0.493652  8.072389
38  0.498779  7.990653  0.504150  7.905025
39  0.504517  7.899187  0.497681  8.008168
40  0.501831  7.942001  0.492432  8.091850
41  0.500977  7.955624  0.504395  7.901133
42  0.497681  8.008168  0.501343  7.949785
43  0.498291  7.998438  0.505127  7.889457
44  0.507324  7.854427  0.499268  7.982869
45  0.501343  7.949785  0.500977  7.955624
46  0.507935  7.844696  0.504028  7.906971
47  0.490112  8.128826  0.498657  7.992599
48  0.494019  8.066551  0.504272  7.903079
49  0.502441  7.932271  0.504517  7.899187
50  0.497925  8.004276  0.500366  7.965354
51  0.507690  7.848589  0.494751  8.054874
52  0.501221  7.951731  0.499634  7.977031
53  0.497559  8.010114  0.486938  8.179424
54  0.498413  7.996492  0.499878  7.973138
55  0.497803  8.006222  0.495483  8.043198
56  0.492676  8.087958  0.492798  8.086012
57  0.502319  7.934217  0.507202  7.856373
58  0.495605  8.041252  0.490845  8.117149
59  0.497437  8.012060  0.508789  7.831074
60  0.498779  7.990653  0.493652  8.072389
61  0.503906  7.908917  0.492554  8.089904
62  0.499634  7.977031  0.502930  7.924486
63  0.491699  8.103527  0.497314  8.014006

2017-12-13 20:28:38.706375 Finish.
Total elapsed time: 07:51:04.71.
