2017-12-13 12:38:35.301663: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:38:35.301818: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:38:35.301828: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:38:35.301832: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:38:35.301837: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 12:38:33.014162 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.503174  8.008484  0.500854  8.045740
1   0.500000  8.059422  0.499634  8.065245
2   0.512939  7.850725  0.496582  8.114261
3   0.503296  8.006073  0.482178  8.339044
4   0.691528  1.160719  0.552368  0.779028
5   0.739746  0.542253  0.542480  0.786032
6   0.735352  0.534846  0.610474  0.802428
7   0.747192  0.524158  0.551514  0.768368
8   0.748047  0.519395  0.623047  0.722977
9   0.750854  0.510767  0.662964  0.868233
10  0.749268  0.510614  0.621338  0.815125
11  0.746216  0.517161  0.607056  0.877951
12  0.753540  0.501610  0.609009  0.815297
13  0.754272  0.500837  0.610840  0.845418
14  0.752808  0.502069  0.632935  0.681491
15  0.765137  0.492791  0.623047  0.879114
16  0.756104  0.498097  0.621582  0.840579
17  0.767334  0.484643  0.644165  0.698253
18  0.754150  0.497064  0.599854  0.911137
19  0.747192  0.507240  0.619385  0.816767
20  0.761230  0.492234  0.616821  0.830167
21  0.767700  0.484794  0.623535  0.758977
22  0.752686  0.499808  0.618774  0.769551
23  0.764648  0.485442  0.637695  0.709352
24  0.756592  0.486988  0.603516  0.849328
25  0.750488  0.495159  0.640869  0.740654
26  0.757568  0.498157  0.615356  0.836786
27  0.755981  0.491549  0.620117  0.716438
28  0.762939  0.481544  0.625000  0.791591
29  0.751709  0.494092  0.629272  0.702861
30  0.753418  0.497800  0.635254  0.700010
31  0.759277  0.489729  0.599854  0.968811
32  0.770508  0.479326  0.591553  0.984314
33  0.760010  0.489093  0.706299  0.769577
34  0.759155  0.484418  0.621704  0.806414
35  0.765747  0.481082  0.628540  0.743424
36  0.759644  0.486037  0.623657  0.769314
37  0.755249  0.495197  0.619629  0.708637
38  0.760132  0.487483  0.628296  0.873584
39  0.757935  0.485672  0.608765  0.832801
40  0.765869  0.482806  0.615601  0.829464
41  0.762085  0.482308  0.627319  0.846898
42  0.769165  0.473441  0.605713  0.976808
43  0.765381  0.479708  0.630615  0.697768
44  0.761719  0.484136  0.690063  0.944485
45  0.764648  0.482454  0.735962  0.863656
46  0.758667  0.482847  0.626709  0.693390
47  0.758423  0.483956  0.720459  0.702766
48  0.757568  0.488398  0.627686  0.754702
49  0.768433  0.471022  0.634888  0.752493
50  0.769287  0.478025  0.635254  0.712939
51  0.752441  0.494684  0.640991  0.725002
52  0.769775  0.474384  0.652222  0.817803
53  0.764893  0.481574  0.652466  0.829226
54  0.760864  0.477957  0.618164  0.747311
55  0.763672  0.479974  0.615601  0.807763
56  0.765137  0.478950  0.616577  0.810644
57  0.766724  0.476863  0.617065  0.744197
58  0.767578  0.474395  0.621460  0.722654
59  0.758911  0.486894  0.611938  0.989205
60  0.766235  0.479148  0.616821  0.883857
61  0.760620  0.482949  0.584106  0.867350
62  0.761353  0.485162  0.614014  0.837990
63  0.762451  0.482113  0.616089  0.883654

2017-12-13 20:14:32.675069 Finish.
Total elapsed time: 07:35:59.68.
