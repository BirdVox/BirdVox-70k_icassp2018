2017-12-13 12:37:36.296095: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:36.296402: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:36.296411: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:36.296416: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:36.296420: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 12:37:34.278617 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit02, unit03, unit05.
Validation set: unit07, unit10.
Test set: unit01.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.497192  8.114337  0.503906  8.004725
1   0.501465  8.042949  0.505371  7.978961
2   0.498779  8.084369  0.500732  8.052119
3   0.502075  8.029844  0.495728  8.131574
4   0.507935  7.934339  0.502930  8.014566
5   0.496094  8.124381  0.501831  8.031569
6   0.498535  8.084413  0.500366  8.054643
7   0.505005  7.979665  0.494873  8.142776
8   0.487915  8.254766  0.501343  8.038191
9   0.494263  8.152189  0.498413  8.085184
10  0.495483  8.132316  0.498901  8.077145
11  0.499390  8.069211  0.502930  8.012094
12  0.506104  7.960892  0.503662  8.000202
13  0.503662  8.000169  0.491943  8.189025
14  0.495728  8.128008  0.497070  8.106346
15  0.500610  8.049272  0.497314  8.102382
16  0.493774  8.159431  0.500122  8.057111
17  0.490723  8.208605  0.503662  8.000040
18  0.502441  8.019711  0.498291  8.086604
19  0.502319  8.021673  0.494141  8.153496
20  0.489746  8.224326  0.491943  8.188909
21  0.495605  8.129882  0.504150  7.992153
22  0.494507  8.147588  0.500000  8.059049
23  0.511841  7.868197  0.494629  8.145620
24  0.490601  8.210549  0.494751  8.143652
25  0.508057  7.929190  0.490234  8.216451
26  0.498291  8.086593  0.496338  8.118074
27  0.489746  8.224321  0.493164  8.169230
28  0.501953  8.027567  0.503418  8.003957
29  0.495850  8.125944  0.499634  8.064950
30  0.502441  8.019697  0.497681  8.096431
31  0.503784  7.998054  0.508057  7.929190
32  0.506714  7.950833  0.498901  8.076756
33  0.496704  8.112171  0.506104  7.960671
34  0.499512  8.066918  0.504150  7.992151
35  0.506104  7.960671  0.500854  8.045275
36  0.498291  8.086593  0.491333  8.198743
37  0.498901  8.076756  0.500610  8.049210
38  0.496338  8.118074  0.501709  8.031502
39  0.500366  8.053145  0.491333  8.198743
40  0.498291  8.086593  0.504150  7.992151
41  0.502075  8.025600  0.495117  8.137749
42  0.496948  8.108236  0.502075  8.025600
43  0.488892  8.238094  0.501587  8.033470
44  0.499878  8.061015  0.497437  8.100366
45  0.507812  7.933125  0.496460  8.116106
46  0.506470  7.954768  0.498901  8.076756
47  0.503662  8.000022  0.503052  8.009859
48  0.497070  8.106269  0.495239  8.135782
49  0.494995  8.139717  0.508545  7.921320
50  0.495483  8.131847  0.497437  8.100366
51  0.493408  8.165295  0.493408  8.165295
52  0.511475  7.874099  0.497803  8.094464
53  0.501099  8.041340  0.496460  8.116106
54  0.505615  7.968541  0.496826  8.110204
55  0.505005  7.978379  0.494629  8.145620
56  0.501831  8.029535  0.494385  8.149555
57  0.495361  8.133814  0.504150  7.992151
58  0.490356  8.214484  0.498413  8.084626
59  0.499878  8.061015  0.506470  7.954768
60  0.496826  8.110204  0.494751  8.143652
61  0.497681  8.096431  0.510498  7.889839
62  0.495972  8.123977  0.498169  8.088561
63  0.494263  8.151522  0.496338  8.118074

2017-12-13 20:30:40.387806 Finish.
Total elapsed time: 07:53:06.39.
