2017-12-13 12:37:36.686042: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:36.686223: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:36.686234: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:36.686238: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:36.686243: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 12:37:34.278666 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit02, unit03, unit05.
Validation set: unit07, unit10.
Test set: unit01.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.498413  8.084626  0.499268  8.070853
1   0.507446  7.939028  0.493652  8.161360
2   0.498657  8.080691  0.511230  7.878034
3   0.500610  8.049210  0.495117  8.137749
4   0.512207  7.862294  0.503540  8.001989
5   0.499878  8.061015  0.505371  7.972476
6   0.499512  8.066918  0.503540  8.001989
7   0.495239  8.135782  0.506104  7.960671
8   0.494995  8.139717  0.505371  7.972476
9   0.501587  8.033470  0.501343  8.037405
10  0.502197  8.023632  0.494507  8.147587
11  0.500488  8.051178  0.500122  8.057080
12  0.502563  8.017729  0.493652  8.161360
13  0.500122  8.057080  0.502686  8.015762
14  0.494507  8.147587  0.496704  8.112171
15  0.499634  8.064950  0.503784  7.998054
16  0.501343  8.037405  0.493896  8.157425
17  0.494263  8.151522  0.498657  8.080691
18  0.496460  8.116106  0.503418  8.003957
19  0.504639  7.984281  0.501709  8.031502
20  0.493164  8.169230  0.501953  8.027567
21  0.502441  8.019697  0.510986  7.881969
22  0.496460  8.116106  0.495117  8.137749
23  0.502930  8.011827  0.505005  7.978379
24  0.505005  7.978379  0.500366  8.053145
25  0.504883  7.980346  0.498657  8.080691
26  0.496582  8.114139  0.500244  8.055113
27  0.503540  8.001989  0.498657  8.080691
28  0.497192  8.104301  0.491089  8.202678
29  0.496826  8.110204  0.496704  8.112171
30  0.495728  8.127912  0.502197  8.023632
31  0.503174  8.007892  0.498657  8.080691
32  0.494385  8.149555  0.499634  8.064950
33  0.503662  8.000022  0.496460  8.116107
34  0.497314  8.102334  0.495483  8.131847
35  0.502563  8.017729  0.496460  8.116106
36  0.501831  8.029535  0.505859  7.964606
37  0.496216  8.120042  0.508423  7.923287
38  0.500977  8.043307  0.495728  8.127912
39  0.497192  8.104301  0.504395  7.988216
40  0.496948  8.108236  0.510132  7.895742
41  0.502197  8.023632  0.500244  8.055113
42  0.492676  8.177100  0.496094  8.122009
43  0.497192  8.104301  0.495483  8.131847
44  0.507080  7.944930  0.495361  8.133814
45  0.502930  8.011827  0.494141  8.153490
46  0.501099  8.041340  0.499634  8.064950
47  0.505737  7.966573  0.508911  7.915417
48  0.501221  8.039372  0.502563  8.017729
49  0.507080  7.944930  0.487427  8.261705
50  0.494751  8.143652  0.509155  7.911482
51  0.499146  8.072821  0.500854  8.045275
52  0.502441  8.019697  0.495972  8.123977
53  0.500732  8.047243  0.504028  7.994119
54  0.487061  8.267607  0.496338  8.118074
55  0.505737  7.966573  0.496826  8.110204
56  0.498047  8.090528  0.492432  8.181035
57  0.510986  7.881969  0.494873  8.141684
58  0.500610  8.049210  0.502319  8.021665
59  0.487671  8.257769  0.496704  8.112171
60  0.497192  8.104301  0.507446  7.939028
61  0.503052  8.009859  0.496826  8.110204
62  0.497070  8.106269  0.499146  8.072821
63  0.500244  8.055113  0.497559  8.098399

2017-12-13 20:34:07.552749 Finish.
Total elapsed time: 07:56:33.55.
