2017-12-13 12:37:35.319500: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:35.319743: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:35.319754: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:35.319758: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:35.319763: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 12:37:33.296281 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.974243  0.108159  0.723145  1.300018
1   0.970215  0.117202  0.700073  2.049491
2   0.973877  0.108763  0.769287  0.738958
3   0.977051  0.100049  0.709717  1.569747
4   0.978760  0.085875  0.723022  1.148192
5   0.976318  0.099051  0.715088  1.445960
6   0.974365  0.094919  0.750610  0.895363
7   0.980591  0.081648  0.732178  1.541074
8   0.978760  0.090234  0.775757  0.652040
9   0.981812  0.079817  0.750488  1.362185
10  0.981445  0.076104  0.798340  0.632796
11  0.981567  0.081880  0.785889  0.679988
12  0.979736  0.081440  0.767334  0.902758
13  0.984619  0.069826  0.755127  1.360492
14  0.981445  0.073147  0.769897  0.905231
15  0.981323  0.079297  0.751221  1.216628
16  0.979614  0.080172  0.743530  0.762280
17  0.981934  0.076897  0.746094  1.216713
18  0.982666  0.076220  0.762207  0.905945
19  0.980469  0.079451  0.756226  1.043212
20  0.979004  0.077497  0.746704  0.998339
21  0.983398  0.067200  0.737549  1.271535
22  0.979858  0.081798  0.779297  0.805145
23  0.979126  0.085880  0.746460  1.019424
24  0.983276  0.073435  0.784058  0.845343
25  0.984741  0.066093  0.772095  0.852749
26  0.984863  0.065214  0.738281  1.354574
27  0.983276  0.069777  0.745117  0.943110
28  0.981079  0.072256  0.783691  0.782844
29  0.982056  0.072710  0.794922  0.672088
30  0.982666  0.069825  0.765259  0.852185
31  0.983276  0.066466  0.750366  1.014167
32  0.982422  0.067722  0.737671  1.553060
33  0.983765  0.065378  0.782349  0.805116
34  0.984131  0.063526  0.773193  1.075482
35  0.985596  0.062778  0.755859  0.951131
36  0.987183  0.062165  0.777466  0.899191
37  0.982178  0.070454  0.773560  0.883983
38  0.985474  0.062880  0.782471  0.711564
39  0.984741  0.061515  0.785400  0.753402
40  0.983154  0.070394  0.761963  0.965003
41  0.983032  0.067036  0.764282  1.007992
42  0.984375  0.064939  0.788086  0.921090
43  0.984253  0.065969  0.758545  1.044381
44  0.985962  0.061611  0.774780  0.943567
45  0.984009  0.064202  0.789185  0.755048
46  0.985107  0.063794  0.832886  0.465444
47  0.986084  0.065626  0.783936  0.842514
48  0.983887  0.064164  0.803345  0.729740
49  0.986450  0.060361  0.794922  0.774045
50  0.984741  0.064009  0.777588  1.032314
51  0.983032  0.072573  0.822510  0.529561
52  0.981934  0.067982  0.803101  0.658509
53  0.984375  0.062110  0.815796  0.547341
54  0.987183  0.057005  0.812378  0.553630
55  0.985962  0.057257  0.852295  0.385394
56  0.984985  0.064106  0.794189  0.765906
57  0.985718  0.068226  0.809937  0.521978
58  0.986450  0.056290  0.801514  0.780499
59  0.982788  0.066907  0.798096  0.690470
60  0.983032  0.067103  0.805298  0.824574
61  0.987183  0.054315  0.794556  0.963201
62  0.984741  0.058139  0.804199  0.590889
63  0.985840  0.055663  0.790527  0.839452

2017-12-13 20:34:46.551183 Finish.
Total elapsed time: 07:57:13.55.
