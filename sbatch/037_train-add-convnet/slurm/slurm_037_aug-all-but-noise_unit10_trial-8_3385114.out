2017-12-13 12:38:47.986936: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:38:47.987073: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:38:47.987084: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:38:47.987088: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:38:47.987092: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 12:38:46.094748 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit01, unit02, unit03.
Validation set: unit05, unit07.
Test set: unit10.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.511475  7.788260  0.498169  8.000384
1   0.504395  7.901133  0.498779  7.990653
2   0.498169  8.000384  0.501099  7.953678
3   0.491211  8.111311  0.501343  7.949785
4   0.503662  7.912810  0.496704  8.023737
5   0.494629  8.056820  0.506592  7.866103
6   0.491821  8.101580  0.504883  7.893349
7   0.505371  7.885564  0.497314  8.014006
8   0.508667  7.833020  0.505371  7.885564
9   0.499268  7.982869  0.496948  8.019845
10  0.498291  7.998438  0.500122  7.969246
11  0.498535  7.994545  0.493164  8.080173
12  0.510620  7.801882  0.506470  7.868050
13  0.498779  7.990653  0.507812  7.846642
14  0.495483  8.043198  0.496826  8.021791
15  0.498657  7.992599  0.500122  7.969246
16  0.508057  7.842750  0.496094  8.033467
17  0.509521  7.819397  0.502441  7.932271
18  0.501465  7.947839  0.502197  7.936163
19  0.501465  7.947839  0.502075  7.938109
20  0.505615  7.881672  0.486084  8.193047
21  0.504517  7.899187  0.505859  7.877780
22  0.494385  8.060713  0.493286  8.078227
23  0.497437  8.012060  0.505005  7.891403
24  0.500366  7.965354  0.505737  7.879726
25  0.503174  7.920594  0.499390  7.980923
26  0.506348  7.869996  0.494141  8.064605
27  0.501831  7.942001  0.498901  7.988707
28  0.501587  7.945893  0.496704  8.023737
29  0.506592  7.866103  0.501465  7.947839
30  0.499390  7.980923  0.494629  8.056820
31  0.505737  7.879726  0.504028  7.906971
32  0.489014  8.146341  0.495850  8.037359
33  0.500610  7.961462  0.499634  7.977031
34  0.506104  7.873888  0.500000  7.971192
35  0.498291  7.998438  0.506592  7.866103
36  0.500488  7.963408  0.492065  8.097688
37  0.496704  8.023737  0.500244  7.967300
38  0.501343  7.949785  0.495117  8.049036
39  0.505249  7.887510  0.506226  7.871942
40  0.493530  8.074335  0.504883  7.893349
41  0.497437  8.012060  0.496460  8.027629
42  0.495972  8.035413  0.511841  7.782421
43  0.499268  7.982869  0.500366  7.965354
44  0.503052  7.922540  0.501221  7.951731
45  0.503540  7.914756  0.508911  7.829128
46  0.500244  7.967300  0.495483  8.043198
47  0.513428  7.757122  0.503784  7.910864
48  0.505615  7.881672  0.502075  7.938109
49  0.507935  7.844696  0.501587  7.945893
50  0.505371  7.885564  0.489624  8.136610
51  0.502808  7.926432  0.498169  8.000384
52  0.498291  7.998438  0.500977  7.955624
53  0.499634  7.977031  0.490479  8.122988
54  0.507690  7.848589  0.491089  8.113257
55  0.500000  7.971192  0.512329  7.774637
56  0.499146  7.984815  0.501099  7.953678
57  0.496704  8.023737  0.502197  7.936163
58  0.503174  7.920594  0.503784  7.910864
59  0.511108  7.794098  0.494751  8.054874
60  0.509766  7.815505  0.494995  8.050982
61  0.497192  8.015952  0.502075  7.938109
62  0.493286  8.078227  0.497559  8.010114
63  0.502563  7.930324  0.515991  7.716254

2017-12-13 20:13:50.116088 Finish.
Total elapsed time: 07:35:04.12.
