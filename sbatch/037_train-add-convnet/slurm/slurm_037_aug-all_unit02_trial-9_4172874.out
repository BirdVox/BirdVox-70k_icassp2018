2018-01-19 14:09:29.827806: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-19 14:09:29.828119: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-19 14:09:29.828130: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-01-19 14:09:29.828135: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-19 14:09:29.828139: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-01-19 14:08:00.371079 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.970337  0.108379  0.797729  0.669041
1   0.973633  0.100114  0.699707  1.142169
2   0.971924  0.099733  0.817139  0.687726
3   0.975952  0.089943  0.790894  0.839698
4   0.976929  0.092428  0.796875  0.820977
5   0.980835  0.074334  0.822388  0.701572
6   0.977783  0.090495  0.835815  0.590013
7   0.974121  0.092971  0.838501  0.598877
8   0.978516  0.082354  0.835083  0.620812
9   0.981201  0.075008  0.808716  0.745389
10  0.982422  0.068443  0.803589  0.754779
11  0.977051  0.085460  0.807251  0.860470
12  0.982422  0.071867  0.817871  0.636467
13  0.980469  0.074632  0.802246  0.697363
14  0.980591  0.073835  0.837769  0.670429
15  0.982422  0.068254  0.827881  0.579119
16  0.980591  0.073222  0.849365  0.567544
17  0.982300  0.070998  0.843872  0.601540
18  0.982300  0.067240  0.845581  0.584401
19  0.979614  0.077178  0.819702  0.699076
20  0.981079  0.075617  0.752075  0.999814
21  0.981689  0.070528  0.876099  0.436794
22  0.982910  0.066338  0.839722  0.657340
23  0.983154  0.065915  0.822754  0.755501
24  0.982666  0.062895  0.816650  0.735112
25  0.981445  0.072926  0.803833  0.742939
26  0.984253  0.067235  0.825562  0.697090
27  0.983032  0.068836  0.843018  0.684901
28  0.980713  0.072721  0.824097  0.619127
29  0.984131  0.062784  0.853271  0.538922
30  0.982422  0.068822  0.842163  0.584412
31  0.984741  0.061679  0.798950  0.870429
32  0.981812  0.067870  0.807617  0.897813
33  0.984253  0.062905  0.842773  0.590590
34  0.984375  0.061755  0.846069  0.582995
35  0.984253  0.061731  0.844849  0.578588
36  0.984375  0.058641  0.850708  0.565153
37  0.985840  0.060731  0.850586  0.634517
38  0.986450  0.055005  0.820190  0.755687
39  0.987305  0.055029  0.847290  0.661607
40  0.984619  0.059728  0.861572  0.513642
41  0.982788  0.064087  0.808105  0.838052
42  0.985596  0.057616  0.789673  0.897175
43  0.984985  0.057899  0.820068  0.753033
44  0.984375  0.058994  0.864380  0.567810
45  0.983398  0.062693  0.834839  0.668730
46  0.987915  0.051038  0.848877  0.660489
47  0.983154  0.064114  0.826538  0.690084
48  0.987549  0.049778  0.849365  0.645391
49  0.986694  0.055064  0.852417  0.675045
50  0.987549  0.055693  0.838867  0.709098
51  0.984253  0.058626  0.845825  0.664488
52  0.988647  0.048392  0.861328  0.619723
53  0.987549  0.050691  0.841797  0.694413
54  0.984985  0.057110  0.807251  0.801412
55  0.986328  0.051253  0.861694  0.604612
56  0.988037  0.050054  0.850708  0.653418
57  0.984497  0.063831  0.871338  0.487968
58  0.984741  0.056056  0.810303  0.778581
59  0.987183  0.055373  0.822998  0.793968
60  0.985107  0.057550  0.848145  0.692568
61  0.987427  0.053862  0.826660  0.807047
62  0.984863  0.055645  0.776978  1.057570
63  0.986572  0.054539  0.850708  0.685514

2018-01-19 22:23:58.959746 Finish.
Total elapsed time: 08:15:58.96.
