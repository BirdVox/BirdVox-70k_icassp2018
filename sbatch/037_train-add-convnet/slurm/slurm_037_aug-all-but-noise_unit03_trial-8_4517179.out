2018-02-12 19:59:50.223890: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 19:59:50.224132: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 19:59:50.224145: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 19:59:36.911586 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.972778  0.114171  0.735962  1.170311
1    0.975464  0.107025  0.740723  1.632369
2    0.970947  0.118814  0.709473  2.050546
3    0.979736  0.101378  0.736938  1.628721
4    0.970337  0.112218  0.795532  0.701271
5    0.976196  0.104945  0.749634  1.200438
6    0.975708  0.102924  0.763550  1.134480
7    0.980713  0.087407  0.745605  1.126282
8    0.978394  0.093025  0.758179  1.007780
9    0.979126  0.087690  0.800903  0.862710
10   0.980713  0.088086  0.778931  0.940646
11   0.980713  0.085358  0.753540  1.422640
12   0.978882  0.088434  0.783081  1.165451
13   0.980225  0.088801  0.803711  0.619667
14   0.977417  0.083716  0.795898  0.687802
15   0.979004  0.078253  0.787476  0.847739
16   0.980469  0.085125  0.786987  0.823289
17   0.980347  0.084570  0.818115  0.504355
18   0.979736  0.084361  0.834351  0.400982
19   0.981567  0.076615  0.822998  0.755256
20   0.981445  0.078312  0.790649  0.688114
21   0.982910  0.073863  0.782715  0.995409
22   0.981812  0.080527  0.799438  0.685022
23   0.981201  0.074532  0.804077  0.651433
24   0.980103  0.078933  0.772461  1.285374
25   0.981934  0.072969  0.763306  1.014581
26   0.980713  0.076967  0.775635  1.058543
27   0.983765  0.065439  0.780029  1.003251
28   0.982056  0.071410  0.774048  1.378654
29   0.980835  0.078218  0.774414  1.015619
30   0.983521  0.066479  0.755127  1.431909
31   0.982910  0.071861  0.807739  0.694987
32   0.981812  0.076432  0.749756  1.420312
33   0.983521  0.075322  0.750610  1.230448
34   0.980591  0.075467  0.809570  0.655791
35   0.982788  0.069050  0.858765  0.400514
36   0.985596  0.063295  0.784302  0.945543
37   0.984131  0.068216  0.821167  0.528632
38   0.983765  0.067039  0.791870  0.842680
39   0.984375  0.066175  0.786255  0.839676
40   0.984009  0.066412  0.810303  0.602447
41   0.985352  0.066158  0.788330  0.897080
42   0.983765  0.067859  0.797852  0.820452
43   0.983276  0.067242  0.819946  0.663265
44   0.985718  0.060567  0.773682  1.348553
45   0.981323  0.075984  0.786377  1.063537
46   0.983398  0.066097  0.793701  0.803479
47   0.984619  0.064658  0.801147  0.750931
48   0.985474  0.063785  0.784668  0.956576
49   0.983765  0.068111  0.782104  0.841009
50   0.982788  0.068834  0.801392  0.727534
51   0.984375  0.065463  0.741821  1.451184
52   0.987793  0.053323  0.811890  0.674564
53   0.984131  0.062809  0.788208  0.815842
54   0.983154  0.066833  0.827515  0.616993
55   0.984497  0.062510  0.794556  0.786729
56   0.984375  0.061886  0.809448  0.876038
57   0.984863  0.061396  0.795288  0.903101
58   0.986450  0.060207  0.828613  0.768271
59   0.983765  0.069393  0.819824  0.782073
60   0.988403  0.051285  0.798340  0.805184
61   0.984985  0.058170  0.791016  0.976713
62   0.984863  0.059886  0.776855  1.200716
63   0.984985  0.065448  0.804077  0.811390
64   0.983765  0.072967  0.796021  0.805705
65   0.984131  0.066481  0.870483  0.431340
66   0.985107  0.063295  0.831787  0.609918
67   0.985107  0.061467  0.777344  1.217571
68   0.987549  0.057411  0.775879  1.418831
69   0.987305  0.058206  0.848267  0.531536
70   0.988403  0.050723  0.830688  0.656397
71   0.984253  0.061392  0.829834  0.567851
72   0.989380  0.049116  0.805054  1.025764
73   0.985596  0.061612  0.807251  0.857441
74   0.984863  0.062829  0.842163  0.552449
75   0.985229  0.061620  0.785767  1.175383
76   0.985352  0.059341  0.865845  0.472116
77   0.987183  0.059114  0.817139  0.780415
78   0.987305  0.059638  0.830688  0.605085
79   0.988647  0.048484  0.784058  1.540070
80   0.986450  0.056838  0.767090  1.742156
81   0.988647  0.055331  0.772095  1.134859
82   0.986938  0.053872  0.814331  0.675822
83   0.986450  0.054485  0.806396  0.873975
84   0.984497  0.056853  0.775513  1.389637
85   0.986816  0.061532  0.820801  0.750275
86   0.987915  0.054360  0.813477  0.944410
87   0.987061  0.054505  0.846313  0.525108
88   0.987427  0.053949  0.783325  1.386815
89   0.988037  0.054564  0.798218  1.173976
90   0.985962  0.060814  0.864502  0.475018
91   0.989014  0.050799  0.799805  1.448423
92   0.987671  0.056683  0.822144  0.757328
93   0.985107  0.054281  0.827515  0.784147
94   0.984863  0.061037  0.802734  0.882092
95   0.984863  0.063564  0.781128  0.853226
96   0.987061  0.056788  0.848145  0.497341
97   0.987793  0.049958  0.812378  0.901980
98   0.987915  0.054556  0.818237  0.782999
99   0.985718  0.061959  0.865601  0.421716
100  0.988281  0.051059  0.818604  0.735720
101  0.986694  0.057024  0.789062  0.908511
102  0.988892  0.046352  0.789795  1.181724
103  0.985840  0.053320  0.786255  1.625715
104  0.986572  0.051823  0.795898  1.238439
105  0.989380  0.051285  0.823120  0.805018
106  0.988403  0.050162  0.798218  1.059716
107  0.985840  0.057060  0.790894  1.062827
108  0.985962  0.058542  0.799194  1.328952
109  0.987915  0.054755  0.808105  1.029161
110  0.987061  0.059997  0.814819  0.661981
111  0.988770  0.046837  0.832275  0.806121
112  0.988892  0.046668  0.819214  0.866643
113  0.989258  0.048454  0.799805  1.138160
114  0.987305  0.051507  0.809814  0.912476
115  0.985962  0.053886  0.847656  0.663064
116  0.989014  0.050162  0.844727  0.667990
117  0.989014  0.047772  0.774292  1.557240
118  0.987549  0.049404  0.810181  1.053135
119  0.988892  0.054646  0.810669  0.983841
120  0.987793  0.057331  0.830688  0.740207
121  0.990601  0.045222  0.817139  0.781413
122  0.988770  0.044899  0.824707  0.855958
123  0.986084  0.057370  0.827515  0.907361
124  0.989624  0.047026  0.821411  0.876266
125  0.989136  0.042881  0.811401  1.142778
126  0.986328  0.054890  0.852905  0.662065
127  0.987549  0.054510  0.845459  0.628306

2018-02-13 11:13:24.403176 Finish.
Total elapsed time: 15:13:48.40.
