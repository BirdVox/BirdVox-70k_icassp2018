2018-02-12 19:59:36.752264: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 19:59:36.752657: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 19:59:36.752671: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 19:59:21.746756 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.498901  8.087412  0.497437  8.109074
1    0.504761  7.989524  0.494385  8.155435
2    0.500366  8.058000  0.493774  8.163337
3    0.501221  8.042615  0.499512  8.069538
4    0.501587  8.035611  0.503174  8.009610
5    0.496216  8.121436  0.494751  8.144761
6    0.497803  8.095357  0.503540  8.002693
7    0.506592  7.953362  0.510254  7.894212
8    0.512695  7.854769  0.496460  8.116372
9    0.493652  8.161568  0.503174  8.008049
10   0.510864  7.884058  0.500732  8.047333
11   0.499390  8.068955  0.494385  8.149605
12   0.505371  7.972514  0.492188  8.184998
13   0.491455  8.196796  0.495483  8.131861
14   0.511353  7.876077  0.504395  7.988224
15   0.494995  8.139722  0.503662  8.000025
16   0.500732  8.047245  0.505981  7.962640
17   0.491577  8.194809  0.498535  8.082658
18   0.493530  8.163328  0.501221  8.039216
19   0.499390  8.068886  0.498413  8.084457
20   0.502441  8.019697  0.500244  8.054770
21   0.497437  8.100366  0.502197  8.023284
22   0.500854  8.045275  0.508179  7.926462
23   0.500244  8.055113  0.500732  8.046776
24   0.495361  8.133814  0.496826  8.109884
25   0.502075  8.025600  0.492554  8.178629
26   0.500610  8.049210  0.500977  8.042718
27   0.490479  8.212516  0.510864  7.883392
28   0.495972  8.123977  0.508667  7.918960
29   0.499634  8.064950  0.499390  8.068626
30   0.494263  8.151522  0.499756  8.062761
31   0.495239  8.135782  0.499756  8.062521
32   0.499878  8.061015  0.504883  7.979924
33   0.503906  7.996086  0.496948  8.107710
34   0.501343  8.037405  0.501831  8.029028
35   0.496216  8.120042  0.509155  7.911101
36   0.496460  8.116106  0.497925  8.092128
37   0.509033  7.913450  0.502319  8.021144
38   0.490479  8.212516  0.499146  8.072399
39   0.503418  8.003957  0.505981  7.962161
40   0.507324  7.940995  0.492310  8.182612
41   0.492554  8.179068  0.509277  7.909216
42   0.500732  8.047243  0.493774  8.158857
43   0.503784  7.998054  0.491455  8.196447
44   0.499634  8.064950  0.501465  8.034867
45   0.498291  8.086593  0.487793  8.255146
46   0.500244  8.055113  0.489746  8.224062
47   0.495117  8.137749  0.499268  8.070380
48   0.509033  7.913450  0.500366  8.052635
49   0.491577  8.194808  0.499512  8.066626
50   0.499268  8.070853  0.497681  8.096105
51   0.505859  7.964606  0.506714  7.950296
52   0.493042  8.171198  0.500366  8.052576
53   0.499634  8.064950  0.500488  8.050678
54   0.497314  8.102334  0.513306  7.844233
55   0.489746  8.224321  0.505981  7.962215
56   0.486328  8.279412  0.496094  8.121560
57   0.503784  7.998054  0.496826  8.109684
58   0.505249  7.974444  0.494873  8.141297
59   0.494019  8.155457  0.502686  8.015414
60   0.500488  8.051178  0.498047  8.090218
61   0.493652  8.161360  0.503540  8.001510
62   0.496582  8.114139  0.503906  7.995585
63   0.498779  8.078723  0.495605  8.129446
64   0.506470  7.954768  0.499634  8.064354
65   0.510376  7.891807  0.497437  8.099858
66   0.505859  7.964606  0.499878  8.060582
67   0.487671  8.257769  0.489014  8.235792
68   0.494873  8.141685  0.503662  7.999777
69   0.503906  7.996086  0.490112  8.218145
70   0.488403  8.245964  0.492920  8.172538
71   0.495728  8.127912  0.497070  8.105831
72   0.501831  8.029535  0.499756  8.062600
73   0.495483  8.131847  0.504395  7.987872
74   0.506226  7.958703  0.505005  7.977851
75   0.489624  8.226289  0.499512  8.066501
76   0.501709  8.031502  0.489868  8.222110
77   0.505981  7.962638  0.509399  7.907268
78   0.498657  8.080691  0.487671  8.257374
79   0.506592  7.952801  0.495728  8.127455
80   0.499756  8.062983  0.500000  8.058826
81   0.504883  7.980346  0.503418  8.003525
82   0.499390  8.068886  0.499878  8.060331
83   0.499756  8.062983  0.504761  7.981758
84   0.510742  7.885904  0.497192  8.103983
85   0.508423  7.923287  0.497314  8.101879
86   0.507935  7.931158  0.499023  8.074548
87   0.500366  8.053145  0.504028  7.993766
88   0.498047  8.090528  0.505005  7.977881
89   0.490479  8.212516  0.503906  7.995600
90   0.500610  8.049210  0.500244  8.054513
91   0.499634  8.064950  0.498901  8.076103
92   0.499268  8.070853  0.490723  8.207925
93   0.516113  7.799332  0.500000  8.058450
94   0.500610  8.049210  0.508789  7.916811
95   0.501099  8.041340  0.497925  8.092075
96   0.493652  8.161360  0.500488  8.050446
97   0.509155  7.911482  0.507812  7.932797
98   0.492065  8.186938  0.493530  8.162803
99   0.503052  8.009859  0.501587  8.033234
100  0.494751  8.143652  0.497070  8.105917
101  0.495483  8.131847  0.495972  8.123593
102  0.496460  8.116106  0.494629  8.145150
103  0.500488  8.051178  0.502930  8.011383
104  0.504883  7.980346  0.496704  8.111645
105  0.494995  8.139717  0.506958  7.946313
106  0.497192  8.104301  0.495972  8.123549
107  0.506836  7.948865  0.510254  7.893342
108  0.504761  7.982314  0.488403  8.245502
109  0.505615  7.968541  0.503418  8.003663
110  0.497681  8.096431  0.504517  7.985795
111  0.490479  8.212516  0.502319  8.020905
112  0.507690  7.935093  0.500732  8.046756
113  0.508545  7.921320  0.497803  8.093814
114  0.493652  8.161360  0.495483  8.131350
115  0.504150  7.992151  0.506226  7.958441
116  0.499390  8.068886  0.500854  8.044990
117  0.494019  8.155457  0.499023  8.074628
118  0.503418  8.003957  0.502197  8.023269
119  0.505249  7.974444  0.497070  8.105518
120  0.499023  8.074788  0.499023  8.074500
121  0.498657  8.080691  0.493774  8.158925
122  0.495239  8.135782  0.496704  8.111991
123  0.505005  7.978379  0.502686  8.015300
124  0.497314  8.102334  0.498047  8.090140
125  0.505005  7.978379  0.495117  8.137191
126  0.499878  8.061015  0.489014  8.235867
127  0.505127  7.976411  0.499756  8.062269

2018-02-13 12:36:46.010259 Finish.
Total elapsed time: 16:37:25.01.
