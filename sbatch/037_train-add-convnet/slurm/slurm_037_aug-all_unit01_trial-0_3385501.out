2017-12-13 17:29:28.203359: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:28.203631: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:28.203643: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 17:29:22.887379 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit02, unit03, unit05.
Validation set: unit07, unit10.
Test set: unit01.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.487793  8.255802  0.495239  8.135782
1   0.501343  8.037405  0.499634  8.064950
2   0.499390  8.068886  0.498535  8.082658
3   0.499023  8.074788  0.498901  8.076756
4   0.507202  7.942963  0.489868  8.222354
5   0.497559  8.098399  0.510376  7.891807
6   0.503174  8.007892  0.493896  8.157425
7   0.498657  8.080691  0.492432  8.181035
8   0.494385  8.149555  0.501221  8.039372
9   0.504517  7.986249  0.498779  8.078723
10  0.502075  8.025600  0.500488  8.051178
11  0.502441  8.019697  0.503174  8.007892
12  0.501465  8.035437  0.496826  8.110204
13  0.493042  8.171198  0.496338  8.118074
14  0.503540  8.001989  0.505615  7.968541
15  0.496826  8.110204  0.495972  8.123977
16  0.499268  8.070853  0.496094  8.122009
17  0.495850  8.125944  0.501953  8.027567
18  0.490356  8.214484  0.499390  8.068885
19  0.498779  8.078723  0.505859  7.964606
20  0.492676  8.177100  0.506348  7.956736
21  0.494141  8.153490  0.498413  8.084626
22  0.492310  8.183003  0.497803  8.094464
23  0.498047  8.090528  0.494019  8.155457
24  0.495728  8.127912  0.503296  8.005924
25  0.508423  7.923287  0.507446  7.939028
26  0.495239  8.135782  0.505981  7.962638
27  0.503784  7.998054  0.495239  8.135782
28  0.502441  8.019697  0.501709  8.031502
29  0.491821  8.190873  0.499146  8.072821
30  0.496826  8.110204  0.500000  8.059048
31  0.500977  8.043307  0.501831  8.029535
32  0.510376  7.891807  0.498779  8.078723
33  0.496948  8.108236  0.502808  8.013794
34  0.489136  8.234159  0.501709  8.031502
35  0.501587  8.033470  0.501465  8.035437
36  0.504517  7.986249  0.506348  7.956736
37  0.499878  8.061015  0.497803  8.094464
38  0.501343  8.037405  0.499268  8.070853
39  0.489746  8.224321  0.497559  8.098399
40  0.497314  8.102334  0.505493  7.970508
41  0.498413  8.084626  0.503052  8.009859
42  0.495483  8.131847  0.497559  8.098399
43  0.492310  8.183003  0.510742  7.885904
44  0.501465  8.035437  0.506592  7.952801
45  0.503540  8.001989  0.493408  8.165295
46  0.494751  8.143652  0.503296  8.005924
47  0.494507  8.147587  0.496704  8.112171
48  0.505737  7.966573  0.500000  8.059048
49  0.503784  7.998054  0.502319  8.021665
50  0.503662  8.000022  0.502563  8.017729
51  0.500488  8.051178  0.510742  7.885904
52  0.494263  8.151522  0.495361  8.133814
53  0.505981  7.962638  0.498657  8.080691
54  0.501953  8.027567  0.492554  8.179068
55  0.501587  8.033470  0.500977  8.043307
56  0.504517  7.986249  0.502686  8.015762
57  0.502441  8.019697  0.496338  8.118074
58  0.497925  8.092496  0.501221  8.039372
59  0.500732  8.047243  0.497314  8.102334
60  0.502075  8.025600  0.499878  8.061015
61  0.502808  8.013794  0.497925  8.092496
62  0.502197  8.023632  0.498901  8.076756
63  0.492065  8.186938  0.496704  8.112171

2017-12-14 01:08:33.012855 Finish.
Total elapsed time: 07:39:11.01.
