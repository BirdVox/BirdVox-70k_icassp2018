2018-01-19 14:08:58.682213: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-19 14:08:58.682492: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-19 14:08:58.682506: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-01-19 14:08:58.682512: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-19 14:08:58.682519: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-01-19 14:08:00.371077 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.499146  7.984815  0.498657  7.992599
1   0.501953  7.940055  0.507446  7.852481
2   0.496338  8.029575  0.496582  8.025683
3   0.492310  8.093796  0.501953  7.940055
4   0.493774  8.070443  0.492920  8.084066
5   0.503662  7.912810  0.503418  7.916702
6   0.504761  7.895295  0.511597  7.786314
7   0.494507  8.058766  0.496582  8.025683
8   0.497437  8.012060  0.499268  7.982869
9   0.496826  8.021791  0.495483  8.043198
10  0.500000  7.971192  0.493530  8.074335
11  0.501587  7.945893  0.499146  7.984815
12  0.492798  8.086012  0.501587  7.945893
13  0.499146  7.984815  0.497681  8.008168
14  0.497925  8.004276  0.500488  7.963408
15  0.497314  8.014006  0.502319  7.934217
16  0.490234  8.126880  0.501221  7.951731
17  0.500488  7.963408  0.504517  7.899187
18  0.513550  7.755176  0.494019  8.066551
19  0.504150  7.905025  0.504883  7.893349
20  0.506104  7.873888  0.498779  7.990653
21  0.497192  8.015952  0.493164  8.080173
22  0.497070  8.017899  0.493164  8.080173
23  0.490356  8.124934  0.496460  8.027629
24  0.501709  7.943947  0.500366  7.965354
25  0.503906  7.908917  0.502075  7.938109
26  0.505005  7.891403  0.501099  7.953678
27  0.498535  7.994545  0.493286  8.078227
28  0.503906  7.908917  0.509399  7.821343
29  0.496338  8.029575  0.499634  7.977031
30  0.505981  7.875834  0.506104  7.873888
31  0.503418  7.916702  0.502197  7.936163
32  0.503174  7.920594  0.503052  7.922540
33  0.503662  7.912810  0.503662  7.912810
34  0.496460  8.027629  0.488403  8.156071
35  0.489624  8.136610  0.496948  8.019845
36  0.501343  7.949785  0.504639  7.897241
37  0.495605  8.041252  0.493408  8.076281
38  0.509277  7.823289  0.499268  7.982869
39  0.492310  8.093796  0.502686  7.928378
40  0.509888  7.813559  0.501831  7.942001
41  0.492065  8.097688  0.498413  7.996492
42  0.501709  7.943947  0.498291  7.998438
43  0.498047  8.002330  0.496094  8.033467
44  0.501465  7.947839  0.500732  7.959516
45  0.501953  7.940055  0.491089  8.113257
46  0.502686  7.928378  0.497314  8.014006
47  0.507568  7.850535  0.493530  8.074335
48  0.497314  8.014006  0.496826  8.021791
49  0.506714  7.864157  0.494629  8.056820
50  0.493652  8.072389  0.501587  7.945893
51  0.502319  7.934217  0.503418  7.916702
52  0.506348  7.869996  0.496338  8.029575
53  0.498657  7.992599  0.487305  8.173586
54  0.491821  8.101580  0.503784  7.910864
55  0.500854  7.957570  0.491455  8.107419
56  0.495605  8.041252  0.494141  8.064605
57  0.502075  7.938109  0.504272  7.903079
58  0.498413  7.996492  0.500244  7.967300
59  0.504028  7.906971  0.490234  8.126880
60  0.504272  7.903079  0.503540  7.914756
61  0.509155  7.825235  0.503662  7.912810
62  0.497681  8.008168  0.499023  7.986761
63  0.494995  8.050982  0.487427  8.171640

2018-01-19 22:25:58.847061 Finish.
Total elapsed time: 08:17:58.85.
