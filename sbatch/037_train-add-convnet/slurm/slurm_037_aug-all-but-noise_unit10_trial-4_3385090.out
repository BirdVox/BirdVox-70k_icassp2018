2017-12-13 12:37:36.012166: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:36.012334: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:36.012344: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:36.012349: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:36.012353: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 12:37:34.076936 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit01, unit02, unit03.
Validation set: unit05, unit07.
Test set: unit10.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.510864  7.797990  0.501221  7.951731
1   0.503296  7.918648  0.505859  7.877780
2   0.503174  7.920594  0.498169  8.000384
3   0.500122  7.969246  0.502197  7.936163
4   0.503784  7.910864  0.499512  7.978977
5   0.499512  7.978977  0.487427  8.171640
6   0.500732  7.959516  0.503174  7.920594
7   0.511108  7.794098  0.501953  7.940055
8   0.498169  8.000384  0.503296  7.918648
9   0.500977  7.955624  0.498779  7.990653
10  0.493286  8.078227  0.497925  8.004276
11  0.501221  7.951731  0.502197  7.936163
12  0.497559  8.010114  0.501465  7.947839
13  0.501099  7.953678  0.506348  7.869996
14  0.495850  8.037359  0.510742  7.799936
15  0.503540  7.914756  0.505371  7.885564
16  0.499634  7.977031  0.504517  7.899187
17  0.500366  7.965354  0.500000  7.971192
18  0.491821  8.101581  0.503418  7.916702
19  0.498779  7.990653  0.502441  7.932271
20  0.492188  8.095742  0.499756  7.975085
21  0.501587  7.945893  0.493652  8.072389
22  0.488770  8.150233  0.506714  7.864157
23  0.507202  7.856373  0.497192  8.015952
24  0.505005  7.891403  0.492432  8.091850
25  0.495483  8.043198  0.489380  8.140502
26  0.499756  7.975085  0.501831  7.942001
27  0.493164  8.080173  0.504150  7.905025
28  0.506958  7.860265  0.496948  8.019845
29  0.498291  7.998438  0.497314  8.014006
30  0.502686  7.928378  0.496460  8.027629
31  0.500244  7.967300  0.505371  7.885564
32  0.499634  7.977031  0.495972  8.035413
33  0.503174  7.920594  0.491577  8.105473
34  0.494019  8.066551  0.492554  8.089904
35  0.496948  8.019845  0.501953  7.940055
36  0.504883  7.893349  0.489014  8.146341
37  0.499512  7.978977  0.507080  7.858319
38  0.512695  7.768799  0.496460  8.027629
39  0.495483  8.043198  0.496460  8.027629
40  0.495605  8.041252  0.498047  8.002330
41  0.495117  8.049036  0.503784  7.910864
42  0.503906  7.908917  0.500854  7.957570
43  0.492188  8.095742  0.498291  7.998438
44  0.508057  7.842750  0.493286  8.078227
45  0.497559  8.010114  0.500488  7.963408
46  0.503540  7.914756  0.502563  7.930324
47  0.506836  7.862211  0.496460  8.027629
48  0.506348  7.869996  0.504639  7.897241
49  0.503784  7.910864  0.494873  8.052928
50  0.509399  7.821343  0.507324  7.854427
51  0.498413  7.996492  0.503174  7.920594
52  0.499023  7.986761  0.497192  8.015952
53  0.499878  7.973138  0.503784  7.910864
54  0.497437  8.012060  0.506348  7.869996
55  0.507935  7.844696  0.502075  7.938109
56  0.497559  8.010114  0.502441  7.932271
57  0.497681  8.008168  0.503906  7.908917
58  0.494995  8.050982  0.502319  7.934217
59  0.493408  8.076281  0.501709  7.943947
60  0.507568  7.850535  0.496704  8.023737
61  0.501587  7.945893  0.505371  7.885564
62  0.502686  7.928378  0.503418  7.916702
63  0.504639  7.897241  0.503174  7.920594

2017-12-13 20:48:57.558268 Finish.
Total elapsed time: 08:11:23.56.
