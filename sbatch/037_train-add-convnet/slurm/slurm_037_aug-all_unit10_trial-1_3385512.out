2017-12-13 17:29:28.731226: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:28.731568: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:28.731580: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 17:29:22.372388 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit01, unit02, unit03.
Validation set: unit05, unit07.
Test set: unit10.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.948608  0.166622  0.943359  0.192713
1   0.957153  0.148758  0.937866  0.204862
2   0.959106  0.141223  0.948242  0.178420
3   0.957031  0.145121  0.959717  0.161256
4   0.962769  0.135857  0.904663  0.241559
5   0.959961  0.140463  0.922852  0.211753
6   0.961182  0.139144  0.902466  0.232222
7   0.963379  0.124754  0.928711  0.201445
8   0.962646  0.135215  0.934814  0.191097
9   0.964111  0.123693  0.945801  0.208087
10  0.964722  0.126361  0.941650  0.185479
11  0.967285  0.123547  0.948486  0.165639
12  0.967529  0.116470  0.966064  0.139325
13  0.965088  0.120313  0.933228  0.196747
14  0.969482  0.107842  0.944946  0.173582
15  0.970947  0.114313  0.950317  0.171688
16  0.969482  0.116356  0.895142  0.238013
17  0.971191  0.106684  0.896606  0.234158
18  0.970947  0.110377  0.946777  0.185053
19  0.965576  0.119109  0.927368  0.195221
20  0.972290  0.107025  0.870239  0.276528
21  0.966309  0.115749  0.943359  0.177852
22  0.969604  0.104530  0.957397  0.144377
23  0.973999  0.098176  0.936035  0.173773
24  0.969360  0.107581  0.904419  0.240133
25  0.973145  0.097792  0.942871  0.184870
26  0.970215  0.107940  0.912109  0.215337
27  0.972778  0.105496  0.944580  0.181869
28  0.973633  0.097676  0.890991  0.249308
29  0.972778  0.104266  0.837158  0.349252
30  0.973633  0.095156  0.909546  0.216883
31  0.973022  0.099630  0.931885  0.183831
32  0.970459  0.099291  0.825439  0.365800
33  0.973145  0.095402  0.919312  0.206735
34  0.970947  0.101825  0.902466  0.233209
35  0.975464  0.089731  0.912354  0.211396
36  0.978394  0.080782  0.779541  0.485141
37  0.973633  0.099444  0.872192  0.293790
38  0.973389  0.098924  0.866943  0.306303
39  0.975586  0.093958  0.905518  0.229280
40  0.979248  0.087191  0.930542  0.185215
41  0.976685  0.086884  0.881348  0.277586
42  0.978394  0.085687  0.886230  0.258194
43  0.975830  0.088375  0.850098  0.354666
44  0.972412  0.094033  0.919922  0.189671
45  0.979614  0.082758  0.930420  0.178741
46  0.979614  0.082383  0.908691  0.216895
47  0.974243  0.095070  0.844971  0.363002
48  0.979370  0.080744  0.935181  0.174156
49  0.978638  0.085446  0.900635  0.237664
50  0.975220  0.092490  0.900879  0.234357
51  0.979370  0.084923  0.914551  0.211877
52  0.980103  0.084909  0.835571  0.417367
53  0.976929  0.086377  0.908569  0.233236
54  0.976318  0.090228  0.901978  0.252991
55  0.977417  0.086119  0.850220  0.379401
56  0.978882  0.080021  0.878296  0.305680
57  0.977783  0.086907  0.826172  0.390732
58  0.978149  0.086897  0.904541  0.222834
59  0.978882  0.079707  0.897095  0.261489
60  0.979126  0.078278  0.898315  0.243351
61  0.977905  0.091718  0.897827  0.247627
62  0.982178  0.069893  0.877197  0.331685
63  0.979126  0.077531  0.829834  0.495179

2017-12-14 01:26:27.148101 Finish.
Total elapsed time: 07:57:05.15.
