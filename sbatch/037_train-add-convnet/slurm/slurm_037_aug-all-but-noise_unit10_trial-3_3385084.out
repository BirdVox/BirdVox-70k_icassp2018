2017-12-13 12:37:34.988305: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:34.988449: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:34.988461: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:34.988465: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:34.988469: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 12:37:33.218438 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit01, unit02, unit03.
Validation set: unit05, unit07.
Test set: unit10.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.497803  8.094464  0.500366  8.053145
1   0.506714  7.950833  0.496826  8.110204
2   0.505005  7.978379  0.490601  8.210548
3   0.496216  8.120042  0.500854  8.045275
4   0.503052  8.009859  0.488892  8.238094
5   0.491089  8.202678  0.504883  7.980346
6   0.507812  7.933125  0.497192  8.104301
7   0.495239  8.135782  0.500488  8.051178
8   0.509888  7.899677  0.513062  7.848521
9   0.498657  8.080691  0.492310  8.183003
10  0.497559  8.098399  0.494873  8.141684
11  0.504395  7.988216  0.499512  8.066918
12  0.505127  7.976411  0.498413  8.084626
13  0.499756  8.062983  0.510132  7.895742
14  0.507690  7.935093  0.505371  7.972476
15  0.499512  8.066918  0.499878  8.061015
16  0.499634  8.064950  0.501831  8.029535
17  0.494141  8.153490  0.507202  7.942963
18  0.497314  8.102334  0.505249  7.974444
19  0.495483  8.131847  0.488159  8.249899
20  0.515381  7.811138  0.501221  8.039372
21  0.488892  8.238094  0.501343  8.037405
22  0.502319  8.021664  0.500488  8.051178
23  0.497437  8.100366  0.498657  8.080691
24  0.498047  8.090528  0.489136  8.234159
25  0.504150  7.992151  0.496704  8.112171
26  0.499146  8.072821  0.497192  8.104301
27  0.500122  8.057080  0.489258  8.232191
28  0.514404  7.826878  0.492920  8.173165
29  0.489624  8.226289  0.491211  8.200711
30  0.506592  7.952801  0.498047  8.090528
31  0.505737  7.966573  0.504639  7.984281
32  0.499512  8.066918  0.506836  7.948865
33  0.501587  8.033470  0.495483  8.131847
34  0.505859  7.964606  0.499756  8.062983
35  0.500977  8.043307  0.511841  7.868196
36  0.509888  7.899677  0.508423  7.923287
37  0.502197  8.023632  0.500000  8.059048
38  0.496338  8.118074  0.501831  8.029535
39  0.494263  8.151522  0.502197  8.023632
40  0.495239  8.135782  0.497925  8.092496
41  0.499756  8.062983  0.502930  8.011827
42  0.493408  8.165295  0.496216  8.120042
43  0.500488  8.051178  0.488037  8.251867
44  0.492920  8.173165  0.489990  8.220386
45  0.507202  7.942963  0.499878  8.061015
46  0.500488  8.051178  0.503052  8.009859
47  0.495483  8.131847  0.492920  8.173165
48  0.502930  8.011827  0.505859  7.964606
49  0.491577  8.194808  0.516602  7.791462
50  0.497437  8.100366  0.509155  7.911482
51  0.498901  8.076756  0.505371  7.972476
52  0.493408  8.165295  0.502563  8.017729
53  0.507202  7.942963  0.492798  8.175133
54  0.498535  8.082658  0.492432  8.181035
55  0.494507  8.147587  0.490112  8.218419
56  0.499756  8.062983  0.506714  7.950833
57  0.496460  8.116106  0.496948  8.108236
58  0.503540  8.001989  0.507080  7.944930
59  0.492432  8.181035  0.493286  8.167263
60  0.500488  8.051178  0.502930  8.011827
61  0.504517  7.986249  0.502686  8.015762
62  0.505371  7.972476  0.489868  8.222354
63  0.502075  8.025600  0.497437  8.100366

2017-12-13 20:20:57.783375 Finish.
Total elapsed time: 07:43:24.78.
