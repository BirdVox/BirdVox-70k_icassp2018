2017-12-13 12:37:36.415032: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:36.415317: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:36.415328: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:36.415333: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:36.415338: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 12:37:34.264887 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit07, unit10, unit01.
Validation set: unit02, unit03.
Test set: unit05.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.956787  0.145182  0.931763  0.243004
1   0.957764  0.146125  0.955078  0.159934
2   0.961548  0.127668  0.937500  0.279004
3   0.959106  0.130045  0.932739  0.272834
4   0.962280  0.135318  0.935791  0.311285
5   0.965942  0.119190  0.929443  0.326429
6   0.966675  0.119017  0.931641  0.309389
7   0.965942  0.121835  0.949829  0.213048
8   0.965088  0.120416  0.959717  0.165910
9   0.964233  0.123288  0.956665  0.219275
10  0.970337  0.109452  0.915161  0.432767
11  0.967773  0.111416  0.946411  0.243105
12  0.965576  0.115291  0.956421  0.171985
13  0.964966  0.116017  0.943848  0.257533
14  0.969849  0.115420  0.932495  0.297695
15  0.970703  0.110915  0.954712  0.206190
16  0.968872  0.109883  0.926392  0.369016
17  0.970947  0.098343  0.942017  0.272372
18  0.966309  0.117291  0.942261  0.247453
19  0.971191  0.108209  0.933716  0.299965
20  0.968262  0.098004  0.956177  0.209589
21  0.968384  0.108461  0.957031  0.211954
22  0.970581  0.106644  0.941040  0.293385
23  0.971924  0.097888  0.943359  0.294366
24  0.972778  0.095859  0.936768  0.334633
25  0.972778  0.100273  0.957031  0.198846
26  0.969604  0.104265  0.962646  0.152543
27  0.976440  0.094975  0.955444  0.205332
28  0.974854  0.093550  0.952393  0.213103
29  0.971436  0.101080  0.945801  0.256533
30  0.972534  0.099345  0.956055  0.192864
31  0.975586  0.097427  0.941650  0.284334
32  0.974365  0.097696  0.935059  0.314295
33  0.971191  0.104996  0.950073  0.221232
34  0.973877  0.097564  0.947021  0.257688
35  0.976440  0.093895  0.938599  0.312602
36  0.971558  0.103306  0.939331  0.285009
37  0.971191  0.097986  0.940796  0.269280
38  0.969482  0.106310  0.933228  0.287156
39  0.973755  0.096142  0.940308  0.288225
40  0.973999  0.092498  0.938721  0.353839
41  0.977539  0.086582  0.958374  0.201742
42  0.974609  0.092840  0.948608  0.232908
43  0.973999  0.094518  0.939331  0.278678
44  0.975098  0.093741  0.947632  0.232784
45  0.971191  0.099843  0.958618  0.174564
46  0.975708  0.092268  0.946899  0.288978
47  0.973999  0.092413  0.957764  0.195380
48  0.971802  0.096564  0.955811  0.209566
49  0.971924  0.097430  0.949097  0.270930
50  0.975586  0.086569  0.950195  0.217109
51  0.977539  0.083912  0.925903  0.416856
52  0.972046  0.097162  0.961182  0.160975
53  0.975952  0.093650  0.937622  0.304670
54  0.977051  0.084390  0.942627  0.257099
55  0.974976  0.090914  0.957153  0.203042
56  0.975220  0.099436  0.947876  0.264281
57  0.972290  0.093039  0.942993  0.278012
58  0.974731  0.089969  0.944092  0.289589
59  0.973755  0.101512  0.947266  0.245677
60  0.976929  0.087021  0.942261  0.303327
61  0.976196  0.088941  0.945435  0.271611
62  0.974487  0.091054  0.939575  0.351412
63  0.973999  0.089910  0.961914  0.178540

2017-12-13 20:30:13.423663 Finish.
Total elapsed time: 07:52:39.42.
