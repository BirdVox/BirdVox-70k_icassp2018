2018-02-12 19:59:42.140329: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 19:59:42.140628: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 19:59:42.140641: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 19:59:24.279801 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.507080  7.858319  0.491455  8.107419
1    0.502808  7.926432  0.502197  7.936163
2    0.495605  8.041252  0.501831  7.942001
3    0.492065  8.097688  0.502441  7.932271
4    0.503174  7.920594  0.506226  7.871942
5    0.499878  7.973138  0.494019  8.066551
6    0.494385  8.060713  0.498047  8.002330
7    0.501465  7.947839  0.494019  8.066551
8    0.501953  7.940055  0.491943  8.099634
9    0.501343  7.949785  0.497070  8.017899
10   0.502319  7.934217  0.495728  8.039306
11   0.495605  8.041252  0.499023  7.986761
12   0.495239  8.047090  0.512085  7.778529
13   0.502075  7.938109  0.507690  7.848589
14   0.495605  8.041252  0.503052  7.922540
15   0.511353  7.790206  0.500488  7.963408
16   0.495972  8.035413  0.498901  7.988707
17   0.495728  8.039306  0.508789  7.831074
18   0.494751  8.054874  0.497559  8.010114
19   0.489502  8.138556  0.500244  7.967300
20   0.494507  8.058766  0.489380  8.140502
21   0.495483  8.043198  0.501465  7.947839
22   0.496338  8.029575  0.497070  8.017899
23   0.506592  7.866103  0.493164  8.080173
24   0.500366  7.965354  0.503662  7.912810
25   0.494995  8.050982  0.515625  7.722093
26   0.500977  7.955624  0.503662  7.912810
27   0.498413  7.996492  0.507690  7.848589
28   0.502441  7.932271  0.502930  7.924486
29   0.506226  7.871942  0.499756  7.975085
30   0.507446  7.852481  0.502197  7.936163
31   0.502075  7.938109  0.506226  7.871942
32   0.500854  7.957570  0.503418  7.916702
33   0.491333  8.109365  0.501343  7.949785
34   0.492554  8.089904  0.507324  7.854427
35   0.500000  7.971192  0.494751  8.054874
36   0.499390  7.980923  0.485229  8.206669
37   0.498779  7.990653  0.505127  7.889457
38   0.494507  8.058766  0.505981  7.875834
39   0.502319  7.934217  0.509644  7.817451
40   0.502808  7.926432  0.502930  7.924486
41   0.507080  7.858319  0.494995  8.050982
42   0.488892  8.148287  0.504272  7.903079
43   0.505737  7.879726  0.508057  7.842750
44   0.492310  8.093796  0.493408  8.076281
45   0.493652  8.072389  0.503784  7.910864
46   0.491699  8.103527  0.500488  7.963408
47   0.502930  7.924486  0.492310  8.093796
48   0.503540  7.914756  0.501465  7.947839
49   0.506958  7.860265  0.502319  7.934217
50   0.501831  7.942001  0.498535  7.994545
51   0.496094  8.033467  0.496094  8.033467
52   0.500244  7.967300  0.506836  7.862211
53   0.496094  8.033467  0.494873  8.052928
54   0.496826  8.021791  0.499146  7.984815
55   0.497803  8.006222  0.501221  7.951731
56   0.498535  7.994545  0.501709  7.943947
57   0.508911  7.829128  0.496826  8.021791
58   0.487305  8.173586  0.505249  7.887510
59   0.502319  7.934217  0.501465  7.947839
60   0.501343  7.949785  0.492432  8.091850
61   0.496338  8.029575  0.499634  7.977031
62   0.496704  8.023737  0.491211  8.111311
63   0.504272  7.903079  0.508789  7.831074
64   0.497925  8.004276  0.502075  7.938109
65   0.503662  7.912810  0.501953  7.940055
66   0.497681  8.008168  0.494263  8.062659
67   0.509888  7.813559  0.503296  7.918648
68   0.495483  8.043198  0.498413  7.996492
69   0.496094  8.033467  0.499634  7.977031
70   0.492188  8.095742  0.503052  7.922540
71   0.507202  7.856373  0.494263  8.062659
72   0.505859  7.877780  0.509766  7.815505
73   0.502563  7.930324  0.513672  7.753230
74   0.503296  7.918648  0.494141  8.064605
75   0.508179  7.840804  0.498047  8.002330
76   0.496460  8.027629  0.512939  7.764907
77   0.516479  7.708470  0.492676  8.087958
78   0.503174  7.920594  0.492188  8.095742
79   0.492432  8.091850  0.506226  7.871942
80   0.501587  7.945893  0.497559  8.010114
81   0.496704  8.023737  0.504272  7.903079
82   0.502930  7.924486  0.505615  7.881672
83   0.499512  7.978977  0.500244  7.967300
84   0.499023  7.986761  0.502319  7.934217
85   0.484863  8.212508  0.492676  8.087958
86   0.497314  8.014006  0.502441  7.932271
87   0.504883  7.893349  0.504639  7.897241
88   0.498779  7.990653  0.495361  8.045144
89   0.495728  8.039306  0.493652  8.072389
90   0.489258  8.142448  0.496826  8.021791
91   0.502563  7.930324  0.505249  7.887510
92   0.493042  8.082120  0.500488  7.963408
93   0.505981  7.875834  0.497070  8.017899
94   0.492920  8.084066  0.502075  7.938109
95   0.503418  7.916702  0.505493  7.883618
96   0.493164  8.080173  0.507690  7.848589
97   0.501099  7.953678  0.502197  7.936163
98   0.494507  8.058766  0.504272  7.903079
99   0.502563  7.930324  0.502563  7.930324
100  0.507690  7.848589  0.506958  7.860265
101  0.503784  7.910864  0.509521  7.819397
102  0.497192  8.015952  0.506470  7.868049
103  0.495239  8.047090  0.495850  8.037359
104  0.495972  8.035413  0.499634  7.977031
105  0.499634  7.977031  0.497559  8.010114
106  0.492676  8.087958  0.492920  8.084066
107  0.501099  7.953678  0.498047  8.002330
108  0.507690  7.848589  0.501831  7.942001
109  0.493408  8.076281  0.501587  7.945893
110  0.503174  7.920594  0.495850  8.037359
111  0.501465  7.947839  0.491333  8.109365
112  0.498535  7.994545  0.497192  8.015952
113  0.507446  7.852481  0.500366  7.965354
114  0.501831  7.942001  0.499878  7.973138
115  0.501343  7.949785  0.505493  7.883618
116  0.502441  7.932271  0.494751  8.054874
117  0.488403  8.156071  0.506470  7.868049
118  0.507446  7.852481  0.501343  7.949785
119  0.510254  7.807721  0.505249  7.887510
120  0.494385  8.060713  0.507324  7.854427
121  0.486816  8.181370  0.499390  7.980923
122  0.494507  8.058766  0.500732  7.959516
123  0.487671  8.167748  0.494629  8.056820
124  0.498047  8.002330  0.499268  7.982869
125  0.504028  7.906971  0.497559  8.010114
126  0.489624  8.136610  0.491089  8.113257
127  0.496460  8.027629  0.495239  8.047090

2018-02-13 12:41:01.169619 Finish.
Total elapsed time: 16:41:37.17.
