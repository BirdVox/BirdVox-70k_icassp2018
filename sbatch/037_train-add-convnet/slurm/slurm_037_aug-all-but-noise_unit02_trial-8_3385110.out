2017-12-13 12:38:35.537736: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:38:35.537909: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:38:35.537920: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:38:35.537924: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:38:35.537929: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 12:38:33.176680 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.971191  0.100144  0.798950  0.554012
1   0.975952  0.097284  0.766235  0.904469
2   0.978149  0.090970  0.768188  0.701390
3   0.976685  0.087063  0.784302  0.584676
4   0.979736  0.080152  0.776001  0.525799
5   0.980225  0.076337  0.769287  0.601928
6   0.982056  0.075553  0.776001  0.556186
7   0.980225  0.078066  0.776611  0.534886
8   0.981201  0.075912  0.805908  0.521780
9   0.981934  0.069999  0.787720  0.522339
10  0.982910  0.066242  0.781250  0.561704
11  0.983154  0.064600  0.763672  0.716898
12  0.984131  0.065803  0.765259  0.522808
13  0.980591  0.073391  0.775879  0.541042
14  0.983887  0.065091  0.761841  0.659500
15  0.985107  0.057938  0.793335  0.583418
16  0.985474  0.061824  0.785645  0.541529
17  0.985474  0.057421  0.785889  0.603881
18  0.983765  0.063884  0.766357  0.626817
19  0.984863  0.063236  0.791748  0.543308
20  0.986328  0.059892  0.800049  0.523095
21  0.987549  0.058377  0.780029  0.564069
22  0.985840  0.061211  0.793579  0.468112
23  0.985229  0.060442  0.758545  0.948394
24  0.983276  0.068913  0.761108  0.519012
25  0.986084  0.054961  0.776733  0.550424
26  0.985229  0.060981  0.770386  0.600521
27  0.985352  0.060365  0.771729  0.525710
28  0.986694  0.054926  0.783936  0.649994
29  0.988159  0.051481  0.752930  0.790618
30  0.988037  0.050783  0.805176  0.566913
31  0.984741  0.058377  0.703125  0.632147
32  0.984497  0.060837  0.782593  0.543744
33  0.985596  0.058895  0.783691  0.579311
34  0.988281  0.048539  0.803345  0.671767
35  0.987305  0.056471  0.801392  0.522254
36  0.987061  0.053022  0.786865  0.636814
37  0.984741  0.058912  0.810669  0.494465
38  0.987061  0.054916  0.792603  0.614496
39  0.987793  0.051380  0.787354  0.693356
40  0.985596  0.056682  0.814087  0.648784
41  0.988892  0.049147  0.791016  0.683014
42  0.987183  0.052088  0.788940  0.706119
43  0.988159  0.052172  0.788452  0.689201
44  0.987183  0.051868  0.770996  0.728886
45  0.988281  0.050801  0.773315  0.725146
46  0.989868  0.046065  0.779053  0.808138
47  0.988281  0.049418  0.751587  1.141094
48  0.987061  0.054784  0.786621  0.749771
49  0.987183  0.051534  0.781250  0.643928
50  0.987061  0.054148  0.801514  0.604698
51  0.989380  0.045762  0.785278  0.614054
52  0.988770  0.049108  0.798706  0.699562
53  0.985962  0.057254  0.791504  0.630357
54  0.988525  0.053368  0.778564  0.736049
55  0.989136  0.047424  0.761719  0.785295
56  0.989136  0.049745  0.785156  0.939092
57  0.989502  0.046894  0.754883  1.087793
58  0.985840  0.057834  0.824463  0.505350
59  0.988281  0.048424  0.798096  0.817196
60  0.989746  0.045367  0.798340  0.733896
61  0.987427  0.054940  0.774292  0.866526
62  0.985962  0.050499  0.794800  0.694460
63  0.990356  0.050160  0.813721  0.569522

2017-12-13 20:41:51.127882 Finish.
Total elapsed time: 08:03:18.13.
