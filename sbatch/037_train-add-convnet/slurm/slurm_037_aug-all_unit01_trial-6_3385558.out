2017-12-13 17:29:28.115222: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:28.115519: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:28.115531: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 17:29:21.539137 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit02, unit03, unit05.
Validation set: unit07, unit10.
Test set: unit01.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.502197  7.936163  0.491577  8.105473
1   0.505493  7.883618  0.503784  7.910864
2   0.489746  8.134664  0.501221  7.951731
3   0.498535  7.994545  0.499390  7.980923
4   0.490356  8.124934  0.497070  8.017899
5   0.503296  7.918648  0.494263  8.062659
6   0.489014  8.146341  0.498901  7.988707
7   0.505371  7.885564  0.492554  8.089904
8   0.487793  8.165802  0.507690  7.848589
9   0.488647  8.152179  0.491943  8.099634
10  0.492065  8.097688  0.504028  7.906971
11  0.508789  7.831074  0.505615  7.881672
12  0.511719  7.784368  0.490967  8.115203
13  0.498779  7.990653  0.500610  7.961462
14  0.507690  7.848589  0.497925  8.004276
15  0.505737  7.879726  0.491333  8.109365
16  0.500610  7.961462  0.504639  7.897241
17  0.501099  7.953678  0.495361  8.045144
18  0.502930  7.924486  0.493774  8.070443
19  0.494385  8.060713  0.486328  8.189155
20  0.514282  7.743500  0.509766  7.815505
21  0.503784  7.910864  0.498657  7.992599
22  0.490601  8.121041  0.503906  7.908917
23  0.503784  7.910864  0.505005  7.891403
24  0.506592  7.866103  0.491577  8.105473
25  0.500000  7.971192  0.499146  7.984815
26  0.499634  7.977031  0.497192  8.015952
27  0.502441  7.932271  0.493896  8.068497
28  0.503540  7.914756  0.493530  8.074335
29  0.504028  7.906971  0.496582  8.025683
30  0.502441  7.932271  0.505005  7.891403
31  0.494385  8.060713  0.503418  7.916702
32  0.501221  7.951731  0.497437  8.012060
33  0.496826  8.021791  0.518188  7.681225
34  0.505981  7.875834  0.496094  8.033467
35  0.504272  7.903079  0.510620  7.801882
36  0.493652  8.072389  0.509033  7.827182
37  0.496582  8.025683  0.495361  8.045144
38  0.503418  7.916702  0.488892  8.148287
39  0.503418  7.916702  0.500732  7.959516
40  0.498535  7.994545  0.505005  7.891403
41  0.501831  7.942001  0.486450  8.187209
42  0.496338  8.029575  0.497681  8.008168
43  0.496582  8.025683  0.493408  8.076281
44  0.501465  7.947839  0.502563  7.930324
45  0.503784  7.910864  0.505493  7.883618
46  0.509644  7.817451  0.496094  8.033467
47  0.501221  7.951731  0.498657  7.992599
48  0.500366  7.965354  0.500610  7.961462
49  0.505615  7.881672  0.506104  7.873888
50  0.505127  7.889457  0.492065  8.097688
51  0.493408  8.076281  0.498291  7.998438
52  0.505005  7.891403  0.493164  8.080173
53  0.497314  8.014006  0.499023  7.986761
54  0.499756  7.975085  0.503418  7.916702
55  0.501343  7.949785  0.493408  8.076281
56  0.494873  8.052928  0.502075  7.938109
57  0.503540  7.914756  0.497314  8.014006
58  0.504517  7.899187  0.496826  8.021791
59  0.499512  7.978977  0.496582  8.025683
60  0.500732  7.959516  0.494629  8.056820
61  0.495361  8.045144  0.491577  8.105473
62  0.505005  7.891403  0.502319  7.934217
63  0.497803  8.006222  0.497437  8.012060

2017-12-14 01:25:27.988137 Finish.
Total elapsed time: 07:56:06.99.
