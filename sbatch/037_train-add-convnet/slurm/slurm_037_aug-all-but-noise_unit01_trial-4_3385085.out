2017-12-13 12:37:36.251008: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:36.251202: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:36.251211: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:36.251215: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:36.251219: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 12:37:33.885465 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit02, unit03, unit05.
Validation set: unit07, unit10.
Test set: unit01.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.978638  0.086227  0.964355  0.146542
1   0.983032  0.070693  0.836548  0.415919
2   0.977661  0.083373  0.926147  0.223473
3   0.979492  0.080289  0.935059  0.200101
4   0.982056  0.074267  0.847046  0.382984
5   0.984131  0.066944  0.942017  0.187315
6   0.983521  0.069887  0.965576  0.142685
7   0.983521  0.065570  0.910522  0.244398
8   0.982666  0.067104  0.878906  0.316174
9   0.983521  0.068188  0.866211  0.330434
10  0.985840  0.058917  0.896729  0.256792
11  0.986206  0.060381  0.941162  0.189950
12  0.986938  0.060988  0.828613  0.453379
13  0.985352  0.061765  0.886963  0.304535
14  0.984497  0.064624  0.867920  0.362172
15  0.986450  0.061552  0.905518  0.252495
16  0.988525  0.051020  0.927734  0.217195
17  0.983398  0.064612  0.962891  0.152138
18  0.989868  0.047626  0.921631  0.233113
19  0.985962  0.058076  0.965210  0.141171
20  0.985962  0.062459  0.960693  0.151060
21  0.988892  0.050731  0.940918  0.193696
22  0.987671  0.056418  0.843018  0.410287
23  0.986816  0.059648  0.909180  0.255806
24  0.988525  0.053282  0.922607  0.224936
25  0.985229  0.053381  0.922119  0.229326
26  0.987183  0.057861  0.901733  0.269297
27  0.988281  0.050816  0.936890  0.189687
28  0.987915  0.053680  0.838135  0.375131
29  0.987183  0.056625  0.891113  0.305449
30  0.987305  0.052007  0.916748  0.262242
31  0.988892  0.049632  0.906494  0.265610
32  0.989624  0.050493  0.897339  0.274021
33  0.990356  0.046143  0.924438  0.218565
34  0.989990  0.046267  0.925903  0.216543
35  0.987427  0.055114  0.837646  0.440889
36  0.989502  0.046942  0.783081  0.604850
37  0.989990  0.045949  0.871826  0.325482
38  0.988525  0.051330  0.912720  0.251524
39  0.991455  0.043406  0.882812  0.316815
40  0.990723  0.047432  0.864014  0.355191
41  0.987427  0.051038  0.919922  0.243092
42  0.987305  0.054222  0.929077  0.221633
43  0.990112  0.047763  0.865723  0.363393
44  0.990601  0.042313  0.902710  0.294069
45  0.991333  0.041280  0.875366  0.384071
46  0.991455  0.041591  0.953247  0.163177
47  0.989380  0.043666  0.807373  0.452413
48  0.991333  0.043591  0.939087  0.191728
49  0.988892  0.048098  0.930176  0.235643
50  0.990845  0.041846  0.865356  0.427220
51  0.992676  0.038008  0.872437  0.353284
52  0.990112  0.041617  0.896484  0.276087
53  0.990112  0.045544  0.927124  0.215051
54  0.991455  0.042722  0.855713  0.406416
55  0.991211  0.042509  0.902588  0.316582
56  0.990479  0.045326  0.840576  0.441711
57  0.990967  0.040863  0.908203  0.288084
58  0.991211  0.040799  0.927490  0.210214
59  0.989624  0.043507  0.954346  0.154869
60  0.989868  0.046338  0.866577  0.344928
61  0.990479  0.042401  0.928955  0.242722
62  0.990723  0.035699  0.948730  0.173686
63  0.990112  0.045302  0.860718  0.359728

2017-12-13 20:47:52.917913 Finish.
Total elapsed time: 08:10:19.92.
