2017-12-13 17:29:28.230606: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:28.230887: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:28.230900: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 17:29:22.892804 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit02, unit03, unit05.
Validation set: unit07, unit10.
Test set: unit01.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.972412  0.119275  0.823486  0.564866
1   0.966064  0.132071  0.848877  0.478875
2   0.974243  0.113208  0.704712  0.812048
3   0.970581  0.109411  0.759644  0.638294
4   0.978516  0.091563  0.816528  0.499094
5   0.979858  0.085508  0.725708  0.797704
6   0.981445  0.079607  0.782227  0.565637
7   0.977539  0.088485  0.666748  1.004190
8   0.979004  0.089330  0.927612  0.238266
9   0.981812  0.072897  0.730591  0.764200
10  0.981812  0.077930  0.711792  0.652368
11  0.980225  0.078096  0.772095  0.593678
12  0.983643  0.069706  0.894531  0.283437
13  0.980957  0.076232  0.724609  0.699303
14  0.983765  0.065047  0.865234  0.358975
15  0.982544  0.074909  0.763916  0.571051
16  0.983887  0.064609  0.780396  0.560541
17  0.982544  0.070368  0.914185  0.245026
18  0.982056  0.072615  0.903198  0.277646
19  0.983643  0.064484  0.907227  0.258628
20  0.982056  0.073253  0.900391  0.271812
21  0.983032  0.070529  0.922852  0.229143
22  0.985474  0.066148  0.927979  0.206500
23  0.986450  0.062698  0.789429  0.574920
24  0.983398  0.066186  0.902710  0.288315
25  0.982056  0.071441  0.836182  0.420920
26  0.984985  0.065083  0.931152  0.210768
27  0.983154  0.063854  0.885376  0.313212
28  0.983521  0.065777  0.878296  0.311104
29  0.985474  0.058386  0.953735  0.157025
30  0.983765  0.063144  0.941650  0.187261
31  0.985718  0.061638  0.886108  0.309460
32  0.985107  0.061615  0.739746  0.665866
33  0.985596  0.060817  0.916992  0.239652
34  0.985718  0.062693  0.877441  0.319258
35  0.986816  0.054002  0.793457  0.499974
36  0.988159  0.051500  0.887939  0.291530
37  0.986816  0.055071  0.845825  0.363594
38  0.985962  0.055319  0.794312  0.519372
39  0.985962  0.055638  0.828613  0.451541
40  0.985840  0.056144  0.929565  0.201865
41  0.986572  0.058494  0.895630  0.278137
42  0.986694  0.059314  0.848999  0.371227
43  0.986816  0.056969  0.920044  0.242878
44  0.986328  0.055264  0.829102  0.473822
45  0.984619  0.069298  0.857788  0.367161
46  0.988037  0.052895  0.878174  0.321304
47  0.987793  0.053056  0.883057  0.333225
48  0.989258  0.046070  0.901001  0.277655
49  0.986938  0.053828  0.834961  0.405293
50  0.986328  0.055342  0.957886  0.168608
51  0.987427  0.051352  0.937012  0.208793
52  0.988403  0.058179  0.891113  0.305164
53  0.985596  0.056059  0.947266  0.183577
54  0.985718  0.053139  0.839966  0.450648
55  0.987793  0.052056  0.801025  0.512014
56  0.988403  0.049834  0.948608  0.185162
57  0.989624  0.044942  0.904297  0.288521
58  0.988037  0.048964  0.933838  0.221348
59  0.989502  0.048416  0.832153  0.431021
60  0.987671  0.050618  0.885254  0.330074
61  0.987671  0.052417  0.875610  0.309433
62  0.989014  0.046799  0.851685  0.404921
63  0.988892  0.048415  0.887817  0.286954

2017-12-14 01:22:21.836182 Finish.
Total elapsed time: 07:52:59.84.
