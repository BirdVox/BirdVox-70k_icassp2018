2017-12-13 17:29:37.860110: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:37.860416: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:37.860429: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 17:29:35.004843 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.969727  0.112995  0.760132  0.892300
1   0.970093  0.111921  0.738403  1.411090
2   0.969360  0.118107  0.767456  0.818318
3   0.972290  0.104394  0.779053  0.701639
4   0.975220  0.091505  0.778320  0.883107
5   0.972656  0.101373  0.787354  0.666433
6   0.975220  0.096819  0.809082  0.653241
7   0.974609  0.096687  0.777710  0.667476
8   0.973267  0.104442  0.747314  1.089515
9   0.977905  0.084094  0.773438  0.978660
10  0.975098  0.085437  0.754517  1.186026
11  0.975830  0.094671  0.757812  1.059281
12  0.980835  0.077603  0.767212  0.946845
13  0.981445  0.069387  0.788696  0.721987
14  0.978394  0.080963  0.774902  0.739799
15  0.979126  0.081376  0.795532  0.597000
16  0.979004  0.081060  0.774780  0.866658
17  0.979004  0.082643  0.762085  1.006314
18  0.979858  0.075273  0.769531  1.081164
19  0.976562  0.087438  0.764893  1.007143
20  0.980103  0.076267  0.799194  0.852471
21  0.978882  0.082222  0.798462  0.784303
22  0.978516  0.080350  0.832031  0.676125
23  0.981201  0.076875  0.801270  0.800660
24  0.979492  0.078938  0.784546  0.861602
25  0.981323  0.073852  0.782471  0.700217
26  0.976807  0.086801  0.811523  0.636148
27  0.979858  0.074432  0.822388  0.662208
28  0.979492  0.073985  0.808838  0.662383
29  0.980103  0.078045  0.797119  0.716956
30  0.979614  0.075694  0.781006  0.873886
31  0.980225  0.071838  0.792847  0.788531
32  0.979248  0.080541  0.787476  0.817751
33  0.980347  0.075898  0.807861  0.874805
34  0.979004  0.077623  0.778076  0.904572
35  0.981445  0.076548  0.818970  0.589655
36  0.980103  0.076210  0.808716  0.692949
37  0.979614  0.080038  0.837769  0.522373
38  0.979370  0.074493  0.816040  0.729248
39  0.980957  0.072826  0.804932  0.589345
40  0.980713  0.074173  0.789551  0.807709
41  0.978760  0.080161  0.813232  0.615209
42  0.982910  0.066612  0.805786  0.792843
43  0.980103  0.077183  0.817627  0.752159
44  0.984375  0.062162  0.800537  1.006828
45  0.979858  0.071508  0.805664  0.696572
46  0.983032  0.064379  0.798340  0.824255
47  0.979980  0.076344  0.797729  0.739248
48  0.981567  0.069110  0.804565  0.673222
49  0.982056  0.069484  0.821899  0.662942
50  0.979492  0.076550  0.815796  0.593332
51  0.981812  0.072008  0.804077  0.656715
52  0.982178  0.074517  0.804443  0.648886
53  0.981934  0.070908  0.810913  0.628583
54  0.977905  0.077149  0.785400  0.969117
55  0.980347  0.074344  0.796875  0.757001
56  0.982666  0.066749  0.847168  0.516210
57  0.980103  0.075152  0.823853  0.632527
58  0.980835  0.070802  0.802734  0.700182
59  0.980225  0.068407  0.814819  0.718907
60  0.982666  0.065577  0.805908  0.850767
61  0.982544  0.068875  0.788086  1.173160
62  0.984741  0.064335  0.777466  0.938950
63  0.980835  0.072515  0.777466  0.988415

2017-12-14 01:30:04.234886 Finish.
Total elapsed time: 08:00:29.23.
