2018-02-12 19:59:46.249520: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 19:59:46.249852: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 19:59:46.249865: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 19:59:31.017918 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.968750  0.123143  0.719727  2.109094
1    0.966919  0.114853  0.724731  1.213920
2    0.976807  0.095092  0.755981  0.947208
3    0.979004  0.090992  0.739380  0.890772
4    0.977905  0.093262  0.737061  1.046309
5    0.979614  0.087532  0.719482  1.171536
6    0.979248  0.089666  0.706665  1.367223
7    0.981812  0.072588  0.794678  0.693023
8    0.979492  0.085668  0.789185  0.801716
9    0.980103  0.081996  0.739014  0.849452
10   0.979492  0.087021  0.762817  0.603454
11   0.980225  0.085236  0.754395  1.080738
12   0.982178  0.075502  0.771118  0.912429
13   0.981812  0.074922  0.785400  0.904484
14   0.984131  0.070005  0.756714  1.288917
15   0.985352  0.062211  0.809326  0.818159
16   0.979736  0.090575  0.785889  0.683092
17   0.982666  0.074030  0.763550  0.761482
18   0.985229  0.066003  0.778320  1.136906
19   0.979980  0.079898  0.798950  0.689722
20   0.981689  0.075768  0.770996  0.708051
21   0.982178  0.073111  0.744263  1.094172
22   0.982422  0.072426  0.764038  1.070988
23   0.982300  0.074007  0.814331  0.594800
24   0.986206  0.057281  0.753784  1.125120
25   0.983521  0.069663  0.757935  1.071718
26   0.980103  0.079407  0.791016  0.786805
27   0.983032  0.073872  0.789917  0.838848
28   0.984497  0.060188  0.786743  0.787247
29   0.983398  0.068465  0.800659  0.743383
30   0.984131  0.068617  0.795776  0.790855
31   0.982178  0.071256  0.780884  0.828758
32   0.986450  0.061758  0.813599  0.750496
33   0.985962  0.060414  0.813477  0.665788
34   0.982056  0.065889  0.796631  0.826436
35   0.984375  0.064719  0.793823  0.773900
36   0.983032  0.070272  0.790283  0.646055
37   0.986450  0.061204  0.789795  0.756244
38   0.987061  0.060885  0.783813  0.883181
39   0.983765  0.062720  0.795532  0.925459
40   0.984497  0.062458  0.791382  0.705684
41   0.987427  0.057070  0.801270  0.783255
42   0.986938  0.057576  0.780273  1.060305
43   0.984741  0.059129  0.796509  0.822224
44   0.985229  0.055829  0.798584  0.768373
45   0.986328  0.063115  0.821655  0.617891
46   0.987305  0.056237  0.788086  0.951705
47   0.985840  0.060438  0.779297  1.017433
48   0.985229  0.059241  0.818726  0.766232
49   0.985229  0.061549  0.783569  1.012750
50   0.986206  0.057769  0.822876  0.787921
51   0.987061  0.057598  0.777344  0.960582
52   0.984131  0.062926  0.788330  0.949594
53   0.985474  0.060892  0.851074  0.464877
54   0.985229  0.058319  0.787720  0.958965
55   0.983643  0.065345  0.802856  0.808972
56   0.982910  0.069574  0.798340  0.711361
57   0.987915  0.056059  0.769531  1.150573
58   0.986450  0.054869  0.806152  0.914924
59   0.984497  0.060784  0.787354  0.889713
60   0.988525  0.049116  0.787720  1.150475
61   0.985840  0.058071  0.785645  1.011340
62   0.986328  0.062090  0.783569  0.793842
63   0.987793  0.053164  0.795288  0.841293
64   0.986572  0.054716  0.795776  0.935343
65   0.985962  0.056721  0.823120  0.647106
66   0.986328  0.058599  0.820312  0.639926
67   0.984619  0.064724  0.823853  0.687887
68   0.985962  0.057308  0.788818  0.819748
69   0.987061  0.055624  0.801392  0.645926
70   0.987793  0.050101  0.803467  0.788577
71   0.988037  0.050508  0.778931  1.063218
72   0.986816  0.054934  0.781738  1.137318
73   0.986816  0.056146  0.833252  0.646488
74   0.987305  0.051269  0.802002  0.877884
75   0.987549  0.051780  0.783203  1.018052
76   0.986328  0.057727  0.788330  0.998599
77   0.987671  0.056846  0.770386  1.171435
78   0.986938  0.048084  0.808838  1.009957
79   0.985474  0.057668  0.805664  0.701108
80   0.987061  0.056359  0.829590  0.629054
81   0.987305  0.054544  0.804810  0.832711
82   0.986084  0.056255  0.767456  1.076077
83   0.987061  0.052019  0.782715  1.103081
84   0.987793  0.050359  0.804565  0.963611
85   0.986816  0.051052  0.778076  1.010253
86   0.985962  0.056049  0.788330  0.818607
87   0.985352  0.059040  0.814697  0.684331
88   0.984985  0.053478  0.802856  0.856497
89   0.986206  0.055491  0.799683  0.802645
90   0.987549  0.051035  0.791382  0.967545
91   0.986206  0.058185  0.779175  1.019370
92   0.986328  0.055615  0.787476  1.120307
93   0.987427  0.053212  0.807739  1.007994
94   0.986084  0.055357  0.776001  1.076397
95   0.989014  0.045460  0.805542  0.926811
96   0.986938  0.051509  0.775024  1.289644
97   0.988525  0.047138  0.776733  1.170411
98   0.987793  0.049188  0.803955  0.853740
99   0.989136  0.052797  0.831909  0.622193
100  0.986206  0.054559  0.826904  0.714240
101  0.988770  0.050034  0.798584  0.980113
102  0.988159  0.052966  0.790771  0.884408
103  0.990845  0.038842  0.787964  1.005712
104  0.987671  0.054471  0.816040  0.800986
105  0.988770  0.047977  0.815674  0.889629
106  0.988770  0.045749  0.789307  1.021803
107  0.988159  0.051650  0.817017  0.817657
108  0.987915  0.055029  0.807739  0.894009
109  0.989868  0.040190  0.796875  1.161970
110  0.988159  0.048571  0.807129  0.932221
111  0.988647  0.047782  0.781616  1.289358
112  0.987793  0.049020  0.784058  1.074687
113  0.988403  0.052458  0.825195  0.623731
114  0.989990  0.046954  0.797485  1.040946
115  0.987549  0.050892  0.782959  1.261115
116  0.990112  0.045559  0.781982  1.083389
117  0.987671  0.057029  0.776489  1.028582
118  0.988281  0.046107  0.812500  0.867186
119  0.985596  0.054341  0.783691  1.003016
120  0.989014  0.046940  0.819336  0.634565
121  0.989258  0.044288  0.787476  1.228439
122  0.988403  0.049829  0.781860  1.100397
123  0.985962  0.048619  0.802979  1.024263
124  0.989258  0.047594  0.807861  0.949623
125  0.989502  0.046458  0.802002  1.165408
126  0.988403  0.043382  0.781982  1.173994
127  0.985840  0.052879  0.776367  1.245602

2018-02-13 13:28:09.316924 Finish.
Total elapsed time: 17:28:38.32.
