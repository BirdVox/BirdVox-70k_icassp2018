2018-02-12 19:59:54.187209: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 19:59:54.187473: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 19:59:54.187486: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 19:59:38.339756 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.974854  0.102247  0.733765  1.764950
1    0.972900  0.101841  0.787720  0.919915
2    0.967896  0.109114  0.805176  0.551879
3    0.977051  0.094179  0.804199  0.831788
4    0.974609  0.088816  0.796631  0.831012
5    0.976929  0.087527  0.792725  0.868364
6    0.979614  0.085541  0.821167  0.658546
7    0.978149  0.091040  0.811646  0.605796
8    0.979370  0.080896  0.803833  0.866502
9    0.976074  0.088394  0.785278  1.059811
10   0.979614  0.078988  0.814819  0.759294
11   0.978394  0.084439  0.833252  0.486754
12   0.979492  0.079917  0.814209  0.585997
13   0.981567  0.071741  0.769775  0.858872
14   0.981323  0.075472  0.831787  0.643093
15   0.980713  0.073145  0.804688  0.934113
16   0.982300  0.073467  0.799072  0.999937
17   0.979370  0.076245  0.825928  0.713076
18   0.982300  0.074508  0.802002  0.634363
19   0.978882  0.085126  0.794312  0.976299
20   0.980591  0.076917  0.827393  0.726469
21   0.980835  0.076718  0.794922  1.121983
22   0.980957  0.072553  0.812744  0.859100
23   0.984497  0.064409  0.802246  1.035581
24   0.983276  0.069644  0.843872  0.583898
25   0.979248  0.073380  0.797607  1.215630
26   0.982178  0.070124  0.813843  0.776768
27   0.982300  0.064548  0.820190  0.831883
28   0.983032  0.071143  0.829468  0.665071
29   0.982544  0.068214  0.824097  0.856322
30   0.983276  0.071421  0.812988  0.928511
31   0.980835  0.071968  0.826050  0.944125
32   0.980347  0.077826  0.858521  0.406918
33   0.986084  0.058707  0.821655  0.883329
34   0.983643  0.064597  0.846558  0.657935
35   0.983765  0.064434  0.836304  0.543482
36   0.985596  0.064119  0.827515  0.702276
37   0.982300  0.072125  0.828369  0.709341
38   0.985840  0.058819  0.822510  0.819948
39   0.982666  0.067721  0.803101  0.812517
40   0.986938  0.060434  0.822876  0.749604
41   0.982910  0.071412  0.849731  0.527667
42   0.984863  0.063353  0.823364  0.849597
43   0.985352  0.069496  0.816650  0.779087
44   0.983521  0.067679  0.864136  0.512521
45   0.985962  0.061777  0.819824  0.849845
46   0.983887  0.062951  0.854004  0.585671
47   0.984253  0.069038  0.833130  0.624613
48   0.983276  0.063458  0.823853  0.847818
49   0.986328  0.058586  0.828491  0.867251
50   0.987305  0.057690  0.807373  1.075246
51   0.984131  0.060875  0.819580  0.871483
52   0.981812  0.066374  0.831543  0.883700
53   0.984253  0.066493  0.844116  0.760551
54   0.986572  0.059816  0.831421  0.679982
55   0.984619  0.058881  0.829346  0.755640
56   0.984009  0.059401  0.811768  0.905908
57   0.988159  0.056372  0.848511  0.615019
58   0.986572  0.056481  0.801636  1.350946
59   0.987061  0.059691  0.849365  0.528247
60   0.986816  0.055095  0.853516  0.483521
61   0.984863  0.062245  0.806885  1.117547
62   0.987549  0.054244  0.818359  1.191545
63   0.982910  0.063312  0.835815  0.831837
64   0.987549  0.055226  0.848511  0.707387
65   0.988281  0.056834  0.829224  0.925155
66   0.986450  0.052233  0.826294  0.829101
67   0.986572  0.056936  0.837280  0.801235
68   0.987183  0.053693  0.839111  0.641753
69   0.988281  0.051767  0.816406  1.182769
70   0.984497  0.060378  0.844238  0.626123
71   0.984497  0.063578  0.850708  0.568730
72   0.988647  0.047962  0.839355  0.626954
73   0.984497  0.064799  0.844727  0.642058
74   0.989136  0.047975  0.864380  0.461647
75   0.987427  0.057914  0.851074  0.611921
76   0.986816  0.052224  0.869507  0.606154
77   0.986206  0.053869  0.853271  0.719852
78   0.987793  0.052726  0.842896  0.672424
79   0.987061  0.054891  0.854492  0.490459
80   0.986694  0.055349  0.863159  0.510872
81   0.986572  0.054314  0.848267  0.633598
82   0.987549  0.052499  0.856934  0.565765
83   0.986572  0.051352  0.845581  0.721463
84   0.986938  0.061339  0.819824  0.827195
85   0.988647  0.048003  0.843384  0.575752
86   0.988770  0.053057  0.848145  0.636206
87   0.986450  0.052256  0.822998  0.737006
88   0.986328  0.057184  0.835815  0.691358
89   0.986816  0.053851  0.838867  1.135669
90   0.988159  0.051227  0.855103  0.597958
91   0.986694  0.054083  0.813110  1.019237
92   0.988281  0.051160  0.829102  0.991631
93   0.985352  0.064964  0.828979  0.909932
94   0.988770  0.050255  0.849731  0.942763
95   0.986206  0.052073  0.830933  0.911795
96   0.986816  0.050513  0.851440  0.616462
97   0.988403  0.050962  0.825439  1.013900
98   0.985596  0.056951  0.803223  1.293865
99   0.986206  0.051586  0.835815  0.727261
100  0.985840  0.059737  0.858765  0.638190
101  0.987915  0.054141  0.849609  0.602725
102  0.988159  0.049344  0.854004  0.657755
103  0.987061  0.055301  0.844116  0.590231
104  0.987671  0.052620  0.850830  0.553677
105  0.985596  0.054345  0.834839  0.739065
106  0.988770  0.049354  0.824463  1.003676
107  0.989014  0.049822  0.860474  0.721132
108  0.987061  0.056199  0.854736  0.668788
109  0.988892  0.046919  0.829834  0.790023
110  0.989624  0.050884  0.834473  0.917207
111  0.989746  0.045866  0.845947  0.606050
112  0.989258  0.048859  0.869507  0.642036
113  0.987915  0.051195  0.846191  0.673758
114  0.988159  0.047679  0.855835  0.549425
115  0.988525  0.047730  0.837769  0.541019
116  0.986572  0.057517  0.847778  0.556497
117  0.988159  0.049074  0.837280  0.740943
118  0.988525  0.044274  0.822510  0.996529
119  0.984375  0.060191  0.833618  0.681303
120  0.985352  0.055531  0.852173  0.542039
121  0.985474  0.054223  0.852539  0.807298
122  0.987549  0.048216  0.845215  0.809051
123  0.987671  0.051847  0.837280  0.715316
124  0.989136  0.048556  0.856079  0.591207
125  0.987305  0.049107  0.861816  0.591883
126  0.988647  0.051469  0.834595  1.116352
127  0.990112  0.042337  0.846191  0.833750

2018-02-13 11:14:47.622598 Finish.
Total elapsed time: 15:15:09.62.
