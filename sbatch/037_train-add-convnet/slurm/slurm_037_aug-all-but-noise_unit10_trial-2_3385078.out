2017-12-13 12:37:36.424199: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:36.424387: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:36.424398: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:36.424402: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:36.424407: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 12:37:34.278623 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit01, unit02, unit03.
Validation set: unit05, unit07.
Test set: unit10.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.492798  8.176860  0.503174  8.009139
1   0.503418  8.004890  0.494995  8.140391
2   0.501953  8.028070  0.510498  7.890201
3   0.495728  8.128180  0.489502  8.228448
4   0.501831  8.029675  0.496338  8.118173
5   0.501831  8.029607  0.504639  7.984332
6   0.501465  8.035474  0.497803  8.094489
7   0.501709  8.031520  0.497070  8.106281
8   0.493774  8.159401  0.498047  8.090534
9   0.503662  8.000026  0.502563  8.017732
10  0.497192  8.104303  0.506714  7.950834
11  0.501343  8.037406  0.508057  7.929191
12  0.497070  8.106269  0.503052  8.009859
13  0.491699  8.192841  0.503906  7.996087
14  0.503784  7.998054  0.495850  8.125944
15  0.508667  7.919352  0.504395  7.988216
16  0.495605  8.129879  0.502808  8.013794
17  0.493652  8.161360  0.498047  8.090528
18  0.498535  8.082658  0.501709  8.031502
19  0.507690  7.935093  0.496094  8.122009
20  0.495605  8.129879  0.503906  7.996086
21  0.503784  7.998054  0.501099  8.041340
22  0.500366  8.053145  0.489258  8.232191
23  0.502808  8.013794  0.503540  8.001989
24  0.491333  8.198743  0.514526  7.824910
25  0.494751  8.143652  0.512085  7.864261
26  0.493164  8.169230  0.499146  8.072821
27  0.490112  8.218419  0.507568  7.937060
28  0.496704  8.112171  0.498901  8.076756
29  0.499023  8.074788  0.506592  7.952801
30  0.505859  7.964606  0.505249  7.974443
31  0.508545  7.921320  0.506348  7.956736
32  0.505249  7.974443  0.497925  8.092496
33  0.505371  7.972476  0.495728  8.127912
34  0.504761  7.982314  0.482910  8.334503
35  0.497437  8.100366  0.496826  8.110204
36  0.494751  8.143652  0.493896  8.157425
37  0.504028  7.994119  0.499634  8.064950
38  0.496582  8.114139  0.506226  7.958703
39  0.504761  7.982314  0.513062  7.848521
40  0.495483  8.131847  0.500122  8.057080
41  0.504639  7.984281  0.497681  8.096431
42  0.510254  7.893774  0.502930  8.011827
43  0.499146  8.072821  0.498169  8.088561
44  0.491089  8.202678  0.495850  8.125944
45  0.495728  8.127912  0.503052  8.009859
46  0.503296  8.005924  0.501343  8.037405
47  0.493530  8.163327  0.504150  7.992151
48  0.502930  8.011827  0.497803  8.094464
49  0.497559  8.098399  0.505615  7.968541
50  0.490723  8.208581  0.498291  8.086593
51  0.497437  8.100366  0.505737  7.966573
52  0.491333  8.198743  0.494141  8.153490
53  0.496338  8.118074  0.502197  8.023632
54  0.496582  8.114139  0.501953  8.027567
55  0.501099  8.041340  0.503662  8.000022
56  0.495850  8.125944  0.503784  7.998054
57  0.505493  7.970508  0.501831  8.029535
58  0.502441  8.019697  0.496338  8.118074
59  0.493042  8.171198  0.501953  8.027567
60  0.499390  8.068885  0.500488  8.051178
61  0.492920  8.173165  0.505859  7.964606
62  0.504761  7.982314  0.503662  8.000022
63  0.507324  7.940995  0.495850  8.125944

2017-12-13 20:33:31.736170 Finish.
Total elapsed time: 07:55:57.74.
