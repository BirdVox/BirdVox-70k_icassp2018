2017-12-13 17:29:28.774149: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:28.774404: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:28.774417: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 17:29:21.846176 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit01, unit02, unit03.
Validation set: unit05, unit07.
Test set: unit10.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.494263  8.151522  0.495117  8.137749
1   0.508301  7.925255  0.501221  8.039372
2   0.504517  7.986249  0.496704  8.112171
3   0.493164  8.169230  0.507690  7.935093
4   0.506104  7.960671  0.501831  8.029535
5   0.501343  8.037405  0.489502  8.228256
6   0.512939  7.850488  0.499146  8.072821
7   0.486084  8.283347  0.507202  7.942963
8   0.511719  7.870164  0.500732  8.047243
9   0.496948  8.108236  0.501343  8.037405
10  0.511841  7.868196  0.513916  7.834748
11  0.494995  8.139717  0.505737  7.966573
12  0.493164  8.169230  0.494629  8.145620
13  0.496094  8.122009  0.499878  8.061015
14  0.504517  7.986249  0.489990  8.220386
15  0.502319  8.021664  0.508911  7.915417
16  0.496094  8.122009  0.499146  8.072821
17  0.508057  7.929190  0.508179  7.927223
18  0.502075  8.025600  0.495483  8.131847
19  0.504028  7.994119  0.511963  7.866229
20  0.501343  8.037405  0.504272  7.990184
21  0.498657  8.080691  0.503174  8.007892
22  0.495728  8.127912  0.501587  8.033470
23  0.498291  8.086593  0.504517  7.986249
24  0.503906  7.996086  0.504028  7.994119
25  0.501587  8.033470  0.501953  8.027567
26  0.494629  8.145620  0.497192  8.104301
27  0.495972  8.123977  0.500488  8.051178
28  0.495728  8.127912  0.508179  7.927223
29  0.509644  7.903612  0.497925  8.092496
30  0.502441  8.019697  0.503052  8.009859
31  0.497559  8.098399  0.511353  7.876066
32  0.494263  8.151522  0.502686  8.015762
33  0.504639  7.984281  0.505615  7.968541
34  0.495239  8.135782  0.499634  8.064950
35  0.502197  8.023632  0.505371  7.972476
36  0.493652  8.161360  0.499512  8.066918
37  0.502319  8.021665  0.502075  8.025600
38  0.506958  7.946898  0.502563  8.017729
39  0.500488  8.051178  0.492798  8.175133
40  0.500122  8.057080  0.503418  8.003957
41  0.505981  7.962638  0.491699  8.192841
42  0.506104  7.960671  0.498779  8.078723
43  0.502441  8.019697  0.492798  8.175133
44  0.501953  8.027567  0.498413  8.084626
45  0.495361  8.133814  0.496826  8.110204
46  0.504639  7.984281  0.493408  8.165295
47  0.497803  8.094464  0.508423  7.923287
48  0.492676  8.177100  0.502319  8.021665
49  0.492310  8.183003  0.500366  8.053145
50  0.495728  8.127912  0.495972  8.123977
51  0.489380  8.230224  0.491089  8.202678
52  0.500244  8.055113  0.502075  8.025600
53  0.505005  7.978379  0.501221  8.039372
54  0.497070  8.106269  0.491821  8.190873
55  0.510010  7.897709  0.502075  8.025600
56  0.501465  8.035437  0.508911  7.915417
57  0.492432  8.181035  0.502686  8.015762
58  0.494019  8.155457  0.493652  8.161360
59  0.499634  8.064950  0.500977  8.043307
60  0.499390  8.068886  0.501343  8.037405
61  0.500122  8.057080  0.501343  8.037405
62  0.496704  8.112171  0.502686  8.015762
63  0.494141  8.153490  0.486450  8.277445

2017-12-14 01:33:55.490984 Finish.
Total elapsed time: 08:04:34.49.
