2017-12-13 12:38:35.519828: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:38:35.519999: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:38:35.520009: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:38:35.520014: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:38:35.520018: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 12:38:33.137196 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.972412  0.106010  0.806274  1.118198
1   0.973267  0.115797  0.802612  0.865849
2   0.975464  0.096741  0.803955  0.938723
3   0.976929  0.089574  0.803955  0.966459
4   0.974365  0.100479  0.800049  1.231828
5   0.977539  0.095435  0.785156  1.265517
6   0.978516  0.084524  0.806885  0.765356
7   0.978149  0.085445  0.759399  1.187912
8   0.978271  0.083787  0.812744  0.566479
9   0.979370  0.080749  0.806641  1.179576
10  0.978760  0.081623  0.781982  0.833606
11  0.979736  0.078839  0.769287  1.316542
12  0.983154  0.071107  0.792603  1.068050
13  0.980591  0.083073  0.824463  0.720468
14  0.981812  0.075053  0.797485  0.851800
15  0.982300  0.072780  0.816528  0.881626
16  0.981079  0.078092  0.799561  0.843193
17  0.981812  0.075197  0.806152  0.874101
18  0.980103  0.077915  0.793335  0.969100
19  0.983276  0.071290  0.817871  0.755229
20  0.983154  0.072202  0.806885  0.817334
21  0.983154  0.068254  0.845337  0.527901
22  0.979126  0.085134  0.816528  0.921718
23  0.982544  0.073200  0.783813  1.092674
24  0.981079  0.071799  0.821533  0.684438
25  0.981079  0.067161  0.816162  1.043468
26  0.985107  0.065501  0.811035  0.922561
27  0.983154  0.073833  0.830811  0.656305
28  0.986572  0.055262  0.846802  0.603292
29  0.982300  0.073276  0.811890  1.062560
30  0.981567  0.066339  0.819336  0.568151
31  0.980835  0.073035  0.814087  0.902577
32  0.982056  0.074352  0.833496  0.552403
33  0.983398  0.063442  0.858032  0.449696
34  0.984375  0.072226  0.791016  0.732841
35  0.983765  0.067674  0.837402  0.519501
36  0.984009  0.059519  0.848755  0.541685
37  0.981689  0.067522  0.799927  0.839132
38  0.984863  0.062109  0.846680  0.553819
39  0.981323  0.072043  0.822510  0.709470
40  0.984009  0.064003  0.843506  0.549311
41  0.985840  0.061342  0.820923  0.849261
42  0.984009  0.067478  0.810791  1.019478
43  0.984619  0.064116  0.835449  0.804312
44  0.985352  0.063074  0.839966  0.656984
45  0.983765  0.059316  0.827148  0.695572
46  0.985718  0.059961  0.848755  0.577422
47  0.984619  0.062949  0.821045  0.808967
48  0.985962  0.062956  0.847290  0.545187
49  0.984375  0.058695  0.787476  1.182228
50  0.981567  0.067755  0.819458  0.681000
51  0.984741  0.062719  0.829346  0.646064
52  0.985962  0.063759  0.857300  0.421014
53  0.987915  0.052115  0.842041  0.620594
54  0.985596  0.058037  0.819702  0.779894
55  0.984497  0.056983  0.834595  0.772448
56  0.984619  0.060952  0.845581  0.693442
57  0.982910  0.064143  0.824341  0.680747
58  0.987427  0.054781  0.810425  0.983187
59  0.987671  0.052397  0.860352  0.510545
60  0.985352  0.058006  0.835083  0.798930
61  0.986694  0.056755  0.846313  0.679168
62  0.984253  0.061151  0.841187  0.729457
63  0.983887  0.063099  0.830811  0.714282

2017-12-13 20:41:51.049447 Finish.
Total elapsed time: 08:03:18.05.
