2017-12-13 12:38:35.524416: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:38:35.524638: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:38:35.524650: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:38:35.524655: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:38:35.524659: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 12:38:33.123959 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.974243  0.105224  0.879883  0.410438
1   0.979736  0.085365  0.871704  0.574460
2   0.977051  0.085355  0.887207  0.447127
3   0.973511  0.097363  0.878174  0.415798
4   0.978882  0.082396  0.888184  0.402290
5   0.978516  0.080300  0.893433  0.366695
6   0.976929  0.083141  0.892090  0.415385
7   0.978271  0.083517  0.915527  0.326395
8   0.983398  0.064620  0.892456  0.401584
9   0.981812  0.073520  0.847168  0.441004
10  0.979736  0.078393  0.881470  0.451333
11  0.982422  0.068207  0.910034  0.336169
12  0.980591  0.069835  0.892822  0.364881
13  0.979492  0.080942  0.891357  0.378647
14  0.978882  0.077550  0.905518  0.295481
15  0.983887  0.065650  0.906860  0.315117
16  0.981689  0.072016  0.891602  0.344164
17  0.980591  0.077296  0.906006  0.287399
18  0.980713  0.072285  0.881592  0.345902
19  0.983276  0.069987  0.891357  0.320387
20  0.985474  0.064318  0.912354  0.274780
21  0.984985  0.060481  0.901978  0.368399
22  0.984253  0.062626  0.901245  0.307404
23  0.984253  0.062073  0.909790  0.304923
24  0.983276  0.068835  0.909668  0.275953
25  0.981812  0.065374  0.892334  0.354661
26  0.985352  0.059041  0.891724  0.351104
27  0.982544  0.069030  0.910278  0.260908
28  0.986328  0.061345  0.906616  0.309862
29  0.985718  0.060691  0.910767  0.304329
30  0.984985  0.059734  0.908569  0.296106
31  0.984131  0.061196  0.904785  0.303271
32  0.984741  0.058832  0.899658  0.329965
33  0.983765  0.062991  0.903687  0.338460
34  0.982544  0.071822  0.888550  0.358671
35  0.985474  0.055388  0.905273  0.357727
36  0.984741  0.060233  0.889771  0.363348
37  0.983765  0.061072  0.902466  0.315623
38  0.983887  0.063209  0.880737  0.362504
39  0.987427  0.053352  0.902100  0.316818
40  0.986572  0.056087  0.906372  0.318344
41  0.985229  0.060272  0.903809  0.301940
42  0.986206  0.057927  0.897827  0.344259
43  0.985229  0.062257  0.896118  0.341149
44  0.983032  0.062300  0.892944  0.346548
45  0.988037  0.054490  0.906616  0.322424
46  0.984863  0.060234  0.903687  0.321714
47  0.985474  0.058369  0.895996  0.344060
48  0.984253  0.061308  0.899902  0.344573
49  0.985352  0.058999  0.909058  0.344839
50  0.984985  0.057201  0.919556  0.299655
51  0.985474  0.061363  0.910278  0.316648
52  0.988892  0.047839  0.911133  0.304607
53  0.989380  0.045177  0.895386  0.364067
54  0.983643  0.062170  0.907104  0.323471
55  0.987427  0.055861  0.924561  0.256371
56  0.987183  0.054848  0.890381  0.363813
57  0.986816  0.060163  0.911011  0.322922
58  0.985474  0.060282  0.894897  0.311240
59  0.988037  0.049698  0.901001  0.341053
60  0.986938  0.053973  0.910156  0.370145
61  0.985596  0.057024  0.888306  0.359497
62  0.986694  0.053424  0.893677  0.336270
63  0.986206  0.057736  0.901245  0.315133

2017-12-13 20:42:39.129243 Finish.
Total elapsed time: 08:04:06.13.
