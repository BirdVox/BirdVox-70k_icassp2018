2017-12-13 12:38:47.897606: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:38:47.897836: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:38:47.897847: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:38:47.897851: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:38:47.897856: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 12:38:46.094845 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit10, unit01, unit02.
Validation set: unit03, unit05.
Test set: unit07.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.949951  0.163074  0.956055  0.139760
1   0.954224  0.155073  0.925415  0.217613
2   0.961426  0.139336  0.931641  0.190547
3   0.961182  0.140824  0.941895  0.163636
4   0.961548  0.139138  0.916260  0.249143
5   0.961182  0.140584  0.849243  0.366963
6   0.961182  0.132158  0.961426  0.133705
7   0.963257  0.133422  0.954102  0.148199
8   0.960327  0.130955  0.952515  0.149716
9   0.964478  0.132858  0.932129  0.197478
10  0.967041  0.121480  0.959839  0.131379
11  0.964600  0.120806  0.923950  0.219728
12  0.971924  0.109974  0.910767  0.247700
13  0.964355  0.125129  0.960693  0.126195
14  0.966797  0.126251  0.954956  0.140313
15  0.969482  0.116874  0.947021  0.165433
16  0.968506  0.113522  0.932617  0.206133
17  0.966064  0.117252  0.949463  0.154040
18  0.967407  0.115570  0.963135  0.120726
19  0.967529  0.118341  0.932739  0.199363
20  0.969116  0.113024  0.962036  0.128803
21  0.970947  0.109746  0.973633  0.095444
22  0.967407  0.115322  0.972778  0.104898
23  0.968384  0.115781  0.958862  0.130765
24  0.970093  0.103779  0.934326  0.189264
25  0.968628  0.111556  0.959595  0.134729
26  0.968262  0.109357  0.944336  0.170903
27  0.970581  0.101617  0.967407  0.112607
28  0.970581  0.110237  0.975342  0.093933
29  0.972168  0.100486  0.935181  0.187639
30  0.970337  0.104116  0.952026  0.148589
31  0.974609  0.096769  0.967529  0.116912
32  0.969849  0.115492  0.966553  0.121023
33  0.971558  0.103839  0.910645  0.233496
34  0.971191  0.100704  0.949829  0.155698
35  0.971069  0.104269  0.975220  0.095634
36  0.973511  0.097137  0.970093  0.100690
37  0.976074  0.092299  0.969727  0.114234
38  0.970215  0.107960  0.975830  0.090214
39  0.969727  0.107461  0.963135  0.122322
40  0.971069  0.104845  0.974731  0.094435
41  0.973877  0.097894  0.961670  0.128801
42  0.972656  0.098433  0.949097  0.160208
43  0.975586  0.090947  0.965942  0.120764
44  0.974121  0.097727  0.942871  0.162972
45  0.974121  0.099190  0.957153  0.126098
46  0.973633  0.094279  0.963745  0.124765
47  0.972534  0.099339  0.965088  0.118431
48  0.974731  0.095185  0.970459  0.105906
49  0.974854  0.089863  0.969116  0.109897
50  0.972168  0.099841  0.964355  0.120127
51  0.974854  0.096345  0.893555  0.281323
52  0.973999  0.101391  0.915405  0.227523
53  0.974121  0.096658  0.972656  0.104403
54  0.975830  0.090669  0.963501  0.121476
55  0.974609  0.088529  0.951050  0.155061
56  0.975952  0.090218  0.965698  0.120147
57  0.979980  0.083049  0.968628  0.113359
58  0.975952  0.091144  0.934692  0.202068
59  0.976685  0.090822  0.940674  0.172778
60  0.974365  0.093591  0.944092  0.170355
61  0.975952  0.088873  0.949951  0.153099
62  0.974365  0.095022  0.948608  0.157120
63  0.974243  0.093182  0.956909  0.136519

2017-12-13 20:13:16.043109 Finish.
Total elapsed time: 07:34:30.04.
