2017-12-13 12:37:36.005541: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:36.005795: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:36.005804: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:36.005809: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:36.005813: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 12:37:34.028923 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit10, unit01, unit02.
Validation set: unit03, unit05.
Test set: unit07.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.949097  0.167612  0.921509  0.219207
1   0.946411  0.173113  0.900024  0.260988
2   0.950439  0.161523  0.874390  0.313537
3   0.955688  0.150505  0.906006  0.259344
4   0.954102  0.141005  0.770142  0.471084
5   0.955933  0.147529  0.927612  0.219440
6   0.958496  0.139042  0.967163  0.115541
7   0.957153  0.148137  0.949829  0.164772
8   0.961548  0.137194  0.941284  0.173309
9   0.959717  0.130633  0.961426  0.131249
10  0.959473  0.136445  0.962280  0.123916
11  0.961670  0.134699  0.944458  0.176498
12  0.957764  0.142695  0.949951  0.151837
13  0.961914  0.131827  0.941772  0.168967
14  0.964966  0.126739  0.970337  0.115559
15  0.965698  0.124630  0.948120  0.150595
16  0.963867  0.134430  0.966187  0.119045
17  0.964966  0.119742  0.971069  0.106336
18  0.967285  0.122475  0.953857  0.150111
19  0.965088  0.126724  0.946411  0.157421
20  0.963257  0.130539  0.961914  0.136818
21  0.965332  0.119615  0.956055  0.133808
22  0.968628  0.113508  0.979248  0.089093
23  0.970459  0.109990  0.954712  0.141803
24  0.965454  0.125184  0.974487  0.104917
25  0.970703  0.110073  0.962769  0.127092
26  0.972168  0.110876  0.953613  0.145353
27  0.966064  0.115428  0.975952  0.096463
28  0.969360  0.112065  0.971069  0.103995
29  0.968994  0.113987  0.957764  0.134241
30  0.964478  0.125707  0.967407  0.109725
31  0.968628  0.110092  0.976685  0.094694
32  0.967529  0.115520  0.953369  0.149502
33  0.967285  0.120289  0.970337  0.109099
34  0.973267  0.104293  0.970581  0.112796
35  0.968506  0.112773  0.976074  0.092725
36  0.968628  0.118921  0.965820  0.119573
37  0.967407  0.121241  0.962158  0.127881
38  0.971924  0.107447  0.973755  0.107640
39  0.970337  0.112875  0.960205  0.131911
40  0.970337  0.116294  0.951294  0.156898
41  0.970093  0.116724  0.974487  0.096876
42  0.972534  0.104824  0.953369  0.145863
43  0.972290  0.106831  0.969849  0.111058
44  0.973633  0.103559  0.969482  0.113404
45  0.975098  0.094259  0.972168  0.106381
46  0.971558  0.101261  0.963379  0.119469
47  0.971313  0.106509  0.979858  0.089250
48  0.968262  0.110850  0.970459  0.106734
49  0.970337  0.103637  0.973755  0.100247
50  0.973267  0.098490  0.972534  0.103710
51  0.972046  0.109113  0.971802  0.104791
52  0.969360  0.107505  0.947144  0.162507
53  0.968140  0.113103  0.968506  0.111560
54  0.974609  0.095099  0.927490  0.205692
55  0.971802  0.102407  0.973999  0.100478
56  0.972168  0.100362  0.978760  0.085582
57  0.972656  0.098832  0.970093  0.110453
58  0.972046  0.106488  0.957886  0.135443
59  0.972534  0.099123  0.955444  0.143439
60  0.978394  0.085426  0.975708  0.102318
61  0.977173  0.088629  0.955811  0.148217
62  0.972534  0.100207  0.949829  0.157973
63  0.974487  0.095023  0.972412  0.105694

2017-12-13 20:48:22.099380 Finish.
Total elapsed time: 08:10:48.10.
