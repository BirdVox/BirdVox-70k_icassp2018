2017-12-13 12:37:36.675278: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:36.675535: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:36.675547: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:36.675552: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:36.675557: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 12:37:34.264949 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.973755  0.101455  0.722412  1.303189
1   0.976074  0.092759  0.729126  0.786961
2   0.976562  0.086140  0.752319  1.114199
3   0.979126  0.081039  0.773926  0.696320
4   0.977783  0.085608  0.774170  0.694197
5   0.976318  0.092956  0.754028  0.627709
6   0.975952  0.093209  0.738281  0.669011
7   0.981445  0.069390  0.753418  0.885788
8   0.979614  0.079131  0.774170  0.729729
9   0.982544  0.073144  0.782227  0.569671
10  0.982544  0.069978  0.763550  0.610276
11  0.982178  0.073875  0.764648  0.781190
12  0.983765  0.072032  0.795044  0.516581
13  0.983765  0.067155  0.756470  0.604841
14  0.981689  0.074550  0.774658  0.734202
15  0.983643  0.061219  0.801880  0.556187
16  0.982178  0.072440  0.793701  0.599337
17  0.981934  0.070477  0.776245  0.557011
18  0.983032  0.071525  0.808228  0.501753
19  0.982788  0.067980  0.773193  0.647302
20  0.982300  0.069104  0.807251  0.520115
21  0.982544  0.070746  0.804565  0.542449
22  0.983521  0.064667  0.801270  0.534032
23  0.982544  0.066952  0.792114  0.535581
24  0.980713  0.073723  0.808838  0.459350
25  0.984375  0.065873  0.790283  0.630820
26  0.984131  0.063429  0.798096  0.508062
27  0.983398  0.066118  0.827271  0.443123
28  0.984741  0.065998  0.793579  0.521262
29  0.984497  0.062891  0.796997  0.540657
30  0.983643  0.064020  0.801270  0.523882
31  0.983887  0.064271  0.802979  0.488730
32  0.986084  0.057317  0.795166  0.530226
33  0.984009  0.062927  0.813965  0.499987
34  0.986938  0.054634  0.791382  0.616570
35  0.984741  0.064208  0.804688  0.597540
36  0.985840  0.057434  0.819458  0.453118
37  0.982666  0.069213  0.808594  0.517135
38  0.987061  0.055683  0.778198  0.681834
39  0.985229  0.062988  0.809204  0.498776
40  0.986206  0.057927  0.818237  0.459434
41  0.984375  0.060534  0.842651  0.422905
42  0.984985  0.060537  0.806396  0.516691
43  0.985107  0.054451  0.807495  0.543981
44  0.984497  0.062405  0.828369  0.464050
45  0.984009  0.068617  0.805786  0.519543
46  0.989868  0.049675  0.803223  0.470255
47  0.984985  0.062585  0.803101  0.493272
48  0.986084  0.057333  0.770508  0.605141
49  0.985107  0.058799  0.809692  0.474565
50  0.984619  0.058914  0.815186  0.492520
51  0.986694  0.054845  0.798950  0.589062
52  0.987915  0.054173  0.816406  0.494110
53  0.987793  0.054858  0.824585  0.504089
54  0.989990  0.045181  0.817749  0.480533
55  0.988037  0.049891  0.823730  0.536183
56  0.986206  0.052281  0.774048  0.706077
57  0.986328  0.053613  0.822510  0.538174
58  0.986206  0.059741  0.796875  0.550644
59  0.987915  0.055035  0.791626  0.649136
60  0.986450  0.056802  0.829468  0.550979
61  0.986938  0.052857  0.829590  0.500619
62  0.986450  0.056619  0.811890  0.590467
63  0.983765  0.060310  0.826294  0.454353

2017-12-13 20:31:42.805683 Finish.
Total elapsed time: 07:54:08.81.
