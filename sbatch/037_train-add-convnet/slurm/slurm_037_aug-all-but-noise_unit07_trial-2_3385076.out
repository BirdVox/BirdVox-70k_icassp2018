2017-12-13 12:37:36.130820: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:36.131098: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:36.131109: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:36.131114: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:36.131118: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 12:37:34.278022 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit10, unit01, unit02.
Validation set: unit03, unit05.
Test set: unit07.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.500854  7.963055  0.504272  7.907229
1   0.502930  7.927689  0.496582  8.028080
2   0.490356  8.126766  0.497437  8.013414
3   0.499390  7.981946  0.504517  7.899933
4   0.499878  7.973696  0.504883  7.893749
5   0.499023  7.987057  0.501099  7.953887
6   0.513550  7.755329  0.498047  8.002436
7   0.507568  7.850612  0.504028  7.907024
8   0.498779  7.990691  0.503540  7.914781
9   0.564697  3.773756  0.804688  0.603770
10  0.914062  0.311589  0.951782  0.208344
11  0.943237  0.221182  0.934570  0.249080
12  0.958740  0.168630  0.944702  0.210013
13  0.955811  0.164350  0.917969  0.243967
14  0.959595  0.148305  0.947632  0.177148
15  0.959351  0.142697  0.929932  0.202937
16  0.962036  0.135363  0.954224  0.150278
17  0.964478  0.122237  0.936523  0.189039
18  0.967041  0.119365  0.946533  0.158468
19  0.966919  0.120084  0.925293  0.213759
20  0.967407  0.108299  0.932861  0.187836
21  0.968872  0.107824  0.944702  0.174784
22  0.968628  0.107749  0.943237  0.172000
23  0.968018  0.110091  0.953491  0.142370
24  0.970337  0.096554  0.961792  0.120589
25  0.971191  0.105228  0.957764  0.134356
26  0.970825  0.099685  0.960571  0.142815
27  0.970825  0.095689  0.918457  0.238900
28  0.966431  0.111037  0.965820  0.114817
29  0.970825  0.098764  0.911865  0.240227
30  0.969238  0.103465  0.961670  0.123075
31  0.972046  0.098331  0.955811  0.135036
32  0.975220  0.082308  0.958862  0.118775
33  0.969360  0.106841  0.959106  0.119110
34  0.970703  0.100151  0.953369  0.144611
35  0.973389  0.094210  0.970215  0.103513
36  0.972656  0.092936  0.961182  0.117820
37  0.973755  0.092852  0.962036  0.136772
38  0.972900  0.093535  0.957153  0.132394
39  0.974487  0.092849  0.967773  0.108362
40  0.971191  0.097898  0.912842  0.246638
41  0.972656  0.095563  0.882690  0.289259
42  0.971313  0.094696  0.953003  0.144962
43  0.971436  0.095400  0.925781  0.202522
44  0.976196  0.089156  0.958252  0.126670
45  0.972290  0.094654  0.965332  0.113405
46  0.975830  0.091110  0.962891  0.110537
47  0.974731  0.091493  0.955200  0.135687
48  0.973145  0.091536  0.965210  0.109425
49  0.974731  0.090983  0.970215  0.104533
50  0.975708  0.086656  0.927002  0.203084
51  0.973755  0.089227  0.945068  0.171960
52  0.972168  0.088897  0.911743  0.252732
53  0.973145  0.092685  0.968262  0.117259
54  0.974487  0.087265  0.967896  0.111897
55  0.977539  0.083261  0.955811  0.139035
56  0.974731  0.086772  0.947144  0.160941
57  0.974976  0.092971  0.973145  0.094541
58  0.976074  0.085032  0.957153  0.131788
59  0.976685  0.081400  0.964478  0.118110
60  0.975708  0.082109  0.959473  0.131449
61  0.977173  0.083486  0.972046  0.100337
62  0.976196  0.083481  0.956665  0.133701
63  0.976685  0.086238  0.944214  0.166326

2017-12-13 20:32:31.022510 Finish.
Total elapsed time: 07:54:57.02.
