2018-02-12 19:59:50.634476: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 19:59:50.634921: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 19:59:50.634935: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 19:59:31.559198 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.502686  7.928378  0.503784  7.910864
1    0.494507  8.058766  0.498169  8.000384
2    0.498901  7.988707  0.499512  7.978977
3    0.497192  8.015952  0.498901  7.988707
4    0.512085  7.778529  0.505371  7.885564
5    0.492554  8.089904  0.496094  8.033467
6    0.498657  7.992599  0.512817  7.766853
7    0.505371  7.885564  0.489380  8.140502
8    0.501343  7.949785  0.493042  8.082120
9    0.498535  7.994545  0.494629  8.056820
10   0.500366  7.965354  0.488525  8.154125
11   0.492310  8.093796  0.501465  7.947839
12   0.494019  8.066551  0.500000  7.971192
13   0.497314  8.014006  0.502319  7.934217
14   0.509766  7.815505  0.499146  7.984815
15   0.509766  7.815505  0.491211  8.111311
16   0.506348  7.869996  0.497559  8.010114
17   0.495239  8.047090  0.496460  8.027629
18   0.509155  7.825235  0.500000  7.971192
19   0.511108  7.794098  0.494507  8.058766
20   0.491455  8.107419  0.489990  8.130772
21   0.497192  8.015952  0.495483  8.043198
22   0.496094  8.033467  0.503662  7.912810
23   0.490234  8.126880  0.505737  7.879726
24   0.501343  7.949785  0.498535  7.994545
25   0.499756  7.975085  0.491455  8.107419
26   0.496826  8.021791  0.496216  8.031521
27   0.500366  7.965354  0.493286  8.078227
28   0.496948  8.019845  0.495483  8.043198
29   0.488892  8.148287  0.494629  8.056820
30   0.509033  7.827182  0.501099  7.953678
31   0.497314  8.014006  0.507812  7.846642
32   0.503662  7.912810  0.502808  7.926432
33   0.501953  7.940055  0.496094  8.033467
34   0.497681  8.008168  0.499023  7.986761
35   0.494507  8.058766  0.498901  7.988707
36   0.498779  7.990653  0.505859  7.877780
37   0.497559  8.010114  0.498779  7.990653
38   0.496582  8.025683  0.501465  7.947839
39   0.503662  7.912810  0.495117  8.049036
40   0.501587  7.945893  0.503052  7.922540
41   0.500732  7.959516  0.487793  8.165802
42   0.509033  7.827182  0.503906  7.908917
43   0.495605  8.041252  0.500854  7.957570
44   0.488159  8.159963  0.495972  8.035413
45   0.505737  7.879726  0.510742  7.799936
46   0.499023  7.986761  0.499512  7.978977
47   0.493408  8.076281  0.494751  8.054874
48   0.505249  7.887510  0.497681  8.008168
49   0.494019  8.066551  0.494751  8.054874
50   0.497681  8.008168  0.503662  7.912810
51   0.500000  7.971192  0.492065  8.097688
52   0.503174  7.920594  0.499268  7.982869
53   0.501099  7.953678  0.505981  7.875834
54   0.507446  7.852481  0.489014  8.146341
55   0.499756  7.975085  0.506714  7.864157
56   0.501953  7.940055  0.492188  8.095742
57   0.501465  7.947839  0.494873  8.052928
58   0.503296  7.918648  0.499023  7.986761
59   0.505737  7.879726  0.481812  8.261160
60   0.494629  8.056820  0.491211  8.111311
61   0.506226  7.871942  0.494995  8.050982
62   0.510864  7.797990  0.510498  7.803828
63   0.502197  7.936163  0.502686  7.928378
64   0.499634  7.977031  0.502808  7.926432
65   0.511597  7.786314  0.506958  7.860265
66   0.503418  7.916702  0.493530  8.074335
67   0.498413  7.996492  0.506592  7.866103
68   0.501099  7.953678  0.504150  7.905025
69   0.498047  8.002330  0.501709  7.943947
70   0.510620  7.801882  0.507080  7.858319
71   0.500488  7.963408  0.504761  7.895295
72   0.506226  7.871942  0.499023  7.986761
73   0.498169  8.000384  0.498413  7.996492
74   0.501343  7.949785  0.502319  7.934217
75   0.504272  7.903079  0.505737  7.879726
76   0.498047  8.002330  0.494873  8.052928
77   0.503906  7.908917  0.498291  7.998438
78   0.489624  8.136610  0.509033  7.827182
79   0.498413  7.996492  0.488770  8.150233
80   0.501099  7.953678  0.500122  7.969246
81   0.501709  7.943947  0.498901  7.988707
82   0.498291  7.998438  0.504395  7.901133
83   0.502075  7.938109  0.501953  7.940055
84   0.500610  7.961462  0.509033  7.827182
85   0.510498  7.803828  0.493042  8.082120
86   0.495972  8.035413  0.507568  7.850535
87   0.501221  7.951731  0.510010  7.811613
88   0.500122  7.969246  0.501709  7.943947
89   0.504639  7.897241  0.504517  7.899187
90   0.500732  7.959516  0.504883  7.893349
91   0.502197  7.936163  0.506104  7.873888
92   0.500122  7.969246  0.501221  7.951731
93   0.497192  8.015952  0.497803  8.006222
94   0.492920  8.084066  0.498291  7.998438
95   0.501465  7.947839  0.501343  7.949785
96   0.499268  7.982869  0.500366  7.965354
97   0.501587  7.945893  0.495728  8.039306
98   0.509888  7.813559  0.501221  7.951731
99   0.505249  7.887510  0.497803  8.006222
100  0.506104  7.873888  0.500366  7.965354
101  0.500732  7.959516  0.497314  8.014006
102  0.500366  7.965354  0.490112  8.128826
103  0.503784  7.910864  0.501831  7.942001
104  0.510132  7.809667  0.499512  7.978977
105  0.510620  7.801882  0.501221  7.951731
106  0.495361  8.045144  0.503296  7.918648
107  0.504272  7.903079  0.499023  7.986761
108  0.495850  8.037359  0.509277  7.823289
109  0.499512  7.978977  0.503052  7.922540
110  0.506958  7.860265  0.495361  8.045144
111  0.495239  8.047090  0.494751  8.054874
112  0.502441  7.932271  0.494507  8.058766
113  0.497070  8.017899  0.498169  8.000384
114  0.489868  8.132718  0.501465  7.947839
115  0.492188  8.095742  0.494385  8.060713
116  0.508179  7.840804  0.500977  7.955624
117  0.496460  8.027629  0.510986  7.796044
118  0.500732  7.959516  0.483765  8.230023
119  0.499512  7.978977  0.505859  7.877780
120  0.508545  7.834966  0.501343  7.949785
121  0.511475  7.788260  0.493042  8.082120
122  0.495972  8.035413  0.499878  7.973138
123  0.498657  7.992599  0.497192  8.015952
124  0.501221  7.951731  0.496094  8.033467
125  0.502441  7.932271  0.506348  7.869996
126  0.507568  7.850535  0.501343  7.949785
127  0.501953  7.940055  0.510376  7.805775

2018-02-13 13:26:58.921652 Finish.
Total elapsed time: 17:27:27.92.
