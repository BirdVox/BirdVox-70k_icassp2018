2017-12-13 17:29:27.520108: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:27.520365: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:27.520378: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 17:29:21.524226 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit07, unit10, unit01.
Validation set: unit02, unit03.
Test set: unit05.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.500610  7.966334  0.500000  7.975030
1   0.505615  7.884758  0.498535  7.996980
2   0.501709  7.945904  0.486206  8.192643
3   0.504395  7.902370  0.514526  7.740579
4   0.507690  7.863071  0.500488  8.071150
5   0.493164  8.178982  0.495361  8.138613
6   0.501465  8.038553  0.507935  7.933118
7   0.505005  7.979751  0.498901  8.077684
8   0.491943  8.189574  0.512451  7.858822
9   0.500732  8.047580  0.498779  8.078959
10  0.496094  8.122180  0.502930  8.011946
11  0.502441  8.019783  0.499023  8.074847
12  0.488770  8.240104  0.497925  8.092525
13  0.492065  8.186958  0.504150  7.992165
14  0.501343  8.037414  0.499756  8.062989
15  0.497803  8.094468  0.494629  8.145622
16  0.505615  7.968543  0.490845  8.206614
17  0.495605  8.088069  0.499878  8.015832
18  0.494019  8.105315  0.508179  7.876012
19  0.496460  8.059846  0.500244  7.996701
20  0.504028  7.933899  0.502441  7.956842
21  0.509399  7.843814  0.504028  7.927431
22  0.507446  7.871138  0.493164  8.097102
23  0.507080  7.873697  0.489990  8.144665
24  0.498657  8.005162  0.496948  8.031135
25  0.503540  7.924913  0.507812  7.855718
26  0.494873  8.061045  0.499390  7.988128
27  0.497192  8.022355  0.507812  7.852286
28  0.495850  8.042340  0.499878  7.977494
29  0.496704  8.027552  0.509277  7.826597
30  0.503418  7.919576  0.504150  7.907494
31  0.500977  7.957750  0.503418  7.918510
32  0.491699  8.105068  0.499390  7.982220
33  0.491333  8.110459  0.490723  8.120004
34  0.495605  8.042010  0.499390  7.981544
35  0.499878  7.973649  0.512817  7.767265
36  0.489380  8.140837  0.499756  7.975349
37  0.501343  7.949997  0.494873  8.053092
38  0.499512  7.979105  0.489258  8.142546
39  0.507202  7.856448  0.500977  7.955679
40  0.501343  7.974011  0.498535  8.180579
41  0.509399  7.973088  0.497803  8.137571
42  0.513062  3.952798  0.503296  0.731334
43  0.495850  0.720961  0.506348  0.714261
44  0.493896  0.711961  0.503418  0.707717
45  0.570190  0.674856  0.683105  0.637276
46  0.813599  0.442905  0.774292  0.642186
47  0.885376  0.311670  0.843994  0.378536
48  0.909546  0.255044  0.827881  0.520586
49  0.920166  0.231733  0.824829  0.593265
50  0.918213  0.228329  0.872803  0.383775
51  0.926880  0.212183  0.856934  0.430726
52  0.928223  0.205540  0.870239  0.355302
53  0.932861  0.203841  0.909546  0.264572
54  0.935547  0.194982  0.910034  0.286021
55  0.936768  0.196914  0.914062  0.304287
56  0.940186  0.189410  0.921631  0.239104
57  0.936646  0.185937  0.906616  0.309827
58  0.941528  0.169988  0.886719  0.324128
59  0.938477  0.185185  0.896484  0.402161
60  0.942505  0.171304  0.922485  0.281508
61  0.942627  0.173616  0.928223  0.230012
62  0.944458  0.167852  0.893433  0.441927
63  0.945557  0.167308  0.920044  0.247058

2017-12-14 01:17:32.020880 Finish.
Total elapsed time: 07:48:11.02.
