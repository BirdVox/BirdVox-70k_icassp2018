2017-12-13 12:37:35.566615: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:35.566725: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:35.566734: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:35.566739: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:35.566743: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 12:37:33.218385 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.970215  0.111046  0.767334  0.672934
1   0.972168  0.113659  0.752563  1.145718
2   0.973267  0.107789  0.755981  0.968293
3   0.976318  0.093846  0.739014  1.361357
4   0.977539  0.089508  0.773438  1.018852
5   0.977173  0.093998  0.790405  0.734850
6   0.976562  0.100688  0.792725  0.833144
7   0.980591  0.084520  0.758179  1.078195
8   0.977783  0.090451  0.793335  0.732564
9   0.979248  0.085384  0.778442  1.132226
10  0.980713  0.078027  0.805786  0.855489
11  0.979980  0.083474  0.755981  1.322121
12  0.979370  0.081208  0.818481  0.761076
13  0.979248  0.081736  0.802856  0.745475
14  0.979492  0.088764  0.781494  0.791542
15  0.981689  0.085370  0.806641  0.649357
16  0.980835  0.085869  0.786987  1.136220
17  0.984619  0.068485  0.824463  0.558753
18  0.982178  0.070877  0.807739  0.865256
19  0.980957  0.079079  0.801514  0.778609
20  0.980347  0.081846  0.785767  0.599451
21  0.983398  0.076739  0.815186  0.691226
22  0.983887  0.072461  0.816162  0.667144
23  0.983032  0.074394  0.793213  0.751798
24  0.984009  0.064394  0.808594  0.837831
25  0.982666  0.068870  0.768188  1.312472
26  0.983398  0.068187  0.792236  0.881166
27  0.984253  0.064542  0.772705  1.103369
28  0.987427  0.062246  0.792236  0.847933
29  0.983765  0.068592  0.815186  0.770392
30  0.983398  0.069226  0.821411  0.821440
31  0.983887  0.073059  0.815552  0.831489
32  0.984375  0.067310  0.805054  1.112865
33  0.980835  0.078570  0.813721  0.701764
34  0.983643  0.071335  0.824341  0.799555
35  0.982788  0.071923  0.761353  1.396326
36  0.985718  0.066783  0.793945  1.192151
37  0.986328  0.059927  0.832886  0.552848
38  0.985962  0.064186  0.801880  0.982650
39  0.985352  0.064445  0.797852  0.933587
40  0.984253  0.065983  0.801392  0.733408
41  0.984375  0.065392  0.793701  0.719978
42  0.982666  0.063536  0.802124  0.768420
43  0.985962  0.059876  0.761597  1.321128
44  0.982910  0.067126  0.805176  0.776159
45  0.984375  0.066628  0.799927  1.120337
46  0.984497  0.065223  0.806396  0.815803
47  0.985352  0.060465  0.820068  0.645529
48  0.986328  0.056520  0.802002  0.904249
49  0.985596  0.059495  0.777954  0.909194
50  0.983154  0.067499  0.802979  0.806112
51  0.988037  0.055925  0.795532  1.120113
52  0.985596  0.060535  0.799072  0.902790
53  0.987793  0.056044  0.827271  0.657067
54  0.986450  0.054468  0.781982  1.134306
55  0.983643  0.063840  0.801025  0.835439
56  0.986816  0.058017  0.804565  0.961146
57  0.987061  0.056534  0.821411  0.779321
58  0.984863  0.062131  0.807373  0.824069
59  0.984863  0.061282  0.810669  0.775900
60  0.986694  0.057520  0.812500  0.795908
61  0.986206  0.063238  0.791138  1.126744
62  0.989624  0.049566  0.807983  0.915474
63  0.986450  0.061471  0.788574  0.852374

2017-12-13 20:20:54.286806 Finish.
Total elapsed time: 07:43:21.29.
