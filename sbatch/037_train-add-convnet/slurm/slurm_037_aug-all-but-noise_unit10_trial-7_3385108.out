2017-12-13 12:38:35.120418: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:38:35.120656: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:38:35.120666: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:38:35.120671: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:38:35.120675: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 12:38:33.188144 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit01, unit02, unit03.
Validation set: unit05, unit07.
Test set: unit10.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.944336  0.166734  0.886108  0.261736
1   0.944092  0.169752  0.856445  0.316588
2   0.953125  0.141240  0.932373  0.187001
3   0.952271  0.145120  0.905396  0.232305
4   0.954712  0.141593  0.950439  0.147880
5   0.959961  0.131016  0.938599  0.169186
6   0.960205  0.131611  0.926880  0.189444
7   0.961792  0.125691  0.868286  0.305221
8   0.958252  0.132352  0.825195  0.417408
9   0.960571  0.125909  0.856689  0.328514
10  0.962402  0.123721  0.856812  0.319422
11  0.959229  0.123773  0.910767  0.202330
12  0.965942  0.123210  0.904541  0.229071
13  0.959839  0.124708  0.948975  0.150417
14  0.960938  0.127117  0.906006  0.226250
15  0.959595  0.127157  0.787231  0.483415
16  0.964355  0.116167  0.865356  0.287188
17  0.962402  0.120972  0.942139  0.159205
18  0.967285  0.111080  0.919434  0.208003
19  0.964966  0.119543  0.919434  0.187619
20  0.967407  0.110270  0.946655  0.160109
21  0.964233  0.117164  0.907349  0.217956
22  0.965576  0.116443  0.941284  0.152769
23  0.969238  0.106670  0.911255  0.205154
24  0.968628  0.107228  0.915649  0.205781
25  0.968506  0.114106  0.947632  0.147534
26  0.967773  0.104086  0.927246  0.190303
27  0.965698  0.113419  0.928589  0.183974
28  0.967041  0.109962  0.958008  0.141989
29  0.970459  0.103618  0.908569  0.202860
30  0.969116  0.108338  0.962280  0.131215
31  0.968628  0.112728  0.953735  0.150351
32  0.968994  0.107066  0.940308  0.171612
33  0.971924  0.102194  0.942383  0.154829
34  0.965698  0.111876  0.904053  0.234411
35  0.968628  0.106235  0.913086  0.220766
36  0.972290  0.103522  0.954712  0.147171
37  0.965332  0.115786  0.906982  0.234305
38  0.965088  0.121661  0.915649  0.221193
39  0.973755  0.092218  0.921509  0.204428
40  0.967651  0.108627  0.896729  0.262560
41  0.968262  0.109292  0.949707  0.147817
42  0.972290  0.099207  0.909546  0.228941
43  0.970459  0.102710  0.902344  0.263818
44  0.971924  0.094110  0.914917  0.223932
45  0.971313  0.101846  0.951416  0.148864
46  0.971436  0.110497  0.927368  0.199609
47  0.978271  0.086770  0.930176  0.198357
48  0.970215  0.103378  0.940552  0.169166
49  0.972900  0.099293  0.938599  0.173960
50  0.971680  0.102130  0.925537  0.204584
51  0.970093  0.103115  0.963867  0.126819
52  0.971191  0.100296  0.900269  0.256864
53  0.972656  0.097337  0.925049  0.195987
54  0.972778  0.099479  0.914307  0.210357
55  0.977661  0.088156  0.878540  0.365641
56  0.972534  0.098105  0.939331  0.169585
57  0.976074  0.092987  0.949341  0.160129
58  0.973877  0.092229  0.926147  0.201137
59  0.972534  0.102199  0.934692  0.197810
60  0.975098  0.095799  0.916504  0.208571
61  0.968994  0.102810  0.949829  0.150348
62  0.972046  0.099361  0.915283  0.220345
63  0.977417  0.084647  0.924683  0.190990

2017-12-13 20:41:29.995306 Finish.
Total elapsed time: 08:02:57.00.
