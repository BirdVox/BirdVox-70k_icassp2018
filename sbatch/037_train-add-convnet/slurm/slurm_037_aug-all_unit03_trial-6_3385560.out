2017-12-13 17:29:26.846181: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:26.846485: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:26.846500: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 17:29:21.539075 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.973877  0.110907  0.821411  0.831728
1   0.968994  0.125263  0.845581  0.484220
2   0.971558  0.111083  0.838135  0.626490
3   0.973511  0.103811  0.871460  0.423516
4   0.977051  0.099519  0.830322  0.797556
5   0.977905  0.090437  0.889893  0.392703
6   0.976807  0.101738  0.869263  0.428944
7   0.977295  0.089085  0.869629  0.545837
8   0.972412  0.107094  0.854736  0.474117
9   0.977173  0.100697  0.865845  0.470143
10  0.979980  0.089027  0.870483  0.505562
11  0.977295  0.089132  0.869995  0.424625
12  0.979858  0.080264  0.876953  0.495291
13  0.979126  0.081718  0.875732  0.403819
14  0.979980  0.096281  0.860229  0.492015
15  0.975708  0.095735  0.869751  0.418297
16  0.980835  0.083194  0.880615  0.393786
17  0.981812  0.078283  0.890503  0.389670
18  0.980957  0.080578  0.824097  0.675223
19  0.983154  0.071619  0.880249  0.463944
20  0.979126  0.083932  0.872192  0.425636
21  0.981689  0.078766  0.887695  0.397610
22  0.980957  0.082008  0.847046  0.504287
23  0.981689  0.076378  0.873413  0.574546
24  0.982544  0.071936  0.868652  0.484896
25  0.979980  0.084391  0.855957  0.498492
26  0.980713  0.081443  0.874878  0.498532
27  0.982056  0.075313  0.902954  0.330749
28  0.980713  0.081562  0.886963  0.451164
29  0.984131  0.068207  0.866333  0.659526
30  0.983521  0.070340  0.887451  0.452891
31  0.983398  0.071581  0.860596  0.630410
32  0.982666  0.073854  0.882568  0.455852
33  0.982910  0.070011  0.903564  0.336584
34  0.982056  0.074873  0.888428  0.414499
35  0.984009  0.074641  0.853394  0.623121
36  0.985352  0.060279  0.879395  0.460128
37  0.980103  0.076924  0.887573  0.400733
38  0.985474  0.066686  0.860718  0.647694
39  0.984253  0.064987  0.885742  0.486917
40  0.985962  0.058802  0.892334  0.408420
41  0.985718  0.063385  0.886108  0.439386
42  0.983398  0.065836  0.879761  0.450951
43  0.985718  0.063137  0.902466  0.322631
44  0.986328  0.065311  0.898682  0.327514
45  0.984741  0.065061  0.893066  0.400502
46  0.982910  0.064837  0.848511  0.705801
47  0.982788  0.073726  0.882568  0.446836
48  0.985596  0.061737  0.884399  0.464106
49  0.984985  0.067074  0.877319  0.554856
50  0.983521  0.073900  0.880981  0.523998
51  0.984741  0.062008  0.862793  0.550168
52  0.984009  0.068198  0.891113  0.387343
53  0.984741  0.068899  0.886475  0.371185
54  0.986328  0.059725  0.894775  0.366301
55  0.986084  0.062484  0.888916  0.387639
56  0.984985  0.059704  0.884155  0.497214
57  0.985596  0.057580  0.911865  0.313378
58  0.985962  0.058566  0.884766  0.510455
59  0.983765  0.060140  0.898071  0.478060
60  0.983887  0.067377  0.869873  0.443724
61  0.983154  0.070481  0.902100  0.316206
62  0.987427  0.057709  0.898560  0.359460
63  0.985718  0.063552  0.894165  0.367020

2017-12-14 01:28:39.966237 Finish.
Total elapsed time: 07:59:18.97.
