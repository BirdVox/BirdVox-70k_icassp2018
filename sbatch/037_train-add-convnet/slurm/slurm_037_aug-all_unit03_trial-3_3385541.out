2017-12-13 17:29:27.818927: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:27.819219: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:27.819231: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 17:29:21.624761 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.494995  8.142975  0.492065  8.189386
1   0.510864  7.885828  0.503296  8.007347
2   0.504883  7.981444  0.493896  8.158250
3   0.497681  8.097066  0.501831  8.030010
4   0.499756  8.063347  0.495361  8.134085
5   0.499756  8.063189  0.502441  8.019849
6   0.485352  8.295268  0.496216  8.120126
7   0.504517  7.986312  0.496948  8.108087
8   0.489502  8.228290  0.491699  8.123119
9   0.494019  8.129163  0.501709  8.073132
10  0.497803  8.086038  0.498047  8.050870
11  0.502441  7.967998  0.505371  7.911870
12  0.503906  7.929802  0.490601  8.137600
13  0.490356  8.138677  0.492432  8.103243
14  0.490723  8.128834  0.500854  7.965881
15  0.493408  8.083526  0.507446  7.858780
16  0.501221  7.957290  0.498047  8.007218
17  0.499756  7.979427  0.493652  8.076230
18  0.495239  8.050511  0.502197  7.939193
19  0.501465  7.950538  0.499268  7.985257
20  0.500122  7.971366  0.497314  8.015875
21  0.506470  7.869701  0.502075  7.939556
22  0.494385  8.061984  0.500244  7.968406
23  0.492676  8.088923  0.501587  7.946725
24  0.501343  7.950504  0.496094  8.034081
25  0.491089  8.113782  0.507568  7.850978
26  0.502563  7.930700  0.491821  8.101893
27  0.508545  7.835228  0.507202  7.856588
28  0.494141  8.064783  0.494019  8.066695
29  0.500244  7.967418  0.489502  8.138650
30  0.498047  8.002405  0.503906  7.908977
31  0.503296  7.918695  0.503418  7.916738
32  0.494629  8.056848  0.506836  7.862232
33  0.500610  7.961478  0.509399  7.821355
34  0.498657  7.992608  0.496460  8.027636
35  0.506348  7.870000  0.502686  7.928382
36  0.501831  7.942003  0.502808  7.926434
37  0.489258  8.175070  0.509033  8.032917
38  0.497437  8.211309  0.493652  8.263623
39  0.504272  8.085219  0.502686  8.103983
40  0.493164  8.251421  0.497070  8.182678
41  0.503418  8.075131  0.498779  8.144840
42  0.490234  8.277938  0.504761  8.039310
43  0.499023  8.127651  0.504639  8.033128
44  0.496948  8.153379  0.504150  8.033693
45  0.499023  8.113009  0.499390  8.103882
46  0.493530  8.195358  0.504272  8.019340
47  0.495728  8.154437  0.494751  8.167633
48  0.492676  8.198768  0.498657  8.100131
49  0.492920  8.190596  0.504639  7.999785
50  0.498779  8.092506  0.494751  8.155793
51  0.500488  8.061870  0.492676  8.186419
52  0.490967  8.212768  0.497314  8.109329
53  0.498413  8.090654  0.497803  8.099588
54  0.502075  8.029961  0.493652  8.165015
55  0.495850  8.129013  0.498535  8.085190
56  0.504028  7.996214  0.489746  8.225407
57  0.501587  8.034854  0.497192  8.077471
58  0.488281  8.248819  0.504517  7.985126
59  0.629639  3.677392  0.664673  1.048446
60  0.919312  0.245757  0.723022  2.030737
61  0.942505  0.181231  0.734009  1.025622
62  0.953491  0.148673  0.766235  0.797348
63  0.954346  0.142073  0.746582  1.420037

2017-12-14 01:27:36.162560 Finish.
Total elapsed time: 07:58:15.16.
