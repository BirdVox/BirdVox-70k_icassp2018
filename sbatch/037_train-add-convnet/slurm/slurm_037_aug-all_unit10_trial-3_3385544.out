2017-12-13 17:29:38.381136: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:38.381405: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:38.381417: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 17:29:35.016483 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit01, unit02, unit03.
Validation set: unit05, unit07.
Test set: unit10.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.496338  8.029575  0.496582  8.025683
1   0.497681  8.008168  0.504150  7.905025
2   0.505005  7.891403  0.503296  7.918648
3   0.491577  8.105473  0.512085  7.778529
4   0.506958  7.860265  0.505005  7.891403
5   0.503052  7.922540  0.506592  7.866103
6   0.501587  7.945893  0.502686  7.928378
7   0.504639  7.897241  0.495850  8.037359
8   0.494629  8.056820  0.502563  7.930324
9   0.495605  8.041252  0.499512  7.978977
10  0.497559  8.010114  0.498535  7.994545
11  0.499023  7.986761  0.498291  7.998438
12  0.493530  8.074335  0.504150  7.905025
13  0.498779  7.990653  0.500366  7.965354
14  0.493042  8.082120  0.500977  7.955624
15  0.492432  8.091850  0.494751  8.054874
16  0.494751  8.054874  0.498413  7.996492
17  0.492798  8.086012  0.497803  8.006222
18  0.504150  7.905025  0.500000  7.971192
19  0.502197  7.936163  0.494141  8.064605
20  0.495972  8.035413  0.488037  8.161909
21  0.492676  8.087958  0.492065  8.097688
22  0.494507  8.058766  0.501587  7.945893
23  0.508545  7.834966  0.496338  8.029575
24  0.492554  8.089904  0.502808  7.926432
25  0.494263  8.062659  0.490356  8.124934
26  0.510986  7.796044  0.492676  8.087958
27  0.496460  8.027629  0.494873  8.052928
28  0.499268  7.982869  0.511841  7.782421
29  0.494751  8.054874  0.498901  7.988707
30  0.503540  7.914756  0.500977  7.955624
31  0.514771  7.735715  0.497314  8.014006
32  0.499756  7.975085  0.494263  8.062659
33  0.496948  8.019845  0.510254  7.807721
34  0.496338  8.029575  0.499512  7.978977
35  0.498291  7.998438  0.500366  7.965354
36  0.505005  7.891403  0.503906  7.908917
37  0.505005  7.891403  0.495605  8.041252
38  0.504395  7.901133  0.505127  7.889457
39  0.499146  7.984815  0.503174  7.920594
40  0.497559  8.010114  0.497925  8.004276
41  0.497437  8.012060  0.502686  7.928378
42  0.497559  8.010114  0.489380  8.140502
43  0.501221  7.951731  0.494385  8.060713
44  0.497803  8.006222  0.497314  8.014006
45  0.510376  7.805775  0.512085  7.778529
46  0.498047  8.002330  0.503784  7.910864
47  0.498901  7.988707  0.498779  7.990653
48  0.492920  8.084066  0.496948  8.019845
49  0.497192  8.015952  0.509277  7.823289
50  0.501221  7.951731  0.508545  7.834966
51  0.487915  8.163855  0.504761  7.895295
52  0.495239  8.047090  0.511475  7.788260
53  0.497559  8.010114  0.506104  7.873888
54  0.497681  8.008168  0.504150  7.905025
55  0.494263  8.062659  0.511719  7.784368
56  0.500732  7.959516  0.495605  8.041252
57  0.505981  7.875834  0.500000  7.971192
58  0.500488  7.963408  0.504028  7.906971
59  0.506348  7.869996  0.493652  8.072389
60  0.496216  8.031521  0.507324  7.854427
61  0.512695  7.768799  0.492310  8.093796
62  0.497070  8.017899  0.507202  7.856373
63  0.502930  7.924486  0.501221  7.951731

2017-12-14 01:18:54.590483 Finish.
Total elapsed time: 07:49:19.59.
