2018-01-19 14:09:14.142916: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-19 14:09:14.143254: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-19 14:09:14.143272: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-01-19 14:09:14.143279: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-19 14:09:14.143286: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-01-19 14:07:49.987501 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit01, unit02, unit03.
Validation set: unit05, unit07.
Test set: unit10.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.953491  0.149548  0.907593  0.244457
1   0.957031  0.137479  0.926880  0.248462
2   0.955811  0.135356  0.926270  0.198517
3   0.960449  0.131486  0.929077  0.202238
4   0.962524  0.123799  0.936279  0.213584
5   0.960815  0.131457  0.910767  0.208408
6   0.962891  0.125461  0.897339  0.248678
7   0.964111  0.117863  0.920898  0.217321
8   0.962524  0.117320  0.925537  0.198689
9   0.964233  0.118812  0.890015  0.254941
10  0.967407  0.117143  0.944946  0.178043
11  0.968750  0.107906  0.930542  0.195871
12  0.972046  0.107611  0.927002  0.246256
13  0.968628  0.109606  0.925293  0.214328
14  0.968628  0.107001  0.941040  0.185589
15  0.969727  0.105286  0.918945  0.235842
16  0.969482  0.111792  0.933716  0.178426
17  0.973633  0.100435  0.922852  0.193729
18  0.974976  0.093350  0.934814  0.199578
19  0.968994  0.106398  0.930908  0.198548
20  0.968994  0.107904  0.945068  0.185507
21  0.973389  0.093351  0.878296  0.291104
22  0.972046  0.094257  0.845337  0.363789
23  0.974976  0.090418  0.879517  0.287013
24  0.975586  0.094367  0.952148  0.149587
25  0.972534  0.096928  0.907715  0.233451
26  0.975586  0.090619  0.880493  0.293727
27  0.972778  0.096996  0.822388  0.407954
28  0.975098  0.093328  0.920166  0.195503
29  0.970825  0.101254  0.942749  0.185232
30  0.976074  0.091116  0.937988  0.193919
31  0.973389  0.092826  0.934937  0.197509
32  0.972412  0.096527  0.871582  0.271561
33  0.976562  0.084142  0.927612  0.187494
34  0.976929  0.086210  0.906128  0.220292
35  0.979614  0.083151  0.867432  0.303256
36  0.975098  0.091190  0.910767  0.215467
37  0.973755  0.088395  0.951660  0.160241
38  0.978271  0.082860  0.880493  0.270286
39  0.979004  0.080775  0.836792  0.396146
40  0.973389  0.095735  0.878296  0.276574
41  0.978149  0.086041  0.927246  0.195002
42  0.976196  0.083882  0.839478  0.373083
43  0.978271  0.083229  0.905884  0.228042
44  0.978516  0.080167  0.818237  0.391646
45  0.978394  0.082159  0.780029  0.570614
46  0.978149  0.081496  0.910400  0.229681
47  0.978027  0.079265  0.903442  0.243779
48  0.977417  0.080852  0.912964  0.230503
49  0.976685  0.080492  0.920532  0.203081
50  0.977783  0.082539  0.946167  0.175011
51  0.978638  0.082844  0.933105  0.202529
52  0.976562  0.083720  0.906494  0.270525
53  0.978882  0.080659  0.937012  0.172724
54  0.978149  0.083211  0.897217  0.267327
55  0.981812  0.075894  0.865967  0.305432
56  0.978760  0.082495  0.895630  0.247374
57  0.979126  0.078933  0.946655  0.179579
58  0.979736  0.079187  0.948120  0.179404
59  0.981079  0.075626  0.953857  0.167652
60  0.979614  0.077324  0.825317  0.369703
61  0.977661  0.080590  0.757690  0.659065
62  0.978882  0.078495  0.890503  0.269104
63  0.978882  0.077181  0.958740  0.144735

2018-01-19 22:14:47.320257 Finish.
Total elapsed time: 08:06:58.32.
