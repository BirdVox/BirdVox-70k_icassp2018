2018-02-12 19:59:48.158972: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 19:59:48.159185: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-02-12 19:59:48.159198: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-02-12 19:59:32.975223 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
          acc      loss   val_acc  val_loss
0    0.972900  0.110723  0.787354  0.880844
1    0.973999  0.101951  0.710693  2.320154
2    0.979492  0.087617  0.768188  1.356528
3    0.978149  0.091027  0.768555  1.357535
4    0.977173  0.097159  0.811890  0.769729
5    0.977661  0.090469  0.769531  1.164143
6    0.983032  0.070444  0.782471  0.972367
7    0.978516  0.086266  0.783447  1.107401
8    0.982178  0.073685  0.787842  1.228576
9    0.981934  0.072055  0.766846  0.843957
10   0.981201  0.081752  0.763428  1.256051
11   0.981689  0.076296  0.830933  0.618711
12   0.982666  0.074694  0.824463  0.874185
13   0.984863  0.072969  0.776245  0.769448
14   0.981323  0.081178  0.796021  0.923973
15   0.981567  0.074380  0.775024  1.239313
16   0.985962  0.061454  0.811035  0.938639
17   0.985474  0.064288  0.833130  0.558995
18   0.983765  0.067149  0.776367  1.257533
19   0.983643  0.066807  0.781616  1.037003
20   0.985596  0.065094  0.795044  0.810174
21   0.983643  0.062904  0.822510  0.781171
22   0.988159  0.056426  0.789307  1.180961
23   0.984619  0.059988  0.815796  0.683143
24   0.983765  0.068636  0.806763  0.818548
25   0.984619  0.063478  0.810547  0.722460
26   0.986938  0.056322  0.818481  0.852177
27   0.981812  0.072519  0.825562  0.644007
28   0.987427  0.063192  0.809448  0.736375
29   0.983643  0.067199  0.802246  0.741917
30   0.983521  0.062371  0.802979  0.824253
31   0.984619  0.062484  0.757812  1.968614
32   0.986572  0.064412  0.809814  0.693014
33   0.985962  0.058179  0.793457  0.938907
34   0.985962  0.060963  0.822021  0.696960
35   0.987427  0.054470  0.828857  0.565441
36   0.986694  0.053935  0.788452  0.749002
37   0.984985  0.061212  0.784058  0.763696
38   0.986328  0.057573  0.840576  0.464406
39   0.987915  0.052025  0.823242  0.535125
40   0.987305  0.060183  0.818115  0.778389
41   0.986206  0.061815  0.809082  0.782707
42   0.987183  0.059032  0.820923  0.623516
43   0.988037  0.054777  0.830688  0.594090
44   0.986572  0.059430  0.846069  0.467851
45   0.988525  0.055296  0.829590  0.644640
46   0.986694  0.059478  0.798950  0.942663
47   0.984253  0.062663  0.846680  0.469775
48   0.987305  0.056740  0.812378  0.739172
49   0.987915  0.050905  0.830933  0.742673
50   0.986816  0.050925  0.831299  0.659132
51   0.989990  0.049111  0.806519  0.921086
52   0.986694  0.054889  0.803589  0.761680
53   0.988403  0.057332  0.822021  0.733500
54   0.987305  0.055488  0.791870  0.962622
55   0.985962  0.058310  0.820435  0.628351
56   0.988647  0.049180  0.802612  1.009664
57   0.990112  0.051866  0.799438  0.832332
58   0.988770  0.049552  0.806396  0.708350
59   0.988892  0.052732  0.816895  0.829142
60   0.987915  0.050291  0.799561  0.887810
61   0.984009  0.061532  0.800171  0.776794
62   0.991577  0.044060  0.796021  0.898984
63   0.989014  0.047675  0.805664  0.700564
64   0.986450  0.052136  0.842651  0.468881
65   0.989380  0.046907  0.810425  0.777904
66   0.986938  0.055151  0.800171  0.680772
67   0.987671  0.055744  0.824341  0.726397
68   0.988037  0.049593  0.802856  0.711393
69   0.987671  0.055353  0.858154  0.506267
70   0.988525  0.051042  0.814453  0.717930
71   0.989868  0.044532  0.810181  0.991960
72   0.989624  0.045492  0.847046  0.570891
73   0.988037  0.052523  0.799805  0.870146
74   0.989990  0.047014  0.796143  0.755533
75   0.988037  0.050651  0.819946  0.669916
76   0.989624  0.047856  0.821167  0.868721
77   0.985352  0.057949  0.838623  0.555956
78   0.990112  0.041408  0.850098  0.610369
79   0.987915  0.050229  0.814819  0.534208
80   0.989258  0.042660  0.796997  1.090208
81   0.990356  0.042076  0.808594  0.864088
82   0.987915  0.049238  0.792603  0.977397
83   0.989990  0.044269  0.783691  0.883810
84   0.985962  0.056735  0.817017  0.980528
85   0.989258  0.050073  0.787842  1.071953
86   0.989502  0.045514  0.849243  0.532309
87   0.990967  0.042078  0.827148  0.671453
88   0.989746  0.044789  0.828247  0.736395
89   0.987793  0.052271  0.844482  0.443587
90   0.988281  0.046016  0.847046  0.464171
91   0.986206  0.054325  0.810669  0.812502
92   0.989746  0.043488  0.796021  1.421875
93   0.989258  0.047760  0.816895  0.789928
94   0.985840  0.052804  0.789307  1.202644
95   0.992676  0.039820  0.820679  0.802595
96   0.989868  0.043814  0.816895  0.763821
97   0.988403  0.049735  0.828491  0.638466
98   0.991333  0.043278  0.815186  0.616645
99   0.988647  0.046366  0.800781  0.883037
100  0.991089  0.043439  0.800903  0.897043
101  0.989502  0.045848  0.821899  0.612432
102  0.989136  0.046567  0.813477  0.765563
103  0.989624  0.049835  0.823853  0.706488
104  0.989136  0.042719  0.798706  1.082975
105  0.988770  0.043494  0.822632  0.878420
106  0.989868  0.039783  0.848267  0.480582
107  0.987427  0.049484  0.830688  0.523454
108  0.990112  0.044069  0.863281  0.533757
109  0.989014  0.042416  0.829224  0.656481
110  0.990234  0.039716  0.820557  0.795980
111  0.988159  0.048458  0.790527  1.260513
112  0.989868  0.040814  0.843262  0.547707
113  0.990601  0.040912  0.833740  0.706250
114  0.989624  0.042479  0.799561  0.903571
115  0.989746  0.046373  0.787598  1.077397
116  0.988281  0.046760  0.794800  1.022049
117  0.988403  0.047021  0.849243  0.549113
118  0.990356  0.042449  0.791870  0.940677
119  0.991699  0.037989  0.790527  1.046461
120  0.987427  0.049165  0.814575  0.767615
121  0.989014  0.040632  0.838745  0.554912
122  0.989624  0.042555  0.794678  1.061055
123  0.988770  0.044225  0.805176  0.754494
124  0.991577  0.038057  0.822266  0.604333
125  0.990723  0.040751  0.804321  1.012929
126  0.990234  0.041078  0.815918  0.675876
127  0.988403  0.042516  0.805542  0.816791

2018-02-13 11:06:11.818414 Finish.
Total elapsed time: 15:06:39.82.
