2017-12-13 12:37:35.125557: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:35.125815: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:35.125826: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:35.125830: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:35.125835: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 12:37:33.299228 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit07, unit10, unit01.
Validation set: unit02, unit03.
Test set: unit05.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.492065  8.186938  0.498535  8.082658
1   0.501099  8.041340  0.502075  8.025600
2   0.501587  8.033470  0.506226  7.958703
3   0.499146  8.072821  0.498169  8.088561
4   0.496338  8.118074  0.499756  8.062983
5   0.497314  8.102334  0.502441  8.019697
6   0.499268  8.070853  0.502563  8.017729
7   0.498413  8.084626  0.498901  8.076756
8   0.496826  8.110204  0.497192  8.104301
9   0.505371  7.972476  0.508667  7.919352
10  0.493774  8.159392  0.498657  8.080691
11  0.493408  8.165295  0.494019  8.155457
12  0.503540  8.001989  0.507202  7.942963
13  0.498779  8.078723  0.499268  8.070853
14  0.497681  8.096431  0.500244  8.055113
15  0.505249  7.974444  0.498169  8.088561
16  0.502197  8.023632  0.496582  8.114139
17  0.500732  8.047243  0.496460  8.116106
18  0.500122  8.057080  0.503540  8.001989
19  0.496094  8.122009  0.494995  8.139717
20  0.504028  7.994119  0.505493  7.970508
21  0.508179  7.927223  0.496582  8.114139
22  0.503784  7.998054  0.501343  8.037405
23  0.505615  7.968541  0.500366  8.053145
24  0.504150  7.992151  0.507202  7.942963
25  0.498779  8.078723  0.494995  8.139717
26  0.503174  8.007892  0.513550  7.840651
27  0.495605  8.129879  0.499390  8.068885
28  0.489868  8.222354  0.506714  7.950833
29  0.497070  8.106269  0.502563  8.017729
30  0.492432  8.181035  0.503540  8.001989
31  0.503418  8.003957  0.501587  8.033470
32  0.496704  8.112171  0.501465  8.035437
33  0.509155  7.911482  0.485962  8.285315
34  0.496216  8.120042  0.508911  7.915417
35  0.498535  8.082658  0.507202  7.942963
36  0.495605  8.129879  0.501709  8.031502
37  0.497559  8.098399  0.493164  8.169230
38  0.497192  8.104301  0.503296  8.005924
39  0.501221  8.039372  0.497437  8.100366
40  0.506226  7.958703  0.504150  7.992151
41  0.507935  7.931158  0.496704  8.112171
42  0.500977  8.043307  0.493774  8.159392
43  0.499756  8.062983  0.484741  8.304990
44  0.499146  8.072821  0.502930  8.011827
45  0.497925  8.092496  0.497070  8.106269
46  0.497314  8.102334  0.498413  8.084626
47  0.491333  8.198743  0.495972  8.123977
48  0.507935  7.931158  0.501587  8.033470
49  0.498657  8.080691  0.493896  8.157425
50  0.498657  8.080691  0.505005  7.978379
51  0.506226  7.958703  0.499390  8.068886
52  0.489136  8.234159  0.496948  8.108236
53  0.491333  8.198743  0.498535  8.082658
54  0.505981  7.962638  0.495972  8.123977
55  0.499756  8.062983  0.500732  8.047243
56  0.497925  8.092496  0.509644  7.903612
57  0.491699  8.192841  0.500977  8.043307
58  0.504028  7.994119  0.510132  7.895742
59  0.498291  8.086593  0.498047  8.090528
60  0.493408  8.165295  0.495483  8.131847
61  0.504395  7.988216  0.514771  7.820975
62  0.500488  8.051178  0.502930  8.011827
63  0.493286  8.167263  0.507446  7.939028

2017-12-13 20:38:51.352321 Finish.
Total elapsed time: 08:01:18.35.
