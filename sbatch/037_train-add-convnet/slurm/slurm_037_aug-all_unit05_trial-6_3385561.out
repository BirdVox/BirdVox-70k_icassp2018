2017-12-13 17:29:27.895294: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:27.895564: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:27.895577: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 17:29:21.539072 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit07, unit10, unit01.
Validation set: unit02, unit03.
Test set: unit05.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.499268  8.078152  0.503174  8.013985
1   0.502441  8.024845  0.499756  8.067283
2   0.494263  8.155151  0.493042  8.174222
3   0.498413  8.087170  0.493286  8.169374
4   0.500244  8.056881  0.497070  8.107728
5   0.505737  7.967788  0.499268  8.071848
6   0.504883  7.981168  0.502686  8.016429
7   0.510010  7.898256  0.504883  7.980786
8   0.495605  8.130236  0.506714  7.951117
9   0.493286  8.167491  0.500488  8.051357
10  0.509155  7.911625  0.499756  8.063094
11  0.500244  8.055199  0.493652  8.161426
12  0.510986  7.882021  0.501587  8.033509
13  0.500000  8.059077  0.501709  8.031524
14  0.490967  8.204662  0.498047  8.090541
15  0.499512  8.066927  0.502319  8.021671
16  0.506958  7.946903  0.501953  8.027570
17  0.498901  8.076758  0.500977  8.043309
18  0.496948  8.108237  0.495361  8.133815
19  0.496338  8.118074  0.510254  7.893775
20  0.492432  8.181035  0.500732  8.047243
21  0.499634  8.064950  0.500610  8.049210
22  0.508667  7.919352  0.500244  8.055113
23  0.495972  8.123977  0.492554  8.179068
24  0.499268  8.070853  0.489258  8.232191
25  0.498413  8.084626  0.500122  8.057080
26  0.496094  8.122009  0.507324  7.940995
27  0.484985  8.301055  0.498047  8.090528
28  0.502197  8.023632  0.500977  8.043307
29  0.510498  7.889839  0.503052  8.009859
30  0.493042  8.171198  0.496338  8.118074
31  0.505859  7.964606  0.496826  8.110204
32  0.506348  7.956736  0.497925  8.092496
33  0.487427  8.261704  0.503784  7.998054
34  0.501709  8.031502  0.494751  8.143652
35  0.502808  8.013794  0.509399  7.907547
36  0.501099  8.041340  0.497314  8.102334
37  0.494873  8.141684  0.496826  8.110204
38  0.508789  7.917385  0.495728  8.127912
39  0.499023  8.074788  0.502563  8.017729
40  0.499023  8.074788  0.500244  8.055113
41  0.489990  8.220386  0.505859  7.964606
42  0.500244  8.055113  0.487671  8.257769
43  0.495728  8.127912  0.503174  8.007892
44  0.493164  8.169230  0.498047  8.090528
45  0.488525  8.243997  0.503540  8.001989
46  0.500488  8.051178  0.496094  8.122009
47  0.503418  8.003957  0.488892  8.238094
48  0.503906  7.996086  0.495239  8.135782
49  0.501099  8.041340  0.497803  8.094463
50  0.507568  7.937060  0.500610  8.049210
51  0.500488  8.051178  0.506836  7.948865
52  0.495361  8.133814  0.496216  8.120042
53  0.494019  8.155457  0.504883  7.980346
54  0.491333  8.198743  0.498657  8.080691
55  0.500854  8.045275  0.505249  7.974444
56  0.502075  8.025600  0.504639  7.984281
57  0.490112  8.218419  0.494995  8.139717
58  0.513550  7.840651  0.498901  8.076756
59  0.500244  8.055113  0.500610  8.049210
60  0.498779  8.078723  0.493774  8.159392
61  0.500000  8.059048  0.500122  8.057080
62  0.501709  8.031502  0.509766  7.901645
63  0.502075  8.025600  0.501587  8.033470

2017-12-14 01:26:06.680446 Finish.
Total elapsed time: 07:56:45.68.
