2017-12-13 12:37:36.037575: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:36.037788: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:36.037799: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:36.037803: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:36.037807: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 12:37:34.030935 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.491089  8.113257  0.499634  7.977031
1   0.493652  8.072389  0.506104  7.873888
2   0.507446  7.852481  0.506714  7.864157
3   0.491455  8.107419  0.506470  7.868049
4   0.498657  7.992599  0.498657  7.992599
5   0.506470  7.868049  0.496460  8.027629
6   0.511597  7.786314  0.495239  8.047090
7   0.493530  8.074335  0.503418  7.916702
8   0.495483  8.043198  0.499146  7.984815
9   0.501587  7.945893  0.497437  8.012060
10  0.507080  7.858319  0.507690  7.848589
11  0.501953  7.940055  0.497437  8.012060
12  0.498047  8.002330  0.501587  7.945893
13  0.500732  7.959516  0.499756  7.975085
14  0.499146  7.984815  0.492188  8.095742
15  0.496582  8.025683  0.499390  7.980923
16  0.505005  7.891403  0.493652  8.072389
17  0.502441  7.932271  0.499023  7.986761
18  0.501465  7.947839  0.508179  7.840804
19  0.496460  8.027629  0.498901  7.988707
20  0.494873  8.052928  0.493408  8.076281
21  0.503784  7.910864  0.500244  7.967300
22  0.497559  8.010114  0.493530  8.074335
23  0.502563  7.930324  0.490723  8.119095
24  0.506104  7.873888  0.498047  8.002330
25  0.504761  7.895295  0.506470  7.868049
26  0.506714  7.864157  0.495483  8.043198
27  0.498779  7.990653  0.499634  7.977031
28  0.496460  8.027629  0.496826  8.021791
29  0.498779  7.990653  0.499756  7.975085
30  0.499268  7.982869  0.499146  7.984815
31  0.507935  7.844696  0.505127  7.889457
32  0.503540  7.914756  0.500488  7.963408
33  0.495117  8.049036  0.501587  7.945893
34  0.499268  7.982869  0.499146  7.984815
35  0.502319  7.934217  0.487305  8.173586
36  0.495239  8.047090  0.500854  7.957570
37  0.503540  7.914756  0.497803  8.006222
38  0.500000  7.971192  0.503540  7.914756
39  0.499634  7.977031  0.495239  8.047090
40  0.502441  7.932271  0.500488  7.963408
41  0.497681  8.008168  0.489868  8.132718
42  0.494995  8.050982  0.497314  8.014006
43  0.499146  7.984815  0.502319  7.934217
44  0.483765  8.230023  0.501587  7.945893
45  0.499756  7.975085  0.498901  7.988707
46  0.499756  7.975085  0.494019  8.066551
47  0.498291  7.998438  0.496582  8.025683
48  0.501465  7.947839  0.506958  7.860265
49  0.497559  8.010114  0.494141  8.064605
50  0.502075  7.938109  0.503540  7.914756
51  0.511108  7.794098  0.494019  8.066551
52  0.495728  8.039306  0.497803  8.006222
53  0.495605  8.041252  0.501343  7.949785
54  0.504639  7.897241  0.500732  7.959516
55  0.490234  8.126880  0.491333  8.109365
56  0.505615  7.881672  0.494629  8.056820
57  0.493164  8.080173  0.501953  7.940055
58  0.502930  7.924486  0.492554  8.089904
59  0.499756  7.975085  0.508423  7.836912
60  0.499878  7.973138  0.497559  8.010114
61  0.492554  8.089904  0.498901  7.988707
62  0.507690  7.848589  0.506104  7.873888
63  0.495239  8.047090  0.500610  7.961462

2017-12-13 20:46:34.920029 Finish.
Total elapsed time: 08:09:00.92.
