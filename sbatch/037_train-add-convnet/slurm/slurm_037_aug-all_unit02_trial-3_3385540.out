2017-12-13 17:29:28.371561: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:28.371893: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:28.371905: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 17:29:21.624813 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.491821  8.101580  0.500610  7.961462
1   0.497192  8.015952  0.498291  7.998438
2   0.503662  7.912810  0.503662  7.912810
3   0.486206  8.191101  0.498535  7.994545
4   0.500244  7.967300  0.497070  8.017899
5   0.494873  8.052928  0.492920  8.084066
6   0.506104  7.873888  0.502197  7.936163
7   0.495239  8.047090  0.500732  7.959516
8   0.496582  8.025683  0.499390  7.980923
9   0.495728  8.039306  0.508789  7.831074
10  0.500854  7.957570  0.494995  8.050982
11  0.498047  8.002330  0.495239  8.047090
12  0.496704  8.023737  0.500732  7.959516
13  0.497803  8.006222  0.495361  8.045144
14  0.500000  7.971192  0.498779  7.990653
15  0.500977  7.955624  0.501465  7.947839
16  0.509399  7.821343  0.502441  7.932271
17  0.506348  7.869996  0.505737  7.879726
18  0.503296  7.918648  0.500977  7.955624
19  0.506958  7.860265  0.504639  7.897241
20  0.490479  8.122988  0.502441  7.932271
21  0.503906  7.908917  0.507935  7.844696
22  0.499023  7.986761  0.501709  7.943947
23  0.496216  8.031521  0.503662  7.912810
24  0.501221  7.951731  0.502808  7.926432
25  0.502197  7.936163  0.502808  7.926432
26  0.499268  7.982869  0.503296  7.918648
27  0.505371  7.885564  0.500000  7.971192
28  0.499390  7.980923  0.519409  7.661764
29  0.506470  7.868049  0.501953  7.940055
30  0.501587  7.945893  0.496704  8.023737
31  0.498535  7.994545  0.498779  7.990653
32  0.504517  7.899187  0.500610  7.961462
33  0.515869  7.718200  0.494995  8.050982
34  0.500732  7.959516  0.495483  8.043198
35  0.498901  7.988707  0.497559  8.010114
36  0.508545  7.834966  0.498901  7.988707
37  0.506958  7.860265  0.507324  7.854427
38  0.510254  7.807721  0.493408  8.076281
39  0.494263  8.062659  0.485962  8.194993
40  0.499390  7.980923  0.494751  8.054874
41  0.505981  7.875834  0.500366  7.965354
42  0.499878  7.973138  0.496338  8.029575
43  0.497314  8.014006  0.501343  7.949785
44  0.495605  8.041252  0.496582  8.025683
45  0.495728  8.039306  0.505859  7.877780
46  0.488159  8.159963  0.500122  7.969246
47  0.510376  7.805775  0.496704  8.023737
48  0.497925  8.004276  0.501831  7.942001
49  0.501465  7.947839  0.500854  7.957570
50  0.503540  7.914756  0.495117  8.049036
51  0.495728  8.039306  0.504028  7.906971
52  0.495483  8.043198  0.497681  8.008168
53  0.492676  8.087958  0.511230  7.792152
54  0.499268  7.982869  0.503784  7.910864
55  0.499390  7.980923  0.503906  7.908917
56  0.510498  7.803828  0.497314  8.014006
57  0.503784  7.910864  0.498291  7.998438
58  0.494019  8.066551  0.502808  7.926432
59  0.503052  7.922540  0.496826  8.021791
60  0.495117  8.049036  0.506226  7.871942
61  0.500854  7.957570  0.500610  7.961462
62  0.502808  7.926432  0.501343  7.949785
63  0.495361  8.045144  0.503662  7.912810

2017-12-14 01:26:33.273333 Finish.
Total elapsed time: 07:57:12.27.
