2017-12-13 12:37:36.120943: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:36.121107: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:36.121117: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:36.121122: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:37:36.121127: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 12:37:34.083464 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit02, unit03, unit05.
Validation set: unit07, unit10.
Test set: unit01.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.970581  0.110395  0.732788  0.615051
1   0.974609  0.098117  0.810303  0.434357
2   0.979980  0.080642  0.907471  0.271383
3   0.976318  0.092240  0.962524  0.167893
4   0.979004  0.077314  0.935547  0.199851
5   0.982544  0.071067  0.899902  0.261238
6   0.977905  0.084989  0.919189  0.218664
7   0.981445  0.075961  0.926514  0.229564
8   0.982422  0.076658  0.876343  0.355983
9   0.978394  0.079205  0.931030  0.210577
10  0.983643  0.072889  0.937622  0.204848
11  0.983032  0.065614  0.943481  0.180108
12  0.984985  0.063758  0.862183  0.328534
13  0.985107  0.065718  0.873047  0.326682
14  0.987549  0.057862  0.955078  0.158404
15  0.984741  0.067687  0.931396  0.218764
16  0.984131  0.066682  0.938965  0.178217
17  0.985840  0.059231  0.922363  0.249745
18  0.986816  0.058715  0.822388  0.460292
19  0.984619  0.062607  0.849243  0.397804
20  0.984497  0.067657  0.928101  0.207837
21  0.984741  0.063329  0.917236  0.218543
22  0.988037  0.052754  0.903809  0.264365
23  0.986084  0.058235  0.907349  0.267950
24  0.985962  0.056947  0.918335  0.230255
25  0.986938  0.058350  0.946777  0.167270
26  0.985352  0.059934  0.905884  0.253841
27  0.988281  0.059515  0.950928  0.166010
28  0.989380  0.050620  0.957520  0.149765
29  0.986938  0.057882  0.942505  0.188131
30  0.986206  0.059150  0.962524  0.145633
31  0.985229  0.059614  0.939941  0.186566
32  0.986694  0.062049  0.896973  0.275435
33  0.987305  0.059145  0.915405  0.269678
34  0.988037  0.050227  0.942871  0.189017
35  0.989502  0.048911  0.903320  0.285915
36  0.988159  0.054309  0.931519  0.222005
37  0.987061  0.053609  0.723633  0.847050
38  0.989624  0.050605  0.855835  0.385764
39  0.989014  0.048280  0.870483  0.333672
40  0.987549  0.053409  0.907837  0.260090
41  0.988159  0.053208  0.878174  0.344108
42  0.987915  0.052035  0.805054  0.512033
43  0.987671  0.053949  0.941284  0.177460
44  0.990234  0.048121  0.963013  0.136837
45  0.988892  0.048967  0.927246  0.213917
46  0.990601  0.045961  0.892334  0.281914
47  0.986694  0.055506  0.887085  0.296241
48  0.989624  0.047492  0.952026  0.173711
49  0.988525  0.051691  0.859985  0.373016
50  0.989380  0.048639  0.772095  0.601394
51  0.989868  0.044601  0.881226  0.341437
52  0.989868  0.050245  0.879883  0.316533
53  0.988525  0.049855  0.932739  0.215829
54  0.989746  0.047348  0.954468  0.166173
55  0.987061  0.051549  0.915894  0.241268
56  0.990723  0.043017  0.844238  0.464316
57  0.989502  0.044251  0.889771  0.307613
58  0.989380  0.047702  0.958618  0.159876
59  0.989746  0.046480  0.919189  0.226799
60  0.991333  0.039731  0.907471  0.289411
61  0.990479  0.040203  0.859863  0.360990
62  0.988281  0.047521  0.955688  0.173003
63  0.988525  0.049194  0.944214  0.208542

2017-12-13 20:48:54.079521 Finish.
Total elapsed time: 08:11:20.08.
