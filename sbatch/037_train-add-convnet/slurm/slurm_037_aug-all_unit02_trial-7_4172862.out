2018-01-19 14:09:12.244406: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-19 14:09:12.244774: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-19 14:09:12.244795: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-01-19 14:09:12.244804: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-19 14:09:12.244812: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-01-19 14:07:50.053365 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit03, unit05, unit07.
Validation set: unit10, unit01.
Test set: unit02.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.495605  8.041252  0.504517  7.841246
1   0.504028  7.906971  0.505371  7.832768
2   0.490845  8.117149  0.496582  7.967712
3   0.492188  8.095742  0.506226  7.812021
4   0.489502  8.138556  0.497192  7.957045
5   0.497559  8.010114  0.498413  7.939264
6   0.494873  8.052928  0.501587  7.887960
7   0.506470  7.868049  0.503296  7.864517
8   0.499756  7.975085  0.493774  8.012475
9   0.502075  7.938109  0.494385  8.003887
10  0.495605  8.041252  0.504272  7.844682
11  0.493896  8.068497  0.497070  7.957286
12  0.498657  7.992599  0.498657  7.937981
13  0.505859  7.877780  0.501343  7.895291
14  0.505737  7.879726  0.495972  7.978339
15  0.492065  8.097688  0.501831  7.885902
16  0.504517  7.899187  0.506348  7.811194
17  0.496094  8.033467  0.488037  8.102276
18  0.508057  7.842750  0.509399  7.765823
19  0.506104  7.873888  0.502808  7.869301
20  0.496460  8.027629  0.486816  8.118789
21  0.503052  7.922540  0.504395  7.841777
22  0.497803  8.006222  0.498169  7.944156
23  0.503784  7.910864  0.503662  7.857247
24  0.504517  7.899187  0.506470  7.813103
25  0.496826  8.021791  0.504761  7.841299
26  0.489746  8.134664  0.499390  7.925914
27  0.502686  7.928378  0.496826  7.965805
28  0.505005  7.891403  0.507080  7.803132
29  0.498779  7.990653  0.494751  7.995319
30  0.499023  7.986761  0.498291  7.939839
31  0.505249  7.887510  0.501953  7.886479
32  0.504883  7.893349  0.503174  7.860363
33  0.512573  7.770745  0.493774  8.015213
34  0.498657  7.992599  0.491943  8.046491
35  0.500854  7.957570  0.495239  7.994703
36  0.494751  8.054874  0.505493  7.826110
37  0.488525  8.154125  0.508423  7.780470
38  0.502686  7.928378  0.505493  7.829912
39  0.500122  7.969246  0.504150  7.847694
40  0.504150  7.905025  0.492310  8.040494
41  0.495850  8.037359  0.490356  8.064106
42  0.496704  8.023737  0.496460  7.972318
43  0.503540  7.914756  0.507202  7.803183
44  0.498291  7.998438  0.493530  8.008385
45  0.504883  7.893349  0.493774  8.011230
46  0.504395  7.901133  0.504639  7.844700
47  0.491333  8.109365  0.498291  7.944587
48  0.494995  8.050982  0.494629  8.003271
49  0.502197  7.936163  0.497070  7.955975
50  0.509155  7.825235  0.505249  7.833053
51  0.501953  7.940055  0.491699  8.049378
52  0.497192  8.015952  0.490234  8.065700
53  0.500854  7.957570  0.500610  7.908276
54  0.508789  7.831074  0.492432  8.033306
55  0.503662  7.912810  0.487671  8.116968
56  0.496460  8.027629  0.500610  7.906636
57  0.499878  7.973138  0.504395  7.846053
58  0.500610  7.961462  0.493530  8.017735
59  0.499023  7.986761  0.502808  7.870154
60  0.487793  8.165802  0.493530  8.013870
61  0.501953  7.940055  0.502197  7.881814
62  0.508789  7.831074  0.500732  7.902520
63  0.502563  7.930324  0.500244  7.910878

2018-01-19 22:17:45.816994 Finish.
Total elapsed time: 08:09:55.82.
