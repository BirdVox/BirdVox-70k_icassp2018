2017-12-13 17:29:29.487796: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:29.488115: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:29.488127: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 17:29:22.372857 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit01, unit02, unit03.
Validation set: unit05, unit07.
Test set: unit10.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.955078  0.153426  0.955688  0.166174
1   0.957153  0.150947  0.955688  0.171650
2   0.962280  0.140876  0.853760  0.332265
3   0.962402  0.136580  0.949097  0.176686
4   0.963013  0.136700  0.949829  0.179848
5   0.964844  0.132681  0.900269  0.241665
6   0.962646  0.136483  0.940918  0.205306
7   0.966553  0.125901  0.951660  0.173756
8   0.967773  0.122415  0.918701  0.206683
9   0.967407  0.122323  0.847778  0.336237
10  0.968628  0.115340  0.896118  0.252143
11  0.969971  0.112788  0.901978  0.254431
12  0.969971  0.117281  0.872314  0.278026
13  0.969604  0.113926  0.923218  0.215992
14  0.970703  0.104860  0.884277  0.284083
15  0.969971  0.110336  0.811157  0.415690
16  0.970825  0.108229  0.876465  0.286193
17  0.968628  0.111589  0.890259  0.253241
18  0.973267  0.099127  0.906128  0.223218
19  0.974976  0.099032  0.830566  0.379970
20  0.968994  0.104842  0.954468  0.154738
21  0.972168  0.108233  0.862671  0.302901
22  0.972778  0.100322  0.887939  0.272665
23  0.971680  0.105436  0.796509  0.420654
24  0.973755  0.099456  0.856445  0.341640
25  0.972534  0.095386  0.853638  0.314950
26  0.974731  0.093336  0.868530  0.278100
27  0.973389  0.099000  0.936890  0.195378
28  0.972534  0.102776  0.892090  0.244998
29  0.975220  0.098683  0.843872  0.355004
30  0.973999  0.098750  0.940796  0.174532
31  0.973389  0.096811  0.914795  0.228400
32  0.973633  0.094583  0.801392  0.472287
33  0.973145  0.096081  0.894165  0.254285
34  0.976196  0.088911  0.896118  0.269541
35  0.974243  0.097070  0.859375  0.314089
36  0.975220  0.096274  0.875000  0.293922
37  0.973389  0.095391  0.935303  0.207126
38  0.979858  0.084366  0.828125  0.396149
39  0.976929  0.091350  0.799561  0.472538
40  0.975464  0.096792  0.938354  0.187261
41  0.971436  0.101696  0.855225  0.340806
42  0.978149  0.085565  0.904663  0.231625
43  0.980103  0.079469  0.896118  0.267563
44  0.977539  0.086996  0.898193  0.269980
45  0.978149  0.088913  0.922729  0.213501
46  0.975464  0.088286  0.885132  0.277491
47  0.974121  0.091073  0.914673  0.222837
48  0.977539  0.087753  0.842651  0.370964
49  0.975464  0.092941  0.840210  0.336007
50  0.979980  0.080345  0.940186  0.177306
51  0.975708  0.089078  0.892578  0.255183
52  0.976929  0.090121  0.908203  0.224432
53  0.978638  0.082750  0.846802  0.381178
54  0.977661  0.084089  0.921021  0.200432
55  0.976685  0.084232  0.920166  0.213070
56  0.977539  0.085708  0.914062  0.227513
57  0.976440  0.092084  0.893677  0.257656
58  0.976807  0.090817  0.894775  0.275065
59  0.976685  0.087903  0.919556  0.205623
60  0.977661  0.088249  0.877197  0.312913
61  0.979248  0.082580  0.884888  0.297985
62  0.979980  0.080577  0.892578  0.277204
63  0.976685  0.087676  0.822388  0.468914

2017-12-14 01:31:32.536942 Finish.
Total elapsed time: 08:02:10.54.
