2017-12-13 17:29:38.683547: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:38.683852: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:38.683865: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 17:29:35.038402 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit07, unit10, unit01.
Validation set: unit02, unit03.
Test set: unit05.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.932739  0.203102  0.877563  0.443424
1   0.936401  0.190123  0.915405  0.381016
2   0.941040  0.192240  0.917969  0.305952
3   0.945801  0.167580  0.929932  0.266849
4   0.947144  0.160007  0.942871  0.221261
5   0.948608  0.164064  0.921753  0.310752
6   0.954224  0.151078  0.947144  0.233316
7   0.946045  0.166479  0.950317  0.186124
8   0.954102  0.149386  0.922852  0.375187
9   0.953369  0.143210  0.927368  0.395456
10  0.958252  0.139511  0.939941  0.250887
11  0.955566  0.141031  0.942627  0.274890
12  0.954956  0.142200  0.926514  0.404972
13  0.956299  0.136996  0.914917  0.391457
14  0.958008  0.137452  0.923462  0.381175
15  0.955444  0.137628  0.925903  0.363724
16  0.955933  0.138129  0.929077  0.376160
17  0.955688  0.144426  0.949219  0.263440
18  0.958008  0.134306  0.943970  0.248627
19  0.958130  0.132483  0.941406  0.277071
20  0.955200  0.142309  0.924072  0.282788
21  0.956665  0.136161  0.920532  0.255586
22  0.959595  0.132194  0.942017  0.271591
23  0.963867  0.121807  0.943359  0.275322
24  0.960449  0.131042  0.926025  0.344615
25  0.958984  0.136572  0.953491  0.248201
26  0.964111  0.130015  0.934570  0.335306
27  0.960571  0.134875  0.950073  0.217914
28  0.959351  0.137752  0.934082  0.315404
29  0.957275  0.134877  0.930786  0.318691
30  0.960205  0.130440  0.932861  0.347089
31  0.964478  0.124192  0.934937  0.264455
32  0.963135  0.125670  0.942627  0.288220
33  0.964722  0.123393  0.934692  0.345537
34  0.966919  0.119743  0.930786  0.337736
35  0.964478  0.125989  0.944458  0.271938
36  0.963867  0.120049  0.933228  0.369384
37  0.964355  0.122091  0.930542  0.372459
38  0.962769  0.122632  0.952271  0.210946
39  0.961548  0.126782  0.916260  0.312779
40  0.970093  0.106050  0.942871  0.286599
41  0.963867  0.121573  0.945923  0.251932
42  0.964233  0.123504  0.940063  0.268221
43  0.965210  0.118508  0.946655  0.235152
44  0.964355  0.129344  0.938965  0.333746
45  0.967529  0.113627  0.942139  0.275793
46  0.966553  0.112244  0.940430  0.286210
47  0.963135  0.122742  0.961060  0.183621
48  0.965698  0.116622  0.937988  0.310141
49  0.966187  0.116206  0.930908  0.266152
50  0.964844  0.110605  0.933716  0.341438
51  0.963623  0.120324  0.945801  0.226883
52  0.968140  0.107708  0.941040  0.287202
53  0.965210  0.119536  0.935547  0.288659
54  0.968140  0.110495  0.935059  0.282077
55  0.968872  0.104881  0.928101  0.375817
56  0.965332  0.121966  0.942749  0.246371
57  0.967651  0.122000  0.952026  0.203826
58  0.969116  0.110299  0.956421  0.214003
59  0.964355  0.114560  0.940552  0.297854
60  0.971802  0.099454  0.922363  0.303150
61  0.967285  0.115849  0.911865  0.364702
62  0.965698  0.117257  0.932373  0.296629
63  0.973511  0.098014  0.932739  0.349252

2017-12-14 01:19:22.621087 Finish.
Total elapsed time: 07:49:47.62.
