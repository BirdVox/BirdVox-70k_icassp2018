2017-12-13 17:29:38.023647: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:38.023810: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:38.023822: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 17:29:35.005000 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit07, unit10, unit01.
Validation set: unit02, unit03.
Test set: unit05.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.500366  7.965354  0.502441  7.932271
1   0.499512  7.978977  0.498169  8.000384
2   0.494995  8.050982  0.490479  8.122988
3   0.497192  8.015952  0.499023  7.986761
4   0.489746  8.134664  0.505737  7.879726
5   0.506714  7.864157  0.500610  7.961462
6   0.495117  8.049036  0.505493  7.883618
7   0.499756  7.975085  0.494141  8.064605
8   0.496094  8.033467  0.507324  7.854427
9   0.508179  7.840804  0.508911  7.829128
10  0.499512  7.978977  0.504272  7.903079
11  0.501221  7.951731  0.497681  8.008168
12  0.501099  7.953678  0.501831  7.942001
13  0.491821  8.101580  0.503540  7.914756
14  0.496338  8.029575  0.497437  8.012060
15  0.492798  8.086012  0.498413  7.996492
16  0.500244  7.967300  0.503174  7.920594
17  0.501343  7.949785  0.498413  7.996492
18  0.506226  7.871942  0.503174  7.920594
19  0.488770  8.150233  0.501831  7.942001
20  0.501221  7.951731  0.505615  7.881672
21  0.502075  7.938109  0.505127  7.889457
22  0.499634  7.977031  0.508423  7.836912
23  0.487793  8.165802  0.501343  7.949785
24  0.499023  7.986761  0.496338  8.029575
25  0.507080  7.858319  0.505127  7.889457
26  0.496948  8.019845  0.503784  7.910864
27  0.496582  8.025683  0.492310  8.093796
28  0.495605  8.041252  0.503662  7.912810
29  0.497314  8.014006  0.498535  7.994545
30  0.498047  8.002330  0.490601  8.121041
31  0.489258  8.142448  0.503418  7.916702
32  0.495483  8.043198  0.493896  8.068497
33  0.491943  8.099634  0.495605  8.041252
34  0.492920  8.084066  0.494019  8.066551
35  0.499390  7.980923  0.496948  8.019845
36  0.504395  7.901133  0.506714  7.864157
37  0.502686  7.928378  0.506470  7.868049
38  0.495972  8.035413  0.502441  7.932271
39  0.504395  7.901133  0.500610  7.961462
40  0.501465  7.947839  0.498169  8.000384
41  0.498535  7.994545  0.504761  7.895295
42  0.499878  7.973138  0.501831  7.942001
43  0.494751  8.054874  0.498901  7.988707
44  0.497803  8.006222  0.504272  7.903079
45  0.501709  7.943947  0.508423  7.836912
46  0.494629  8.056820  0.505859  7.877780
47  0.504517  7.899187  0.498779  7.990653
48  0.502075  7.938109  0.496338  8.029575
49  0.501343  7.949785  0.493408  8.076281
50  0.499146  7.984815  0.494263  8.062659
51  0.499268  7.982869  0.492310  8.093796
52  0.506836  7.862211  0.496704  8.023737
53  0.507568  7.850535  0.502686  7.928378
54  0.497437  8.012060  0.498413  7.996492
55  0.510620  7.801882  0.499512  7.978977
56  0.496094  8.033467  0.492676  8.087958
57  0.494873  8.052928  0.494751  8.054874
58  0.489990  8.130772  0.502441  7.932271
59  0.505371  7.885564  0.502075  7.938109
60  0.506104  7.873888  0.504272  7.903079
61  0.498047  8.002330  0.506958  7.860265
62  0.503906  7.908917  0.494873  8.052928
63  0.513428  7.757122  0.502319  7.934217

2017-12-14 01:17:53.911007 Finish.
Total elapsed time: 07:48:18.91.
