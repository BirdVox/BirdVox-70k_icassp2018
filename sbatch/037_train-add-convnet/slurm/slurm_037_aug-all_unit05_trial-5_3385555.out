2017-12-13 17:29:27.529433: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:27.529695: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 17:29:27.529708: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 17:29:21.835902 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit07, unit10, unit01.
Validation set: unit02, unit03.
Test set: unit05.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.937012  0.187306  0.882324  0.443447
1   0.941406  0.179874  0.868042  0.689564
2   0.947388  0.171143  0.872925  0.524672
3   0.946899  0.172418  0.907715  0.414201
4   0.952026  0.150193  0.876953  0.584059
5   0.949707  0.159870  0.842651  0.796404
6   0.951538  0.161727  0.916748  0.392162
7   0.953979  0.144336  0.886841  0.603919
8   0.955811  0.144245  0.901001  0.490645
9   0.952148  0.153418  0.883301  0.502121
10  0.952515  0.145166  0.905762  0.494650
11  0.957520  0.146100  0.895386  0.455393
12  0.957520  0.145022  0.897827  0.566913
13  0.958984  0.138521  0.895386  0.536830
14  0.959229  0.143490  0.927734  0.368221
15  0.958374  0.139004  0.925049  0.391365
16  0.959351  0.136875  0.905151  0.489900
17  0.962646  0.128573  0.919434  0.402995
18  0.962769  0.133298  0.947632  0.270861
19  0.957886  0.138962  0.926025  0.332848
20  0.961792  0.127792  0.927124  0.318925
21  0.960083  0.137789  0.928345  0.263742
22  0.964722  0.131834  0.932007  0.322174
23  0.968018  0.115330  0.943237  0.317756
24  0.962646  0.125598  0.941650  0.242411
25  0.963501  0.125224  0.921631  0.368335
26  0.963989  0.126550  0.924194  0.345731
27  0.965332  0.123414  0.910767  0.477295
28  0.965088  0.121339  0.928345  0.391108
29  0.966309  0.121263  0.915283  0.426155
30  0.963135  0.129753  0.908813  0.404664
31  0.968140  0.112791  0.916504  0.292927
32  0.967896  0.116730  0.936035  0.282857
33  0.970703  0.114809  0.938110  0.311841
34  0.965820  0.114972  0.928833  0.410313
35  0.961426  0.132310  0.939331  0.357675
36  0.966553  0.118756  0.940063  0.264877
37  0.968262  0.117733  0.933472  0.326406
38  0.965820  0.121201  0.935059  0.312488
39  0.966431  0.122949  0.955078  0.208704
40  0.962036  0.128569  0.927856  0.312510
41  0.969238  0.117550  0.911255  0.497118
42  0.970337  0.111839  0.937622  0.318202
43  0.964722  0.123929  0.942627  0.298202
44  0.964844  0.116658  0.934814  0.343814
45  0.967407  0.119359  0.915649  0.413422
46  0.970093  0.110078  0.939209  0.324852
47  0.971436  0.109828  0.918701  0.522928
48  0.970947  0.107746  0.938232  0.267750
49  0.971802  0.108236  0.944946  0.285095
50  0.972412  0.115262  0.942627  0.266414
51  0.968628  0.117033  0.926147  0.336263
52  0.970581  0.113538  0.934448  0.307100
53  0.975098  0.101220  0.925903  0.437386
54  0.968506  0.113442  0.908813  0.445894
55  0.970581  0.113260  0.925049  0.438556
56  0.973877  0.100885  0.946045  0.279921
57  0.970093  0.110458  0.938477  0.272793
58  0.970215  0.112031  0.940063  0.292278
59  0.966919  0.110297  0.934937  0.312700
60  0.973267  0.106901  0.937256  0.309442
61  0.973145  0.107208  0.945435  0.255001
62  0.975098  0.096126  0.938721  0.370068
63  0.969727  0.109793  0.918701  0.409969

2017-12-14 01:17:22.380148 Finish.
Total elapsed time: 07:48:01.38.
