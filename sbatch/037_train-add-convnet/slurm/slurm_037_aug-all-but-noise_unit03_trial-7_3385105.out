2017-12-13 12:38:35.100139: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:38:35.100343: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:38:35.100353: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:38:35.100358: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 12:38:35.100362: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 12:38:33.137368 Start.
Training additive context-aware convnet on BirdVox-70k. 
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
spec_input (InputLayer)          (None, 128, 104, 1)   0                                            
____________________________________________________________________________________________________
spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 
____________________________________________________________________________________________________
spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    
____________________________________________________________________________________________________
spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 
____________________________________________________________________________________________________
bg_input (InputLayer)            (None, 128, 9)        0                                            
____________________________________________________________________________________________________
spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 
____________________________________________________________________________________________________
bg_pool (AveragePooling1D)       (None, 32, 9)         0           bg_input[0][0]                   
____________________________________________________________________________________________________
spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 
____________________________________________________________________________________________________
bg_permute (Permute)             (None, 9, 32)         0           bg_pool[0][0]                    
____________________________________________________________________________________________________
spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 
____________________________________________________________________________________________________
bg_conv (Conv1D)                 (None, 9, 8)          264         bg_permute[0][0]                 
____________________________________________________________________________________________________
spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 
____________________________________________________________________________________________________
bg_flatten (Flatten)             (None, 72)            0           bg_conv[0][0]                    
____________________________________________________________________________________________________
spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               
____________________________________________________________________________________________________
bg_dense1 (Dense)                (None, 64)            4672        bg_flatten[0][0]                 
____________________________________________________________________________________________________
spec_dense2 (Dense)              (None, 1)             64          spec_dense1[0][0]                
____________________________________________________________________________________________________
bg_dense2 (Dense)                (None, 1)             64          bg_dense1[0][0]                  
____________________________________________________________________________________________________
add (Add)                        (None, 1)             0           spec_dense2[0][0]                
                                                                   bg_dense2[0][0]                  
____________________________________________________________________________________________________
dense (Dense)                    (None, 1)             2           add[0][0]                        
====================================================================================================
Total params: 682,078
Trainable params: 682,076
Non-trainable params: 2
____________________________________________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.971680  0.113367  0.735718  1.136258
1   0.975220  0.108249  0.736694  1.213633
2   0.976685  0.106222  0.759644  0.748775
3   0.977417  0.107709  0.773315  0.658483
4   0.977295  0.098743  0.738037  1.049057
5   0.979736  0.097542  0.747559  1.177571
6   0.979248  0.094175  0.750244  1.003280
7   0.978882  0.092523  0.755493  0.946664
8   0.979370  0.092830  0.762939  1.272130
9   0.982178  0.083254  0.764282  0.728866
10  0.981323  0.084993  0.765869  0.932731
11  0.980591  0.078246  0.773804  0.915473
12  0.984741  0.074038  0.792358  0.727118
13  0.981445  0.082188  0.784302  1.023212
14  0.981201  0.088036  0.784912  0.882319
15  0.984863  0.073780  0.799927  0.798129
16  0.984375  0.076548  0.813232  0.643980
17  0.984009  0.074922  0.738770  1.724820
18  0.983643  0.076756  0.794434  0.760054
19  0.984619  0.068952  0.772827  1.050799
20  0.986084  0.065798  0.746460  1.701633
21  0.983887  0.066329  0.791626  0.820350
22  0.982910  0.074259  0.795044  1.051211
23  0.985840  0.068741  0.788330  0.936838
24  0.984741  0.070730  0.754028  1.001658
25  0.984619  0.070346  0.795288  0.947113
26  0.984253  0.065739  0.775024  0.898325
27  0.983398  0.074888  0.797607  0.709783
28  0.984619  0.066947  0.829834  0.491780
29  0.985474  0.063384  0.749023  1.469885
30  0.984009  0.072638  0.796509  0.578000
31  0.986084  0.062291  0.756592  1.006369
32  0.984619  0.068736  0.815918  0.545558
33  0.983276  0.072123  0.802979  0.673472
34  0.983765  0.073099  0.768433  1.384333
35  0.986572  0.059825  0.766479  1.295745
36  0.986084  0.063632  0.786499  0.726139
37  0.983521  0.074571  0.767944  1.042173
38  0.984131  0.071225  0.786743  0.963336
39  0.985596  0.062530  0.789429  0.892287
40  0.986084  0.055090  0.749268  1.471370
41  0.986694  0.059841  0.791260  0.898468
42  0.988525  0.054873  0.775391  0.972896
43  0.987061  0.060819  0.799927  0.818890
44  0.986084  0.068519  0.782471  0.855541
45  0.985962  0.063271  0.818726  0.586438
46  0.984985  0.067967  0.794067  0.718890
47  0.987061  0.055669  0.784058  0.800275
48  0.987305  0.058551  0.819946  0.511511
49  0.986084  0.064658  0.779541  0.862992
50  0.986450  0.061996  0.816162  0.528984
51  0.985962  0.060947  0.799561  0.776928
52  0.983643  0.063016  0.814453  0.548213
53  0.987427  0.059453  0.759888  1.220687
54  0.987915  0.060463  0.834595  0.503556
55  0.987427  0.057644  0.788574  1.013742
56  0.985962  0.058005  0.761841  1.601007
57  0.986694  0.054198  0.813110  0.716354
58  0.985962  0.060276  0.774170  1.161123
59  0.987793  0.057138  0.809082  0.722830
60  0.985596  0.063158  0.838013  0.483588
61  0.987427  0.055263  0.741333  1.438225
62  0.987549  0.059087  0.801636  0.874760
63  0.989502  0.056418  0.787598  0.979631

2017-12-13 20:38:10.749034 Finish.
Total elapsed time: 07:59:37.75.
