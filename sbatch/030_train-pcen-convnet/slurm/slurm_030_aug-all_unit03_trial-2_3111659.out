2017-11-26 04:04:58.386348: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-11-26 04:04:58.386547: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-11-26 04:04:58.386561: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-11-26 04:04:43.201052 Start.
Training Salamon's ICASSP 2017 convnet on BirdVox-70k
with PCEN input instead of logmelspec.
Training set: unit05, unit07, unit10.
Validation set: unit01, unit02.
Test set: unit03.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 128, 104, 1)       0         
_________________________________________________________________
batch_normalization_1 (Batch (None, 128, 104, 1)       4         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 128, 104, 24)      624       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 64, 26, 24)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 64, 26, 48)        28848     
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 32, 6, 48)         0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 32, 6, 48)         57648     
_________________________________________________________________
flatten_1 (Flatten)          (None, 9216)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 64)                589888    
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 65        
=================================================================
Total params: 677,077
Trainable params: 677,075
Non-trainable params: 2
_________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.528320  0.843824  0.715088  0.677215
1   0.669434  2.745883  0.598267  5.194442
2   0.688599  3.405621  0.568970  5.527140
3   0.717407  3.449859  0.574463  5.512201
4   0.721558  3.493960  0.595581  5.454103
5   0.722046  3.526804  0.605957  5.388472
6   0.734131  3.502730  0.613037  5.288741
7   0.733887  3.546306  0.625854  5.305097
8   0.748291  3.442542  0.621216  5.299624
9   0.756470  3.376526  0.619995  5.353334
10  0.747437  3.410432  0.635742  5.262560
11  0.768677  3.309529  0.631592  5.327254
12  0.769043  3.319009  0.629883  5.275250
13  0.754272  3.538890  0.635010  5.372355
14  0.764038  3.408479  0.629028  5.317003
15  0.770874  3.305246  0.639160  5.333624
16  0.762695  3.439237  0.627686  5.400682
17  0.768066  3.370345  0.635864  5.281762
18  0.761353  3.443853  0.647461  5.271506
19  0.771606  3.346832  0.640503  5.330215
20  0.770142  3.393516  0.658325  5.081096
21  0.764893  3.461923  0.652466  5.175054
22  0.773926  3.341708  0.639404  5.335560
23  0.772583  3.339449  0.627563  5.424198
24  0.756836  3.465804  0.626831  5.396283
25  0.768921  3.384131  0.645874  5.220945
26  0.769165  3.354018  0.639893  5.315905
27  0.781860  3.249082  0.642578  5.343652
28  0.771484  3.425484  0.637329  5.340978
29  0.768066  3.389417  0.601074  5.767884
30  0.495972  8.079095  0.504761  7.954906
31  0.498535  8.050273  0.506592  7.918263

2017-11-26 08:09:49.406986 Finish.
Total elapsed time: 04:05:06.41.
