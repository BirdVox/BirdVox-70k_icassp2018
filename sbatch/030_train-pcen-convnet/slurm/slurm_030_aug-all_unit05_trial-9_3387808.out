2017-12-13 22:29:55.535252: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 22:29:55.535502: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-13 22:29:55.535516: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-12-13 22:29:37.404310 Start.
Training Salamon's ICASSP 2017 convnet on BirdVox-70k
with PCEN input instead of logmelspec.
Training set: unit07, unit10, unit01.
Validation set: unit02, unit03.
Test set: unit05.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 128, 104, 1)       0         
_________________________________________________________________
batch_normalization_1 (Batch (None, 128, 104, 1)       4         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 128, 104, 24)      624       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 64, 26, 24)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 64, 26, 48)        28848     
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 32, 6, 48)         0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 32, 6, 48)         57648     
_________________________________________________________________
flatten_1 (Flatten)          (None, 9216)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 64)                589888    
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 65        
=================================================================
Total params: 677,077
Trainable params: 677,075
Non-trainable params: 2
_________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.938232  0.206978  0.918091  0.289701
1   0.934448  0.210005  0.932617  0.215834
2   0.941528  0.184803  0.911011  0.349009
3   0.934326  0.212586  0.950562  0.240413
4   0.940186  0.193541  0.930542  0.261373
5   0.920288  0.235933  0.939453  0.228779
6   0.929443  0.239510  0.922363  0.421642
7   0.527710  7.516636  0.501953  7.989983
8   0.507202  7.900422  0.505981  7.914878
9   0.495361  8.080454  0.500366  7.997330
10  0.510864  7.827293  0.494995  8.077834
11  0.505371  7.910362  0.511108  7.816978
12  0.509033  7.848404  0.492188  8.115396
13  0.491821  8.119849  0.497437  8.029005
14  0.495972  8.051170  0.501465  7.962454
15  0.503540  7.928336  0.497437  8.024642
16  0.505249  7.899182  0.502441  7.943062
17  0.511597  7.796300  0.503906  7.918125
18  0.504150  7.913520  0.497925  8.012082
19  0.496094  8.040644  0.508057  7.849319
20  0.490356  8.130949  0.493530  8.079816
21  0.497437  8.017056  0.501221  7.956261
22  0.502075  7.942217  0.507935  7.848399
23  0.500977  7.958963  0.499756  7.978075
24  0.496094  8.036147  0.500366  7.967737
25  0.505615  7.883791  0.497314  8.015875
26  0.499878  7.974787  0.500977  7.957064
27  0.499023  7.988020  0.493530  8.075424
28  0.505493  7.884560  0.498413  7.997296
29  0.498413  7.997180  0.500366  7.965935
30  0.505493  7.884109  0.493530  8.074743
31  0.508667  7.833360  0.493408  8.076560

2017-12-14 03:11:14.859747 Finish.
Total elapsed time: 04:41:37.86.
