2018-01-19 19:47:31.510787: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-19 19:47:31.511075: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-19 19:47:31.511090: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2018-01-19 19:46:34.708194 Start.
Training Salamon's ICASSP 2017 convnet on BirdVox-70k
with PCEN input instead of logmelspec.
Training set: unit01, unit02, unit03.
Validation set: unit05, unit07.
Test set: unit10.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 128, 104, 1)       0         
_________________________________________________________________
batch_normalization_1 (Batch (None, 128, 104, 1)       4         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 128, 104, 24)      624       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 64, 26, 24)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 64, 26, 48)        28848     
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 32, 6, 48)         0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 32, 6, 48)         57648     
_________________________________________________________________
flatten_1 (Flatten)          (None, 9216)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 64)                589888    
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 65        
=================================================================
Total params: 677,077
Trainable params: 677,075
Non-trainable params: 2
_________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.933228  0.210260  0.944336  0.181730
1   0.929565  0.244561  0.942627  0.184164
2   0.941162  0.193881  0.924316  0.231810
3   0.916626  0.253706  0.829346  0.408713
4   0.758301  1.236487  0.488770  8.203944
5   0.502930  7.971902  0.503174  7.962049
6   0.494751  8.091908  0.499756  8.008143
7   0.490112  8.158647  0.497314  8.040841
8   0.498779  8.014972  0.503052  7.944511
9   0.498169  8.020346  0.497437  8.030137
10  0.503662  7.929259  0.508423  7.851829
11  0.496826  8.035378  0.504761  7.907628
12  0.502930  7.935727  0.497681  8.018376
13  0.488403  8.165377  0.508911  7.837581
14  0.494141  8.072311  0.504883  7.900347
15  0.508301  7.851643  0.504395  8.062498
16  0.503418  8.083411  0.501099  8.104397
17  0.496460  8.169085  0.501343  8.082004
18  0.494141  8.192073  0.506714  7.984133
19  0.500244  8.084323  0.499512  8.092445
20  0.499023  8.097351  0.500854  8.065130
21  0.496826  8.127829  0.506104  7.976241
22  0.494629  8.159473  0.498779  8.090985
23  0.503174  8.018809  0.510498  7.899505
24  0.497681  8.105033  0.500732  8.054851
25  0.496948  8.114996  0.498291  8.092560
26  0.509277  7.914802  0.500488  8.055830
27  0.499146  8.076929  0.496094  8.125609
28  0.505249  7.977610  0.501831  8.032295
29  0.493652  8.163775  0.498291  8.088687
30  0.507690  7.936914  0.496704  8.113739
31  0.503052  8.011214  0.507446  7.940185

2018-01-20 00:49:02.347972 Finish.
Total elapsed time: 05:02:28.35.
