2017-11-24 14:42:35.469257: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-11-24 14:42:35.469501: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-11-24 14:42:35.469516: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
Using TensorFlow backend.
2017-11-24 14:42:34.427949 Start.
Training Salamon's ICASSP 2017 convnet on BirdVox-70k
with PCEN input instead of logmelspec.
Training set: unit02, unit03, unit05.
Validation set: unit07, unit10.
Test set: unit01.

h5py version: 2.6.0
keras version: 2.0.6
numpy version: 1.13.1
pandas version: 0.20.3
pescador version: 1.0.0
tensorflow version: 1.2.1

_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 128, 104, 1)       0         
_________________________________________________________________
batch_normalization_1 (Batch (None, 128, 104, 1)       4         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 128, 104, 24)      624       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 64, 26, 24)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 64, 26, 48)        28848     
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 32, 6, 48)         0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 32, 6, 48)         57648     
_________________________________________________________________
flatten_1 (Flatten)          (None, 9216)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 64)                589888    
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 65        
=================================================================
Total params: 677,077
Trainable params: 677,075
Non-trainable params: 2
_________________________________________________________________
         acc      loss   val_acc  val_loss
0   0.497559  7.960717  0.493042  8.123906
1   0.497559  8.045593  0.499634  8.007410
2   0.502319  7.960987  0.496948  8.043420
3   0.503174  7.941621  0.498535  8.013233
4   0.503662  7.929539  0.499023  8.001666
5   0.494751  8.068220  0.507324  7.866309
6   0.496826  8.032410  0.493896  8.077927
7   0.500122  7.977645  0.507690  7.856014
8   0.501831  7.948582  0.499268  7.988653
9   0.499634  7.982125  0.504028  7.911416
10  0.499023  7.990647  0.498779  7.994016
11  0.494263  8.065574  0.505249  7.890008
12  0.492065  8.099833  0.508911  7.830946
13  0.502441  7.933816  0.498047  8.000642
14  0.500488  7.964496  0.508667  7.833919
15  0.495728  8.040053  0.503784  7.911388
16  0.505005  7.891902  0.488525  8.154526
17  0.495239  8.047414  0.498291  7.994440
18  0.498413  7.996695  0.496582  8.023800
19  0.502686  7.928507  0.502197  7.929531
20  0.511841  7.845136  0.511230  7.915020
21  0.509155  7.931115  0.498413  8.094483
22  0.495605  8.135951  0.504639  7.987759
23  0.503906  7.998316  0.489258  8.233520
24  0.493774  8.159633  0.497925  8.160023
25  0.499023  8.118950  0.496948  8.115992
26  0.501465  8.027347  0.492798  8.152686
27  0.491455  8.165669  0.503052  7.973522
28  0.497681  8.053533  0.501831  7.982246
29  0.502319  7.970187  0.497559  8.042089
30  0.494019  8.095080  0.503418  7.941980
31  0.490234  8.149325  0.501831  7.961769

2017-11-24 18:45:56.903440 Finish.
Total elapsed time: 04:03:22.90.
