{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-12-12 19:34:35.711788 Start.\n",
      "Using NTT-like convnet for binary classification in BirdVox-70k clips with logmelspec input. \n",
      "Training set: unit02, unit03, unit05.\n",
      "Validation set: unit07, unit10.\n",
      "Test set: unit01.\n",
      "\n",
      "h5py version: 2.6.0\n",
      "keras version: 2.0.6\n",
      "numpy version: 1.13.1\n",
      "tensorflow version: 1.2.1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import h5py\n",
    "import keras\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "import localmodule\n",
    "\n",
    "\n",
    "# Define constants.\n",
    "data_dir = localmodule.get_data_dir()\n",
    "dataset_name = localmodule.get_dataset_name()\n",
    "folds = localmodule.fold_units()\n",
    "models_dir = localmodule.get_models_dir()\n",
    "n_input_hops = 104\n",
    "bg_duration = 1800\n",
    "\n",
    "\n",
    "# Read command-line arguments.\n",
    "args = [\"all-but-noise\", \"unit01\", \"trial-0\", \"unit01\"]\n",
    "aug_kind_str = args[0]\n",
    "test_unit_str = args[1]\n",
    "trial_str = args[2]\n",
    "predict_unit_str = args[3]\n",
    "\n",
    "\n",
    "# Retrieve fold such that unit_str is in the test set.\n",
    "fold = [f for f in folds if test_unit_str in f[0]][0]\n",
    "test_units = fold[0]\n",
    "training_units = fold[1]\n",
    "validation_units = fold[2]\n",
    "\n",
    "\n",
    "# Print header.\n",
    "start_time = int(time.time())\n",
    "print(str(datetime.datetime.now()) + \" Start.\")\n",
    "print(\"Using NTT-like convnet for binary classification in \" +\n",
    "    dataset_name + \" clips with logmelspec input. \")\n",
    "print(\"Training set: \" + \", \".join(training_units) + \".\")\n",
    "print(\"Validation set: \" + \", \".join(validation_units) + \".\")\n",
    "print(\"Test set: \" + \", \".join(test_units) + \".\")\n",
    "print(\"\")\n",
    "print('h5py version: {:s}'.format(h5py.__version__))\n",
    "print('keras version: {:s}'.format(keras.__version__))\n",
    "print('numpy version: {:s}'.format(np.__version__))\n",
    "print('tensorflow version: {:s}'.format(tf.__version__))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# Load model.\n",
    "model_name = \"icassp-ntt-convnet\"\n",
    "if not aug_kind_str == \"none\":\n",
    "    model_name = \"_\".join([model_name, \"aug-\" + aug_kind_str])\n",
    "model_dir = os.path.join(models_dir, model_name)\n",
    "unit_dir = os.path.join(model_dir, test_unit_str)\n",
    "trial_dir = os.path.join(unit_dir, trial_str)\n",
    "network_name = \"_\".join(\n",
    "    [dataset_name, model_name, test_unit_str, trial_str, \"network\"])\n",
    "network_path = os.path.join(trial_dir, network_name + \".hdf5\")\n",
    "model = keras.models.load_model(network_path)\n",
    "\n",
    "\n",
    "# Open logmelspec container with h5py.\n",
    "aug_str = \"original\"\n",
    "logmelspec_dir = os.path.join(data_dir,\n",
    "    \"_\".join([dataset_name, \"clip-logmelspec\"]))\n",
    "original_logmelspec_dir = os.path.join(logmelspec_dir, aug_str)\n",
    "hdf5_name = \"_\".join([dataset_name, aug_str, predict_unit_str])\n",
    "hdf5_path = os.path.join(original_logmelspec_dir, hdf5_name + \".hdf5\")\n",
    "lms_container = h5py.File(hdf5_path, \"r\")\n",
    "lms_group = lms_container[\"logmelspec\"]\n",
    "keys = list(lms_group.keys())\n",
    "\n",
    "\n",
    "# Create HDF5 container for predictions.\n",
    "clip_predictions_name = \"_\".join([\n",
    "    dataset_name,\n",
    "    model_name,\n",
    "    \"test-\" + test_unit_str,\n",
    "    \"predict-\" + predict_unit_str,\n",
    "    \"clip-predictions\"\n",
    "])\n",
    "\n",
    "\n",
    "# Create CSV file.\n",
    "prediction_name = \"_\".join([dataset_name, model_name,\n",
    "    \"test-\" + test_unit_str, trial_str, \"predict-\" + predict_unit_str,\n",
    "    \"clip-predictions\"])\n",
    "prediction_path = os.path.join(trial_dir, prediction_name + \".csv\")\n",
    "csv_file = open(prediction_path, 'w')\n",
    "csv_writer = csv.writer(csv_file, delimiter=',')\n",
    "\n",
    "\n",
    "# Create CSV header.\n",
    "csv_header = [\"Dataset\", \"Test unit\", \"Prediction unit\", \"Timestamp\",\n",
    "    \"Key\", \"Predicted probability\"]\n",
    "csv_writer.writerow(csv_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "key = keys[0]\n",
    "\n",
    "# Load logmelspec.\n",
    "X_lms = lms_group[key]\n",
    "\n",
    "# Trim logmelspec in time to required number of hops.\n",
    "X_width = X_lms.shape[1]\n",
    "first_col = int((X_width-n_input_hops) / 2)\n",
    "last_col = int((X_width+n_input_hops) / 2)\n",
    "X_lms = X_lms[:, first_col:last_col]\n",
    "\n",
    "# Add trailing singleton dimension for Keras interoperability.\n",
    "X_lms = X_lms[np.newaxis, :, :, np.newaxis]\n",
    "\n",
    "# Predict.\n",
    "#predicted_probability = model.predict(X)[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bg_key = \"_\".join(key.split(\"_\")[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 file \"BirdVox-70k_background_summaries_unit01_T-1800.hdf5\" (mode r)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open background container with h5py.\n",
    "bg_name = \"_\".join(\n",
    "    [dataset_name, \"clip-logmelspec-backgrounds\"])\n",
    "bg_dir = os.path.join(data_dir, bg_name)\n",
    "T_str = \"T-\" + str(bg_duration).zfill(4)\n",
    "T_dir = os.path.join(bg_dir, T_str)\n",
    "bg_name = \"_\".join(\n",
    "    [dataset_name, \"background_summaries\",\n",
    "     test_unit_str, T_str + \".hdf5\"])\n",
    "bg_path = os.path.join(T_dir, bg_name)\n",
    "bg_container = h5py.File(bg_path, \"r\")\n",
    "bg_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
