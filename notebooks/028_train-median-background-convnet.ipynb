{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-26 17:11:58.611296 Start.\n",
      "Training Salamon's ICASSP 2017 convnet on BirdVox-70k. \n",
      "Training set: unit02, unit03, unit05.\n",
      "Validation set: unit07, unit10.\n",
      "Test set: unit01.\n",
      "\n",
      "h5py version: 2.6.0\n",
      "keras version: 2.0.6\n",
      "numpy version: 1.13.1\n",
      "pandas version: 0.20.3\n",
      "pescador version: 1.0.0\n",
      "tensorflow version: 1.2.1\n",
      "\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "spec_input (InputLayer)          (None, 128, 104, 1)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "spec_bn (BatchNormalization)     (None, 128, 104, 1)   4           spec_input[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "spec_conv1 (Conv2D)              (None, 128, 104, 24)  624         spec_bn[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "bg_input (InputLayer)            (None, 128, 5)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "spec_pool1 (MaxPooling2D)        (None, 64, 26, 24)    0           spec_conv1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "bg_pool (AveragePooling1D)       (None, 32, 5)         0           bg_input[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "spec_conv2 (Conv2D)              (None, 64, 26, 48)    28848       spec_pool1[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "bg_permute (Permute)             (None, 5, 32)         0           bg_pool[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "spec_pool2 (MaxPooling2D)        (None, 32, 6, 48)     0           spec_conv2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "bg_conv (Conv1D)                 (None, 5, 8)          264         bg_permute[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "spec_conv3 (Conv2D)              (None, 32, 6, 48)     57648       spec_pool2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "bg_flatten (Flatten)             (None, 40)            0           bg_conv[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "spec_flatten (Flatten)           (None, 9216)          0           spec_conv3[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "bg_dense1 (Dense)                (None, 16)            656         bg_flatten[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "spec_dense1 (Dense)              (None, 64)            589888      spec_flatten[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "bg_dense2 (Dense)                (None, 4)             68          bg_dense1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "spec_reshape (Reshape)           (None, 16, 4)         0           spec_dense1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "bg_reshape (Reshape)             (None, 1, 4)          0           bg_dense2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "multiply (Multiply)              (None, 16, 4)         0           spec_reshape[0][0]               \n",
      "                                                                   bg_reshape[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "flatten (Flatten)                (None, 64)            0           multiply[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense (Dense)                    (None, 1)             65          flatten[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 678,065\n",
      "Trainable params: 678,063\n",
      "Non-trainable params: 2\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import h5py\n",
    "import keras\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pescador\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "import localmodule\n",
    "\n",
    "# Define constants.\n",
    "dataset_name = localmodule.get_dataset_name()\n",
    "folds = localmodule.fold_units()\n",
    "models_dir = localmodule.get_models_dir()\n",
    "n_input_hops = 104\n",
    "n_filters = [24, 48, 48]\n",
    "kernel_size = [5, 5]\n",
    "pool_size = [2, 4]\n",
    "n_hidden_units = 64\n",
    "steps_per_epoch = 256\n",
    "epochs = 32\n",
    "validation_steps = 256\n",
    "batch_size = 32\n",
    "n_context_classes = 4\n",
    "\n",
    "\n",
    "# Read command-line arguments.\n",
    "args = [\"3600\", \"unit01\"]\n",
    "aug_kind_str = \"none\"\n",
    "bg_duration = int(args[0])\n",
    "unit_str = args[1]\n",
    "\n",
    "\n",
    "# Retrieve fold such that unit_str is in the test set.\n",
    "fold = [f for f in folds if unit_str in f[0]][0]\n",
    "test_units = fold[0]\n",
    "training_units = fold[1]\n",
    "validation_units = fold[2]\n",
    "\n",
    "\n",
    "# Print header.\n",
    "start_time = int(time.time())\n",
    "print(str(datetime.datetime.now()) + \" Start.\")\n",
    "print(\"Training Salamon's ICASSP 2017 convnet on \" + dataset_name + \". \")\n",
    "print(\"Training set: \" + \", \".join(training_units) + \".\")\n",
    "print(\"Validation set: \" + \", \".join(validation_units) + \".\")\n",
    "print(\"Test set: \" + \", \".join(test_units) + \".\")\n",
    "print(\"\")\n",
    "print('h5py version: {:s}'.format(h5py.__version__))\n",
    "print('keras version: {:s}'.format(keras.__version__))\n",
    "print('numpy version: {:s}'.format(np.__version__))\n",
    "print('pandas version: {:s}'.format(pd.__version__))\n",
    "print('pescador version: {:s}'.format(pescador.__version__))\n",
    "print('tensorflow version: {:s}'.format(tf.__version__))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# Define and compile Keras model.\n",
    "# NB: the original implementation of Justin Salamon in ICASSP 2017 relies on\n",
    "# glorot_uniform initialization for all layers, and the optimizer is a\n",
    "# stochastic gradient descent (SGD) with a fixed learning rate of 0.1.\n",
    "# Instead, we use a he_normal initialization for the layers followed\n",
    "# by rectified linear units (see He ICCV 2015), and replace the SGD by\n",
    "# the Adam adaptive stochastic optimizer (see Kingma ICLR 2014).\n",
    "# Moreover, we disable dropout because we found that it consistently prevented\n",
    "# the model to train at all.\n",
    "\n",
    "# Main channel.\n",
    "# Input\n",
    "spec_input = keras.layers.Input(\n",
    "    shape=(128, n_input_hops, 1), name=\"spec_input\")\n",
    "\n",
    "# Layer 1\n",
    "spec_bn = keras.layers.normalization.BatchNormalization(\n",
    "    name=\"spec_bn\")(spec_input)\n",
    "spec_conv1 = keras.layers.Convolution2D(n_filters[0], kernel_size,\n",
    "    padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "    name=\"spec_conv1\")(spec_bn)\n",
    "spec_pool1 = keras.layers.MaxPooling2D(\n",
    "    pool_size=pool_size, name=\"spec_pool1\")(spec_conv1)\n",
    "\n",
    "# Layer 2\n",
    "spec_conv2 = keras.layers.Convolution2D(n_filters[1], kernel_size,\n",
    "    padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "    activation=\"relu\", name=\"spec_conv2\")(spec_pool1)\n",
    "spec_pool2 = keras.layers.MaxPooling2D(\n",
    "    pool_size=pool_size, name=\"spec_pool2\")(spec_conv2)\n",
    "\n",
    "# Layer 3\n",
    "spec_conv3 = keras.layers.Convolution2D(n_filters[2], kernel_size,\n",
    "    padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "    activation=\"relu\", name=\"spec_conv3\")(spec_pool2)\n",
    "\n",
    "# Layer 4\n",
    "spec_flatten = keras.layers.Flatten(\n",
    "    name=\"spec_flatten\")(spec_conv3)\n",
    "spec_dense = keras.layers.Dense(n_hidden_units,\n",
    "    kernel_initializer=\"he_normal\", activation=\"relu\",\n",
    "    kernel_regularizer=keras.regularizers.l2(0.001),\n",
    "    name=\"spec_dense1\")(spec_flatten)\n",
    "\n",
    "# Reshape.\n",
    "spec_reshape = keras.layers.Reshape((-1, 4),\n",
    "    name=\"spec_reshape\")(spec_dense)\n",
    "\n",
    "\n",
    "# Side channel.\n",
    "# Input\n",
    "bg_input = keras.layers.Input(\n",
    "    shape=(128, 5), name=\"bg_input\")\n",
    "\n",
    "# Pool\n",
    "bg_pool = keras.layers.AveragePooling1D(\n",
    "    pool_size=4, name=\"bg_pool\")(bg_input)\n",
    "\n",
    "# Permute\n",
    "bg_permute = keras.layers.Permute(\n",
    "    (2, 1), name=\"bg_permute\")(bg_pool)\n",
    "\n",
    "# Conv\n",
    "bg_conv = keras.layers.Conv1D(\n",
    "    8, 1, kernel_initializer=\"he_normal\",\n",
    "    activation=\"relu\", name=\"bg_conv\")(bg_permute)\n",
    "\n",
    "# Flatten\n",
    "bg_flatten = keras.layers.Flatten(\n",
    "    name=\"bg_flatten\")(bg_conv)\n",
    "\n",
    "# Dense 1\n",
    "bg_dense1 = keras.layers.Dense(16,\n",
    "    kernel_initializer=\"he_normal\",\n",
    "    activation=\"relu\", name=\"bg_dense1\")(bg_flatten)\n",
    "\n",
    "# Dense 2\n",
    "bg_dense2 = keras.layers.Dense(4,\n",
    "    kernel_initializer=\"he_normal\",\n",
    "    activation=\"softmax\", name=\"bg_dense2\")(bg_dense1)\n",
    "\n",
    "# Reshape\n",
    "bg_reshape = keras.layers.Reshape((1, 4),\n",
    "    name=\"bg_reshape\")(bg_dense2)\n",
    "\n",
    "\n",
    "# Element-wise multiplication\n",
    "multiply = keras.layers.Multiply(\n",
    "    name=\"multiply\")([spec_reshape, bg_reshape])\n",
    "\n",
    "# Flatten\n",
    "flatten = keras.layers.Flatten(\n",
    "    name=\"flatten\")(multiply)\n",
    "\n",
    "\n",
    "# Layer 5\n",
    "# We put a single output instead of 43 in the original paper, because this\n",
    "# is binary classification instead of multilabel classification.\n",
    "# Furthermore, this layer contains 43 times less connections than in the\n",
    "# original paper, so we divide the l2 weight penalization by 50, which is\n",
    "# of the same order of magnitude as 43.\n",
    "# 0.001 / 50 = 0.00002\n",
    "dense = keras.layers.Dense(1,\n",
    "    kernel_initializer=\"normal\", activation=\"sigmoid\",\n",
    "    kernel_regularizer=keras.regularizers.l2(0.00002),\n",
    "    name=\"dense\")(flatten)\n",
    "\n",
    "\n",
    "# Compile model, print model summary.\n",
    "inputs = [spec_input, bg_input]\n",
    "model = keras.models.Model(inputs=inputs, outputs=dense)\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "    optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define function for multiplexing streamers.\n",
    "def multiplex_lms_with_background(\n",
    "        augs, fold_units, n_hops, batch_size, percentile_ids):\n",
    "\n",
    "    # Define constants.\n",
    "    aug_dict = localmodule.get_augmentations()\n",
    "    data_dir = localmodule.get_data_dir()\n",
    "    dataset_name = localmodule.get_dataset_name()\n",
    "    tfr_name = \"_\".join([dataset_name, \"clip-logmelspec\"])\n",
    "    tfr_dir = os.path.join(data_dir, tfr_name)\n",
    "    bg_name = \"_\".join(\n",
    "        [dataset_name, \"clip-logmelspec-backgrounds\"])\n",
    "    bg_dir = os.path.join(data_dir, bg_name)\n",
    "    T_str = \"T-\" + str(bg_duration).zfill(4)\n",
    "    T_dir = os.path.join(bg_dir, T_str)\n",
    "\n",
    "    # Loop over augmentations.\n",
    "    streams = []\n",
    "    for aug_str in augs:\n",
    "\n",
    "        # Define instances.\n",
    "        aug_dir = os.path.join(tfr_dir, aug_str)\n",
    "        if aug_str == \"original\":\n",
    "            instances = [aug_str]\n",
    "        else:\n",
    "            n_instances = aug_dict[aug_str]\n",
    "            instances = [\"-\".join([aug_str, str(instance_id)])\n",
    "                for instance_id in range(n_instances)]\n",
    "\n",
    "        # Define bias.\n",
    "        if aug_str[:5] == \"noise\":\n",
    "            bias = np.float32(-17.0)\n",
    "        else:\n",
    "            bias = np.float32(0.0)\n",
    "\n",
    "        # Loop over instances.\n",
    "        for instanced_aug_str in instances:\n",
    "\n",
    "            # Loop over units.\n",
    "            for unit_str in fold_units:\n",
    "\n",
    "                # Define path to time-frequency representation.\n",
    "                lms_name = \"_\".join(\n",
    "                    [dataset_name, instanced_aug_str, unit_str])\n",
    "                lms_path = os.path.join(aug_dir, lms_name + \".hdf5\")\n",
    "\n",
    "                # Define path to background.\n",
    "                bg_name = \"_\".join(\n",
    "                    [dataset_name, \"background-summaries\",\n",
    "                     unit_str, T_str + \".hdf5\"])\n",
    "                bg_path = os.path.join(bg_dir, bg_name)\n",
    "\n",
    "                # Define pescador streamer.\n",
    "                stream = pescador.Streamer(yield_lms_and_background,\n",
    "                    lms_path, n_hops, bias, bg_path, percentile_ids)\n",
    "                streams.append(stream)\n",
    "\n",
    "    # Multiplex streamers together.\n",
    "    mux = pescador.Mux(streams,\n",
    "        k=len(streams), lam=None, with_replacement=True, revive=True)\n",
    "\n",
    "    # Create buffered streamer with specified batch size.\n",
    "    buffered_streamer = pescador.BufferedStreamer(mux, batch_size)\n",
    "\n",
    "    return buffered_streamer.tuples(\"X\", \"y\", cycle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def yield_lms_and_background(tfr_path, n_hops, bias, bg_path, percentile_ids):\n",
    "    \n",
    "    # Open HDF5 container.\n",
    "    with h5py.File(tfr_path, \"r\") as tfr_container:\n",
    "        # Open HDF5 group corresponding to time-freq representation (TFR).\n",
    "        tfr_group = tfr_container[tfr_str]\n",
    "\n",
    "        # The naming convention of a key is\n",
    "        # [unit]_[time]_[freq]_[y]_[aug]_[instance]\n",
    "        # where y=1 if the key corresponds to a positive clip and 0 otherwise.\n",
    "        keys = list(tfr_group.keys())\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Infinite \"yield\" loop.\n",
    "        while True:\n",
    "            # Pick a key uniformly as random.\n",
    "            key = random.choice(keys)\n",
    "\n",
    "            # Load time-frequency spectrogram (TFR).\n",
    "            X_spec = tfr_group[key]\n",
    "\n",
    "            # Trim TFR in time to required number of hops.\n",
    "            X_width = X_spec.shape[1]\n",
    "            first_col = int((X_width-n_hops) / 2)\n",
    "            last_col = int((X_width+n_hops) / 2)\n",
    "            X_spec = X_spec[:, first_col:last_col]\n",
    "\n",
    "            # Add trailing singleton dimension for Keras interoperability.\n",
    "            X_spec = X_spec[:, :, np.newaxis]\n",
    "\n",
    "            # Apply bias\n",
    "            X_spec = X_spec + bias\n",
    "            \n",
    "            # Load background.\n",
    "            X_bg = bg_group[key]\n",
    "            \n",
    "            # Retrieve label y from key name.\n",
    "            y = np.array([np.float32(key.split(\"_\")[3])])\n",
    "\n",
    "            # Yield data and label as dictionary.\n",
    "            yield dict(X_spec=X_spec, X_bg=X_bg, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BirdVox-70k_background-summaries_unit02_T-3600.hdf5\n",
      "BirdVox-70k_background-summaries_unit03_T-3600.hdf5\n",
      "BirdVox-70k_background-summaries_unit05_T-3600.hdf5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "import localmodule\n",
    "\n",
    "aug_kind_str = \"all\"\n",
    "    \n",
    "        \n",
    "# Define constants.\n",
    "dataset_name = localmodule.get_dataset_name()\n",
    "folds = localmodule.fold_units()\n",
    "models_dir = localmodule.get_models_dir()\n",
    "n_input_hops = 104\n",
    "n_filters = [24, 48, 48]\n",
    "kernel_size = [5, 5]\n",
    "pool_size = [2, 4]\n",
    "n_hidden_units = 64\n",
    "steps_per_epoch = 256\n",
    "epochs = 32\n",
    "validation_steps = 256\n",
    "batch_size = 32\n",
    "n_context_classes = 4\n",
    "\n",
    "\n",
    "# Read command-line arguments.\n",
    "args = [\"3600\", \"unit01\"]\n",
    "aug_kind_str = \"none\"\n",
    "bg_duration = int(args[0])\n",
    "unit_str = args[1]\n",
    "\n",
    "# Retrieve fold such that unit_str is in the test set.\n",
    "fold = [f for f in folds if unit_str in f[0]][0]\n",
    "test_units = fold[0]\n",
    "training_units = fold[1]\n",
    "validation_units = fold[2]\n",
    "fold_units = training_units            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Parse augmentation kind string (aug_kind_str).\n",
    "if aug_kind_str == \"none\":\n",
    "    augs = [\"original\"]\n",
    "elif aug_kind_str == \"pitch\":\n",
    "    augs = [\"original\", \"pitch\"]\n",
    "elif aug_kind_str == \"stretch\":\n",
    "    augs = [\"original\", \"stretch\"]\n",
    "else:\n",
    "    noise_augs = [\"noise-\" + unit_str for unit_str in fold_units]\n",
    "    if aug_kind_str == \"all\":\n",
    "        augs = noise_augs + [\"original\", \"pitch\", \"stretch\"]\n",
    "    elif aug_kind_str == \"noise\":\n",
    "        augs = noise_augs + [\"original\"]\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
